[
  {
    "objectID": "posts/FelixBetancourt_HW2.html",
    "href": "posts/FelixBetancourt_HW2.html",
    "title": "Homework 2",
    "section": "",
    "text": "Code\nknitr::opts_chunk$set(echo = TRUE, warning = FALSE)"
  },
  {
    "objectID": "posts/FelixBetancourt_HW2.html#homework-2",
    "href": "posts/FelixBetancourt_HW2.html#homework-2",
    "title": "Homework 2",
    "section": "Homework 2",
    "text": "Homework 2\nDACSS 603, Spring 2023\n\n\nCode\n# Loading packages\n\nsuppressPackageStartupMessages(library(dplyr))\nsuppressPackageStartupMessages(library(tidyverse))\nlibrary(formattable)\nsuppressPackageStartupMessages(library(kableExtra))\nlibrary(ggplot2)\nlibrary(readxl)\n\n\n\n1. The time between the date a patient was recommended for heart surgery and the surgery date for cardiac patients in Ontario was collected by the Cardiac Care Network (“Wait Times Data Guide,” Ministry of Health and Long-Term Care, Ontario, Canada, 2006). The sample mean and sample standard deviation for wait times (in days) of patients for two cardiac procedures are given in the accompanying table. Assume that the sample is representative of the Ontario population.\n\n\n\n\n\n\n\n\n\nSurgical Procedure\nSample Size\nMean wait time\nStandard Deviation\n\n\n\n\nBypass\n539\n19\n10\n\n\nAngiography\n847\n18\n9\n\n\n\n\n\nConstruct the 90% confidence interval to estimate the actual mean wait time for each of the two procedures. Is the confidence interval narrower for angiography or bypass surgery?\n\n\nCode\nbypass_n &lt;- 539\nbypass_mean &lt;- 19\nbypass_sd &lt;- 10\n\n\nConfidence interval (90%) for Bypass type of surgery\n\n\nCode\nerror &lt;- qnorm(p=0.9)*bypass_sd/sqrt(bypass_n)\nb_left &lt;- bypass_mean-error\nb_right &lt;- bypass_mean+error\nci_b &lt;- c(b_left, b_right)\nprint(ci_b)\n\n\n[1] 18.448 19.552\n\n\nConfidence interval (90%) for Angiography type of surgery\n\n\nCode\nangio_n &lt;- 847\nangio_mean &lt;- 18\nangio_sd &lt;- 9\n\nerror &lt;- qnorm(p=0.9)*angio_sd/sqrt(angio_n)\na_left &lt;- angio_mean-error\na_right &lt;- angio_mean+error\nci_a &lt;- c(a_left, a_right)\nprint(ci_a)\n\n\n[1] 17.60369 18.39631\n\n\n\n\nWhich is the narrower method?\n\n\nCode\nb_diff &lt;- b_right-b_left\na_diff &lt;- a_right - a_left\n\n\nBypass CI length\n\n\nCode\n#bypass\nprint(b_diff)\n\n\n[1] 1.104007\n\n\nAngiography CI length\n\n\nCode\n#Angiography\nprint(a_diff)\n\n\n[1] 0.7926234\n\n\nCI for Angiography is the narrower method.\n\n\n\n2. A survey of 1031 adult Americans was carried out by the National Center for Public Policy. Assume that the sample is representative of adult Americans. Among those surveyed, 567 believed that college education is essential for success. Find the point estimate, p, of the proportion of all adult Americans who believe that a college education is essential for success.\n\n\nConstruct and interpret a 95% confidence interval for p.\n\n\nCode\n# p value\n\np &lt;- 567/1031\np\n\n\n[1] 0.5499515\n\n\nCode\nz &lt;- qnorm(0.95)\n\n#Confidence interval for p\nCI &lt;- p + c(-1, 1) * z * sqrt((p*(1-p))/1031)\nCI\n\n\n[1] 0.5244662 0.5754368\n\n\nBased on the sample of 1031 adult Americans, we estimate, with 95% confidence, that between 52.45% and 57.54% of all adult Americans believe that a college education is essential for success.\n--------------------------------------------------------------------------------------------------------------------\n\n\n3. Suppose that the financial aid office of UMass Amherst seeks to estimate the mean cost of textbooks per semester for students. The estimate will be useful if it is within $5 of the true population mean (i.e. they want the confidence interval to have a length of $10 or less). The financial aid office is pretty sure that the amount spent on books varies widely, with most values between $30 and $200. They think that the population standard deviation is about a quarter of this range (in other words, you can assume they know the population standard deviation).\n\n\nAssuming the significance level to be 5%, what should be the size of the sample.\n\n\nCode\nsigma &lt;- (200 - 30) / 4 \nerror &lt;- 5\n\nn &lt;- ceiling((qnorm(0.95) * sigma / error) ^ 2)\nn\n\n\n[1] 196\n\n\nThe sample size should be 196.\n\n\nAccording to a union agreement, the mean income for all senior-level workers in a large service company equals $500 per week. A representative of a women’s group decides to analyze whether the mean income μ for female employees matches this norm. For a random sample of nine female employees, ȳ = $410 and s = 90\n\n\nA. Test whether the mean income of female employees differs from $500 per week. Include assumptions, hypotheses, test statistic, and P-value. Interpret the result.\n\n\nCode\nn_s &lt;- 9  \nsample_mean &lt;- 410  \nsample_sd &lt;- 90  \n\n\nAssuming that:\n\nThe sample of female employees is a random sample from the population of all female employees.\nThe population of female employees’ incomes follows a normal distribution or the sample size is large enough for the central limit theorem to apply.\nThe population standard deviation is unknown.\n\nNull Hypothesis H0: Women’s income mean (μ) = $500 per week\n\n\nCode\nh0_mean &lt;- 500\n\n# t value\nt_stat &lt;- (sample_mean - h0_mean) / (sample_sd / sqrt(n_s))\n\n#p-value\np_val &lt;- 2 * pt(-abs(t_stat), df = n_s - 1)\np_val\n\n\n[1] 0.01707168\n\n\nP-value is significantly lower than 0.05 significance level, therefore we reject the null hypothesis and we can’t say that woman’s income mean equals the population mean ($500 per week).\nMean income of female employees in the company is significantly lower than $500 per week.\n\n\nB. Report the P-value for Ha: μ &lt; 500. Interpret.\n\n\nCode\np_left &lt;- pt(t_stat, df = n_s-1)\np_left\n\n\n[1] 0.008535841\n\n\nP-value for the left-tailed test is less than the significance level of 0.05. Therefore, we reject the null hypothesis and conclude that there is sufficient evidence to suggest that the mean income of female employees is less than $500 per week\n\n\nC. Report and interpret the P-value for Ha: μ &gt; 500. (Hint: The P-values for the two possible one-sided tests must sum to 1.\n\n\nCode\np_right &lt;- 1 - p_left\np_right\n\n\n[1] 0.9914642\n\n\n\n\n5. Jones and Smith separately conduct studies to test H0: μ = 500 against Ha: μ ≠ 500, each with n = 1000. Jones gets ȳ = 519.5, with se = 10.0. Smith gets ȳ = 519.7, with se = 10.0.\nA. Show that t = 1.95 and P-value = 0.051 for Jones. Show that t = 1.97 and P-value = 0.049 for Smith.\n\n\nCode\nn_s2 &lt;- 1000 \nh0_mean2 &lt;- 500\n\n#Jones\nsample_meanj &lt;- 519.5  \nsample_sdj &lt;- 10  \n\n# t value\nt_statj &lt;- (sample_meanj - h0_mean2) / (sample_sdj)\nt_statj\n\n\n[1] 1.95\n\n\nCode\n#p-value\np_valj &lt;- 2 * pt(-abs(t_statj), df = n_s2 - 1)\np_valj\n\n\n[1] 0.05145555\n\n\n\n\nCode\n#Smith\nsample_meansm &lt;- 519.7  \nsample_sdsm &lt;- 10  \n\n# t value\nt_statsm &lt;- (sample_meansm - h0_mean2) / (sample_sdsm)\nt_statsm\n\n\n[1] 1.97\n\n\nCode\n#p-value\np_valsm &lt;- 2 * pt(-abs(t_statsm), df = n_s2 - 1)\np_valsm\n\n\n[1] 0.04911426\n\n\nB. Using α = 0.05, for each study indicate whether the result is “statistically significant.”\nWe fail to reject the null hypothesis. In this case, both studies have p-values slightly above 0.05, but below it. So we can say that both studies have marginally significant results at the α = 0.05 level.\nC. Using this example, explain the misleading aspects of reporting the result of a test as “P ≤ 0.05” versus “P &gt; 0.05,” or as “reject H0” versus “Do not reject H0,” without reporting the actual P-value.\nReporting the result of a hypothesis test simply as “P ≤ 0.05” or “reject H0” without reporting the actual p-value can be misleading in several ways:\n\nIt doesn’t provide information about the magnitude of the effect or the strength of evidence against the null hypothesis.\nIt does not convey the uncertainty associated with the p-value. In this example, both studies have p-values slightly above and below 0.05. Reporting the result as simply “P ≤ 0.05” implies a false sense of certainty.\n\n\n\n\n6. A school nurse wants to determine whether age is a factor in whether children choose a healthy snack after school. She conducts a survey of 300 middle school students, with the results below. Test at α = 0.05 the claim that the proportion who choose a healthy snack differs by grade level.\n\n\n\n\n\n\n\n\n\nGrade Level\n6th grade\n7th grade\n8th grade\n\n\n\n\nHealthy Snack\n31\n43\n51\n\n\nUnhealthy Snack\n69\n57\n49\n\n\n\n\n\nWhat is the null hypothesis?\nThe proportion of observed children choosing healthy or unhealthy snack is equal to the expected proportion in all grades.\n\n\nWhich test should we use?\nChi-square\n\n\nCode\nsnack_obs &lt;- matrix(c(31, 43, 51, 69, 57, 49), nrow = 2, byrow = TRUE)\nrownames(snack_obs) &lt;- c(\"Healthy\", \"Unhealthy\")\ncolnames(snack_obs) &lt;- c(\"6th grade\", \"7th grade\", \"8th grade\")\n\nobs &lt;- table(snack_obs)\n\ntotal_obs &lt;- sum(snack_obs)\nsnack_exp &lt;- rep(sum(snack_obs)/3, 3)\nsnack_exp &lt;- rbind(snack_exp, snack_exp)\nrownames(snack_exp) &lt;- c(\"Healthy\", \"Unhealthy\")\ncolnames(snack_exp) &lt;- c(\"6th grade\", \"7th grade\", \"8th grade\")\nsnack_exp &lt;- snack_exp * total_obs\n\nchisq.test(snack_obs, snack_exp, 0.05)\n\n\n\n    Pearson's Chi-squared test\n\ndata:  snack_obs\nX-squared = 8.3383, df = 2, p-value = 0.01547\n\n\n\n\nWhat is the conclusion?\nWe reject the Null Hypothesis. Seems that there is a significant difference between observed proportion of children choosing healthy snack based on the grade versus the expected proportion. In this case seems that in low grades children the proportion of children choosing healthy snack are lower than higher grades.\n\n\n7. Per-pupil costs (in thousands of dollars) for cyber charter school tuition for school districts in three areas are shown. Test the claim that there is a difference in means for the three areas, using an appropriate test.\n“Area 1” 6.2 9.3 6.8 6.1 6.7 7.5 “Area 2” 7.5 8.2 8.5 8.2 7.0 9.3 “Area 3” 5.8 6.4 5.6 7.1 3.0 3.5\n\n\nWhat is the null hypothesis?\nThe means for the Per-pupil costs in the 3 school districts areas are equal.\n\n\nWhich test should we use?\nOne-way Anova.\n\n\nCode\narea1 &lt;- c(6.2, 9.3, 6.8, 6.1, 6.7, 7.5)\narea2 &lt;- c(7.5, 8.2, 8.5, 8.2, 7.0, 9.3)\narea3 &lt;- c(5.8, 6.4, 5.6, 7.1, 3.0, 3.5)\n\nperpupil &lt;- data.frame(area1, area2, area3)\n\nsummary(perpupil)\n\n\n     area1           area2           area3      \n Min.   :6.100   Min.   :7.000   Min.   :3.000  \n 1st Qu.:6.325   1st Qu.:7.675   1st Qu.:4.025  \n Median :6.750   Median :8.200   Median :5.700  \n Mean   :7.100   Mean   :8.117   Mean   :5.233  \n 3rd Qu.:7.325   3rd Qu.:8.425   3rd Qu.:6.250  \n Max.   :9.300   Max.   :9.300   Max.   :7.100  \n\n\nCode\nperpupil2 &lt;- stack(perpupil[,1:3])\n\nmodel &lt;- aov(values ~ ind, data = perpupil2)\n\nsummary(model)\n\n\n            Df Sum Sq Mean Sq F value  Pr(&gt;F)   \nind          2  25.66  12.832   8.176 0.00397 **\nResiduals   15  23.54   1.569                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\nWhat is the conclusion?\nThe per-pupil cost is significantly different based on the area (p&lt;0.05)."
  },
  {
    "objectID": "posts/HW4_AkhileshKumar.html",
    "href": "posts/HW4_AkhileshKumar.html",
    "title": "Homework 4",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.0.10     ✔ readr     2.1.4 \n✔ forcats   1.0.0      ✔ stringr   1.5.0 \n✔ ggplot2   3.4.1      ✔ tibble    3.1.8 \n✔ lubridate 1.9.2      ✔ tidyr     1.2.1 \n✔ purrr     0.3.5      \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nCode\nlibrary(ggplot2)\nlibrary(ggthemes)\nlibrary(stats)\nlibrary(alr4)\n\n\nLoading required package: car\nLoading required package: carData\n\nAttaching package: 'car'\n\nThe following object is masked from 'package:dplyr':\n\n    recode\n\nThe following object is masked from 'package:purrr':\n\n    some\n\nLoading required package: effects\nlattice theme set by effectsTheme()\nSee ?effectsTheme for details.\n\n\nCode\nlibrary(smss)\nlibrary(kableExtra)\n\n\n\nAttaching package: 'kableExtra'\n\nThe following object is masked from 'package:dplyr':\n\n    group_rows\n\n\nCode\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/HW4_AkhileshKumar.html#question-1",
    "href": "posts/HW4_AkhileshKumar.html#question-1",
    "title": "Homework 4",
    "section": "Question 1",
    "text": "Question 1\nFor recent data in Jacksonville, Florida, on y = selling price of home (in dollars), x1 = size of home (in square feet), and x2 = lot size (in square feet), the prediction equation is ŷ = −10,536 + (53.8 * x1) + (2.84* x2)."
  },
  {
    "objectID": "posts/HW4_AkhileshKumar.html#a",
    "href": "posts/HW4_AkhileshKumar.html#a",
    "title": "Homework 4",
    "section": "A",
    "text": "A\nA particular home of 1240 square feet on a lot of 18,000 square feet sold for $145,000. Find the predicted selling price and the residual, and interpret.\n\n\nCode\nx1 = 1240\nx2 = 18000\n\nPredicted_selling_price &lt;-  -10536 + (53.8 * x1) + (2.84 * x2)\n\nPredicted_selling_price\n\n\n[1] 107296\n\n\n\n\nCode\nActual_selling_price = 145000\n\nResidual &lt;- Actual_selling_price-Predicted_selling_price\n\nResidual\n\n\n[1] 37704\n\n\nBased on the given data, the prediction equation suggests that a home with 1240 square feet on a lot of 18,000 square feet in Jacksonville, Florida should have a selling price of $107,296. However, the actual selling price of this particular home was $145,000, which is higher than the predicted selling price. This results in a residual of $37,704, indicating that the prediction equation underestimated the selling price of this home.\nThis suggests that while the prediction equation provides some insight into the selling prices of homes in Jacksonville, Florida based on size of the home and lot size, it is not a perfect predictor. Other factors, such as the condition of the home, location, and other amenities, may also play a role in determining the selling price of a home. Therefore, it is important to consider multiple factors when predicting the selling price of a home in Jacksonville, Florida."
  },
  {
    "objectID": "posts/HW4_AkhileshKumar.html#b",
    "href": "posts/HW4_AkhileshKumar.html#b",
    "title": "Homework 4",
    "section": "B",
    "text": "B\nSince the prediction equation for home selling price is ŷ = −10,536 + (53.8 * x1) + (2.84 * x2), where x1 represents the home size and x2 represents the lot size, the impact of changing home size on the selling price while holding lot size constant, can be determined.\nSpecifically, for each additional square foot of home size (x1), the house selling price is expected to increase by 53.8 dollars, assuming lot size is fixed. This is because the coefficient of x1 in the equation is 53.8.\nThe impact of lot size (x2) on the selling price is also accounted for in the equation, with a coefficient of 2.84. However, if lot size is held constant, then this coefficient does not affect the output, so only coefficient of x1 is required to be considered .\nTherefore, if the home size is increased by one unit (representing one additional square foot), the predicted selling price would increase by 53.8 units, assuming the lot size remains constant."
  },
  {
    "objectID": "posts/HW4_AkhileshKumar.html#c",
    "href": "posts/HW4_AkhileshKumar.html#c",
    "title": "Homework 4",
    "section": "C",
    "text": "C\nAccording to this prediction equation, for fixed home size, how much would lot size need to increase to have the same impact as a one-square-foot increase in home size?\nFor fixed home size, 53.8 * 1 = 2.84x2\n\n\nCode\nx2 &lt;- 53.8/2.84\nx2\n\n\n[1] 18.94366\n\n\nIt can be observed that the predicted selling price of a house is estimated to increase by approximately $3 for every additional square foot of lot size, assuming the house size is fixed. Therefore, to achieve a similar impact on the predicted selling price as an increment of one square foot in house size, lot size should be increased by roughly 19 square feet. This is because every one-square-foot increase in house size is predicted to increase the selling price by around $53.8, which is equivalent to an increase of approximately 18.94 square feet in lot size. Therefore, an increase in lot size of about 18.94 square feet would have an identical effect on the predicted selling price as an increase of one square foot in home size."
  },
  {
    "objectID": "posts/HW4_AkhileshKumar.html#question-2",
    "href": "posts/HW4_AkhileshKumar.html#question-2",
    "title": "Homework 4",
    "section": "Question 2",
    "text": "Question 2\n(Data file: salary in alr4 R package). The data file concerns salary and other characteristics of all faculty in a small Midwestern college collected in the early 1980s for presentation in legal proceedings for which discrimination against women in salary was at issue. All persons in the data hold tenured or tenure track positions; temporary faculty are not included. The variables include degree, a factor with levels PhD and MS; rank, a factor with levels Asst, Assoc, and Prof; sex, a factor with levels Male and Female; Year, years in current rank; ysdeg, years since highest degree, and salary, academic year salary in dollars."
  },
  {
    "objectID": "posts/HW4_AkhileshKumar.html#a-1",
    "href": "posts/HW4_AkhileshKumar.html#a-1",
    "title": "Homework 4",
    "section": "A",
    "text": "A\nTest the hypothesis that the mean salary for men and women is the same, without regard to any other variable but sex. Explain your findings.\n\n\nCode\n# Box plot\ndata(\"salary\")\nggplot(salary, aes(x = sex, y = salary, fill = sex)) +\n  geom_boxplot() +\n  labs(x = \"Sex\", y = \"Salary\", title = \"Salary Comparison between Male & Female\") +\n  theme_void() +\n  theme(axis.title = element_text(color = \"navyblue\", size = 13, face = \"bold\"), \n        axis.text.x = element_text(color = \"navyblue\", size = 10), \n        axis.text.y = element_text(color = \"navyblue\", size = 8), \n        plot.title = element_text(color = \"navyblue\", size = 14, face = \"bold\", hjust = 0.5)) +\n  scale_fill_manual(values = c(\"#FF5C35\", \"#00B945\"))\n\n\n\n\n\n\n\nCode\n# Summarize salary data\nsummary_salary &lt;- salary %&gt;%\n  group_by(sex) %&gt;%\n  summarise(average_salary = mean(salary), min_salary = min(salary), max_salary = max(salary))\n\n# Create salary comparison plot\nggplot(summary_salary, aes(x = sex, y = average_salary)) +   \n  geom_point(color = \"red\", size = 4) +\n  geom_errorbar(aes(ymin = min_salary, ymax = max_salary), color = \"#32CD32\", width = 0.5) +\n  labs(x = \"Sex\", y = \"Salary\", title = \"Salary Comparison between Male & Female\") +\n  geom_text(aes(y = max_salary, label = max_salary), size = 3, color = \"#000080\", hjust = -3) +\n  geom_text(aes(y = min_salary, label = min_salary), size = 3, color = \"#000080\", hjust = -3) +\n  geom_text(aes(label = round(average_salary, digits = 2)), size = 3, color = \"#000080\", hjust = -0.5) +\n  theme_minimal() +\n  theme(axis.text.x = element_text(color = \"#33475b\", size = 10, face = \"bold\"),\n        axis.text.y = element_text(color = \"#33475b\", size = 8, face = \"bold\"),\n        axis.title.y = element_text(color = \"#33475b\", size = 13, face = \"bold\"),\n        axis.title.x = element_text(color = \"#33475b\", size = 13, face = \"bold\"),\n        plot.title = element_text(color = \"#000080\", size = 14, face = \"bold\", hjust = 0.5))\n\n\n\n\n\nBased solely on the visuals, it is clear that the average salary for men and women differ. However, it is uncertain if this difference is statistically significant. Additionally, while men generally have a higher mean salary, there is one outlier female who earns the most, indicating a wider salary range for women. Nonetheless, further analysis is necessary to determine whether these findings are significant.\n\n\nCode\n# t.test to text the null hypothesis that men and women make the same amount. \n\nt.test(salary~sex, data = salary)\n\n\n\n    Welch Two Sample t-test\n\ndata:  salary by sex\nt = 1.7744, df = 21.591, p-value = 0.09009\nalternative hypothesis: true difference in means between group Male and group Female is not equal to 0\n95 percent confidence interval:\n -567.8539 7247.1471\nsample estimates:\n  mean in group Male mean in group Female \n            24696.79             21357.14 \n\n\nBased on a Welch Two Sample t-test, I found that the mean salary for men is higher than women. However, the p-value of 0.09 is higher than a significance level of 5%, indicating that the null hypothesis of ‘the mean salary for men and women are the same’, cannot be rejected. Therefore, It cannot be concluded that there is a significant difference in the mean salary between men and women."
  },
  {
    "objectID": "posts/HW4_AkhileshKumar.html#b-1",
    "href": "posts/HW4_AkhileshKumar.html#b-1",
    "title": "Homework 4",
    "section": "B",
    "text": "B\nRun a multiple linear regression with salary as the outcome variable and everything else as predictors, including sex. Assuming no interactions between sex and the other predictors, obtain a 95% confidence interval for the difference in salary between males and females.\n\n\nCode\nmodel &lt;- lm(salary ~ ., data = salary)\nsummary(model)\n\n\n\nCall:\nlm(formula = salary ~ ., data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4045.2 -1094.7  -361.5   813.2  9193.1 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 15746.05     800.18  19.678  &lt; 2e-16 ***\ndegreePhD    1388.61    1018.75   1.363    0.180    \nrankAssoc    5292.36    1145.40   4.621 3.22e-05 ***\nrankProf    11118.76    1351.77   8.225 1.62e-10 ***\nsexFemale    1166.37     925.57   1.260    0.214    \nyear          476.31      94.91   5.018 8.65e-06 ***\nysdeg        -124.57      77.49  -1.608    0.115    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2398 on 45 degrees of freedom\nMultiple R-squared:  0.855, Adjusted R-squared:  0.8357 \nF-statistic: 44.24 on 6 and 45 DF,  p-value: &lt; 2.2e-16\n\n\n\n\nCode\nconfint(model)\n\n\n                 2.5 %      97.5 %\n(Intercept) 14134.4059 17357.68946\ndegreePhD    -663.2482  3440.47485\nrankAssoc    2985.4107  7599.31080\nrankProf     8396.1546 13841.37340\nsexFemale    -697.8183  3030.56452\nyear          285.1433   667.47476\nysdeg        -280.6397    31.49105\n\n\nThe linear regression analysis shows that there is a significant relationship between salary and the variables degree, rank, sex, year, and years since degree completion. However, the p-value for the variable sex is 0.214, which is higher than the significance level of 0.05, indicating that null hypothesis: that there is no difference in mean salary between males and females, cannot be rejected. Further, at a 95% confidence interval, the difference in mean salary between males and females ranges from -$697.82 to $3,030.56, suggesting that 95% of the time, males will make between $697.82 more and $3,030.56 less than females."
  },
  {
    "objectID": "posts/HW4_AkhileshKumar.html#c-1",
    "href": "posts/HW4_AkhileshKumar.html#c-1",
    "title": "Homework 4",
    "section": "C",
    "text": "C\nInterpret your finding for each predictor variable; discuss (a) statistical significance, (b) interpretation of the coefficient / slope in relation to the outcome variable and other variables\ndegreePhD:\n\nThe coefficient estimate is 1388.61 with a standard error of 1018.75.\nThe p-value is 0.180, which is greater than 0.05, indicating that the coefficient for “degreePhD” is not statistically significant.\nThis means that there is insufficient evidence to suggest that having a PhD degree has a statistically significant effect on salary, after accounting for other variables in the model. This suggests that having a PhD degree does not have a significant effect on salary after accounting for the other variables in the model.\n\nrankAssoc:\n\nThe coefficient estimate is 5292.36 with a standard error of 1145.40.\nThe p-value is 3.22e-05, which is less than 0.05, indicating that the coefficient for “rankAssoc” is statistically significant.\nThis means that having an associate professor rank is associated with a salary increase of $5292.36 on average, compared to assistant professors, after controlling for the other variables in the model.\n\nrankProf:\n\nThe coefficient estimate is 11118.76 with a standard error of 1351.77.\nThe p-value is 1.62e-10, which is less than 0.05, indicating that the coefficient for “rankProf” is statistically significant.\nThis means that having a professor rank is associated with a salary increase of $11118.76 on average, compared to assistant professors, after controlling for the other variables in the model.\n\nsexFemale:\n\nThe coefficient estimate is 1166.37 with a standard error of 925.57.\nThe p-value is 0.214, which is greater than 0.05, indicating that the coefficient for “sexFemale” is not statistically significant.\nThis suggests that gender does not have a significant effect on salary after accounting for the other variables in the model.\n\nyear:\n\nThe coefficient estimate is 476.31 with a standard error of 94.91.\nThe p-value is 8.65e-06, which is less than 0.05, indicating that the coefficient for “year” is statistically significant.\nThis means that for each additional year, there is an increase in salary of $476.31 on average, after controlling for the other variables in the model.\n\nysdeg:\n\nThe coefficient estimate is -124.57 with a standard error of 77.49.\nThe p-value is 0.115, which is greater than 0.05, indicating that the coefficient for “ysdeg” is not statistically significant.\nThis suggests that the years since receiving a degree does not have a significant effect on salary after accounting for the other variables in the model.\n\nOverall, the model shows that rank and years of experience are the most significant predictors of salary in this dataset."
  },
  {
    "objectID": "posts/HW4_AkhileshKumar.html#d",
    "href": "posts/HW4_AkhileshKumar.html#d",
    "title": "Homework 4",
    "section": "D",
    "text": "D\nChange the baseline category for the rank variable. Interpret the coefficients related to rank again.\n\n\nCode\nsalary$rank &lt;- relevel(salary$rank, ref = 'Prof')\ncolnames(salary)\n\n\n[1] \"degree\" \"rank\"   \"sex\"    \"year\"   \"ysdeg\"  \"salary\"\n\n\nCode\nsummary(lm(salary ~ ., salary))\n\n\n\nCall:\nlm(formula = salary ~ ., data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4045.2 -1094.7  -361.5   813.2  9193.1 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  26864.81    1375.29  19.534  &lt; 2e-16 ***\ndegreePhD     1388.61    1018.75   1.363    0.180    \nrankAsst    -11118.76    1351.77  -8.225 1.62e-10 ***\nrankAssoc    -5826.40    1012.93  -5.752 7.28e-07 ***\nsexFemale     1166.37     925.57   1.260    0.214    \nyear           476.31      94.91   5.018 8.65e-06 ***\nysdeg         -124.57      77.49  -1.608    0.115    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2398 on 45 degrees of freedom\nMultiple R-squared:  0.855, Adjusted R-squared:  0.8357 \nF-statistic: 44.24 on 6 and 45 DF,  p-value: &lt; 2.2e-16\n\n\nAfter changing the baseline category for the rank variable, an Associate can expect a 6483.0 dollar decrease in salary compared to Professor, while a Assistant can expect a 11890.3 dollar salary decrease compared to Professor. Both ranks have significance levels well below 0.05 and it can be determined that rank does have a statistically significant impact on salary.\nInterpreting the coefficients related to rank after changing the baseline category to “Prof”, I see that the coefficients for both rankAsst and rankAssoc are negative. This means that the average salary for assistant professors and associate professors is lower than the average salary for full professors, holding all other variables constant.\nThe coefficient for rankAsst is -11118.76, indicating that assistant professors earn, on average, $11,118.76 less than full professors when all other variables are held constant. The coefficient for rankAssoc is -5826.40, indicating that associate professors earn, on average, $5,826.40 less than full professors when all other variables are held constant.\nThe interpretation of these coefficients assumes that the other variables included in the model (degreePhD, sexFemale, year, and ysdeg) are held constant."
  },
  {
    "objectID": "posts/HW4_AkhileshKumar.html#e",
    "href": "posts/HW4_AkhileshKumar.html#e",
    "title": "Homework 4",
    "section": "E",
    "text": "E\nFinkelstein (1980), in a discussion of the use of regression in discrimination cases, wrote, “a variable may reflect a position or status bestowed by the employer, in which case if there is discrimination in the award of the position or status, the variable may be ‘tainted.’” Thus, for example, if discrimination is at work in promotion of faculty to higher ranks, using rank to adjust salaries before comparing the sexes may not be acceptable to the courts.Exclude the variable rank, refit, and summarize how your findings changed, if they did\n\n\nCode\nsummary(lm(salary ~ degree + sex + year + ysdeg, salary))\n\n\n\nCall:\nlm(formula = salary ~ degree + sex + year + ysdeg, data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8146.9 -2186.9  -491.5  2279.1 11186.6 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 17183.57    1147.94  14.969  &lt; 2e-16 ***\ndegreePhD   -3299.35    1302.52  -2.533 0.014704 *  \nsexFemale   -1286.54    1313.09  -0.980 0.332209    \nyear          351.97     142.48   2.470 0.017185 *  \nysdeg         339.40      80.62   4.210 0.000114 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3744 on 47 degrees of freedom\nMultiple R-squared:  0.6312,    Adjusted R-squared:  0.5998 \nF-statistic: 20.11 on 4 and 47 DF,  p-value: 1.048e-09\n\n\n\nThe coefficient for the predictor variable degreePhD is -3299.35, with a standard error of 1302.52, and a p-value of 0.014704, indicating a significant negative relationship between having a PhD degree and salary.\nThe coefficient for the predictor variable sexFemale is -1286.54, with a standard error of 1313.09, and a p-value of 0.332209, indicating that there is no significant difference in salary between male and female employees.\nThe coefficient for the predictor variable year is 351.97, with a standard error of 142.48, and a p-value of 0.017185, indicating a significant positive relationship between the year of employment and salary.\nThe coefficient for the predictor variable ysdeg is 339.40, with a standard error of 80.62, and a p-value of 0.000114, indicating a significant positive relationship between the number of years of service and salary.\n\nThe R-squared value of the model is 0.6312, indicating that the predictors explain 63.12% of the variability in salary. The adjusted R-squared value, which adjusts for the number of predictors in the model, is 0.5998.\nThe F-statistic of the model is 20.11 with 4 and 47 degrees of freedom, and the p-value is 1.048e-09, indicating that the model is a good fit for the data and that at least one of the predictors is significantly related to salary."
  },
  {
    "objectID": "posts/HW4_AkhileshKumar.html#f",
    "href": "posts/HW4_AkhileshKumar.html#f",
    "title": "Homework 4",
    "section": "F",
    "text": "F\nEveryone in this dataset was hired the year they earned their highest degree. It is also known that a new Dean was appointed 15 years ago, and everyone in the dataset who earned their highest degree 15 years ago or less than that has been hired by the new Dean. Some people have argued that the new Dean has been making offers that are a lot more generous to newly hired faculty than the previous one and that this might explain some of the variation in Salary.\nCreate a new variable that would allow you to test this hypothesis and run another multiple regression model to test this. Select variables carefully to make sure there is no multicollinearity. Explain why multicollinearity would be a concern in this case and how you avoided it. Do you find support for the hypothesis that the people hired by the new Dean are making higher than those that were not?\n\n\nCode\nsalary$NewDean &lt;- ifelse(salary$ysdeg &lt;= 15, 1, 0)\n\nsummary(lm(salary ~ sex + rank + degree + year+ NewDean, data = salary))\n\n\n\nCall:\nlm(formula = salary ~ sex + rank + degree + year + NewDean, data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3403.3 -1387.0  -167.0   528.2  9233.8 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  24425.32    1107.52  22.054  &lt; 2e-16 ***\nsexFemale      907.14     840.54   1.079   0.2862    \nrankAsst    -11096.95    1191.00  -9.317 4.54e-12 ***\nrankAssoc    -6124.28    1028.58  -5.954 3.65e-07 ***\ndegreePhD      818.93     797.48   1.027   0.3100    \nyear           434.85      78.89   5.512 1.65e-06 ***\nNewDean       2163.46    1072.04   2.018   0.0496 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2362 on 45 degrees of freedom\nMultiple R-squared:  0.8594,    Adjusted R-squared:  0.8407 \nF-statistic: 45.86 on 6 and 45 DF,  p-value: &lt; 2.2e-16\n\n\nTo test the hypothesis, a dummy variable (NewDean) was created. It is coded as 1 if ysdeg is 15 years or less, and 0 otherwise.\nThese correlations indicate that there is a relationship between the variables and they might have a similar impact on the response variable (salary). To address this issue, the variable ysdeg was removed due to its overlap with NewDean variable (since the NewDean variable was based on ysdeg).\nThen, I fit a new regression model, including the variables of sex, rank, degree, year and NewDean. According to the regression results, employees hired under the new dean are expected to earn $2163.46 more than those hired prior to the new dean’s tenure. However, this correlation is only significant at the 0.0496 level. While it is statistically significant, it is not the most powerful predictor of salary."
  },
  {
    "objectID": "posts/HW4_AkhileshKumar.html#question-3",
    "href": "posts/HW4_AkhileshKumar.html#question-3",
    "title": "Homework 4",
    "section": "Question 3",
    "text": "Question 3\n\n\nCode\ndata(\"house.selling.price\")\nhouse.selling.price"
  },
  {
    "objectID": "posts/HW4_AkhileshKumar.html#a-2",
    "href": "posts/HW4_AkhileshKumar.html#a-2",
    "title": "Homework 4",
    "section": "A",
    "text": "A\nUsing the house.selling.price data, run and report regression results modeling y = selling price (in dollars) in terms of size of home (in square feet) and whether the home is new (1 = yes; 0 = no). In particular, for each variable; discuss statistical significance and interpret the meaning of the coefficient.\n\n\nCode\nsummary(lm(Price ~ Size + New, data = house.selling.price))\n\n\n\nCall:\nlm(formula = Price ~ Size + New, data = house.selling.price)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-205102  -34374   -5778   18929  163866 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -40230.867  14696.140  -2.738  0.00737 ** \nSize           116.132      8.795  13.204  &lt; 2e-16 ***\nNew          57736.283  18653.041   3.095  0.00257 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 53880 on 97 degrees of freedom\nMultiple R-squared:  0.7226,    Adjusted R-squared:  0.7169 \nF-statistic: 126.3 on 2 and 97 DF,  p-value: &lt; 2.2e-16\n\n\nThe coefficient of Size is estimated to be 116.132, which means that for each unit increase in Size, it is expected that the Price would increase by an average of 116.132. The t-value of 13.204 is highly significant (p &lt; 0.001), indicating that the coefficient is significantly different from zero. Therefore, the null hypothesis that there is no linear relationship between Size and Price is rejected.\nThe coefficient of New is estimated to be 57736.283, which means that its price would be $57736.283 higher than a house that is not new, all other things being equal. The t-value of 3.095 is also significant (p = 0.00257), indicating that the coefficient is significantly different from zero. Therefore, the null hypothesis that there is no linear relationship between New and Price, is rejected.\nThe intercept coefficient is estimated to be -40230.867, which is the expected value of Price when both Size and New are equal to zero. However, since there are no houses with zero Size and zero New, this value does not have any practical meaning in this context.\nThe adjusted R-squared of this model is 0.7169, which means that approximately 71.7% of the variability in Price can be explained by Size and New. The F-statistic of 126.3 with p-value &lt; 2.2e-16 indicates that the model as a whole is statistically significant, and that the predictor variables are jointly significant in explaining the variation in the response variable."
  },
  {
    "objectID": "posts/HW4_AkhileshKumar.html#b-2",
    "href": "posts/HW4_AkhileshKumar.html#b-2",
    "title": "Homework 4",
    "section": "B",
    "text": "B\nReport and interpret the prediction equation, and form separate equations relating selling price to size for new and for not new homes\nBased on the above regression model, the prediction equation for the selling price of a home would be selling price = -40230.867 + 116.132 * size + 57736.283 * new where size = the size of the home and new = 1 if the home is new or new = 0 if the home is not new.\n\nFor new homes, z = 1 so the prediction equation for the selling price of a new home is selling price = -40230.867 + 116.132 * size + 57736.283 or selling price = 17505.416 + 116.132 * size\nFor old homes, z = 0 so the prediction equation for the selling price of a not-new home is selling price = -40230.867 + 116.132 * size + 0 or selling price = -40230.867 + 116.132 * size\n\nBased on the regression analysis and the obtained equations, we can infer that the variables Size and New have a positive influence on the selling price of a home. The coefficient of Size denotes that there is a linear association between the size of a home and its selling prce. This implies that for every 1 sq foot increase in the size of a home, we can expect an estimated increase of ~$116 in the selling price, while holding all other factors constant. On the other hand, the coefficient of New indicates that new homes, on average, are sold for ~$57,736 more than old homes of the same size.\nSince there is no interaction term in the model, the impact of each variable is analyzed separately. This suggests that the effect of Size on the selling price is consistent for both new and old homes. Moreover, both Size and New have been found to be statistically significant, respectively, based on their small p-values."
  },
  {
    "objectID": "posts/HW4_AkhileshKumar.html#c-2",
    "href": "posts/HW4_AkhileshKumar.html#c-2",
    "title": "Homework 4",
    "section": "C",
    "text": "C\nFind the predicted selling price for a home of 3000 square feet that is (i) new, (ii) not new.\n\n\nCode\nSize &lt;- 3000\n\n#predicted selling price for a new home\nNew_Price = 17505.416 + (116.132 * Size)\n\n#predicted selling price for a not new home\nOld_Price = -40230.867 + (116.132 * Size)\n\nNew_Price\n\n\n[1] 365901.4\n\n\nCode\nOld_Price\n\n\n[1] 308165.1\n\n\nBased on the predictive formulas from the regression model, the predicted selling price for a new home of 3000 sq. feet is $365,901 and the predicted selling price for a not-new home of the same size is $308,165."
  },
  {
    "objectID": "posts/HW4_AkhileshKumar.html#d-1",
    "href": "posts/HW4_AkhileshKumar.html#d-1",
    "title": "Homework 4",
    "section": "D",
    "text": "D\nFit another model, this time with an interaction term allowing interaction between size and new, and report the regression results\n\n\nCode\nsummary(lm(Price ~ Size*New, data = house.selling.price))\n\n\n\nCall:\nlm(formula = Price ~ Size * New, data = house.selling.price)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-175748  -28979   -6260   14693  192519 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -22227.808  15521.110  -1.432  0.15536    \nSize           104.438      9.424  11.082  &lt; 2e-16 ***\nNew         -78527.502  51007.642  -1.540  0.12697    \nSize:New        61.916     21.686   2.855  0.00527 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 52000 on 96 degrees of freedom\nMultiple R-squared:  0.7443,    Adjusted R-squared:  0.7363 \nF-statistic: 93.15 on 3 and 96 DF,  p-value: &lt; 2.2e-16\n\n\n\nThe new model includes an interaction term between Size and New, allowing for the possibility that the effect of Size on Price differs depending on whether a home is new or not.\nThe coefficient for the interaction term is positive and statistically significant (p &lt; 0.01), indicating that the effect of Size on Price is stronger for new homes compared to not-new homes.\nThe coefficient for New is negative but not statistically significant (p &gt; 0.05), suggesting that being a new home does not have a significant impact on Price when Size and the interaction term are taken into account.\nThe coefficient for Size is positive and highly significant (p &lt; 0.001), indicating that increasing the size of a home leads to a higher predicted selling price, all else being equal.\nThe adjusted R-squared for the model is 0.7363, indicating that the model explains around 74% of the variation in selling prices, after accounting for the effects of Size, New, and the interaction term."
  },
  {
    "objectID": "posts/HW4_AkhileshKumar.html#e-1",
    "href": "posts/HW4_AkhileshKumar.html#e-1",
    "title": "Homework 4",
    "section": "E",
    "text": "E\nReport the lines relating the predicted selling price to the size for homes that are (i) new, (ii) not new.\nThe predicted selling price, based on the new regression that includes interaction between Size and Newness, would look like:\nHouse_Price = -22227.808 + 104.438 * Size - 78527.50 * z + 61.916 * Size * z\nWhere Size = the size of the home and z = 1 if the home is new or z = 0 if the home is not new.\nNew_Price = -22227.808 + 104.438 * Size - 78527.50 * 1 + 61.916 * Size * 1\n      = -22227.808 + 104.438 * Size - 78527.50  + 61.916 * Size\n      \n      = -100755 + 166.354 * Size\nOld_Price = -22227.808 + 104.438 * Size - 78527.50 * 0 + 61.916 * Size * 0\n      = -22227.808 + 104.438 * Size"
  },
  {
    "objectID": "posts/HW4_AkhileshKumar.html#f-1",
    "href": "posts/HW4_AkhileshKumar.html#f-1",
    "title": "Homework 4",
    "section": "F",
    "text": "F\nFind the predicted selling price for a home of 3000 square feet that is (i) new, (ii) not new.\n\n\nCode\nSize &lt;- 3000\n\n# predicted selling price for a new home\n\nNew_Price = -22227.808 + 104.438 * Size - 78527.50 * 1 + 61.916 * Size * 1\n\n#predicted selling price for a not new home\n\nOld_Price = -22227.808 + 104.438 * Size\n\nNew_Price\n\n\n[1] 398306.7\n\n\nCode\nOld_Price\n\n\n[1] 291086.2\n\n\nBased on the predictive formulas from the regression model, the predicted selling price for a new home that is 3000 sq. feet is $398,306.70 and the predicted selling price for a not new home of the same size is $291,086.20."
  },
  {
    "objectID": "posts/HW4_AkhileshKumar.html#g",
    "href": "posts/HW4_AkhileshKumar.html#g",
    "title": "Homework 4",
    "section": "G",
    "text": "G\nFind the predicted selling price for a home of 1500 square feet that is (i) new, (ii) not new. Comparing to (F), explain how the difference in predicted selling prices changes as the size of home increases\n\n\nCode\nSize &lt;- 1500\n\n# predicted selling price for a new home\n\nNew_Price = -22227.81 + 104.438 * Size - 78527.50 * 1 + 61.916 * Size * 1\n\n#predicted selling price for a not new home\n\nOld_Price = -22227.81 + 104.438 * Size\n\nNew_Price\n\n\n[1] 148775.7\n\n\nCode\nOld_Price\n\n\n[1] 134429.2\n\n\nAs size of home goes up, the difference in predicted selling prices between old and new homes become larger.\nFor a 3,000-square-foot house, the difference in price between one that’s new and one that’s not is $107220.5.\nFor a 1,500-square-food-house, the difference in price between one that’s new and one that’s not is $14352.5.\nAs the houses get bigger in size, the impact of whether the house is new or not new increases."
  },
  {
    "objectID": "posts/HW4_AkhileshKumar.html#h",
    "href": "posts/HW4_AkhileshKumar.html#h",
    "title": "Homework 4",
    "section": "H",
    "text": "H\nDo you think the model with interaction or the one without it represents the relationship of size and new to the outcome price? What makes you prefer one model over another?\nIn the context of predicting the selling price of a house, I analyzed two regression models - one that includes an interaction term between the size of a house and whether it’s new or not, and another that does not include the interaction term. I compared the two models to determine which one better represents the relationship between the size of a house, its age, and its selling price.\nUpon comparing the two models, I found that the model that allows for interactions does a better job of representing the relationship between the size of a house and whether it’s new or not to the selling price. The adjusted R-squared for the model with interaction is 0.7363, which is slightly higher than the adjusted R-squared for the model without interaction, which is 0.7169. This suggests that the model with interaction explains slightly more of the variation in the selling price due to the interaction between size, new and selling price.\nFurthermore, the model with interaction includes a created variable Size:New, which is statistically significant. This means that null hypothesis that Size:New is not correlated to selling price can be rejected, and it can be concluded that the interaction between the size of a house and whether it’s new or not has a significant impact on its selling price.\nThe model that includes the interaction term it more preferable, as it explains slightly more of the variation in the selling price due to the interaction between size, new and selling price. Additionally, the statistical significance of the Size:New variable suggests that this interaction term is necessary to create an accurate equation for predicting the selling price of a house."
  },
  {
    "objectID": "posts/Derian-Toth_Homework3.html",
    "href": "posts/Derian-Toth_Homework3.html",
    "title": "Homework 3",
    "section": "",
    "text": "library(“alr4”)\nlibrary(“smss”)\n\n\nCode\nlibrary(\"alr4\")\n\n\nLoading required package: car\n\n\nLoading required package: carData\n\n\nLoading required package: effects\n\n\nlattice theme set by effectsTheme()\nSee ?effectsTheme for details.\n\n\nCode\nlibrary(\"smss\")\nlibrary(\"ggplot2\")\n        \ndata(UN11)\nhead(UN11)\n\n\n                region  group fertility   ppgdp lifeExpF pctUrban\nAfghanistan       Asia  other     5.968   499.0    49.49       23\nAlbania         Europe  other     1.525  3677.2    80.40       53\nAlgeria         Africa africa     2.142  4473.0    75.00       67\nAngola          Africa africa     5.135  4321.9    53.17       59\nAnguilla     Caribbean  other     2.000 13750.1    81.10      100\nArgentina   Latin Amer  other     2.172  9162.1    79.89       93\n\n\n\nQuestion 1:\nGraph ppgdp with fertility and transform the data\na) the predictor variable is the independent variable, this is per person gross national product (ppgdp). And the response is the dependent variable, this is fertility.\n\n\nCode\nggplot(data = UN11, aes(x = ppgdp, y = fertility)) +\n  geom_point() +\n  geom_smooth(method = 'lm', se=F)\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nb) The scatterplot above shows the level of fertility based on per person gdp across different countries. What we can see is that as the gdp person increases, the level of fertility decreases, until it reaches a specific point, and then it stays pretty level.\n\n\nCode\nggplot(data = UN11, aes(x = log(ppgdp), y = log(fertility))) +\n  geom_point() +\n  geom_smooth()\n\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\nc) Now that we have transformed this data into a logarithm, a simple linear regression does seem plausible to summarize the graph.\n\n\nQuestion 2:\na) When the independent variable is increased by 1.33, the slope of the prediction will increase by 1.33.\nb) When the independent variable is increased by 1.33, the correlation will not change. The correlation is the standardized version of the slope and does not depend on unit of measurement.\n\n\nQuestion 3:\nCan Southern California’s water supply in future years be predicted from past data?\n\n\nCode\ndata(water)\nhead(water)\n\n\n  Year APMAM APSAB APSLAKE OPBPC  OPRC OPSLAKE  BSAAM\n1 1948  9.13  3.58    3.91  4.10  7.43    6.47  54235\n2 1949  5.28  4.82    5.20  7.55 11.11   10.26  67567\n3 1950  4.20  3.77    3.67  9.52 12.20   11.35  66161\n4 1951  4.60  4.46    3.93 11.14 15.15   11.13  68094\n5 1952  7.15  4.99    4.88 16.34 20.05   22.81 107080\n6 1953  9.70  5.65    4.91  8.88  8.15    7.41  67594\n\n\nCode\nggplot(data = water, aes(x = Year, y = BSAAM)) +\n  geom_point() +\n  geom_smooth()\n\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\nDue to the shape of this relationship, it is best to transform the data.\n\n\nCode\npairs(water)\n\n\n\n\n\nWhat we see in the scatterplot matrix above is a relationship between locations, but not necesarily location and year (looking at the top row of scatterplots). Therefore further analysis is necessary in order to attempt a prediction of water runoff.\n\n\nQuestion 4:\nProfessor Ratings Overtime\n\n\nCode\ndata(\"Rateprof\")\npairs(~ quality + helpfulness + clarity + easiness + raterInterest, data=Rateprof)\n\n\n\n\n\nFrom the scatterplot matrix above we can see that there is a positivie relationship between professor rating and quality of professor, helpfulness of professor, and their clarity. However the professor rating is not related to the easiness of their class or the rater interest in the material.\n\n\nQuestion 5:\n\n\nRegression Analysis\n\n\nCode\ndata(\"student.survey\")\nhead(student.survey)\n\n\n  subj ge ag  hi  co   dh   dr tv sp ne ah    ve pa           pi           re\n1    1  m 32 2.2 3.5    0  5.0  3  5  0  0 FALSE  r conservative   most weeks\n2    2  f 23 2.1 3.5 1200  0.3 15  7  5  6 FALSE  d      liberal occasionally\n3    3  f 27 3.3 3.0 1300  1.5  0  4  3  0 FALSE  d      liberal   most weeks\n4    4  f 35 3.5 3.2 1500  8.0  5  5  6  3 FALSE  i     moderate occasionally\n5    5  m 23 3.1 3.5 1600 10.0  6  6  3  0 FALSE  i very liberal        never\n6    6  m 39 3.5 3.5  350  3.0  4  5  7  0 FALSE  d      liberal occasionally\n     ab    aa    ld\n1 FALSE FALSE FALSE\n2 FALSE FALSE    NA\n3 FALSE FALSE    NA\n4 FALSE FALSE FALSE\n5 FALSE FALSE FALSE\n6 FALSE FALSE    NA\n\n\nCode\n#regression analysis i\n\nstudent_survey_reg_i&lt;-lm(as.numeric(pi) ~ as.numeric(re), data = student.survey)\nsummary(student_survey_reg_i)\n\n\n\nCall:\nlm(formula = as.numeric(pi) ~ as.numeric(re), data = student.survey)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.81243 -0.87160  0.09882  1.12840  3.09882 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      0.9308     0.4252   2.189   0.0327 *  \nas.numeric(re)   0.9704     0.1792   5.416 1.22e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.345 on 58 degrees of freedom\nMultiple R-squared:  0.3359,    Adjusted R-squared:  0.3244 \nF-statistic: 29.34 on 1 and 58 DF,  p-value: 1.221e-06\n\n\nCode\nggplot(student.survey, aes(re, fill = pi)) +\n  geom_bar(position = \"fill\")\n\n\n\n\n\nAbove we can see the relationship between religiosity and political ideology. We see that the students who attend church more regularly also identify more as conservatives than liberals. According to the regression analysis, the relationship between political ideology and religiosity is significant with a p value very close to 0. From this we can see that as religiosity increases, political ideology leans conservative.\n\n\nCode\n#regression analysis (ii)\n\nstudent_survey_reg_ii &lt;- lm(hi ~ tv, data = student.survey)\nsummary(student_survey_reg_ii)\n\n\n\nCall:\nlm(formula = hi ~ tv, data = student.survey)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.2583 -0.2456  0.0417  0.3368  0.7051 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  3.441353   0.085345  40.323   &lt;2e-16 ***\ntv          -0.018305   0.008658  -2.114   0.0388 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4467 on 58 degrees of freedom\nMultiple R-squared:  0.07156,   Adjusted R-squared:  0.05555 \nF-statistic: 4.471 on 1 and 58 DF,  p-value: 0.03879\n\n\nCode\nggplot(data = student.survey, aes(x = tv, y = hi)) +\n  geom_point() +\n  geom_smooth(method = 'lm', se=F)\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nAbove we can see the relationship between average rate of tv watching per week and high school GPA. We see that the students who watch more TV on average have a lower GPS. According to the regression analysis, the relationship between average hours of tv watched per week and high school GPS is significant with a p value very close to 0.01. The adjusted R squared is quite low and the p value is not at a strong significance level. We can see that the relationship in the regression analysis above is stronger."
  },
  {
    "objectID": "posts/HW1_XiaoyanHu.html",
    "href": "posts/HW1_XiaoyanHu.html",
    "title": "Homework 1",
    "section": "",
    "text": "Code\nlibrary(tidyr)\nlibrary(dplyr)\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nCode\nlibrary(readxl)\nlibrary(ggplot2)\ndata&lt;-read_excel(\"/Users/cassie199/Desktop/23spring/603_Spring_2023-1/posts/_data/LungCapData.xls\")\nhead(data)\n\n\n# A tibble: 6 × 6\n  LungCap   Age Height Smoke Gender Caesarean\n    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;    \n1    6.48     6   62.1 no    male   no       \n2   10.1     18   74.7 yes   female no       \n3    9.55    16   69.7 no    female yes      \n4   11.1     14   71   no    male   no       \n5    4.8      5   56.9 no    male   no       \n6    6.22    11   58.7 no    female no       \n\n\n\nQuestion 1\n\nUse the LungCapData to answer the following questions. (Hint: Using dplyr, especially group_by() and summarize() can help you answer the following questions relatively efficiently.)\n\n\nWhat does the distribution of LungCap look like? (Hint: Plot a histogram with probability density on the y axis)\n\n-the distribution of LungCap looks like a normal distribution\n\n\nCode\nggplot(data, aes(x=LungCap)) + geom_histogram()\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\nCompare the probability distribution of the LungCap with respect to Males and Females? (Hint:make boxplots separated by gender using the boxplot() function)\n\n\n\nCode\nboxplot(data$LungCap~data$Gender)\n\n\n\n\n\n\nCompare the mean lung capacities for smokers and non-smokers. Does it make sense?\n\n\n\nCode\nboxplot(data$LungCap~data$Smoke)\n\n\n\n\n\n\nExamine the relationship between Smoking and Lung Capacity within age groups: “less than or equal to 13”, “14 to 15”, “16 to 17”, and “greater than or equal to 18”.\n\n\n\nCode\nage_ranges &lt;- c(\"&lt;=13\", \"14-15\", \"16-17\",  \"&gt;18\")\ndata$age_ranges &lt;- cut(data$Age, breaks = c(13, 14,15, 16,17, 18),\n                     include.lowest = TRUE)\nggplot(data, aes(x = age_ranges, y = LungCap)) +\n  geom_boxplot() +\n  labs(x = \"Age Range\", y = \"LungCap\")\n\n\n\n\n\n\nCompare the lung capacities for smokers and non-smokers within each age group. Is your answer different from the one in part c. What could possibly be going on here?\n\n#Question 2\n\nLet X = number of prior convictions for prisoners at a state prison at which there are 810 prisoners.\n\nX 0 1 2 3 4 Frequency 128 434 160 64 24\n\nWhat is the probability that a randomly selected inmate has exactly 2 prior convictions?\n\n\n\nCode\nn&lt;-810\nX&lt;-tibble(x=0:4,\n          F=c(128,434,160,64,24))\nX\n\n\n# A tibble: 5 × 2\n      x     F\n  &lt;int&gt; &lt;dbl&gt;\n1     0   128\n2     1   434\n3     2   160\n4     3    64\n5     4    24\n\n\nCode\npa&lt;-160/n\npa\n\n\n[1] 0.1975309\n\n\n\nWhat is the probability that a randomly selected inmate has fewer than 2 prior convictions?\n\n\n\nCode\npb&lt;-(128+434)/n\npb\n\n\n[1] 0.6938272\n\n\n\nWhat is the probability that a randomly selected inmate has 2 or fewer prior convictions?\n\n\n\nCode\npc&lt;-(128+434+160)/n\npc\n\n\n[1] 0.891358\n\n\n\nWhat is the probability that a randomly selected inmate has more than 2 prior convictions?\n\n\n\nCode\npd&lt;-(64+24)/n\npd\n\n\n[1] 0.108642\n\n\n\nWhat is the expected value1 for the number of prior convictions?\n\n\n\nCode\nprior&lt;-c(434,160,64,24)\nmean(prior)\n\n\n[1] 170.5\n\n\n\nCalculate the variance and the standard deviation for the Prior Convictions.\n\n\n\nCode\nvar(prior)\n\n\n[1] 34115.67\n\n\nCode\nsd(prior)\n\n\n[1] 184.7043"
  },
  {
    "objectID": "posts/EmmaNarkewicz_FinalProject_CheckIn1.html",
    "href": "posts/EmmaNarkewicz_FinalProject_CheckIn1.html",
    "title": "Final Project Proposal",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/EmmaNarkewicz_FinalProject_CheckIn1.html#research-question",
    "href": "posts/EmmaNarkewicz_FinalProject_CheckIn1.html#research-question",
    "title": "Final Project Proposal",
    "section": "Research Question",
    "text": "Research Question\nFor my final project I want to expand on research on the mental health, empathy, and burnout of medical school students using a data set of 886 medical students in Switzerland. The COVID-19 pandemic heightened the mental health challenges of health care workers around the world (Teisman et al., 2021). Numerous studies show that health care workers are prone to compassion fatigue due to working long hours in stressful work environments with continuous exposure to trauma (Jennings, 2009; Rodriguez & Carlotta, 2017; Peters, 2018; Yayha et al., 2021; Carrard et al., 2022; Shin et al., 2022).\nThe Association of American Medical Colleges (AAMC) found that 30% of surveyed medical students and residents met the criteria for depression and 10% reported having suicidal thoughts (Pasturel, 2020). Previous studies conducted on samples of health care workers in Switzerland, Iraq, and South Korea examined the impact of gender on burnout, finding that female medical students had higher rates of empathy and burnout than male coworkers (Carrard et al., 2022; Yahya et al., 2021; Shin et al., 2022). A 2009 multi-site study of medical students in the U.S. found statistically significant differences in depression by gender but not by ethnicity (Goebert et al., 2009). In contrast, the same study found statistically significant differences in suicidal ideation by ethnicity, but not by gender, with Black students reporting the highest rates of suicidal ideation & Caucasian students reporting the lowest rates of suicidal ideation (Goebert et al., 2009).\nResearch Question: Why are some medical students more likely to experience burnout than others?"
  },
  {
    "objectID": "posts/EmmaNarkewicz_FinalProject_CheckIn1.html#hypothesis-testing",
    "href": "posts/EmmaNarkewicz_FinalProject_CheckIn1.html#hypothesis-testing",
    "title": "Final Project Proposal",
    "section": "Hypothesis Testing",
    "text": "Hypothesis Testing\nI want to explore further how ethnic identity might serve as a protective or risk factor for the burnout of medical students, specifically for international medical students. A 2022 study of medical school students in Croatia found that international medical students experience higher rates of burnout mediated by social and familial loneliness (Gradiski et al., 2022). For my final project I will test whether or not a student’s first language being a national language of Switzerland – where the sample was taken – impacts their burnout. The commonly spoken national languages of Switzerland are French, German, and Italian (Kużelewska, 2016).\nHypothesis: Medical students whose native language is a national language of the country where they are studying will experience lower rates of burnout than medical students with other native languages.\nThe reasoning behind my hypothesis is if a medical student’s native language is one of the national language of Switzerland, they will have benefit from potential protective factors of social, cultural, and familial connections. In contrast, I expect medical students whose native language is not German, French, or Italian to be at higher risk for burnout mediated through increased stress from coping with different culture, language, and physical separation from their family."
  },
  {
    "objectID": "posts/EmmaNarkewicz_FinalProject_CheckIn1.html#descriptive-statistics",
    "href": "posts/EmmaNarkewicz_FinalProject_CheckIn1.html#descriptive-statistics",
    "title": "Final Project Proposal",
    "section": "Descriptive Statistics",
    "text": "Descriptive Statistics\nThe data set I will be analyzing contains demographic information on 886 medical students in Switzerland. Students answered demographic information about their age, gender, their year in school and well as the results of self-reported empathy, depression, anxiety, and burnout. The data set was downloaded from Kaggle at https://www.kaggle.com/datasets/thedevastator/medical-student-mental-health?select=Codebook+Carrard+et+al.+2022+MedTeach.csv but originally sourced for a 2022 publication in the Medical Teacher Journe by Carrard et al.\nImportant variables I want to explore in my data set as potential risk and protective factors:\n\nNative Language\nAge\nGender\nHaving a romantic partner\nSeeing a psychotherapist\nHours worked\nJefferson Scale Empathy (JSPE) total empathy score\nQuestionnaire of Cognitive and Affective Empathy (QCAE) Cognitive empathy score\nQuestionnaire of Cognitive and Affective Empathy QCAE Affective empathy score\nCenter for Epidemiologic Studies Depression Scale (CES-D) total score\nState & Trait Anxiety (STAI) score\nMaslach Burnout Inventory (MBI) Emotional Exhaustion\nMaslach Burnout Inventory (MBI) Cynicism\nMaslach Burnout Inventory (MBI) Academic Efficacy\n\nEach of the various empathy, mental health, and burnout scales are scored differently, so care needs to be taken in interpreting these findings.For example, a higher score on the emotional exhaustion and cynicism scales of the MBI indicate higher burn out, while a higher score on the MBI personal achievement indicates lower levels of burnout (Maslach et al., 1996).\n\n\nCode\n#Readin Final data set\n\nFinalDataSet &lt;- read_csv(\"_data/med_student_burnout.csv\")\n\n\nRows: 886 Columns: 20\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (20): id, age, year, sex, glang, part, job, stud_h, health, psyt, jspe, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nCode\nFinalDataSet\n\n\n# A tibble: 886 × 20\n      id   age  year   sex glang  part   job stud_h health  psyt  jspe qcae_cog\n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n 1     2    18     1     1   120     1     0     56      3     0    88       62\n 2     4    26     4     1     1     1     0     20      4     0   109       55\n 3     9    21     3     2     1     0     0     36      3     0   106       64\n 4    10    21     2     2     1     0     1     51      5     0   101       52\n 5    13    21     3     1     1     1     0     22      4     0   102       58\n 6    14    26     5     2     1     1     1     10      2     0   102       48\n 7    17    23     5     2     1     1     0     15      3     0   117       58\n 8    21    23     4     1     1     1     1      8      4     0   118       65\n 9    23    23     4     2     1     1     1     20      2     0   118       69\n10    24    22     2     2     1     1     0     20      5     0   108       56\n# … with 876 more rows, and 8 more variables: qcae_aff &lt;dbl&gt;, amsp &lt;dbl&gt;,\n#   erec_mean &lt;dbl&gt;, cesd &lt;dbl&gt;, stai_t &lt;dbl&gt;, mbi_ex &lt;dbl&gt;, mbi_cy &lt;dbl&gt;,\n#   mbi_ea &lt;dbl&gt;\n\n\nPrior to examining the descriptive statistics from the med school data set I recoded qualitative variables stored as numeric values, using the Carrard et al., 2022 code book, replacing 0, 1 with clear demographic information about age, gender, having a partner etc. The explanatory variable NatLang which collapses down into if medical students native language is German, French, or Italian (NatSpeaker) or not (NotNatSpeaker).\n\n\nCode\n#Recoding categorical variables based on code book\n\nFinalRecoded &lt;- FinalDataSet %&gt;%\n  mutate(NatLang = case_when(\n    glang == 1 | glang == 15 | glang == 90 ~ \"NatSpeaker\",\n    glang &gt; 19  & glang &lt; 90 | glang &gt; 90 ~ \"NonNatSpeaker\")\n  ) %&gt;% \n  relocate(`NatLang`, .before = `age`)%&gt;%\n  select(!contains(\"glang\")) %&gt;%\nmutate(gender = case_when(\n         sex == 1  ~ \"Male\",\n         sex == 2 ~ \"Female\", \n         sex == 3 ~ \"Non-Binary\")\n        ) %&gt;%\n  relocate(`gender`, .before = `age`)%&gt;%\n  select(!contains(\"sex\")) %&gt;%\nmutate(partner = case_when(\n         part == 0  ~ \"single\",\n         part == 1 ~ \"partnered\")\n        ) %&gt;%\n  relocate(`partner`, .before = `age`)%&gt;%\n  select(!\"part\") %&gt;%\nmutate(paid_job = case_when(\n         job == 0  ~ \"no_job\",\n         job == 1 ~ \"yes_job\")\n        ) %&gt;%\n  relocate(`paid_job`, .before = `age`)%&gt;%\n  select(!\"job\") %&gt;%\nmutate(health_sat = case_when(\n         health == 1  ~ \"very_dis\",\n         health == 2 ~ \"dis\",\n         health == 3 ~ \"neutral\",\n         health == 4 ~ \"sat\",\n         health  == 5 ~ \"dis_sat\")\n) %&gt;%\n  relocate(`health_sat`, .before = `age`)%&gt;%\n  select(!\"health\") %&gt;%\nmutate(MHcare = case_when(\n         psyt == 0  ~ \"no_ther\",\n         psyt == 1 ~ \"yes_ther\")\n        ) %&gt;%\nrelocate(`MHcare`, .before = `age`)%&gt;%\n  select(!\"psyt\") \n\nFinalRecoded\n\n\n# A tibble: 886 × 20\n      id NatLang  gender partner paid_…¹ healt…² MHcare   age  year stud_h  jspe\n   &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n 1     2 NonNatS… Male   partne… no_job  neutral no_th…    18     1     56    88\n 2     4 NatSpea… Male   partne… no_job  sat     no_th…    26     4     20   109\n 3     9 NatSpea… Female single  no_job  neutral no_th…    21     3     36   106\n 4    10 NatSpea… Female single  yes_job dis_sat no_th…    21     2     51   101\n 5    13 NatSpea… Male   partne… no_job  sat     no_th…    21     3     22   102\n 6    14 NatSpea… Female partne… yes_job dis     no_th…    26     5     10   102\n 7    17 NatSpea… Female partne… no_job  neutral no_th…    23     5     15   117\n 8    21 NatSpea… Male   partne… yes_job sat     no_th…    23     4      8   118\n 9    23 NatSpea… Female partne… yes_job dis     no_th…    23     4     20   118\n10    24 NatSpea… Female partne… no_job  dis_sat no_th…    22     2     20   108\n# … with 876 more rows, 9 more variables: qcae_cog &lt;dbl&gt;, qcae_aff &lt;dbl&gt;,\n#   amsp &lt;dbl&gt;, erec_mean &lt;dbl&gt;, cesd &lt;dbl&gt;, stai_t &lt;dbl&gt;, mbi_ex &lt;dbl&gt;,\n#   mbi_cy &lt;dbl&gt;, mbi_ea &lt;dbl&gt;, and abbreviated variable names ¹​paid_job,\n#   ²​health_sat\n\n\n\n\nCode\n#Descriptive statistics for quantitative variables\n\nsummary(FinalRecoded)\n\n\n       id           NatLang             gender            partner         \n Min.   :   2.0   Length:886         Length:886         Length:886        \n 1st Qu.: 447.5   Class :character   Class :character   Class :character  \n Median : 876.0   Mode  :character   Mode  :character   Mode  :character  \n Mean   : 889.7                                                           \n 3rd Qu.:1341.8                                                           \n Max.   :1790.0                                                           \n   paid_job          health_sat           MHcare               age       \n Length:886         Length:886         Length:886         Min.   :17.00  \n Class :character   Class :character   Class :character   1st Qu.:20.00  \n Mode  :character   Mode  :character   Mode  :character   Median :22.00  \n                                                          Mean   :22.38  \n                                                          3rd Qu.:24.00  \n                                                          Max.   :49.00  \n      year           stud_h           jspe          qcae_cog    \n Min.   :1.000   Min.   : 0.00   Min.   : 67.0   Min.   :37.00  \n 1st Qu.:1.000   1st Qu.:12.00   1st Qu.:101.0   1st Qu.:54.00  \n Median :3.000   Median :25.00   Median :107.0   Median :58.00  \n Mean   :3.103   Mean   :25.29   Mean   :106.4   Mean   :58.53  \n 3rd Qu.:5.000   3rd Qu.:36.00   3rd Qu.:113.0   3rd Qu.:63.00  \n Max.   :6.000   Max.   :70.00   Max.   :125.0   Max.   :76.00  \n    qcae_aff          amsp         erec_mean           cesd      \n Min.   :18.00   Min.   : 6.00   Min.   :0.3571   Min.   : 0.00  \n 1st Qu.:31.00   1st Qu.:20.00   1st Qu.:0.6667   1st Qu.: 9.00  \n Median :35.00   Median :23.00   Median :0.7262   Median :16.00  \n Mean   :34.78   Mean   :23.15   Mean   :0.7201   Mean   :18.05  \n 3rd Qu.:39.00   3rd Qu.:26.75   3rd Qu.:0.7857   3rd Qu.:25.00  \n Max.   :48.00   Max.   :35.00   Max.   :0.9524   Max.   :56.00  \n     stai_t         mbi_ex          mbi_cy          mbi_ea     \n Min.   :20.0   Min.   : 5.00   Min.   : 4.00   Min.   :10.00  \n 1st Qu.:34.0   1st Qu.:13.00   1st Qu.: 6.00   1st Qu.:21.00  \n Median :43.0   Median :17.00   Median : 9.00   Median :24.00  \n Mean   :42.9   Mean   :16.88   Mean   :10.08   Mean   :24.21  \n 3rd Qu.:51.0   3rd Qu.:20.00   3rd Qu.:13.00   3rd Qu.:28.00  \n Max.   :77.0   Max.   :30.00   Max.   :24.00   Max.   :36.00  \n\n\nNote that id is not a true numeric variable and therefor the descriptive statistics for it should be disregarded.\nMedical students in the sample studied for an average of 25 hours a week, with a maximum of 70 hours.\nScores on the Jefferson Scale of Physician Empathy (JSPE) range from 20-140 with a higher score indicating higher empathy. The mean JSPE score in the sample was was 106.2 and the median JSPE score was 107.0 indicating relatively high empathy. There was a broad range from as low to 67-125, with the IQR indicating most medical students scored in the low 100s range.\nThere were are scores for all 3 components of MBI burnout: emotional exhaustion, cynicism, and personal achievement.\n\nOn the mbi-ex, medical school students’ scores ranged from 5-30, with a median score of 17 and a mean score of 16.88. According to the MBI score guide, half of medical students in the sample exhibit low-level burnout (scoring 17 or less), and the other half exhibiting moderate burnout in terms of emotional exhaustion.\nOn the mbi-cyn, medical students’ scores ranged from 4-24, with a median of 9 and mean of 10.08. According to the scoring guide, the majority of the sample exhibit moderate burnout (6-11) with some exhibiting high level burnout (12+) in the dimension of cynicism.\nOn the mbi-ea, medical students scores ranged from 10-36, with a median score of 24 and a mean score of 24.01. A score of 33 or less indicates high level of burnout and a score between 24-39 indicates moderate level burnout, with medical students falling in the high and moderate burnout range for personal achievement.\n\n\n\nCode\n#Frequency of categorical & ordinal variables\n\n prop.table(table(select(FinalRecoded, NatLang)))\n\n\nNatLang\n   NatSpeaker NonNatSpeaker \n    0.8950339     0.1049661 \n\n\nCode\n prop.table(table(select(FinalRecoded, gender)))\n\n\ngender\n     Female        Male  Non-Binary \n0.683972912 0.310383747 0.005643341 \n\n\nCode\n prop.table(table(select(FinalRecoded, partner)))\n\n\npartner\npartnered    single \n0.5632054 0.4367946 \n\n\nCode\n prop.table(table(select(FinalRecoded, paid_job)))\n\n\npaid_job\n   no_job   yes_job \n0.6512415 0.3487585 \n\n\nCode\n prop.table(table(select(FinalRecoded, health_sat)))\n\n\nhealth_sat\n       dis    dis_sat    neutral        sat   very_dis \n0.09819413 0.25282167 0.15349887 0.45372460 0.04176072 \n\n\nCode\n prop.table(table(select(FinalRecoded, MHcare)))\n\n\nMHcare\n no_ther yes_ther \n0.775395 0.224605 \n\n\nFrom the proportion tables above it can be seen that majority (90%) of the sample speaks one of the national languages of Switzerland, while only 10% are non native speakers. The sample is also mostly female (68%), with less than 1% identifying as non-binary. Over half (56%) of the medical students reported having partners, but only about a third of medical students had a paid job (34.9%). The most common (45%) report from medical students was that they were satisfied with their health and less than one quarter (22.5%) of medical student reported seeing a therapist in the last 12 months.\nFinally, I created a several box plots comparing the empathy, mental health, and burnout scores of medical students whose native language is a national language of Switzerland vs. students whose native language is not.\n\n\nCode\n# Empathy Score\n\nggplot(data = FinalRecoded, aes(x= NatLang, y = jspe, fill = NatLang)) +\n  geom_boxplot() +\n  labs(title = \"Box Plot - JSPE measure of Empathy\", x = \"Native Language Spoken\", y = \"JSPE\") \n\n\n\n\n\nFrom the box plot of JSPE scores, native speakers of national languages have slightly higher median empathy than non-native students, though they are very close and there appear to be several low outliers for native speakers.\n\n\nCode\n#Depression Score\nggplot(data = FinalRecoded, aes(x= NatLang, y = cesd, fill = NatLang)) + geom_boxplot() +\n  labs(title = \"Box Plot - cesd measure of Depression\", x = \"Native Language Spoken\", y = \"cesd\") \n\n\n\n\n\nNon-native national language speakers have a higher median on CESD than native national language speakers, suggesting higher depression.\n\n\nCode\n#Anxiety Score\nggplot(data = FinalRecoded, aes(x= NatLang, y = stai_t, fill = NatLang)) + geom_boxplot() +\n  labs(title = \"Box Plot - stai_t measure of Anxiety\", x = \"Native Language Spoken\", y = \"stai_t\") \n\n\n\n\n\nNon-native national languages speakers also seem to score higher for anxiety as measured by the stai_t.\n\n\nCode\n#Emotional Exhaustion Burnout Score\nggplot(data = FinalRecoded, aes(x= NatLang, y = mbi_ex, fill = NatLang)) + geom_boxplot() +\n  labs(title = \"Box Plot - mbi_ex Emotional Burnout\", x = \"Native Language Spoken\", y = \"mbi_ex\") \n\n\n\n\n\nNative language speaking and non-native language speaking medical students seemed to have nearly identical median scores for Emotional Exhaustion (mbi_ex).\n\n\nCode\n#Cynicism Score\nggplot(data = FinalRecoded, aes(x= NatLang, y = mbi_cy, fill = NatLang)) + geom_boxplot() +\n  labs(title = \"Box Plot - mbi_cy Cynicism Burnout\", x = \"Native Language Spoken\", y = \"mbi_cy\") \n\n\n\n\n\nNon-native speaking medical students appeared to score slightly higher on average than non-native speaking medical students on Cynicism as measured by the mbi-cy, with a higher median score.\n\n\nCode\n#Academic Efficacy Score\nggplot(data = FinalRecoded, aes(x= NatLang, y = mbi_ea, fill = NatLang)) + geom_boxplot() +\n  labs(title = \"Box Plot - mbi-ea Academic Efficacy Burnout\", x = \"Native Language Spoken\", y = \"mbi_ea\") \n\n\n\n\n\nLastly, Native language speaking and non-native language speaking medical students seemed to have nearly identical median scores for Personal Achievement burnout (mbi_ea).\nI look forward to analyzing the data set and testing my hypothesis in the coming weeks, as well as getting feedback on this proposal."
  },
  {
    "objectID": "posts/EmmaNarkewicz_FinalProject_CheckIn1.html#works-cited",
    "href": "posts/EmmaNarkewicz_FinalProject_CheckIn1.html#works-cited",
    "title": "Final Project Proposal",
    "section": "Works Cited",
    "text": "Works Cited\nCarrard, V., Bourquin, C., Berney, S, Schlegel, K., Gaume, J., Bart, P-A., Preisig M., Mast, M. A., & Berney, A. (2022) The relationship between medical students’ empathy, mental health, and burnout: A cross-sectional study, Medical Teacher, 44:12, 1392-1399, DOI: 10.1080/0142159X.2022.2098708\nGradiski, I. P., Borovecki, A., Ćurković, M., San-Martín, M., Delgado Bolton, R. C., & Vivanco, L. (2022). Burnout in International Medical Students: Characterization of Professionalism and Loneliness as Predictive Factors of Burnout. International journal of environmental research and public health, 19(3), 1385. https://doi.org/10.3390/ijerph19031385\nGoebert., D., Thompson., D., Takeshita., J., Beach, C., Bryson, P., Ephgrave, K., Kent. A., Kunkel., M., Schechter., J., Tate., J. (2009). Depressive Symptoms in Medical Students and Residents: A Multischool Study. Academic Medicine 84(2):p 236-241, DOI: 10.1097/ACM.0b013e31819391bb\nJennings, M.L. Medical Student Burnout: Interdisciplinary Exploration and Analysis. J Med Humanit 30, 253–269 (2009). https://doi.org/10.1007/s10912-009-9093-5\nKużelewska,E. (2016).Language Policy in Switzerland. Studies in Logic, Grammar and Rhetoric,45(1) 125-140. https://doi.org/10.1515/slgr-2016-0020\nMaslach, C., Jackson, S.E., & Jackson, Leiter, M. P. (Eds.) (1996). Maslach Burnout Inventory manual (3rd ed.).\nPaturel, A. (2020). Healing the very youngest healers. American Association of Medical Colleges (AAMC). https://www.aamc.org/news-insights/healing-very-youngest-healers#:~:text=In%20a%20recent%20study%20%2C%209.4,as%20their%20same%2Dage%20peers.\nPeters E. (2018). Compassion fatigue in nursing: A concept analysis. Nursing forum, 53(4), 466–480. https://doi.org/10.1111/nuf.12274\nRadloff, L.S. (1977). The CES-D Scale: a self-report depression scale for research in the general population. Applied Psychological Measurement, 1:385-401.\nRodriguez, S. Y. S., Carlotta, M. S.. (2017). Predictors of Burnout Syndrome in psychologists. Estudos De Psicologia (campinas), 34(Estud. psicol. (Campinas), 2017 34(1)), 141–150. https://doi.org/10.1590/1982-02752017000100014\nShin, H. S., Park, H., & Lee, Y. M. (2022). The relationship between medical students’ empathy and burnout levels by gender and study years. Patient education and counseling, 105(2), 432–439. https://doi.org/10.1016/j.pec.2021.05.036\nTiesman, H., Weissman, D., Stone., D., Quinlan, K., & Chosewood, L. (2021). Suicide Prevention for Healthcare Workers. CDC. https://blogs.cdc.gov/niosh-science-blog/2021/09/17/suicide-prevention-hcw/\nWilliams, B., Beovich, B. Psychometric properties of the Jefferson Scale of Empathy: a COSMIN systematic review protocol. Syst Rev 8, 319 (2019). https://doi.org/10.1186/s13643-019-1240-0\nYahya, M. S., Abutiheen, A. A., & Al- Haidary, A. F. (2021). Burnout among medical students of the University of Kerbala and its correlates. Middle East Current Psychiatry, Ain Shams University, 28(1), 78. https://doi.org/10.1186/s43045-021-00152-2"
  },
  {
    "objectID": "posts/Final_Project_check_in_2_Diana_Rinker.html#final-project-check-in-2-diana-rinker.",
    "href": "posts/Final_Project_check_in_2_Diana_Rinker.html#final-project-check-in-2-diana-rinker.",
    "title": "Final Project check-in 2",
    "section": "Final Project check-in 2, Diana Rinker.",
    "text": "Final Project check-in 2, Diana Rinker."
  },
  {
    "objectID": "posts/Final_Project_check_in_2_Diana_Rinker.html#popularity",
    "href": "posts/Final_Project_check_in_2_Diana_Rinker.html#popularity",
    "title": "Final Project check-in 2",
    "section": "1.1.Popularity",
    "text": "1.1.Popularity\n\nPage views\nThis is the most general metric, representing how many views the post received. Views do not distinguish repeated views by the same person.\n\n\nCode\n# str(merged)\nggplot(data=merged, mapping=aes(x=`Page views`))+\n  geom_histogram()\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\nCode\nmerged$post.month &lt;-as.numeric(merged$post.month)\nmerged$year_month &lt;- paste0(merged$post.year, \"-\", sprintf(\"%02d\", merged$post.month))\n\nggplot( data=merged, mapping=aes(y=`Page views`, x=year_month))+\n          geom_boxplot()+\n  labs(title=\"Number of post wiews per month\", x=\"Month\", y=\"Number of vews\")+ \n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\nCode\n# ggplot(data=merged, mapping=aes(x=log(`Page views`)))+\n#   geom_histogram()\n\n\nFrom this graph we can see that the numbers of views is increased over time. To get a better understanding of it, lets review other metrics.\n\n\nPage visits and unique users\nVisit is an instance of a user engaging with bdc website. The user can visit a few pages of the site and re-visit them, creaing few views in once visit.Once th user left the website, the visit is over.\nThere also can be many visits by the same viewer. To account for repeated users there is “Uniques” variable. It tells us how many unique users saw the post. The relationship will always be that\nUniques &lt; Visits &lt; Views\nLets plot them together:\n\n\nCode\n# str(merged)\nggplot(merged, aes(x =post.date )) +\n     geom_bar(aes(y = `Page views`, fill = \"Page views\"), stat = \"identity\", position = \"dodge\", width = 0.6) +\n     geom_bar(aes(y = Visits, fill = \"Visits\"), stat = \"identity\", position = \"dodge\", width = 0.6) +\n     geom_bar(aes(y = Uniques, fill = \"Unique users\"), stat = \"identity\", position = \"dodge\", width = 0.6)    +\n     labs(title = \"Views, visits and unique users per month\", x = \"Post.date\", y = \"Value\") + \n     theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\nThis graph also shows increase in all three metrics in 2022-2023. To see how these metrics crrespond to each other, I will calculate “visits ratio” and “unique.ratio”.\n\n\nPage unique viewers.\nBecuse “Uniques” variable represents number of unique people who came to the page and viewed it at least once, his metric represents popularity of the post. It’s distribution shows us that not all posts are equally popular:\n\n\nCode\n# colnames(merged)\nggplot(data=merged, mapping=aes(x=Uniques))+\n  geom_histogram()\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nWe can see a long tail on the right, showing that there is a number of posts who are way more popular. If we look at the distribution of this variable over time, we will see significant increase of average popularity after December 2021.\n\n\nCode\nggplot( data=merged, mapping=aes(y=Uniques, x=merged$year_month))+\n          geom_boxplot()+\n  labs(title=\"Number of unique viewers per month\", x=\"Month\", y=\"Number of unique viewers\")+ \n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\nWarning: Use of `merged$year_month` is discouraged.\nℹ Use `year_month` instead."
  },
  {
    "objectID": "posts/Final_Project_check_in_2_Diana_Rinker.html#engagement-metrics",
    "href": "posts/Final_Project_check_in_2_Diana_Rinker.html#engagement-metrics",
    "title": "Final Project check-in 2",
    "section": "1.2. Engagement metrics:",
    "text": "1.2. Engagement metrics:\n\nVisit ratio and unique ratio:\n“visits ratio” is the number of visits per total views. This metric tells us how often the page being re-viewed. The lower the ratio means that the page was viewed more times during single visit. Since repeated view can be considered a higher engagement, the lower visit ratio indicates higher engagement.\n“unique ratio” - number of unique users per total views. It is telling us how often the same person re-visited the post. Lower ratio indicates repeated users, therefore higher engagement.\nBoth ratios are always in the range of 0 - 1.\n\n\nCode\n  merged &lt;- merged %&gt;%\n     mutate(uniques.ratio = Uniques / `Page views`)%&gt;%\n      mutate(visits.ratio = Visits / `Page views`)\n# str(merged)\n\n     ggplot(merged, aes( y = uniques.ratio, x =post.date, )) +\n          geom_point(color =\"green\" )    +\n          labs(title = \"Unique.ratio per post \", x = \"Post.date\", y = \"Value\") + \n          theme(axis.text.x = element_text(angle = 45, hjust = 1))  \n\n\n\n\n\nCode\n      ggplot(merged, aes( y = visits.ratio, x =post.date, )) +\n          geom_point(color =\"blue\" )    +\n          labs(title = \"Visits.ratio per post \", x = \"Post.date\", y = \"Value\") + \n          theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\nFrom the two graphs above we can see that the two variables are distributed very similarly.\nIf we plot them against each other and visualize their correlation, we can see that they highly correlated especially in higher values.\n\n\nCode\n      plot(merged$uniques.ratio, merged$visits.ratio)\n\n\n\n\n\nCode\ncorrelation &lt;- cor(merged$uniques.ratio, merged$visits.ratio)\ncorrelation\n\n\n[1] 0.915789\n\n\nDensity of the distribution also showing that most posts are visited once and by unique users, i.e. post is read once by each user.\n\n\nCode\n# DENSITY DISTRIBUTION HERE \n\n\n\n\nExit rate\nThis variable is measuring how many people visited the page and then left the website after the first view. This metric is the best measure of engagement for all users, as it represents the first step after being exposed to the post - either quitting the site or remaining on the site.\nHere can see the distribution of this variable :\n\n\nCode\n# str(merged)\nmerged$`Exit rate` &lt;- as.numeric(sub(\"%\", \"\", merged$`Exit rate`)) / 100\n\nggplot(data=merged, mapping=aes(x=`Exit rate`))+\n  geom_histogram()\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nWe can also see consistent increase of this value ovr time, similar to the trend seen before:\n\n\nCode\nggplot(merged, mapping = aes(x=year_month , y=`Exit rate`, fill=year_month ))+\n  geom_boxplot() +\n  labs(title = \"distribution of `Exit rate` per post \", y = \"Exit rate\" , x=\"Month\")+ \n     theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\nNumber of reactions\nThis represents total amount of likes and dislikes of comments per post.To like/dislike the comment, user doesn’t have to log in.\n\n\nCode\n# merged$year_month \n# colnames(merged)\n\nggplot(data=merged, mapping=aes(x=post.total.likes))+\n  geom_histogram()+\n  labs(title=\" Number of all reactions per post\")\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nSince we already saw that there is large variability in popularity of posts, which can natually impact absolute amount of reactions. To account for number of views, we calculate reactions.ratio:\n\n\nCode\n  merged &lt;- merged %&gt;%\n     mutate(reactions.ratio = post.total.likes / `Page views`)\n  \n# str(merged)\nggplot(merged, mapping = aes(x=year_month , y=reactions.ratio, fill=year_month ))+\n  geom_boxplot() +\n  labs(title = \"distribution of reactions \", y = \"Percentage of reactions \" )+\n  scale_y_continuous(breaks = seq (from=0, to= 10000, by= 100)) + \n     theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\nThe absolute number of reactions and the ratio appear to decrease over time.\n\n\nNumber of comments: engagement metric for subset of readers. Absolute and per views.\nTo comment on the requires logging in from a user, which is an indicator of greater engagement of an individual user. Therefore this variable represents engagement of a subset of users - more loyal readers who have created an account.\n\n\nCode\n# merged$year_month \nggplot(data=merged, mapping=aes(x=n.comments))+\n  geom_histogram()\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nAnd change in the distribution over time:\n\n\nCode\nggplot(merged, mapping = aes(x=year_month , y=n.comments, fill=year_month ))+\n  geom_boxplot() +\n  labs(title = \"distribution of comments per post \", y = \"Number of comments\" )+\n  scale_y_continuous(breaks = seq (from=0, to= 10000, by= 100)) + \n     theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\nThe trend in amount of comments over time is different. While previously reviewed variables seemed to increase over time, this variable decreases.\nWe can calculate the ratio of comments per If we compare absolute values fo comments and ratio of comments per page views, we see the decrease in both cases, while the ratio’s decrease is more dramatic:\n\n\nCode\n  merged &lt;- merged %&gt;%\n     mutate(comments.ratio = n.comments/ `Page views`)\n\nggplot(merged, mapping = aes(x=year_month , y=comments.ratio, fill=year_month ))+\n  geom_boxplot() +\n  labs(title = \"distribution of comments ratio (comments/page views) \", y = \"Number of comments\" )+\n  scale_y_continuous(breaks = seq (from=0, to= 10000, by= 100)) + \n     theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\nWE can see that with an increase of views (mostly driven by post popularity), comments ratio decreeases. So, newly added viewers are not adding to the engaged subset of readers. \nThe distribution of comments per post over time:\n\n\nCode\n# str(merged)\nggplot(merged, mapping = aes(x=year_month , y=post.total.likes, fill=year_month ))+\n  geom_boxplot() +\n  labs(title = \"distribution of reactions over time  \", y = \"Percentage of reactions \" )+\n  scale_y_continuous(breaks = seq (from=0, to= 10000, by= 100)) + \n     theme(axis.text.x = element_text(angle = 45, hjust = 1))"
  },
  {
    "objectID": "posts/Final_Project_check_in_2_Diana_Rinker.html#impact-of-popularity-on-engagement.",
    "href": "posts/Final_Project_check_in_2_Diana_Rinker.html#impact-of-popularity-on-engagement.",
    "title": "Final Project check-in 2",
    "section": "1.3 Impact of popularity on engagement.",
    "text": "1.3 Impact of popularity on engagement.\nSince we noticed a pattern in distribution of describbed variables, lets find out how they are interconnected, and how popularity of the post impact users’ engagement:\n\n\nCode\npairs(subset (merged, select=c(Uniques, `Exit rate`, uniques.ratio, visits.ratio, reactions.ratio )))\n\n\n\n\n\nThis graph showing that popularity is strongly correlated with all engagement metrics, such as Exit rate, uniques ratio, visit ratio and reaction’s ratio, where an increase in popularity causes decrease of engagement across all 4 metrics for engagement.\nWe can also see that the relationship between Popularity and engagement rate are curvi-linear. If we log the values, we will see linear relationship:\n\n\nCode\npairs(subset (merged, select=c(log(Uniques), log(`Exit rate`), uniques.ratio, visits.ratio, reactions.ratio )))\n\n\nError in `x[r, vars, drop = drop]`:\n! Can't subset columns with `vars`.\n✖ Can't convert from `j` &lt;double&gt; to &lt;integer&gt; due to loss of precision.\n\n\nCode\n# WHT DOES THE GRAPH SHOWS POST.DATE and POST.DATE1  INSTEAD OF \"UNIQUES\" and  \"Exit rate\"? \n\n\nAs we saw in the distributions of variables above, all engagement metrics are strongly correlate with each other the popularity metric.\n\n1.3.1. Modeling engagement ~ popularity:\nLets review how it is impacting exit rate : I will use “Exit rate” s my main engagement variable, as it reporesents the first engagement chouce that all site users make: to stay on the site or leave. Using log() of popularity is shoing a better fitted model:\n\n\nCode\nggplot(merged, mapping=aes(x=log(Uniques), y=`Exit rate` ))+\n  geom_point()\n\n\n\n\n\nCode\nsummary(lm(`Exit rate`~ + log(Uniques) , data = merged))\n\n\n\nCall:\nlm(formula = `Exit rate` ~ +log(Uniques), data = merged)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.259486 -0.016818  0.001917  0.019850  0.073240 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -0.332581   0.027678  -12.02   &lt;2e-16 ***\nlog(Uniques)  0.113870   0.002855   39.89   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.03073 on 533 degrees of freedom\nMultiple R-squared:  0.749, Adjusted R-squared:  0.7486 \nF-statistic:  1591 on 1 and 533 DF,  p-value: &lt; 2.2e-16\n\n\nWe can see significance of popularity and high R^2 of the model, and can conclude that popularity overall decreases engagement. To explore this connection further, lets mreview Sources of users that are coming to the website.\n\n\n1.3.2. How popularity impacts engagement: Referral sources.\nThe website analytics provides information on where the viewers are coming from to the blog page. for example, if people clicked on the blog link posted on FaceBook, that would be referral from social media. If people clicked on the blog link within BDC website, that would be “BDC referral visit”.\nThere are 5 sources of referrals, each corresponding with a variable in the data set. Variable’s value is a number of visits from this referral source.\n    \"Search + amp referral visits\"\n    \"Direct (non-email) referral visits\"\n    \"Other website referral visits\"\n    \"Social referral visits\"\n    \"BDC referral visits\"\n    \"Visits when post was on LL HP\" \n\n\nCode\n#renaming variables for convenience: \nmerged&lt;-merged%&gt;%\n  rename(google =\"Search + amp referral visits\",\n         direct =\"Direct (non-email) referral visits\",\n         other.web = \"Other website referral visits\",\n          social= \"Social referral visits\",\n          bdc= \"BDC referral visits\",\n          ll= \"Visits when post was on LL HP\" )\n\nggplot(merged, mapping=aes(x=post.date))+\n  geom_point(aes(y=google), color=\"red\")+\n  geom_point(aes(y=direct), color=\"green\")+\n  geom_point(aes(y=other.web), color=\"yellow\")+\n  geom_point(aes(y=social), color=\"purple\")+\n  geom_point(aes(y=bdc), color=\"blue\")+\n  geom_point(aes(y=ll), color=\"pink\")  +\n  labs(title = \"referral sources per post \", y = \"Number of referrals\" , x=\"Post\")+ \n     theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\nThis graph clearly demonstrates increase of “Search + amp referral visits” after December 2021, while other referral sources maintain the same level over time. This increase matches changes observed in popularity ( Page Views), and engagement metrics that are highly correlated with popularity. It also doubles views of the posts in many cases and might significanly influence our model and conclusions.\n\n\nEarly comments’impact\nWe saw that increased popularity is associated with the sources of the viewers and might come from the search engines. We want to find out if early engagement in the post ( comments within first 10 hours of the post time) trigger its appearance in google news listing and therefore cause sharp increase in popularity.\nWe are interested to find out, what characteristics of the post (prior to it being picked up by google), correlate with its appearance on google news.\nNumber of comments within 1st 10 hours from the post.\n\n\nCode\n#  calculating number of comments within first 10 hours \n\n# date and time of the post \npost.date  &lt;-comments.data %&gt;%\n  group_by(post_id)%&gt;%\n  arrange(written_at)%&gt;%\n  summarize(post.dt = first(written_at), \n            n.coms =n())%&gt;%\n  filter (n.coms&gt;100)%&gt;%\n  select(post_id, post.dt, n.coms)%&gt;%\narrange(desc(post_id))\n# head(post.date)\n\n# Calculating age of the post \ncomments.data &lt;- merge( comments.data , post.date , by = \"post_id\", all = TRUE)\ncomments.data &lt;-comments.data %&gt;%\n  mutate (age = difftime(  written_at,post.dt, units = \"hours\"), \n          early = ifelse(age&lt;10, 1, 0))\n\nearly.coms  &lt;-comments.data%&gt;%\n  group_by(post_id)%&gt;%\n  arrange(written_at)%&gt;%\n  summarize(early.sum = sum(early), \n            n.coms =n())%&gt;%\n  filter (n.coms&gt;100)%&gt;%\n  select(post_id, early.sum)%&gt;%\narrange(desc(post_id))\n\n# Adding number of early comments to merged dataset:\nmerged &lt;- merge( merged, early.coms , by = \"post_id\", all = TRUE)\n\nhead(merged )\n\n\n   post_id  post.date\n1 27067757 2023-03-01\n2 27067763 2023-02-28\n3 27067769 2023-02-27\n4 27067775 2023-02-24\n5 27067781 2023-02-23\n6 27067787 2023-02-22\n                                                                             Letter\n1                          love letters | blog | We&'ve gained weight – as a couple\n2           love letters | blog | She was a friendly ghost, but I was still ghosted\n3 love letters | blog | I waited for her to initiate sex – and it ended my marriage\n4                         love letters | blog | My husband doesn&'t share his money\n5                              love letters | blog | My wife won&'t go back to work\n6                                love letters | blog | He doesn&'t have time for me\n  Page views google direct Visits Uniques other.web social   bdc   ll Exits\n1      24890  10407  10180  22766   20765       349     56  7088 2785 18547\n2      15575   1210  10297  13597   11677       214     85  7461 2832 10127\n3      28594   7027  15609  26115   23914       909     87 12957 3016 22464\n4      72718  43419  19921  67007   62122       311    134 14106 4216 60915\n5      67537  43173  16863  62888   59272       310    152 12344 2739 56929\n6      28778  14466   9698  26269   23865       331     67  6332 3003 21515\n  Exit rate dup n.comments post.year post.month post.likes post.dislikes\n1      0.81   0        208      2023          3       1001           167\n2      0.74   0        264      2023          2       1908           118\n3      0.86   0        241      2023          2       1195           167\n4      0.91   0        321      2023          2       1921           341\n5      0.91   0        294      2023          2       1445           549\n6      0.82   0        182      2023          2       1337            85\n  post.total.likes blocked.sum pct.positive year_month uniques.ratio\n1             1168           1     85.70205    2023-03     0.8342708\n2             2026           1     94.17572    2023-02     0.7497271\n3             1362           1     87.73862    2023-02     0.8363293\n4             2262           3     84.92485    2023-02     0.8542864\n5             1994           0     72.46740    2023-02     0.8776226\n6             1422           0     94.02250    2023-02     0.8292793\n  visits.ratio reactions.ratio comments.ratio early.sum\n1    0.9146645      0.04692648    0.008356770       192\n2    0.8730016      0.13008026    0.016950241       247\n3    0.9133035      0.04763237    0.008428342       214\n4    0.9214637      0.03110647    0.004414313       261\n5    0.9311637      0.02952456    0.004353169       240\n6    0.9128153      0.04941275    0.006324275       142\n\n\nNow, as I calculated number of early comments for all posts, I can see if that number correlated with Google referrals.\n\n\nCode\nmerged.2&lt;-merged%&gt;%\n  filter(google&gt;10000)\ncorrelation.2 &lt;- cor(merged.2$early.sum, merged.2$google)\ncorrelation.2\n\n\n[1] 0.2736053\n\n\nCode\nplot(merged.2$early.sum, merged.2$google)\n\n\n\n\n\n\n\nCode\nsummary(lm(google ~early.sum, data=merged.2))\n\n\n\nCall:\nlm(formula = google ~ early.sum, data = merged.2)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-15035  -6141  -2482   3559  54900 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)  5908.84    5265.21   1.122  0.26406   \nearly.sum      94.56      30.73   3.077  0.00261 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 10600 on 117 degrees of freedom\nMultiple R-squared:  0.07486,   Adjusted R-squared:  0.06695 \nF-statistic: 9.467 on 1 and 117 DF,  p-value: 0.002605\n\n\nCode\nsummary(lm(google ~early.sum, data=merged))\n\n\n\nCall:\nlm(formula = google ~ early.sum, data = merged)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n -7036  -5548  -4652   2036  68716 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 8921.388   1789.530   4.985 8.38e-07 ***\nearly.sum    -12.627      9.921  -1.273    0.204    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 9806 on 533 degrees of freedom\nMultiple R-squared:  0.00303,   Adjusted R-squared:  0.001159 \nF-statistic:  1.62 on 1 and 533 DF,  p-value: 0.2037\n\n\nWe can see significant connection between early comments and google referrals, only on the data set of google referrals above 10000 visits. Also, R^2 is low in this model, which suggests that there are other factors that are not considered in this model. I will explore how days of the week and time of the post contribute to this relationship.\n\n\nCode\n# calculating day of the week for the post: \ncolnames(comments.data)\n\n\n [1] \"post_id\"           \"content\"           \"message_id\"       \n [4] \"user_id\"           \"user_name\"         \"display_name\"     \n [7] \"image_url\"         \"email\"             \"email_verified\"   \n[10] \"created_at\"        \"private_profile\"   \"approved\"         \n[13] \"written_at\"        \"parent\"            \"absolute_likes\"   \n[16] \"absolute_dislikes\" \"comment.year\"      \"com.year\"         \n[19] \"post.dt\"           \"n.coms\"            \"age\"              \n[22] \"early\"            \n\n\nCode\n# Calculating age of the post \nlibrary(lubridate)\n\n\n\nAttaching package: 'lubridate'\n\n\nThe following objects are masked from 'package:base':\n\n    date, intersect, setdiff, union\n\n\nCode\ncomments.data &lt;-comments.data %&gt;%\n  mutate (weekday = wday(post.dt, label = TRUE), \n          weekend = ifelse(weekday ==\"Fri\" | weekday == \"Sat\", 1, 0))\n\n# class(week.days$weekday)\n# levels(week.days$weekday) \n# week.days$weekend\n\nweekdays &lt;-comments.data %&gt;%\n  group_by(post_id)%&gt;%\n  arrange(written_at)%&gt;%\n  summarize(post.weekday = first(weekday), \n            post.weekend =first(weekend),\n            n.coms =n())%&gt;%\n  filter (n.coms&gt;100)%&gt;%\n  select(post_id, post.weekday,post.weekend )%&gt;%\narrange(desc(post_id))\n\n# Adding number of early comments to merged dataset:\nmerged &lt;- merge( merged, weekdays , by = \"post_id\", all = TRUE)\ncolnames(merged)\n\n\n [1] \"post_id\"          \"post.date\"        \"Letter\"           \"Page views\"      \n [5] \"google\"           \"direct\"           \"Visits\"           \"Uniques\"         \n [9] \"other.web\"        \"social\"           \"bdc\"              \"ll\"              \n[13] \"Exits\"            \"Exit rate\"        \"dup\"              \"n.comments\"      \n[17] \"post.year\"        \"post.month\"       \"post.likes\"       \"post.dislikes\"   \n[21] \"post.total.likes\" \"blocked.sum\"      \"pct.positive\"     \"year_month\"      \n[25] \"uniques.ratio\"    \"visits.ratio\"     \"reactions.ratio\"  \"comments.ratio\"  \n[29] \"early.sum\"        \"post.weekday\"     \"post.weekend\"    \n\n\nCode\n# levels(merged$post.weekday)\n\n\n\n\nPost weekdays to the model\n\n\nCode\nmerged.2&lt;-merged %&gt;%\n  filter(google&gt;10000)\n\n plot(merged$post.weekday, merged$google)\n\n\n\n\n\nCode\nsummary(lm(log(google) ~early.sum +post.weekday , data=merged.2))\n\n\n\nCall:\nlm(formula = log(google) ~ early.sum + post.weekday, data = merged.2)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.85556 -0.25080 -0.05083  0.23281  1.25348 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)     9.30191    0.20350  45.710  &lt; 2e-16 ***\nearly.sum       0.00352    0.00119   2.958  0.00377 ** \npost.weekday.L  0.01571    0.08732   0.180  0.85751    \npost.weekday.Q -0.02154    0.08544  -0.252  0.80145    \npost.weekday.C -0.02274    0.08280  -0.275  0.78409    \npost.weekday^4  0.15962    0.08043   1.985  0.04962 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4058 on 113 degrees of freedom\nMultiple R-squared:  0.102, Adjusted R-squared:  0.06229 \nF-statistic: 2.568 on 5 and 113 DF,  p-value: 0.03066\n\n\nCode\n# table(merged.weekdays$post.weekday)\n\nggplot (merged, mapping =aes(x=early.sum, y=google, color =post.weekday))+\n  geom_point( ) +\n  geom_smooth(method=\"lm\")+\n  labs(title = \"All google referrals and early comments\", y = \"Google referrals\" , x=\"Early comments\")+ \n     theme(axis.text.x = element_text(angle = 45, hjust = 1))+\n  facet_wrap(~post.weekday)\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nCode\nggplot (merged.2, mapping =aes(x=early.sum, y=google, color =post.weekday))+\n  geom_point( ) +\n  geom_smooth(method=\"lm\")+\n  labs(title = \"Google referrals over 10K and early comments\", y = \"Google referrals\" , x=\"Early comments\")+ \n     theme(axis.text.x = element_text(angle = 45, hjust = 1))+\n  facet_wrap(~post.weekday)\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nEarly comments indeed influence Google referrals. However, this influence is different on different days of the week and appears more pronounced on Thursdays and Fridays. This overall supports our suggestion that early engagement of loyal users through comments impacts post popularity. further investigation of factors would be helpful, with the goal to model fit (R^2).\nOur hypothesis that large amount of early comments triggers google referrals has some preliminary support with the few restrictiions to be considere: - the connection only appears on the posts with google referrals above 10000. - This connection is atronger for only thursdays and Fridays and appears very weak on other days of the week. - low coefficient and high deviation of data from predicated valaues (low R^2) suggest that there are other , more influential, factors that were not considered."
  },
  {
    "objectID": "posts/Final_Project_check_in_2_Diana_Rinker.html#conclusion",
    "href": "posts/Final_Project_check_in_2_Diana_Rinker.html#conclusion",
    "title": "Final Project check-in 2",
    "section": "1.4.Conclusion",
    "text": "1.4.Conclusion\nAs we reviewed a variety of engagement metric, I found: 1. That they are all strongly correlated to each other and influenced by post popularity: Engagement overall decreases as popularity increases.In our further analysis I will need to include “Uniques” variable in the model to control for popularity\n\nAlso as a result of exploration of engagement metrics, we can distinguish two types of users: superficial visitors and loyal readers. The main difference is that loyal readers have created accounts and therefore can comment. It is possible that htese two groups of readers respond differently to popularity increase, as well as other Independent variables. Therefore, the difference in types of readers must be considered when addressing our main hypothesi: whether authors engagement and other factors impacts reader’s engagement.\nEarly engagement of loyal users through comments might contribute to dramatic changes in post popularity.\n\nNow, lets consider independent variables of our main hypothesis. # 2. Independent variables (IV):\n\n\nCode\nlibrary(stargazer)\n\n\n\nPlease cite as: \n\n\n Hlavac, Marek (2022). stargazer: Well-Formatted Regression and Summary Statistics Tables.\n\n\n R package version 5.2.3. https://CRAN.R-project.org/package=stargazer \n\n\nCode\nstargazer()\n\n\nError in if (substr(inside[i], 1, nchar(\"list(\")) == \"list(\") {: missing value where TRUE/FALSE needed\n\n\n\nAuthors comments\nTo identify, how much the author of the blog is engaged in the post, I will create an additional variable derived from a user_name field.\n\n\nCode\n# str(merged)\ncomments.data$user_name&lt;-  ifelse (is.na(comments.data$user_name), 0, comments.data$user_name)\ncomments.data$author&lt;-  ifelse (comments.data$user_name==\"MeredithGoldstein\", 1, 0)\n\ncomments.grouped &lt;-comments.data %&gt;%\n  group_by(post_id)%&gt;%\n  summarize(n.comments=n(),\n            author.sum = sum(author))\n\n# dim(comments.grouped )\n# colnames(comments.grouped )\n# class(comments.grouped$author.sum)\n\n# Comments data contains rows that dont actually reporesent posts, and were crearted by web support team for troubleshooting. I need to remove these rows. They typically have very low number of comments\ncomments.grouped &lt;-comments.grouped %&gt;%\nfilter(n.comments &gt;100)  # removing invalid posts created by the  website management team.\n\ncomments.grouped &lt;-comments.grouped %&gt;%\n  select(post_id, author.sum)\n dim(comments.grouped)\n\n\n[1] 535   2\n\n\nCode\n#adding author.sum to main data set: \nmerged &lt;- merge( merged , comments.grouped, by = \"post_id\", all = TRUE)\n\n\nggplot(data=merged, mapping = aes(x=year_month , y=author.sum, fill=year_month ))+\n  geom_boxplot() +\n  labs(title = \"distribution of Author's comments by months\", y = \"Author's comments\" , x=\"Month\")+ \n     theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\nThis graph shows, that majority of posts have no author’s comments. However, if we look at the box plots’ upper ranges, we can suggest a trend of decrease in author’s comments with time.\n\n\nMood of the post.\nThis is a numerical variable, calculated as percentage of “thumbs up” from all likes (both “thumbs up” and “thumbs down”).\n\n\nCode\nggplot(merged, mapping = aes(x=year_month , y=pct.positive, fill=year_month ))+\n  geom_boxplot() +\n  labs(title = \"distribution of Mood per post \", y = \"Mood\" , x=\"Month\")+ \n     theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\nBlocked comments per post.\nNow I will visualize amount of blocked comments per post:\n\n\nCode\nggplot(merged, mapping = aes(x=year_month , y=blocked.sum, fill=year_month ))+\n  geom_boxplot() +\n  labs(title = \"Number Blocked comments  per post \", y = \"Number of blocked comments per post\" , x=\"Month\")+ \n     theme(axis.text.x = element_text(angle = 45, hjust = 1))"
  },
  {
    "objectID": "posts/Final_Project_check_in_2_Diana_Rinker.html#basic-model.",
    "href": "posts/Final_Project_check_in_2_Diana_Rinker.html#basic-model.",
    "title": "Final Project check-in 2",
    "section": "3.1 Basic model.",
    "text": "3.1 Basic model.\nI will start with creating a simple model of engagement ~ popularity and author’s engagement:\n\n\nCode\n# colnames(merged)\nsummary(lm(`Exit rate` ~ log(Uniques)+ author.sum, data = merged))\n\n\n\nCall:\nlm(formula = `Exit rate` ~ log(Uniques) + author.sum, data = merged)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.257486 -0.016558  0.001669  0.019958  0.072030 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -0.326263   0.027749 -11.758   &lt;2e-16 ***\nlog(Uniques)  0.113326   0.002857  39.663   &lt;2e-16 ***\nauthor.sum   -0.003389   0.001602  -2.116   0.0348 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.03063 on 532 degrees of freedom\nMultiple R-squared:  0.7511,    Adjusted R-squared:  0.7502 \nF-statistic: 802.9 on 2 and 532 DF,  p-value: &lt; 2.2e-16\n\n\nCode\nsummary(lm(`Exit rate` ~ log(Uniques)+ author.sum, data = merged.2021))\n\n\n\nCall:\nlm(formula = `Exit rate` ~ log(Uniques) + author.sum, data = merged.2021)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.257995 -0.018855  0.000548  0.027265  0.069625 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -0.402793   0.096359  -4.180 4.18e-05 ***\nlog(Uniques)  0.121731   0.010266  11.857  &lt; 2e-16 ***\nauthor.sum   -0.002533   0.002568  -0.986    0.325    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.0372 on 224 degrees of freedom\nMultiple R-squared:  0.3856,    Adjusted R-squared:  0.3801 \nF-statistic:  70.3 on 2 and 224 DF,  p-value: &lt; 2.2e-16\n\n\nCode\nsummary(lm(`Exit rate` ~ log(Uniques)+ author.sum, data = merged.2022))\n\n\n\nCall:\nlm(formula = `Exit rate` ~ log(Uniques) + author.sum, data = merged.2022)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.083434 -0.014968  0.002043  0.017982  0.053266 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -0.354200   0.029528 -11.995  &lt; 2e-16 ***\nlog(Uniques)  0.115989   0.002974  38.996  &lt; 2e-16 ***\nauthor.sum   -0.005402   0.001997  -2.705  0.00722 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.02455 on 305 degrees of freedom\nMultiple R-squared:  0.8369,    Adjusted R-squared:  0.8359 \nF-statistic: 782.7 on 2 and 305 DF,  p-value: &lt; 2.2e-16\n\n\nTo see how engagement of a subset of readers changes (loyal readers), I will use another measure for engagement as DV, which represents only registered users: number of comments.\n\n\nCode\nsummary(lm(n.comments ~ log(Uniques) + author.sum , data = merged))\n\n\n\nCall:\nlm(formula = n.comments ~ log(Uniques) + author.sum, data = merged)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-98.670 -35.171  -8.842  26.596 181.918 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   238.521     45.076   5.292 1.78e-07 ***\nlog(Uniques)   -4.667      4.641  -1.006   0.3151    \nauthor.sum      4.872      2.602   1.872   0.0617 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 49.75 on 532 degrees of freedom\nMultiple R-squared:  0.009115,  Adjusted R-squared:  0.00539 \nF-statistic: 2.447 on 2 and 532 DF,  p-value: 0.08752\n\n\nThis shows that engagement is decreasing with popularity and someone increasing with author’s comments. for loyal readers : engagement is also sensitive to author’s comments, but not sensitive to popularity of the post.\n\nanalize change in x1 and x2 to y , considering log()"
  },
  {
    "objectID": "posts/Final_Project_check_in_2_Diana_Rinker.html#adding-other-variables.",
    "href": "posts/Final_Project_check_in_2_Diana_Rinker.html#adding-other-variables.",
    "title": "Final Project check-in 2",
    "section": "3.2 Adding other variables.",
    "text": "3.2 Adding other variables.\nNow I will add other variables to see if there is any impact on the results of the model with engagement of both groups of readers. For number of comments, I would have to select either early.sum or n.comments, due to high correlation between them\n\n\nCode\n(cor(merged$early.sum, merged$n.comments))\n\n\n[1] 0.9507822\n\n\nI am including n/comments in the model, to see if engagement of loyal readers through comments impact overall engagement:\n\n\nCode\n# colnames(merged)\nsummary(lm(`Exit rate` ~ log(Uniques)+ author.sum + pct.positive +blocked.sum+post.weekday +n.comments, data = merged))\n\n\n\nCall:\nlm(formula = `Exit rate` ~ log(Uniques) + author.sum + pct.positive + \n    blocked.sum + post.weekday + n.comments, data = merged)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.249664 -0.014653  0.001237  0.018954  0.061566 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    -3.658e-01  3.738e-02  -9.787  &lt; 2e-16 ***\nlog(Uniques)    1.135e-01  2.803e-03  40.492  &lt; 2e-16 ***\nauthor.sum     -4.158e-03  1.551e-03  -2.681  0.00756 ** \npct.positive    1.488e-04  2.369e-04   0.628  0.53032    \nblocked.sum    -3.634e-04  2.099e-04  -1.732  0.08391 .  \npost.weekday.L -1.959e-03  3.001e-03  -0.653  0.51425    \npost.weekday.Q  1.332e-02  2.879e-03   4.627 4.69e-06 ***\npost.weekday.C  1.109e-04  2.825e-03   0.039  0.96869    \npost.weekday^4  3.235e-03  2.797e-03   1.157  0.24793    \nn.comments      1.384e-04  2.642e-05   5.237 2.36e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.02933 on 525 degrees of freedom\nMultiple R-squared:  0.7748,    Adjusted R-squared:  0.771 \nF-statistic: 200.7 on 9 and 525 DF,  p-value: &lt; 2.2e-16\n\n\nAhd it is shown to be significant, negativelly impacting overall engagement. considering our finding s earlier, that early comments increase popularity and therefore decrease overall engagement, this is expected. Tuesday as level of “weekday” also shows to significantly decrease engagement.\nAuthors engagement, day of the week(Tuesday) and blocked comments seem to increase engagement.\n\n\nCode\nsummary(lm(n.comments ~ log(Uniques) + author.sum + pct.positive +blocked.sum +post.weekday, data = merged)) #not adding early.sum due to high correlation with DV: cor(merged$early.sum, merged$n.comments): 0.95\n\n\n\nCall:\nlm(formula = n.comments ~ log(Uniques) + author.sum + pct.positive + \n    blocked.sum + post.weekday, data = merged)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-84.53 -33.89  -9.63  25.39 183.31 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    106.8646    61.5191   1.737  0.08296 .  \nlog(Uniques)    -1.8812     4.6253  -0.407  0.68438    \nauthor.sum       3.9147     2.5540   1.533  0.12593    \npct.positive     1.1648     0.3878   3.004  0.00279 ** \nblocked.sum      0.8864     0.3443   2.575  0.01030 *  \npost.weekday.L  21.7036     4.8624   4.464 9.87e-06 ***\npost.weekday.Q   6.0475     4.7449   1.275  0.20304    \npost.weekday.C   1.0266     4.6622   0.220  0.82581    \npost.weekday^4  -0.1307     4.6164  -0.028  0.97743    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 48.41 on 526 degrees of freedom\nMultiple R-squared:  0.07252,   Adjusted R-squared:  0.05841 \nF-statistic: 5.141 on 8 and 526 DF,  p-value: 3.455e-06\n\n\nWhile general viewers show no impact by mood of the post or blocked comments, we see significant positive correlation of engagement with all of these metrics for loyal readers(=commenters).\nNumber of comments, representing engagement is somewhat impacted by three independent variables, but this model has low R^2 , which suggests that there are other factors impacting loyal reader’s engagement that are not considered in this model."
  },
  {
    "objectID": "posts/Final_Project_check_in_2_Diana_Rinker.html#using-different-datasets-for-comparing",
    "href": "posts/Final_Project_check_in_2_Diana_Rinker.html#using-different-datasets-for-comparing",
    "title": "Final Project check-in 2",
    "section": "3.3 Using different datasets for comparing:",
    "text": "3.3 Using different datasets for comparing:\n\nExit rate for 2021 and 2022:\n\n\nCode\n# colnames(merged)\nsummary(lm(`Exit rate` ~ log(Uniques)+ author.sum + pct.positive +blocked.sum, data = merged.2021))\n\n\n\nCall:\nlm(formula = `Exit rate` ~ log(Uniques) + author.sum + pct.positive + \n    blocked.sum, data = merged.2021)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.258483 -0.017314  0.000667  0.027730  0.077470 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -5.315e-01  1.194e-01  -4.451 1.35e-05 ***\nlog(Uniques)  1.259e-01  1.053e-02  11.949  &lt; 2e-16 ***\nauthor.sum   -2.673e-03  2.563e-03  -1.043   0.2981    \npct.positive  1.009e-03  5.573e-04   1.811   0.0715 .  \nblocked.sum   7.438e-05  4.341e-04   0.171   0.8641    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.03709 on 222 degrees of freedom\nMultiple R-squared:  0.3946,    Adjusted R-squared:  0.3837 \nF-statistic: 36.18 on 4 and 222 DF,  p-value: &lt; 2.2e-16\n\n\nCode\nsummary(lm(`Exit rate` ~ log(Uniques)+ author.sum + pct.positive +blocked.sum, data = merged.2022))\n\n\n\nCall:\nlm(formula = `Exit rate` ~ log(Uniques) + author.sum + pct.positive + \n    blocked.sum, data = merged.2022)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.08367 -0.01553  0.00204  0.01839  0.05326 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -0.3315325  0.0368576  -8.995  &lt; 2e-16 ***\nlog(Uniques)  0.1153984  0.0029870  38.633  &lt; 2e-16 ***\nauthor.sum   -0.0052803  0.0019958  -2.646  0.00858 ** \npct.positive -0.0001812  0.0002344  -0.773  0.44014    \nblocked.sum  -0.0003622  0.0002190  -1.654  0.09915 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.0245 on 303 degrees of freedom\nMultiple R-squared:  0.8386,    Adjusted R-squared:  0.8365 \nF-statistic: 393.7 on 4 and 303 DF,  p-value: &lt; 2.2e-16\n\n\nExit rate (engagement) still correlates with popularity of the site or bot 2021 and 2021,confirming the same finding from un-splitted data set earlier. However, author’s comments appear only be relevant to all user’s engagement in 2022, and not in 2021. Mood of the post appears significant for user’s engagement only in 2021, and blocked comments only appear significant in 2022. ## 3.2 Diagnostic of the model:\n\n\nCode\n# colnames(merged)\nmodel.ex.r&lt;- lm(`Exit rate` ~ log(Uniques)+ author.sum + pct.positive +blocked.sum, data = merged)\n\npar(mfrow = c(2,3))\nplot(model.ex.r, which = 1:6)\n\n\n\n\n\nCode\nex.r.2021 &lt;- lm(`Exit rate` ~ log(Uniques)+ author.sum + pct.positive +blocked.sum, data = merged.2021)\nex.r.2022 &lt;-lm(`Exit rate` ~ log(Uniques)+ author.sum + pct.positive +blocked.sum, data = merged.2022)\npar(mfrow = c(2,3))\nplot(ex.r.2021, which = 1:6)\n\n\n\n\n\nCode\npar(mfrow = c(2,3))\nplot(ex.r.2022, which = 1:6)\n\n\n\n\n\n\n\nCode\nmodel.2&lt;- lm(n.comments ~ log(Uniques) + author.sum + pct.positive +blocked.sum, data = merged)\n\n\npar(mfrow = c(2,3))\nplot(model.2, which = 1:6)\n\n\n\n\n\n\n\nNumber of comments for 2021 and 2022:\n\n\nCode\nsummary(lm(n.comments ~ log(Uniques) + author.sum + pct.positive +blocked.sum, data = merged.2021))\n\n\n\nCall:\nlm(formula = n.comments ~ log(Uniques) + author.sum + pct.positive + \n    blocked.sum, data = merged.2021)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-106.632  -38.547   -8.915   28.987  167.497 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)  209.0074   180.9229   1.155    0.249\nlog(Uniques)   3.0321    15.9599   0.190    0.849\nauthor.sum     4.5830     3.8832   1.180    0.239\npct.positive  -0.3590     0.8443  -0.425    0.671\nblocked.sum    0.8618     0.6577   1.310    0.191\n\nResidual standard error: 56.19 on 222 degrees of freedom\nMultiple R-squared:  0.01739,   Adjusted R-squared:  -0.000314 \nF-statistic: 0.9823 on 4 and 222 DF,  p-value: 0.418\n\n\nCode\nsummary(lm(n.comments ~ log(Uniques) + author.sum + pct.positive +blocked.sum, data = merged.2022))\n\n\n\nCall:\nlm(formula = n.comments ~ log(Uniques) + author.sum + pct.positive + \n    blocked.sum, data = merged.2022)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-76.762 -28.410  -7.407  25.375 155.874 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -72.7563    59.4359  -1.224 0.221861    \nlog(Uniques)  18.5894     4.8169   3.859 0.000139 ***\nauthor.sum     1.9188     3.2184   0.596 0.551481    \npct.positive   0.7834     0.3780   2.072 0.039094 *  \nblocked.sum    1.1017     0.3531   3.120 0.001981 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 39.51 on 303 degrees of freedom\nMultiple R-squared:  0.07768,   Adjusted R-squared:  0.0655 \nF-statistic:  6.38 on 4 and 303 DF,  p-value: 6.108e-05\n\n\nNone of the variables show significance for loyal user’s engagement(comments) in 2021. However, loyal users are significantly influenced by popularity, mood of the post and blocked comments in 2022.\n\n\nAdding a weekday:\nAdding the weekday of the post ad additional IV increases R^2 and shows significance for one of the levels:\n\n\nCode\nsummary(lm(n.comments ~ log(Uniques) + author.sum + pct.positive +blocked.sum + post.weekday, data = merged))\n\n\n\nCall:\nlm(formula = n.comments ~ log(Uniques) + author.sum + pct.positive + \n    blocked.sum + post.weekday, data = merged)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-84.53 -33.89  -9.63  25.39 183.31 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    106.8646    61.5191   1.737  0.08296 .  \nlog(Uniques)    -1.8812     4.6253  -0.407  0.68438    \nauthor.sum       3.9147     2.5540   1.533  0.12593    \npct.positive     1.1648     0.3878   3.004  0.00279 ** \nblocked.sum      0.8864     0.3443   2.575  0.01030 *  \npost.weekday.L  21.7036     4.8624   4.464 9.87e-06 ***\npost.weekday.Q   6.0475     4.7449   1.275  0.20304    \npost.weekday.C   1.0266     4.6622   0.220  0.82581    \npost.weekday^4  -0.1307     4.6164  -0.028  0.97743    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 48.41 on 526 degrees of freedom\nMultiple R-squared:  0.07252,   Adjusted R-squared:  0.05841 \nF-statistic: 5.141 on 8 and 526 DF,  p-value: 3.455e-06\n\n\nCode\nsummary(lm(`Exit rate` ~ log(Uniques) + author.sum + pct.positive+blocked.sum +post.weekday, data = merged))\n\n\n\nCall:\nlm(formula = `Exit rate` ~ log(Uniques) + author.sum + pct.positive + \n    blocked.sum + post.weekday, data = merged)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.253949 -0.016021  0.001578  0.019637  0.065920 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    -0.3510322  0.0381950  -9.191  &lt; 2e-16 ***\nlog(Uniques)    0.1132211  0.0028717  39.427  &lt; 2e-16 ***\nauthor.sum     -0.0036165  0.0015857  -2.281    0.023 *  \npct.positive    0.0003099  0.0002408   1.287    0.199    \nblocked.sum    -0.0002408  0.0002137  -1.127    0.260    \npost.weekday.L  0.0010440  0.0030189   0.346    0.730    \npost.weekday.Q  0.0141570  0.0029459   4.806 2.02e-06 ***\npost.weekday.C  0.0002529  0.0028946   0.087    0.930    \npost.weekday^4  0.0032168  0.0028662   1.122    0.262    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.03006 on 526 degrees of freedom\nMultiple R-squared:  0.7631,    Adjusted R-squared:  0.7595 \nF-statistic: 211.8 on 8 and 526 DF,  p-value: &lt; 2.2e-16\n\n\nCode\nclass(merged$post.weekday)\n\n\n[1] \"ordered\" \"factor\" \n\n\n\n\nAdding early comments:\nMeasuring how early comments impact engagement:\n\n\nCode\nmerged&lt;- merged %&gt;%\n  mutate (e.rate = `Exit rate`*10000)\nsummary(lm(e.rate ~ log(Uniques) + author.sum +post.weekday +early.sum, data = merged))\n\n\n\nCall:\nlm(formula = e.rate ~ log(Uniques) + author.sum + post.weekday + \n    early.sum, data = merged)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2491.71  -146.36    15.88   188.10   638.62 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    -3587.5846   276.2872 -12.985  &lt; 2e-16 ***\nlog(Uniques)    1138.5047    27.5061  41.391  &lt; 2e-16 ***\nauthor.sum       -38.3691    15.4708  -2.480   0.0134 *  \npost.weekday.L   -14.2529    29.1268  -0.489   0.6248    \npost.weekday.Q   143.4069    28.7226   4.993 8.10e-07 ***\npost.weekday.C     5.1103    28.2598   0.181   0.8566    \npost.weekday^4    35.8574    27.9676   1.282   0.2004    \nearly.sum          1.5903     0.2998   5.305 1.66e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 293.5 on 527 degrees of freedom\nMultiple R-squared:  0.7737,    Adjusted R-squared:  0.7707 \nF-statistic: 257.4 on 7 and 527 DF,  p-value: &lt; 2.2e-16\n\n\nCode\nsummary(lm(e.rate ~ log(Uniques) + author.sum+early.sum, data = merged))\n\n\n\nCall:\nlm(formula = e.rate ~ log(Uniques) + author.sum + early.sum, \n    data = merged)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2538.89  -143.55    24.05   198.90   685.26 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -3634.7905   281.4951 -12.912  &lt; 2e-16 ***\nlog(Uniques)  1144.2628    28.0370  40.813  &lt; 2e-16 ***\nauthor.sum     -35.9680    15.6776  -2.294   0.0222 *  \nearly.sum        1.5193     0.3042   4.994 8.04e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 299.6 on 531 degrees of freedom\nMultiple R-squared:  0.7623,    Adjusted R-squared:  0.761 \nF-statistic: 567.6 on 3 and 531 DF,  p-value: &lt; 2.2e-16\n\n\nThe model’s predictive value is pretty good (probably due to popularity being included) : 0.77. It also appears that this connection We can check it, by excluding the popularity, as see that R^2 drops dramatically:\n\n\nCode\nsummary(lm(e.rate ~ author.sum+early.sum, data = merged))\n\n\n\nCall:\nlm(formula = e.rate ~ author.sum + early.sum, data = merged)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3227.6  -410.9   -65.3   383.0  1613.4 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 7634.2416   111.2795  68.604   &lt;2e-16 ***\nauthor.sum   -92.0013    31.7347  -2.899   0.0039 ** \nearly.sum      0.5435     0.6163   0.882   0.3783    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 608.8 on 532 degrees of freedom\nMultiple R-squared:  0.01667,   Adjusted R-squared:  0.01298 \nF-statistic:  4.51 on 2 and 532 DF,  p-value: 0.01142"
  },
  {
    "objectID": "posts/Tyler_Tewksbury_Final2.html",
    "href": "posts/Tyler_Tewksbury_Final2.html",
    "title": "Final Project Part 2",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/Tyler_Tewksbury_Final2.html#background-and-research-question",
    "href": "posts/Tyler_Tewksbury_Final2.html#background-and-research-question",
    "title": "Final Project Part 2",
    "section": "Background and Research Question",
    "text": "Background and Research Question\nIn 2020, after the release of the Netflix series The Queen’s Gambit, interest in chess was at an all-time high. This led many fans of the show to use popular websites such as Chess.com and Lichess.org to begin learning the game. These websites use a rating system that mimics that of official in-person chess leagues, increasing your rating number as you win and decreasing as you lose. This can be used to measure one’s skill in chess, and determining if they can enter certain competitions.\nWhen playing chess online, as you face someone completely random that the website matches you against, there is no guarantee that you will play against someone with an identical rating. Thus, there will typically be a difference between the two players’ rating. Obviously the player with the higher rating would be more likely to win, right? That is where this study comes in. By quantifying the effect of rating difference on win chance, players may be able to understand more about the match they are currently in. Knowing how likely they are to win, how likely their opponent is to win, and this could lead to further interesting research about making the most fair chess matches possible. As there are not any academic studies on the topic, there is no proven indicator that a slight discrepancy in rating has is an indicator to a player’s win chance. This poses the research question:\nHow strong of a predictor is the difference between players chess rating in determining the victor?"
  },
  {
    "objectID": "posts/Tyler_Tewksbury_Final2.html#dataset",
    "href": "posts/Tyler_Tewksbury_Final2.html#dataset",
    "title": "Final Project Part 2",
    "section": "Dataset",
    "text": "Dataset\nThe dataset being used is sourced from Kaggle: https://www.kaggle.com/datasets/datasnaek/chess\nGathered in 2016, the dataset contains information from over 20,000 matches on Lichess.org via the Lichess API. Information on the players’, their opening moves, the results of the match, and more are all columns within the dataset. There was no selection criteria - it was simply the most recent games taken from the top 100 teams on the Lichess website, consisting of over 1,500 players. This should provide an adequate sample for building regression models.\n\n\nCode\n#reading in the dataset\nchess &lt;- read.csv(\"_data/chess_games.csv\")"
  },
  {
    "objectID": "posts/Tyler_Tewksbury_Final2.html#descriptive-statistics",
    "href": "posts/Tyler_Tewksbury_Final2.html#descriptive-statistics",
    "title": "Final Project Part 2",
    "section": "Descriptive Statistics",
    "text": "Descriptive Statistics\n\n\nCode\nstr(chess)\n\n\n'data.frame':   20058 obs. of  16 variables:\n $ id            : chr  \"TZJHLljE\" \"l1NXvwaE\" \"mIICvQHh\" \"kWKvrqYL\" ...\n $ rated         : chr  \"FALSE\" \"TRUE\" \"TRUE\" \"TRUE\" ...\n $ created_at    : num  1.5e+12 1.5e+12 1.5e+12 1.5e+12 1.5e+12 ...\n $ last_move_at  : num  1.5e+12 1.5e+12 1.5e+12 1.5e+12 1.5e+12 ...\n $ turns         : int  13 16 61 61 95 5 33 9 66 119 ...\n $ victory_status: chr  \"outoftime\" \"resign\" \"mate\" \"mate\" ...\n $ winner        : chr  \"white\" \"black\" \"white\" \"white\" ...\n $ increment_code: chr  \"15+2\" \"5+10\" \"5+10\" \"20+0\" ...\n $ white_id      : chr  \"bourgris\" \"a-00\" \"ischia\" \"daniamurashov\" ...\n $ white_rating  : int  1500 1322 1496 1439 1523 1250 1520 1413 1439 1381 ...\n $ black_id      : chr  \"a-00\" \"skinnerua\" \"a-00\" \"adivanov2009\" ...\n $ black_rating  : int  1191 1261 1500 1454 1469 1002 1423 2108 1392 1209 ...\n $ moves         : chr  \"d4 d5 c4 c6 cxd5 e6 dxe6 fxe6 Nf3 Bb4+ Nc3 Ba5 Bf4\" \"d4 Nc6 e4 e5 f4 f6 dxe5 fxe5 fxe5 Nxe5 Qd4 Nc6 Qe5+ Nxe5 c4 Bb4+\" \"e4 e5 d3 d6 Be3 c6 Be2 b5 Nd2 a5 a4 c5 axb5 Nc6 bxc6 Ra6 Nc4 a4 c3 a3 Nxa3 Rxa3 Rxa3 c4 dxc4 d5 cxd5 Qxd5 exd5 \"| __truncated__ \"d4 d5 Nf3 Bf5 Nc3 Nf6 Bf4 Ng4 e3 Nc6 Be2 Qd7 O-O O-O-O Nb5 Nb4 Rc1 Nxa2 Ra1 Nb4 Nxa7+ Kb8 Nb5 Bxc2 Bxc7+ Kc8 Qd\"| __truncated__ ...\n $ opening_eco   : chr  \"D10\" \"B00\" \"C20\" \"D02\" ...\n $ opening_name  : chr  \"Slav Defense: Exchange Variation\" \"Nimzowitsch Defense: Kennedy Variation\" \"King's Pawn Game: Leonardis Variation\" \"Queen's Pawn Game: Zukertort Variation\" ...\n $ opening_ply   : int  5 4 3 3 5 4 10 5 6 4 ...\n\n\nThe dataset contains 20058 observations across 16 variables.\n\n\nCode\nsummary(chess)\n\n\n      id               rated             created_at         last_move_at      \n Length:20058       Length:20058       Min.   :1.377e+12   Min.   :1.377e+12  \n Class :character   Class :character   1st Qu.:1.478e+12   1st Qu.:1.478e+12  \n Mode  :character   Mode  :character   Median :1.496e+12   Median :1.496e+12  \n                                       Mean   :1.484e+12   Mean   :1.484e+12  \n                                       3rd Qu.:1.503e+12   3rd Qu.:1.503e+12  \n                                       Max.   :1.504e+12   Max.   :1.504e+12  \n     turns        victory_status        winner          increment_code    \n Min.   :  1.00   Length:20058       Length:20058       Length:20058      \n 1st Qu.: 37.00   Class :character   Class :character   Class :character  \n Median : 55.00   Mode  :character   Mode  :character   Mode  :character  \n Mean   : 60.47                                                           \n 3rd Qu.: 79.00                                                           \n Max.   :349.00                                                           \n   white_id          white_rating    black_id          black_rating \n Length:20058       Min.   : 784   Length:20058       Min.   : 789  \n Class :character   1st Qu.:1398   Class :character   1st Qu.:1391  \n Mode  :character   Median :1567   Mode  :character   Median :1562  \n                    Mean   :1597                      Mean   :1589  \n                    3rd Qu.:1793                      3rd Qu.:1784  \n                    Max.   :2700                      Max.   :2723  \n    moves           opening_eco        opening_name        opening_ply    \n Length:20058       Length:20058       Length:20058       Min.   : 1.000  \n Class :character   Class :character   Class :character   1st Qu.: 3.000  \n Mode  :character   Mode  :character   Mode  :character   Median : 4.000  \n                                                          Mean   : 4.817  \n                                                          3rd Qu.: 6.000  \n                                                          Max.   :28.000  \n\n\nLooking at the summary, it is clear what variables will be used and if any new columns will be added. The following will prove relevance to the research question:\n\n`rated``\n`victory_status``\nwinner\nwhite_id\nwhite_rating\nblack_id\nblack_rating\nturns\nopening_eco\nopening_name\n\nA new column containing the difference between the rating will be added in the next iteration for analysis. Having this added column will make the functions necessary for analysis easier, as calculating the difference will not need to be repeated for each observation.\n\nwhite_rating and black_rating\n\n\nCode\nrange(chess$white_rating)\n\n\n[1]  784 2700\n\n\nCode\nrange(chess$black_rating)\n\n\n[1]  789 2723\n\n\nThe ranges of the two sides are nearly identical, and are quite large nearing 2000. This could be both good and bad for the study, as the large range could prove significant, but it may be necessary to break the models into smaller ranges. This could also be interesting, perhaps seeing if the rating differences at a lower level matter more than that of a higher level, or vice versa."
  },
  {
    "objectID": "posts/Tyler_Tewksbury_Final2.html#proposed-models",
    "href": "posts/Tyler_Tewksbury_Final2.html#proposed-models",
    "title": "Final Project Part 2",
    "section": "Proposed Models",
    "text": "Proposed Models\nThe obvious model for this question will be a a linear probability regression, as the victory status is a binary variable. Proposed models initially are:\nLinear probability including unranked Linear probability excluding unranked\nThere will be more models, potentially differentiating between the different ranges as stated earlier. More possibilities include looking at exclusively drawn game data, analyzing favored openings depending on rank, or possibly finding other predictors if the rank difference is not significant."
  },
  {
    "objectID": "posts/Tyler_Tewksbury_Final2.html#part-2-constructing-models",
    "href": "posts/Tyler_Tewksbury_Final2.html#part-2-constructing-models",
    "title": "Final Project Part 2",
    "section": "Part 2: Constructing Models",
    "text": "Part 2: Constructing Models\nTo begin constructing the models, the difference in rank column needs to be added. This can be done using simple R commands. The victor will also be made into bingary numeric of 0 and 1, 0 being a white victory and 1 being a black victory. Draw results will be removed to allow use of linear probability.\n\n\nCode\nchess &lt;- chess[chess$winner != 'draw' ,]\nchess &lt;- chess %&gt;%\n  mutate(winner = recode(winner, \"white\" = \"0\", \"black\" = \"1\")) %&gt;%\n  mutate(winner = as.numeric(winner))\nchess$rating_diff &lt;- chess$white_rating - chess$black_rating\nrange(chess$rating_diff)\n\n\n[1] -1605  1499\n\n\nCode\nhist(chess$rating_diff)\n\n\n\n\n\nWith the difference in rating defined, an initial model can be made with the difference as an explanatory variable, and the winner as the response variable.\n\n\nCode\nwinner1 &lt;- lm(winner ~ rating_diff, data = chess)\nsummary(winner1)\n\n\n\nCall:\nlm(formula = winner ~ rating_diff, data = chess)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.3770 -0.4462 -0.1038  0.4733  1.2555 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  4.821e-01  3.362e-03  143.37   &lt;2e-16 ***\nrating_diff -7.317e-04  1.341e-05  -54.57   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4646 on 19106 degrees of freedom\nMultiple R-squared:  0.1349,    Adjusted R-squared:  0.1348 \nF-statistic:  2978 on 1 and 19106 DF,  p-value: &lt; 2.2e-16\n\n\nThe barebones model shows what was to be expected. A statistically significant relationship between the difference in rating and the outcome of the match. The p value being astronomically low confirms that the explanatory variable undoubtably affectts the dependent variable. Just from this barebones graph though, it is interesting to see how few outliers there are when visualized. Much more can be done both visually and statistically with this dataset however, so let’s make more models, first controlling for the amount of turns taken in the match.\n\n\nCode\nwinner2 &lt;- lm(winner ~ rating_diff + turns, data = chess)\nsummary(winner2)\n\n\n\nCall:\nlm(formula = winner ~ rating_diff + turns, data = chess)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.34504 -0.44679 -0.09632  0.47048  1.24682 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  4.524e-01  7.018e-03  64.467  &lt; 2e-16 ***\nrating_diff -7.295e-04  1.341e-05 -54.399  &lt; 2e-16 ***\nturns        5.009e-04  1.040e-04   4.815 1.48e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4643 on 19105 degrees of freedom\nMultiple R-squared:  0.1359,    Adjusted R-squared:  0.1358 \nF-statistic:  1502 on 2 and 19105 DF,  p-value: &lt; 2.2e-16\n\n\nCode\nplot(x = chess$rating_diff, \n     y = chess$winner)\nabline(winner1, col = \"green\")\nabline(h = .5, lty = 2, col = \"black\")\nabline(v = 0, lty = 2, col = \"darkred\")\nabline(winner2, col = \"red\")\n\n\nWarning in abline(winner2, col = \"red\"): only using the first two of 3\nregression coefficients\n\n\n\n\n\nI included the original regression line to show for comparison. The color will be green to easily differentiate it from future regression models. When controlling for the amount of turns taken in the match, a line slightly favoring a black victory is created. This is interesting, potentially implying that the amount of turns may matter more as the white player. This result is also statistically significant, as all of the models likely will be, given the obvious relationship between the two main variables.\nThe third model that will be tested is a probit regression, as well as using a predict function to create a more appealing and more applicable regression line.\n\n\nCode\nwinner3 &lt;- glm(winner ~ rating_diff, family = binomial(link = \"logit\"), data = chess)\nplot(x = chess$rating_diff, \n     y = chess$winner)\nx &lt;- seq(-1500, 1500, 10)\ny &lt;- predict(winner3, list(rating_diff = x), type = \"response\")\nlines(x, y)\n\n\n\n\n\nThe probit model aims to determine the likelihood that a victory will occur based on the rating difference. This differs from the previous linear models, as the predicted value can exceed the binary range that we are looking for (the victory). By using a probit (or logit) model, as well as an s curve to visualize, this will prove more beneficial for future models.\n##Continued Work\nMore models can and will be made using this data. As I further my knowledge of regression, as well as spend time working with the dataset. This check-in was just to show that I have something, and I plan to upload a “check-in 2.5” as time permits prior to the final submission.\nSome example changes to future models are: Excluding extreme outliers Narrowing down the difference range significantly to see if a difference of just 10-20 points is significant Testing with more different opening moves to see the difference they could make\nI also anticipate to improve the visualization aspect, with more interesting labels and colors to differentiate between models on a single visualization. The final poster will not have room to have many different models with differing visualizations, so it is important to make the few visualizations as clear and detailed as possible."
  },
  {
    "objectID": "posts/abigailbalint_hw2.html",
    "href": "posts/abigailbalint_hw2.html",
    "title": "Homework 2",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(readxl)\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/abigailbalint_hw2.html#question-1",
    "href": "posts/abigailbalint_hw2.html#question-1",
    "title": "Homework 2",
    "section": "Question 1",
    "text": "Question 1\nGenerating a table here to calculate standard deviation\n\n\nCode\nheart &lt;- matrix(c(539, 19, 10, 847, 18, 9), ncol=3, byrow=TRUE)\ncolnames(heart) &lt;- c('Sample Size','Mean Wait Time','Standard Deviation')\nrownames(heart) &lt;- c('Bypass','Angiography')\nheart &lt;- as.table(heart)\nhead(heart)\n\n\n            Sample Size Mean Wait Time Standard Deviation\nBypass              539             19                 10\nAngiography         847             18                  9\n\n\nCalculating confidence interval for Bypass:\n\n\nCode\nstander1 &lt;- 10/sqrt(539)\nconfidence_level &lt;- 0.90 \ntail_area &lt;- (1-confidence_level)/2\nt_score &lt;- qt(p = 1-tail_area, df = 539-1)\nt_score\n\n\n[1] 1.647691\n\n\nCode\nCI &lt;- c(19 - t_score * stander1,\n        19 + t_score * stander1)\nprint(CI)\n\n\n[1] 18.29029 19.70971\n\n\nCalculating confidence interval for Angiography:\n\n\nCode\nstander2 &lt;- 9/sqrt(847)\nconfidence_level &lt;- 0.90 \ntail_area &lt;- (1-confidence_level)/2\nt_score &lt;- qt(p = 1-tail_area, df = 847-1)\nt_score\n\n\n[1] 1.646657\n\n\nCode\nCI &lt;- c(18 - t_score * stander2,\n        18 + t_score * stander2)\nprint(CI)\n\n\n[1] 17.49078 18.50922\n\n\n\n\nCode\nBypassInt &lt;- 19.70971 - 18.29029\nAngiographyInt &lt;- 18.50922-17.49078 \nprint(BypassInt)  \n\n\n[1] 1.41942\n\n\nCode\nprint(AngiographyInt) \n\n\n[1] 1.01844\n\n\nQuestion: Is the confidence interval narrower for angiography or bypass surgery? Answer: The interval for angiography surgery is shorter at an interval of 1.01."
  },
  {
    "objectID": "posts/abigailbalint_hw2.html#question-2",
    "href": "posts/abigailbalint_hw2.html#question-2",
    "title": "Homework 2",
    "section": "Question 2",
    "text": "Question 2\nHere I am using a prop test function to find the p value -\n\n\nCode\nprop.test(567, 1031, conf.level = 0.95)\n\n\n\n    1-sample proportions test with continuity correction\n\ndata:  567 out of 1031, null probability 0.5\nX-squared = 10.091, df = 1, p-value = 0.00149\nalternative hypothesis: true p is not equal to 0.5\n95 percent confidence interval:\n 0.5189682 0.5805580\nsample estimates:\n        p \n0.5499515 \n\n\nAnswer: To interpret this, I am looking at my confidence interval and seeing that the true population mean of those who believe college is needed for success is somewhere between 52-58% which makes sense because the reported percentage from this random sample who believe college is needed for success is 54%, inside of that range."
  },
  {
    "objectID": "posts/abigailbalint_hw2.html#question-3",
    "href": "posts/abigailbalint_hw2.html#question-3",
    "title": "Homework 2",
    "section": "Question 3",
    "text": "Question 3\nMy calculations to find the sample size are step by step in the below code block:\n\n\nCode\n#Starting by trying to find margin of error:\n#Formula is error= z * SD/sqrt of n\n#By squaring the whole thing, can transform this formular to to n=z^2*SD^2/error^2\n# Z score for 95% confidence is 1.96\n1.96^2\n\n\n[1] 3.8416\n\n\nCode\n#n=3.8416*SD^2/error^2\n#Finding the range and dividing it by 4 since we know SD is quarter of the range\n(200-30)/4\n\n\n[1] 42.5\n\n\nCode\n42.5^2\n\n\n[1] 1806.25\n\n\nCode\n#Now formula looks like this and just need to find standard error n=3.8416*1806.25/error^2\n#We know that margin of error is 5 (within $5)\n5^2\n\n\n[1] 25\n\n\nCode\n#Now we can fill in the final formula n=3.8416*1806.25/25\n(3.8416*1806.25)/25\n\n\n[1] 277.5556\n\n\nCode\n#n=277.56\n\n\nAnswer: The sample size should be about 278 (rounded)"
  },
  {
    "objectID": "posts/abigailbalint_hw2.html#question-4",
    "href": "posts/abigailbalint_hw2.html#question-4",
    "title": "Homework 2",
    "section": "Question 4",
    "text": "Question 4\nA. Test whether the mean income of female employees differs from $500 per week. Include assumptions, hypotheses, test statistic, and P-value. Interpret the result.\nAssumptions - -Randomly generated sample -From the same general population -Distribution is normal\nNull hypothesis - females earn $500 a week Alternative hypothesis - females earn more or less than $500 a week\nCalculating the t value below using standard formula -\nt= (sample mean-population mean)/(standard deviation/sample size^2)\n\n\nCode\ntvalue =  (410 - 500) / (90/sqrt(9))\ntvalue\n\n\n[1] -3\n\n\nAnswer: t=-3\nThis tells us that the mean of the female group is three standard deviations away from the mean of the overall group’s pay.\nB. Report the P-value for alternative hypothesis: μ &lt; 500. Interpret.\nCalculating p-value -\nFormula pt(q = t, df =standardeviation-1, lower.tail = TRUE)\n\n\nCode\npt(q = tvalue, df =8, lower.tail = TRUE)\n\n\n[1] 0.008535841\n\n\nAnswer: .01 (rounded)\nA p-value of .01 means we can reject the alternative hypothesis (mean for females &lt; 500 a week)\nC. Report and interpret the P-value for alternative hypothesis: μ &gt; 500.\n\n\nCode\npt(q = tvalue, df =8, lower.tail = FALSE)\n\n\n[1] 0.9914642\n\n\nAnswer: .99 (rounded) This is the same but flipped, can reject the alternative hypothesis (mean for females &gt; 500 a week)"
  },
  {
    "objectID": "posts/abigailbalint_hw2.html#question-5",
    "href": "posts/abigailbalint_hw2.html#question-5",
    "title": "Homework 2",
    "section": "Question 5",
    "text": "Question 5\nUsing formula t= sample mean-population mean/standard error\n\n\nCode\nt=(519.5-500)/10\nprint(t)\n\n\n[1] 1.95\n\n\nCode\npt(q = t, df = 999, lower.tail = FALSE)*2\n\n\n[1] 0.05145555\n\n\nAnswer: Jones t=1.95, p=.051\n\n\nCode\nt=(519.7-500)/10\nprint(t)\n\n\n[1] 1.97\n\n\nCode\npt(q = t, df = 999, lower.tail = FALSE)*2\n\n\n[1] 0.04911426\n\n\nAnswer: Smith t=1.97, p=.049\nB. This makes Smith statistically significant because .049 falls below .05 but .051 does not.\nC. This shows that results presented this way can be misleading because even though the p-values are extremely close here, one would report rejecting the null hypothesis and one wouldn’t even though the differences in results are marginal."
  },
  {
    "objectID": "posts/abigailbalint_hw2.html#question-6",
    "href": "posts/abigailbalint_hw2.html#question-6",
    "title": "Homework 2",
    "section": "Question 6",
    "text": "Question 6\nNull hypothesis: Proportion of those who choose healthy snacks is not equal by grade level.\nGenerating table -\n\n\nCode\nsnack &lt;- matrix(c(31, 43, 51, 69, 57, 49), ncol=3, byrow=TRUE)\ncolnames(snack) &lt;- c('6th','7th','8th')\nrownames(snack) &lt;- c('healthy','unhealthy')\nsnack &lt;- as.table(snack)\nhead(snack)\n\n\n          6th 7th 8th\nhealthy    31  43  51\nunhealthy  69  57  49\n\n\nPerforming chi squared test -\n\n\nCode\nchisq.test(snack, .05, correct = FALSE)\n\n\n\n    Pearson's Chi-squared test\n\ndata:  snack\nX-squared = 8.3383, df = 2, p-value = 0.01547\n\n\nSince the p value is .01 we can assume that there is a difference by grade level in those who choose unhealthy vs healthy snacks, rejecting null hypothesis."
  },
  {
    "objectID": "posts/abigailbalint_hw2.html#question-7",
    "href": "posts/abigailbalint_hw2.html#question-7",
    "title": "Homework 2",
    "section": "Question 7",
    "text": "Question 7\nNull hypothesis: There is no difference in per-pupil costs between areas.\nGenerating data frame -\n\n\nCode\narea &lt;- c(rep(\"Area1\", 6), rep(\"Area2\", 6), rep(\"Area3\", 6))\ncost &lt;- c(6.2, 9.3, 6.8, 6.1, 6.7, 7.5, 7.5, 8.2, 8.5, 8.2, 7.0, 9.3,\n          5.8, 6.4, 5.6, 7.1, 3.0, 3.5)\ntuition &lt;- data.frame(area,cost)\nhead(tuition)\n\n\n   area cost\n1 Area1  6.2\n2 Area1  9.3\n3 Area1  6.8\n4 Area1  6.1\n5 Area1  6.7\n6 Area1  7.5\n\n\n\n\nCode\nanova &lt;- aov(cost ~ area, data = tuition)\nsummary(anova)\n\n\n            Df Sum Sq Mean Sq F value  Pr(&gt;F)   \narea         2  25.66  12.832   8.176 0.00397 **\nResiduals   15  23.54   1.569                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nInterpretation: The p value is very low indicating there is a difference and we can reject the null hypothesis. :"
  },
  {
    "objectID": "posts/AdithyaParupudi_finalproject2.html",
    "href": "posts/AdithyaParupudi_finalproject2.html",
    "title": "Final Project - Post 2",
    "section": "",
    "text": "The birthwt dataset(part of the MASS package) is a widely-used data collection in the field of medical statistics and public health research, focusing on the factors influencing birth weight in newborns. It contains records of various factors such as maternal age, weight, race, smoking habits during pregnancy, and the number of prenatal visits, among others. By analyzing the relationships between these variables and birth weight, researchers and medical professionals can identify potential risk factors, better understand the determinants of low birth weight, and develop effective interventions to improve maternal and neonatal health outcomes."
  },
  {
    "objectID": "posts/AdithyaParupudi_finalproject2.html#general-visualizations",
    "href": "posts/AdithyaParupudi_finalproject2.html#general-visualizations",
    "title": "Final Project - Post 2",
    "section": "General Visualizations",
    "text": "General Visualizations\n\n\nCode\nggplot1 &lt;- ggplot(data = data.frame(table(birthwt$race)), aes(x = Var1, y = Freq, fill = Var1)) +\n  geom_bar(stat = \"identity\") +\n  geom_text(aes(label = Freq), vjust = -0.5) +\n  xlab(\"Race\") + ylab(\"Frequency\")+\n  labs(fill = \"Race\")\n\nggplot2 &lt;- ggplot(data = data.frame(table(birthwt$ht)), aes(x = Var1, y = Freq, fill = Var1)) +\n  geom_bar(stat = \"identity\") +\n  geom_text(aes(label = Freq), vjust = -0.5) +\n  xlab(\"Hypertension State\") + ylab(\"Frequency\") +\n  labs(fill = \"Hypertension\")\n\nggplot3 &lt;- ggplot(data = data.frame(table(birthwt$ui)), aes(x = Var1, y = Freq, fill = Var1)) +\n  geom_bar(stat = \"identity\") +\n  geom_text(aes(label = Freq), vjust = -0.5) +\n  xlab(\"Uterine Irritability\") + ylab(\"Frequency\")+\n  labs(fill = \"Uterine Irritability\")\n\nggplot4 &lt;- ggplot(data = data.frame(table(birthwt$ptl)), aes(x = Var1, y = Freq, fill = Var1)) +\n  geom_bar(stat = \"identity\") +\n  geom_text(aes(label = Freq), vjust = -0.5) +\n  xlab(\"Previous Premature Labors\") + ylab(\"Frequency\")+\n  labs(fill = \"Premature Labors\")\n\nggplot5 &lt;- ggplot(data = data.frame(table(birthwt$smoke)), aes(x = Var1, y = Freq, fill = Var1)) +\n  geom_bar(stat = \"identity\") +\n  geom_text(aes(label = Freq), vjust = -0.5) +\n  xlab(\"Smoking Status\") + ylab(\"Frequency\")+\n  labs(fill = \"Smoking Status\")\n\nggplot6 &lt;- ggplot(data = data.frame(table(birthwt$low)), aes(x = Var1, y = Freq, fill = Var1)) +\n  geom_bar(stat = \"identity\") +\n  geom_text(aes(label = Freq), vjust = -0.5) +\n  xlab(\"Low bwt\") + ylab(\"Frequency\")+\n  labs(fill = \"&lt;2.5kg\")\n\nggplot7 &lt;- ggplot(data = data.frame(table(birthwt$ftv)), aes(x = Var1, y = Freq, fill = Var1)) +\n  geom_bar(stat = \"identity\") +\n  geom_text(aes(label = Freq), vjust = -0.5) +\n  xlab(\"Frequenct of physician visits\") + ylab(\"Frequency\")+\n  labs(fill = \"Visits\")\n\n# Arrange the ggplot objects in a grid with different colors\ngrid.arrange(ggplot1 + scale_fill_brewer(palette = \"Set2\"),\n             ggplot2 + scale_fill_brewer(palette = \"Pastel1\"),\n             ggplot3 + scale_fill_brewer(palette = \"Set3\"),\n             ggplot4 + scale_fill_brewer(palette = \"Accent\"),\n             ggplot5 + scale_fill_brewer(palette = \"Pastel2\"),\n             ggplot6 + scale_fill_brewer(palette = \"Pastel1\"),\n             ggplot7 + scale_fill_brewer(palette = \"Pastel2\"),\n             ncol = 2)"
  },
  {
    "objectID": "posts/AdithyaParupudi_finalproject2.html#checking-association-between-variables",
    "href": "posts/AdithyaParupudi_finalproject2.html#checking-association-between-variables",
    "title": "Final Project - Post 2",
    "section": "Checking association between variables",
    "text": "Checking association between variables\n\nScatter plot of Mother’s Age vs Birth Weight\nThis plot shows the relationship between the mother’s age (x-axis) and the birth weight of the baby (y-axis). Each point represents a single data point from the ‘birthwt’ dataset. From the plot, we can visually assess if there is any trend or pattern between the mother’s age and the birth weight of the baby.\nIt is known that birthwt greater than 2500 grams is considered healthy weight. In the below scatterplot, we can observe that there are many cases of low birth weight registered in mothers who are aged between 20 and 30, with fewer such cases reported in mothers aged less than 20. As the mother’s age increased, there are fewer cases of low birthwt weight.\n\n\nCode\nggplot(birthwt, aes(x = age, y = bwt)) + \n  geom_point() +\n  labs(title = \"Scatter plot of Mother's Age vs Birth Weight\",\n       x = \"Mother's Age\",\n       y = \"Birth Weight (grams)\") +\n  theme_minimal()\n\n\n\n\n\n\n\nHistogram of Birth Weight\nThis plot is a histogram of the birth weights (x-axis) in the dataset, showing the frequency (y-axis) of the birth weights within specific intervals (bins). The histogram roughly looks like a bell-curve with a peak from 2500-3500 grams. There are fewer cases of low birth weight in bwt less than 2000 grams and greater than 4000 grams. The majority count is between weight ranges of 2500-2500 grams. There are still quite a few cases of low birthweight(less than 2500 grams), but the count of healthy baby weight is more.\n\n\nCode\nggplot(birthwt, aes(x = bwt)) + \n  geom_histogram(binwidth = 100, fill = \"steelblue\", color = \"black\") +\n  labs(title = \"Histogram of Birth Weight\",\n       x = \"Birth Weight (grams)\",\n       y = \"Frequency\") +\n  theme_minimal()\n\n\n\n\n\n\n\nScatter plot of Mother’s Age vs Birth Weight\nThis plot is similar to the first scatter plot, but with an added color dimension based on the smoking status of the mother. Non-smokers are represented by dark green points, while smokers are represented by red points. This plot enables us to visually compare the birth weight patterns between smokers and non-smokers across different ages of mothers.\nFrom the scatter plot it can be noted that there are many points in age groups ‘&lt;20’ and ‘20-30’, with no smokers above 40 years. It looks like the number of smokers and non-smokers belonging to age group 20-30 are almost equal. And as age increases, the number of smokers descreases.\n\n\nCode\nggplot(birthwt, aes(x = age, y = bwt, color = factor(smoke))) + \n  geom_point() +\n  scale_color_manual(values = c(\"darkgreen\", \"red\"), labels = c(\"Non-smoker\", \"Smoker\")) +\n  labs(title = \"Scatter plot of Mother's Age vs Birth Weight\",\n       x = \"Mother's Age\",\n       y = \"Birth Weight (grams)\",\n       color = \"Smoking Status\") +\n  theme_minimal()\n\n\n\n\n\n\n\nBox plot of Birth Weight by Race\nThis plot shows the distribution of birth weights (y-axis) across three racial categories (x-axis): White (1), Black (2), and Other (3). From the box plot we can see that there are more mothers belonging to ‘white’ race, followed by ‘other’ and ‘black’ races. There is one outlier each observed for race - black and other. The highest birthweight of ~5000grams is observed in white mothers, followed by ~4000 grams in other race mothers, with the black mothers having their child’s birthwt just below 4000 grams. The lowest birthwt of ~1000 grams is observed for white mothers.\n\n\nCode\nggplot(birthwt, aes(x = factor(race), y = bwt, fill = factor(race))) + \n  geom_boxplot() +\n  scale_fill_manual(values = c(\"white\", \"Turquoise\", \"orange\"), labels = c(\"White\", \"Black\", \"Other\")) +\n  labs(title = \"Box plot of Birth Weight by Race\",\n       x = \"Race (1 = White, 2 = Black, 3 = Other)\",\n       y = \"Birth Weight (grams)\",\n       fill = \"Race\") +\n  theme_minimal()\n\n\n\n\n\n\n\nCode\nstr(birthwt)\n\n\n'data.frame':   189 obs. of  10 variables:\n $ low  : int  0 0 0 0 0 0 0 0 0 0 ...\n $ age  : int  19 33 20 21 18 21 22 17 29 26 ...\n $ lwt  : int  182 155 105 108 107 124 118 103 123 113 ...\n $ race : int  2 3 1 1 1 3 1 3 1 1 ...\n $ smoke: int  0 0 1 1 1 0 0 0 1 1 ...\n $ ptl  : int  0 0 0 0 0 0 0 0 0 0 ...\n $ ht   : int  0 0 0 0 0 0 0 0 0 0 ...\n $ ui   : int  1 0 0 1 1 0 0 0 0 0 ...\n $ ftv  : int  0 3 1 2 0 0 1 1 1 0 ...\n $ bwt  : int  2523 2551 2557 2594 2600 2622 2637 2637 2663 2665 ..."
  },
  {
    "objectID": "posts/AdithyaParupudi_finalproject2.html#test-for-rq1",
    "href": "posts/AdithyaParupudi_finalproject2.html#test-for-rq1",
    "title": "Final Project - Post 2",
    "section": "Test for RQ1",
    "text": "Test for RQ1\nThe logistic regression model examines the relationship between history of hypertension (ht) and the risk of low infant birth weight (low), while controlling for the interaction between uterine irritability (ui) and the number of previous premature labors (ptl).\nFrom the output, we observe that the history of hypertension (htYes) has a significant positive effect on the probability of low birth weight (Estimate = 1.4438, p-value = 0.0244). This indicates that mothers with a history of hypertension are more likely to have infants with low birth weight.\nThe significant interaction term between uterine irritability (uiYes) and one previous premature labor (ptl1) (Estimate = -1.1253, p-value = 0.3163) suggests that the effect of uterine irritability on low birth weight may vary depending on the number of previous premature labors, although this interaction is not significant at the 0.05 level.\n\n\nCode\nmodel1 &lt;- glm(low ~ ht + ui * ptl, data = birthwt, family = binomial)\n\n# Perform hypothesis tests for model1 (ht, ui, ptl, and ui * ptl)\nsummary(model1)\n\n\n\nCall:\nglm(formula = low ~ ht + ui * ptl, family = binomial, data = birthwt)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-1.7074  -0.6952  -0.6952   1.1542   1.7542  \n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  -1.2970     0.2111  -6.145 7.99e-10 ***\nht            1.4552     0.6292   2.313  0.02074 *  \nui            1.3518     0.4883   2.768  0.00564 ** \nptl           1.2448     0.4284   2.906  0.00366 ** \nui:ptl       -1.3632     0.6495  -2.099  0.03585 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 234.67  on 188  degrees of freedom\nResidual deviance: 215.19  on 184  degrees of freedom\nAIC: 225.19\n\nNumber of Fisher Scoring iterations: 4"
  },
  {
    "objectID": "posts/AdithyaParupudi_finalproject2.html#test-for-rq2",
    "href": "posts/AdithyaParupudi_finalproject2.html#test-for-rq2",
    "title": "Final Project - Post 2",
    "section": "Test for RQ2",
    "text": "Test for RQ2\nThe logistic regression model investigates how maternal smoking during pregnancy (smoke) and maternal age (age) influence the risk of low infant birth weight (low).\nThe output shows that maternal smoking (smokeYes) has a significant positive effect on the probability of low birth weight (Estimate = 0.7380, p-value = 0.0232), indicating that mothers who smoke during pregnancy are more likely to have infants with low birth weight. None of the age group variables are significant (age&gt;40: p-value = 0.9879; age20-30: p-value = 0.3891; age30-40: p-value = 0.2613), suggesting that there is no strong evidence of a relationship between maternal age and the risk of low birth weight in this model.\nHere, no significant relationship is found between maternal age and low birth weight.\n\n\nCode\nmodel2 &lt;- glm(low ~ smoke + age, data = birthwt, family = binomial)\n\n# Perform hypothesis tests for model2 (smoke, age)\nsummary(model2)\n\n\n\nCall:\nglm(formula = low ~ smoke + age, family = binomial, data = birthwt)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-1.1589  -0.8668  -0.7470   1.2821   1.7925  \n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)  \n(Intercept)  0.06091    0.75732   0.080   0.9359  \nsmoke        0.69185    0.32181   2.150   0.0316 *\nage         -0.04978    0.03197  -1.557   0.1195  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 234.67  on 188  degrees of freedom\nResidual deviance: 227.28  on 186  degrees of freedom\nAIC: 233.28\n\nNumber of Fisher Scoring iterations: 4"
  },
  {
    "objectID": "posts/AdithyaParupudi_finalproject2.html#full-model",
    "href": "posts/AdithyaParupudi_finalproject2.html#full-model",
    "title": "Final Project - Post 2",
    "section": "Full model",
    "text": "Full model\nThe full_model includes all the predictors available in the birthwt dataset. The output of the summary shows that none of the predictors have a significant p-value (all are much greater than the typical threshold of 0.05). This suggests that, in this full model, none of the predictors appear to be significantly associated with low birth weight.\nThe residual deviance is very low (3.8067e-08) compared to the null deviance (234.67), which might indicate that the model is overfitting the data.\n\n\nCode\nfull_model = glm(low~., data=birthwt, family = binomial )\n\n\nWarning: glm.fit: algorithm did not converge\n\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\n\nCode\nsummary(full_model)\n\n\n\nCall:\nglm(formula = low ~ ., family = binomial, data = birthwt)\n\nDeviance Residuals: \n       Min          1Q      Median          3Q         Max  \n-1.890e-04  -2.100e-08  -2.100e-08   2.100e-08   1.593e-04  \n\nCoefficients:\n              Estimate Std. Error z value Pr(&gt;|z|)\n(Intercept)  1.161e+03  2.074e+05   0.006    0.996\nage          3.223e-01  1.787e+03   0.000    1.000\nlwt         -1.733e-01  3.202e+02  -0.001    1.000\nrace         6.494e-01  3.165e+04   0.000    1.000\nsmoke       -1.746e+01  7.668e+04   0.000    1.000\nptl          1.267e+02  3.406e+05   0.000    1.000\nht           3.636e+01  1.237e+05   0.000    1.000\nui          -6.183e+01  7.547e+04  -0.001    0.999\nftv         -8.925e+00  1.624e+04  -0.001    1.000\nbwt         -4.466e-01  6.468e+01  -0.007    0.994\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 2.3467e+02  on 188  degrees of freedom\nResidual deviance: 1.0537e-07  on 179  degrees of freedom\nAIC: 20\n\nNumber of Fisher Scoring iterations: 25"
  },
  {
    "objectID": "posts/AdithyaParupudi_finalproject2.html#temp-model-1",
    "href": "posts/AdithyaParupudi_finalproject2.html#temp-model-1",
    "title": "Final Project - Post 2",
    "section": "Temp model 1",
    "text": "Temp model 1\nThis model investigates whether the interaction between race and the number of previous premature labors (ptl) plays a role in predicting low birth weight. While the main effects of race (specifically being black) and having one previous premature labor (ptl1) are significantly associated with low birth weight, the interaction terms are not significant. This suggests that the combined effect of race and ptl does not provide additional information in predicting low birth weight beyond their individual main effects.\n\n\nCode\ntemp_mod1 &lt;- glm(low ~ race * ptl, data = birthwt, family = binomial)\nsummary(temp_mod1)\n\n\n\nCall:\nglm(formula = low ~ race * ptl, family = binomial, data = birthwt)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-1.7757  -0.9205  -0.7137   1.4122   1.7276  \n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  -1.5367     0.4095  -3.752 0.000175 ***\nrace          0.2990     0.1913   1.563 0.117984    \nptl           0.5552     0.6708   0.828 0.407895    \nrace:ptl      0.1457     0.3471   0.420 0.674606    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 234.67  on 188  degrees of freedom\nResidual deviance: 224.10  on 185  degrees of freedom\nAIC: 232.1\n\nNumber of Fisher Scoring iterations: 4"
  },
  {
    "objectID": "posts/AdithyaParupudi_finalproject2.html#temp-model-2",
    "href": "posts/AdithyaParupudi_finalproject2.html#temp-model-2",
    "title": "Final Project - Post 2",
    "section": "Temp model 2",
    "text": "Temp model 2\nThis model examines the relationship between low birth weight and the main effects of the number of previous premature labors (ptl), maternal smoking during pregnancy (smoke), and the frequency of doctor visits (ftv). Among the predictors, only having one previous premature labor (ptl1) is significantly associated with low birth weight. This implies that having one previous premature labor is an important predictor of low birth weight in this model. However, maternal smoking and the frequency of doctor visits do not seem to play a significant role in predicting low birth weight in this model.\n\n\nCode\ntemp_mod2 = glm(low ~ ptl + smoke + ftv , data=birthwt, family = binomial )\nsummary(temp_mod2)\n\n\n\nCall:\nglm(formula = low ~ ptl + smoke + ftv, family = binomial, data = birthwt)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-1.8792  -0.8818  -0.7230   1.1492   1.8132  \n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  -1.0978     0.2543  -4.316 1.59e-05 ***\nptl           0.7004     0.3251   2.154   0.0312 *  \nsmoke         0.5747     0.3291   1.746   0.0808 .  \nftv          -0.1105     0.1569  -0.704   0.4814    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 234.67  on 188  degrees of freedom\nResidual deviance: 224.27  on 185  degrees of freedom\nAIC: 232.27\n\nNumber of Fisher Scoring iterations: 4"
  },
  {
    "objectID": "posts/AdithyaParupudi_finalproject2.html#temp-model-3",
    "href": "posts/AdithyaParupudi_finalproject2.html#temp-model-3",
    "title": "Final Project - Post 2",
    "section": "Temp model 3",
    "text": "Temp model 3\nIn this model, the relationship between low birth weight and the interaction between the history of hypertension (ht) and uterine irritability (ui) is explored. Both main effects of having a history of hypertension and uterine irritability are significantly associated with low birth weight. The interaction term is not defined, which could be due to collinearity or insufficient data. However, the significant main effects suggest that both hypertension and uterine irritability are important predictors of low birth weight in this model.\n\n\nCode\nmodel3 &lt;- glm(low ~ ht * ui, data = birthwt, family = binomial)\nsummary(model3)\n\n\n\nCall:\nglm(formula = low ~ ht * ui, family = binomial, data = birthwt)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-1.3232  -0.7673  -0.7673   1.1774   1.6531  \n\nCoefficients: (1 not defined because of singularities)\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  -1.0719     0.1879  -5.703 1.17e-08 ***\nht            1.4084     0.6150   2.290   0.0220 *  \nui            1.0719     0.4221   2.539   0.0111 *  \nht:ui             NA         NA      NA       NA    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 234.67  on 188  degrees of freedom\nResidual deviance: 224.32  on 186  degrees of freedom\nAIC: 230.32\n\nNumber of Fisher Scoring iterations: 4"
  },
  {
    "objectID": "posts/AdithyaParupudi_finalproject2.html#plotting-model-1",
    "href": "posts/AdithyaParupudi_finalproject2.html#plotting-model-1",
    "title": "Final Project - Post 2",
    "section": "Plotting Model 1",
    "text": "Plotting Model 1\nFrom the plots we can conclude that:\n\nResiduals vs Fitted: This looks like a well-fitted model where a roughly horizontal line around the zero residual value\nNormal Q-Q: There are deviations observed in this line which indicate non-normality in the residuals, which could affect the model’s performance.\nScale-Location: There are visible patterns, which goes against the ideal distribution of this plot (equal distribution across a horizontal line). This means that the model is likely over or underfitting (which reiterates the point made in the Residuals vs Fitted plot).\nResiduals vs Leverage: This looks like a well-fitted model where a roughly horizontal line around the zero residual value\n\n\n\nCode\nplot(model1)"
  },
  {
    "objectID": "posts/AdithyaParupudi_finalproject2.html#plotting-model-2",
    "href": "posts/AdithyaParupudi_finalproject2.html#plotting-model-2",
    "title": "Final Project - Post 2",
    "section": "Plotting Model 2",
    "text": "Plotting Model 2\nFrom the plots we can conclude that:\n\nResiduals vs Fitted: The graph indicates that the model does not fit the data well, or there might be issues with the underlying assumptions.\nNormal Q-Q: Deviations from this line is observed in this graph which may indicate non-normality in the residuals, which could affect the model’s performance\nScale-Location: It might suggest that the model has non-constant variance in the residuals, indicating heteroskedasticity.\nResiduals vs Leverage: It is not along the straight line, hence it is not a suitable model.\n\n\n\nCode\nplot(model2)"
  },
  {
    "objectID": "posts/HW1_Guanhua_Tan.html",
    "href": "posts/HW1_Guanhua_Tan.html",
    "title": "Homework 1",
    "section": "",
    "text": "First, let’s read in the data from the Excel file:\n\n\nCode\nlibrary(readxl)\nlibrary(tidyverse)\n\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.0      ✔ purrr   0.3.5 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.4.1 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\nCode\nlibrary(ggplot2)\ndf &lt;- read_excel(\"_data/LungCapData.xls\")\nhead(df)\n\n\n# A tibble: 6 × 6\n  LungCap   Age Height Smoke Gender Caesarean\n    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;    \n1    6.48     6   62.1 no    male   no       \n2   10.1     18   74.7 yes   female no       \n3    9.55    16   69.7 no    female yes      \n4   11.1     14   71   no    male   no       \n5    4.8      5   56.9 no    male   no       \n6    6.22    11   58.7 no    female no       \n\n\nThe distribution of LungCap looks as follows:\n\n\nCode\nhist(df$LungCap)\n\n\n\n\n\nThe histogram suggests that the distribution is close to a normal distribution. Most of the observations are close to the mean. Very few observations are close to the margins (0 and 15).\n\n\n\n\n\nCode\ndf %&gt;%\n  ggplot(aes(x=Gender,y=LungCap))%+%\n  stat_boxplot(geom = \"errorbar\", # Error bars\n               width = 0.2)+\n  geom_boxplot()\n\n\n\n\n\nThe box graphic suggests that the median of male lung capacities are slightly larger than the one of female ones.\n\n\n\n\n\nCode\ndf_c &lt;- df %&gt;%\n  group_by(Smoke) %&gt;%\n  mutate(mean_lungcap=mean(LungCap))%&gt;%\n  distinct(mean_lungcap)\ndf_c \n\n\n# A tibble: 2 × 2\n# Groups:   Smoke [2]\n  Smoke mean_lungcap\n  &lt;chr&gt;        &lt;dbl&gt;\n1 no            7.77\n2 yes           8.65\n\n\nThe data indicates smokers’ lung capacities are larger than no-smokers’ ones. It runs counter to the intuition.\n\n\n\n\n\nCode\n# less than or equal to 13\ndf_d_13&lt;-df %&gt;%\n  filter(Smoke == \"yes\" & Age &lt;= 13) %&gt;%\n  mutate(mean_lungcap=mean(LungCap)) %&gt;%\n  distinct(mean_lungcap)\n\n\ndf_d_14_15 &lt;-df %&gt;%\n   filter(Smoke == \"yes\" & Age &lt;= 15 | Age &gt;= 14) %&gt;%\n  mutate(mean_lungcap=mean(LungCap)) %&gt;%\n  distinct(mean_lungcap)\n\n\ndf_d_16_17 &lt;-df %&gt;%\n   filter(Smoke == \"yes\" & Age &lt;= 17 | Age &gt;= 16) %&gt;%\n  mutate(mean_lungcap=mean(LungCap)) %&gt;%\n  distinct(mean_lungcap)\n\n\ndf_d_18 &lt;-df %&gt;%\n   filter(Smoke == \"yes\" & Age &gt;= 18) %&gt;%\n  mutate(mean_lungcap=mean(LungCap)) %&gt;%\n  distinct(mean_lungcap)\nresult &lt;-c(df_d_13, df_d_14_15, df_d_16_17, df_d_18)\nprint(result)\n\n\n$mean_lungcap\n[1] 7.201852\n\n$mean_lungcap\n[1] 9.725077\n\n$mean_lungcap\n[1] 10.00616\n\n$mean_lungcap\n[1] 10.51333\n\n\nThe data indicates that with the increase of the age, the lung capacities grows larger.\n\n\n\n\n\nCode\ndf_e_13&lt;-df %&gt;%\n  filter(Age &lt;= 13) %&gt;%\n  group_by(Smoke) %&gt;%\n  mutate(mean_lungcap=mean(LungCap)) %&gt;%\n  distinct(mean_lungcap)\n\ndf_e_14&lt;-df %&gt;%\n  filter(Age == 15 | Age == 14) %&gt;%\n  group_by(Smoke) %&gt;%\n  mutate(mean_lungcap=mean(LungCap)) %&gt;%\n  distinct(mean_lungcap)\n\n\ndf_e_16&lt;-df %&gt;%\n  filter(Age == 17 | Age == 16) %&gt;%\n  group_by(Smoke) %&gt;%\n  mutate(mean_lungcap=mean(LungCap)) %&gt;%\n  distinct(mean_lungcap)\n\n\ndf_e_18&lt;-df %&gt;%\n  filter( Age &gt;= 18) %&gt;%\n  group_by(Smoke) %&gt;%\n  mutate(mean_lungcap=mean(LungCap)) %&gt;%\n  distinct(mean_lungcap)\n\ndf_e_13\n\n\n# A tibble: 2 × 2\n# Groups:   Smoke [2]\n  Smoke mean_lungcap\n  &lt;chr&gt;        &lt;dbl&gt;\n1 no            6.36\n2 yes           7.20\n\n\nCode\ndf_e_14\n\n\n# A tibble: 2 × 2\n# Groups:   Smoke [2]\n  Smoke mean_lungcap\n  &lt;chr&gt;        &lt;dbl&gt;\n1 no            9.14\n2 yes           8.39\n\n\nCode\ndf_e_16\n\n\n# A tibble: 2 × 2\n# Groups:   Smoke [2]\n  Smoke mean_lungcap\n  &lt;chr&gt;        &lt;dbl&gt;\n1 no           10.5 \n2 yes           9.38\n\n\nCode\ndf_e_18\n\n\n# A tibble: 2 × 2\n# Groups:   Smoke [2]\n  Smoke mean_lungcap\n  &lt;chr&gt;        &lt;dbl&gt;\n1 yes           10.5\n2 no            11.1\n\n\nThe shows a big difference from the part C. Only in age group under 13, smokers have larger lung capacities than non-smokers. In other age groups, unlike what the part C suggests, non-smokers have large lung capacities that smokers."
  },
  {
    "objectID": "posts/HW1_Guanhua_Tan.html#a",
    "href": "posts/HW1_Guanhua_Tan.html#a",
    "title": "Homework 1",
    "section": "",
    "text": "First, let’s read in the data from the Excel file:\n\n\nCode\nlibrary(readxl)\nlibrary(tidyverse)\n\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.0      ✔ purrr   0.3.5 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.4.1 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\nCode\nlibrary(ggplot2)\ndf &lt;- read_excel(\"_data/LungCapData.xls\")\nhead(df)\n\n\n# A tibble: 6 × 6\n  LungCap   Age Height Smoke Gender Caesarean\n    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;    \n1    6.48     6   62.1 no    male   no       \n2   10.1     18   74.7 yes   female no       \n3    9.55    16   69.7 no    female yes      \n4   11.1     14   71   no    male   no       \n5    4.8      5   56.9 no    male   no       \n6    6.22    11   58.7 no    female no       \n\n\nThe distribution of LungCap looks as follows:\n\n\nCode\nhist(df$LungCap)\n\n\n\n\n\nThe histogram suggests that the distribution is close to a normal distribution. Most of the observations are close to the mean. Very few observations are close to the margins (0 and 15)."
  },
  {
    "objectID": "posts/HW1_Guanhua_Tan.html#b-compare-the-probability-distribution-of-the-lungcap-with-respect-to-males-and-females",
    "href": "posts/HW1_Guanhua_Tan.html#b-compare-the-probability-distribution-of-the-lungcap-with-respect-to-males-and-females",
    "title": "Homework 1",
    "section": "",
    "text": "Code\ndf %&gt;%\n  ggplot(aes(x=Gender,y=LungCap))%+%\n  stat_boxplot(geom = \"errorbar\", # Error bars\n               width = 0.2)+\n  geom_boxplot()\n\n\n\n\n\nThe box graphic suggests that the median of male lung capacities are slightly larger than the one of female ones."
  },
  {
    "objectID": "posts/HW1_Guanhua_Tan.html#c-compare-the-mean-lung-capacities-for-smokers-and-non-smokers.-does-it-make-sense",
    "href": "posts/HW1_Guanhua_Tan.html#c-compare-the-mean-lung-capacities-for-smokers-and-non-smokers.-does-it-make-sense",
    "title": "Homework 1",
    "section": "",
    "text": "Code\ndf_c &lt;- df %&gt;%\n  group_by(Smoke) %&gt;%\n  mutate(mean_lungcap=mean(LungCap))%&gt;%\n  distinct(mean_lungcap)\ndf_c \n\n\n# A tibble: 2 × 2\n# Groups:   Smoke [2]\n  Smoke mean_lungcap\n  &lt;chr&gt;        &lt;dbl&gt;\n1 no            7.77\n2 yes           8.65\n\n\nThe data indicates smokers’ lung capacities are larger than no-smokers’ ones. It runs counter to the intuition."
  },
  {
    "objectID": "posts/HW1_Guanhua_Tan.html#d-examine-the-relationship-between-smoking-and-lung-capacity-within-age-groups-less-than-or-equal-to-13-14-to-15-16-to-17-and-greater-than-or-equal-to-18.",
    "href": "posts/HW1_Guanhua_Tan.html#d-examine-the-relationship-between-smoking-and-lung-capacity-within-age-groups-less-than-or-equal-to-13-14-to-15-16-to-17-and-greater-than-or-equal-to-18.",
    "title": "Homework 1",
    "section": "",
    "text": "Code\n# less than or equal to 13\ndf_d_13&lt;-df %&gt;%\n  filter(Smoke == \"yes\" & Age &lt;= 13) %&gt;%\n  mutate(mean_lungcap=mean(LungCap)) %&gt;%\n  distinct(mean_lungcap)\n\n\ndf_d_14_15 &lt;-df %&gt;%\n   filter(Smoke == \"yes\" & Age &lt;= 15 | Age &gt;= 14) %&gt;%\n  mutate(mean_lungcap=mean(LungCap)) %&gt;%\n  distinct(mean_lungcap)\n\n\ndf_d_16_17 &lt;-df %&gt;%\n   filter(Smoke == \"yes\" & Age &lt;= 17 | Age &gt;= 16) %&gt;%\n  mutate(mean_lungcap=mean(LungCap)) %&gt;%\n  distinct(mean_lungcap)\n\n\ndf_d_18 &lt;-df %&gt;%\n   filter(Smoke == \"yes\" & Age &gt;= 18) %&gt;%\n  mutate(mean_lungcap=mean(LungCap)) %&gt;%\n  distinct(mean_lungcap)\nresult &lt;-c(df_d_13, df_d_14_15, df_d_16_17, df_d_18)\nprint(result)\n\n\n$mean_lungcap\n[1] 7.201852\n\n$mean_lungcap\n[1] 9.725077\n\n$mean_lungcap\n[1] 10.00616\n\n$mean_lungcap\n[1] 10.51333\n\n\nThe data indicates that with the increase of the age, the lung capacities grows larger."
  },
  {
    "objectID": "posts/HW1_Guanhua_Tan.html#e-compare-the-lung-capacities-for-smokers-and-non-smokers-within-each-age-group.-is-your-answer-different-from-the-one-in-part-c.-what-could-possibly-be-going-on-here",
    "href": "posts/HW1_Guanhua_Tan.html#e-compare-the-lung-capacities-for-smokers-and-non-smokers-within-each-age-group.-is-your-answer-different-from-the-one-in-part-c.-what-could-possibly-be-going-on-here",
    "title": "Homework 1",
    "section": "",
    "text": "Code\ndf_e_13&lt;-df %&gt;%\n  filter(Age &lt;= 13) %&gt;%\n  group_by(Smoke) %&gt;%\n  mutate(mean_lungcap=mean(LungCap)) %&gt;%\n  distinct(mean_lungcap)\n\ndf_e_14&lt;-df %&gt;%\n  filter(Age == 15 | Age == 14) %&gt;%\n  group_by(Smoke) %&gt;%\n  mutate(mean_lungcap=mean(LungCap)) %&gt;%\n  distinct(mean_lungcap)\n\n\ndf_e_16&lt;-df %&gt;%\n  filter(Age == 17 | Age == 16) %&gt;%\n  group_by(Smoke) %&gt;%\n  mutate(mean_lungcap=mean(LungCap)) %&gt;%\n  distinct(mean_lungcap)\n\n\ndf_e_18&lt;-df %&gt;%\n  filter( Age &gt;= 18) %&gt;%\n  group_by(Smoke) %&gt;%\n  mutate(mean_lungcap=mean(LungCap)) %&gt;%\n  distinct(mean_lungcap)\n\ndf_e_13\n\n\n# A tibble: 2 × 2\n# Groups:   Smoke [2]\n  Smoke mean_lungcap\n  &lt;chr&gt;        &lt;dbl&gt;\n1 no            6.36\n2 yes           7.20\n\n\nCode\ndf_e_14\n\n\n# A tibble: 2 × 2\n# Groups:   Smoke [2]\n  Smoke mean_lungcap\n  &lt;chr&gt;        &lt;dbl&gt;\n1 no            9.14\n2 yes           8.39\n\n\nCode\ndf_e_16\n\n\n# A tibble: 2 × 2\n# Groups:   Smoke [2]\n  Smoke mean_lungcap\n  &lt;chr&gt;        &lt;dbl&gt;\n1 no           10.5 \n2 yes           9.38\n\n\nCode\ndf_e_18\n\n\n# A tibble: 2 × 2\n# Groups:   Smoke [2]\n  Smoke mean_lungcap\n  &lt;chr&gt;        &lt;dbl&gt;\n1 yes           10.5\n2 no            11.1\n\n\nThe shows a big difference from the part C. Only in age group under 13, smokers have larger lung capacities than non-smokers. In other age groups, unlike what the part C suggests, non-smokers have large lung capacities that smokers."
  },
  {
    "objectID": "posts/HW1_Guanhua_Tan.html#a-what-is-the-probability-that-a-randomly-selected-inmate-has-exactly-2-prior-convictions",
    "href": "posts/HW1_Guanhua_Tan.html#a-what-is-the-probability-that-a-randomly-selected-inmate-has-exactly-2-prior-convictions",
    "title": "Homework 1",
    "section": "a) What is the probability that a randomly selected inmate has exactly 2 prior convictions?",
    "text": "a) What is the probability that a randomly selected inmate has exactly 2 prior convictions?\n\n\nCode\nc &lt;- 160/810\nc\n\n\n[1] 0.1975309\n\n\nThe probability is 19.8%."
  },
  {
    "objectID": "posts/HW1_Guanhua_Tan.html#b-what-is-the-probability-that-a-randomly-selected-inmate-has-fewer-than-2-prior-convictions",
    "href": "posts/HW1_Guanhua_Tan.html#b-what-is-the-probability-that-a-randomly-selected-inmate-has-fewer-than-2-prior-convictions",
    "title": "Homework 1",
    "section": "b) What is the probability that a randomly selected inmate has fewer than 2 prior convictions?",
    "text": "b) What is the probability that a randomly selected inmate has fewer than 2 prior convictions?\n\n\nCode\nc&lt;-(128+434)/810\nc\n\n\n[1] 0.6938272\n\n\nThe probability is 69.4%."
  },
  {
    "objectID": "posts/HW1_Guanhua_Tan.html#c-what-is-the-probability-that-a-randomly-selected-inmate-has-2-or-fewer-prior-convictions",
    "href": "posts/HW1_Guanhua_Tan.html#c-what-is-the-probability-that-a-randomly-selected-inmate-has-2-or-fewer-prior-convictions",
    "title": "Homework 1",
    "section": "c) What is the probability that a randomly selected inmate has 2 or fewer prior convictions?",
    "text": "c) What is the probability that a randomly selected inmate has 2 or fewer prior convictions?\n\n\nCode\nc &lt;- (434+160+128)/810\nc\n\n\n[1] 0.891358\n\n\nThe probability is 89.1%."
  },
  {
    "objectID": "posts/HW1_Guanhua_Tan.html#d-what-is-the-probability-that-a-randomly-selected-inmate-has-more-than-2-prior-convictions",
    "href": "posts/HW1_Guanhua_Tan.html#d-what-is-the-probability-that-a-randomly-selected-inmate-has-more-than-2-prior-convictions",
    "title": "Homework 1",
    "section": "d) What is the probability that a randomly selected inmate has more than 2 prior convictions?",
    "text": "d) What is the probability that a randomly selected inmate has more than 2 prior convictions?\n\n\nCode\nc&lt;-(64+24)/810\nc\n\n\n[1] 0.108642\n\n\nThe probability is 10.9%."
  },
  {
    "objectID": "posts/HW1_Guanhua_Tan.html#e-what-is-the-expected-value1-for-the-number-of-prior-convictions",
    "href": "posts/HW1_Guanhua_Tan.html#e-what-is-the-expected-value1-for-the-number-of-prior-convictions",
    "title": "Homework 1",
    "section": "e) What is the expected value1 for the number of prior convictions?",
    "text": "e) What is the expected value1 for the number of prior convictions?\n\n\nCode\nvals&lt;-c(0,1,2,3,4)\nprobs&lt;-c(128/810, 434/810, 160/810, 64/801, 24/810)\nexv&lt;-weighted.mean(vals, probs)\nexv\n\n\n[1] 1.28794\n\n\nThe expected value is 1.29.\n\nCalculate the variance and the standard deviation for the Prior Convictions.\n\n\n\nCode\nvar &lt;- sum((vals-exv)^2*probs)\nvar\n\n\n[1] 0.8588399\n\n\nCode\nsd &lt;- sqrt(var)\nsd\n\n\n[1] 0.9267361\n\n\nThe variance is 0.8588. The standard deviation is 0.9267."
  },
  {
    "objectID": "posts/AdithyaParupudi_finalproject1.html",
    "href": "posts/AdithyaParupudi_finalproject1.html",
    "title": "Final Project - Post 1",
    "section": "",
    "text": "Description\nThe birthwt dataset(part of the MASS package) is a widely-used data collection in the field of medical statistics and public health research, focusing on the factors influencing birth weight in newborns. It contains records of various factors such as maternal age, weight, race, smoking habits during pregnancy, and the number of prenatal visits, among others. By analyzing the relationships between these variables and birth weight, researchers and medical professionals can identify potential risk factors, better understand the determinants of low birth weight, and develop effective interventions to improve maternal and neonatal health outcomes.\n\n\nResearch Questions\n\nDoes maternal smoking during pregnancy have a significant impact on newborn birth weight?\nIs there a correlation between maternal age and the number of prenatal visits?\nDo racial differences influence birth weight, when controlling for other factors such as maternal age, weight, and smoking habits?\n\n\n\nHypothesis\n\nNull Hypothesis (H0): There is no significant relationship between maternal smoking during pregnancy and newborn birth weight, after controlling for other factors such as maternal age, weight, and race.\nThere is a significant relationship between maternal smoking during pregnancy and newborn birth weight.\nThe relationship between maternal age and newborn birth weight is moderated by the number of prenatal visits, such that the positive association between maternal age and birth weight is stronger for mothers with a higher number of prenatal visits.\n\n\n\nDescriptive Statistics\nThe birthwt data frame has 189 rows and 10 columns. The data were collected at Baystate Medical Center, Springfield, Mass during 1986.\n\nlow: an indicator of birth weight less than 2.5 kg.\nage: mother’s age in years.\nlwt: mother’s weight in pounds at last menstrual period.\nrace: mother’s race (1 = white, 2 = black, 3 = other).\nsmoke: smoking status during pregnancy.\nptl: number of previous premature labors.\nht: history of hypertension.\nui: presence of uterine irritability.\niv: number of physician visits during the first trimester.\nbwt: birth weight in grams.\n\n\n\nExploratory Data Analysis\n\n\nCode\n# column names\ncolnames(birthwt)\n\n\n [1] \"low\"   \"age\"   \"lwt\"   \"race\"  \"smoke\" \"ptl\"   \"ht\"    \"ui\"    \"ftv\"  \n[10] \"bwt\"  \n\n\n\n\nCode\nglimpse(birthwt)\n\n\nRows: 189\nColumns: 10\n$ low   &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ age   &lt;int&gt; 19, 33, 20, 21, 18, 21, 22, 17, 29, 26, 19, 19, 22, 30, 18, 18, …\n$ lwt   &lt;int&gt; 182, 155, 105, 108, 107, 124, 118, 103, 123, 113, 95, 150, 95, 1…\n$ race  &lt;int&gt; 2, 3, 1, 1, 1, 3, 1, 3, 1, 1, 3, 3, 3, 3, 1, 1, 2, 1, 3, 1, 3, 1…\n$ smoke &lt;int&gt; 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0…\n$ ptl   &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0…\n$ ht    &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ ui    &lt;int&gt; 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1…\n$ ftv   &lt;int&gt; 0, 3, 1, 2, 0, 0, 1, 1, 1, 0, 0, 1, 0, 2, 0, 0, 0, 3, 0, 1, 2, 3…\n$ bwt   &lt;int&gt; 2523, 2551, 2557, 2594, 2600, 2622, 2637, 2637, 2663, 2665, 2722…\n\n\n\n\nCode\nhead(birthwt)\n\n\n   low age lwt race smoke ptl ht ui ftv  bwt\n85   0  19 182    2     0   0  0  1   0 2523\n86   0  33 155    3     0   0  0  0   3 2551\n87   0  20 105    1     1   0  0  0   1 2557\n88   0  21 108    1     1   0  0  1   2 2594\n89   0  18 107    1     1   0  0  1   0 2600\n91   0  21 124    3     0   0  0  0   0 2622\n\n\n\n\nReferences\nVenables, W. N. and Ripley, B. D. (2002) Modern Applied Statistics with S. Fourth edition. Springer"
  },
  {
    "objectID": "posts/HW2_XiaoyanHu.html",
    "href": "posts/HW2_XiaoyanHu.html",
    "title": "Homework 2",
    "section": "",
    "text": "Code\nlibrary(tidyr)\nlibrary(dplyr)\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nCode\nlibrary(readxl)\nlibrary(ggplot2)\n\n\n\nQuestion 1\n\nThe time between the date a patient was recommended for heart surgery and the surgery date for cardiac patients in Ontario was collected by the Cardiac Care Network (“Wait Times Data Guide,” Ministry of Health and Long-Term Care, Ontario, Canada, 2006). The sample mean and sample standard deviation for wait times (in days) of patients for two cardiac procedures are given in the accompanying table. Assume that the sample is representative of the Ontario population\n\nConstruct the 90% confidence interval to estimate the actual mean wait time for each of the two procedures. Is the confidence interval narrower for angiography or bypass surgery?\n\n\nCode\n# mean mean()\nMean1&lt;-19\nMean2&lt;-18\n# standard deviation sd()\nsd1&lt;-10\nsd2&lt;-9\n# sample size\nn1&lt;-539\nn2&lt;-847\n# standard error sd/sqrt(n)\nse1&lt;-sd1/sqrt(n1)\nse2&lt;-sd2/sqrt(n2)\nse1\n\n\n[1] 0.4307305\n\n\nCode\nse2\n\n\n[1] 0.3092437\n\n\nCode\n# t-value\n\n# tail_area&lt;-(1-confidence_level)/2\nTA&lt;-(1-0.9)/2\n#t score &lt;-qt(p=1-tail_area,df=s_size-1 )\ntscore1&lt;-qt(p=1-TA, df = n1-1)\ntscore2&lt;-qt(p=1-TA, df = n2-1)\ntscore1\n\n\n[1] 1.647691\n\n\nCode\ntscore2\n\n\n[1] 1.646657\n\n\nCode\n#CI &lt;-c(s_mean-t_score*SE,s_mean+t_score*SE )\nCI1&lt;-c(Mean1-tscore1*se1, Mean1+tscore1*se1)\nCI2&lt;-c(Mean2-tscore2*se2, Mean2+tscore2*se2)\nCI1\n\n\n[1] 18.29029 19.70971\n\n\nCode\nCI2\n\n\n[1] 17.49078 18.50922\n\n\nCode\nDiffCI1&lt;-18.29029- 19.70971\nDiffCI2&lt;-17.49078-18.50922\nDiffCI1\n\n\n[1] -1.41942\n\n\nCode\nDiffCI2\n\n\n[1] -1.01844\n\n\n\n\nQuestion 2\nA survey of 1031 adult Americans was carried out by the National Center for Public Policy. Assume that the sample is representative of adult Americans. Among those surveyed, 567 believed that college education is essential for success. Find the point estimate, p, of the proportion of all adult Americans who believe that a college education is essential for success. Construct and interpret a 95% confidence interval for p.\n\n\nCode\n#p \np1&lt;-567/1031\n#sample size \nn&lt;-1031\n#alpha = 0.05\nse&lt;-sqrt((p1*(1-p1))/n)\n#z score1.96\nCI&lt;-c(p1-1.96*se,p1+1.96*se)\nCI\n\n\n[1] 0.5195833 0.5803197\n\n\n\n\nQuestion 3\nSuppose that the financial aid office of UMass Amherst seeks to estimate the mean cost of textbooks per semester for students. The estimate will be useful if it is within $5 of the true population mean (i.e. they want the confidence interval to have a length of $10 or less). The financial aid office is pretty sure that the amount spent on books varies widely, with most values between $30 and $200. They think that the population standard deviation is about a quarter of this range (in other words, you can assume they know the population standard deviation).Assuming the significance level to be 5%, what should be the size of the sample\n\n\nCode\n#z*sd/sqrt(n)=5\n#population sd = (200-30)/4 = 42.5\n#alpha = 0.05\n#n=(z*sd/5)^2\n#SD = 5\nn&lt;-(1.96*42.5/5)^2\n\nn\n\n\n[1] 277.5556\n\n\nCode\nqnorm(0.975)\n\n\n[1] 1.959964\n\n\n\n\nQuestion 4\nAccording to a union agreement, the mean income for all senior-level workers in a large service company equals $500 per week. A representative of a women’s group decides to analyze whether the mean income μ for female employees matches this norm. For a random sample of nine female employees, ȳ = $410 and s = 90 A. Test whether the mean income of female employees differs from $500 per week. Include assumptions, hypotheses, test statistic, and P-value. Interpret the result.\n\n\nCode\n#assumptions\n#hypothesis\n  #h0:u=500\n  #ha:u≠500\n#t-test\nu4&lt;-500\ny4&lt;-410\nsd4&lt;-90\nn4&lt;-9\n\nt4&lt;-(y4-u4)/(sd4/sqrt(n4))\nt4\n\n\n[1] -3\n\n\nCode\n#pvalue\np4= 2*pt(t4, df = n4-1)\np4\n\n\n[1] 0.01707168\n\n\nCode\n#P3&lt;- P-value = P(|t| &gt; |t3|) = P(t &lt; -t3) + P(t &gt; t3)\n\n\nreject the null hypothesis\nB. Report the P-value for Ha: μ &lt; 500. Interpret. B.\n\n\nCode\n#hypothesis\n  #h0:u&gt;=500\n  #ha:u&lt;500\np4.2&lt;-pt(t4, df = n4-1)\np4.2\n\n\n[1] 0.008535841\n\n\nreject ha C. Report and interpret the P-value for Ha: μ &gt; 500. (Hint: The P-values for the two possible one-sided tests must sum to 1.\n\n\nCode\n#hypothesis\n  #h0:u&lt;=500\n  #ha:u&gt;500\np4.3&lt;-pt(t4, df = n4-1)\n1-p4.3\n\n\n[1] 0.9914642\n\n\n\n\nQuestion5.\nJones and Smith separately conduct studies to test H0: μ = 500 against Ha: μ ≠ 500, each with n = 1000. Jones gets ȳ = 519.5, with se = 10.0. Smith gets ȳ = 519.7, with se = 10.0. A. Show that t = 1.95 and P-value = 0.051 for Jones. Show that t = 1.97 and P-value = 0.049 for Smith.\n\n\nCode\n#H0: μ = 500, Ha: μ ≠ 500\nu5&lt;-500\nn5&lt;-1000\nn5&lt;-1000\ny51&lt;-519.5\nse51&lt;-10\ny52&lt;-519.7\nse52&lt;-10\n\nt51&lt;-(y51-u5)/se51\nt51\n\n\n[1] 1.95\n\n\nCode\nt52&lt;-(y52-u5)/se52\nt52\n\n\n[1] 1.97\n\n\nCode\np51&lt;-2*pt(t51, df=n5-1,lower.tail = F)\np52&lt;-2*pt(t52, df=n5-1,lower.tail = F)\np51\n\n\n[1] 0.05145555\n\n\nCode\np52\n\n\n[1] 0.04911426\n\n\nwhy lower tail is false here?\nB. Using α = 0.05, for each study indicate whether the result is “statistically significant.”\nJones is not significant but Smith’s is\nC. Using this example, explain the misleading aspects of reporting the result of a test as “P ≤ 0.05” versus “P &gt; 0.05,” or as “reject H0” versus “Do not reject H0,” without reporting the actual P-value.\nIt is important to have a significant level at 0.05 and report the accurate and true p-value to evaluate the siginifance.\n\n\nQuestion6.\nA school nurse wants to determine whether age is a factor in whether children choose a healthy snack after school. She conducts a survey of 300 middle school students, with the results below. Test at α = 0.05 the claim that the proportion who choose a healthy snack differs by grade level. What is the null hypothesis? Which test should we use? What is the conclusion? Grade level 6th grade 7th grade 8th grade Healthy snack 31 43 51 Unhealthy snack 69 57 49 hypothesis:there is a relationship between chosing snacks and grade\n\n\nCode\ngrade_level &lt;- c(rep(\"6th grade\", 100), rep(\"7th grade\", 100), rep(\"8th grade\", 100))\nsnack &lt;- c(rep(\"healthy snack\", 31), rep(\"unhealthy snack\", 69), rep(\"healthy snack\", 43),\n           rep(\"unhealthy snack\", 57), rep(\"healthy snack\", 51), rep(\"unhealthy snack\", 49))\nsnack_data &lt;- data.frame(grade_level, snack)\nsnack_data\n\n\n    grade_level           snack\n1     6th grade   healthy snack\n2     6th grade   healthy snack\n3     6th grade   healthy snack\n4     6th grade   healthy snack\n5     6th grade   healthy snack\n6     6th grade   healthy snack\n7     6th grade   healthy snack\n8     6th grade   healthy snack\n9     6th grade   healthy snack\n10    6th grade   healthy snack\n11    6th grade   healthy snack\n12    6th grade   healthy snack\n13    6th grade   healthy snack\n14    6th grade   healthy snack\n15    6th grade   healthy snack\n16    6th grade   healthy snack\n17    6th grade   healthy snack\n18    6th grade   healthy snack\n19    6th grade   healthy snack\n20    6th grade   healthy snack\n21    6th grade   healthy snack\n22    6th grade   healthy snack\n23    6th grade   healthy snack\n24    6th grade   healthy snack\n25    6th grade   healthy snack\n26    6th grade   healthy snack\n27    6th grade   healthy snack\n28    6th grade   healthy snack\n29    6th grade   healthy snack\n30    6th grade   healthy snack\n31    6th grade   healthy snack\n32    6th grade unhealthy snack\n33    6th grade unhealthy snack\n34    6th grade unhealthy snack\n35    6th grade unhealthy snack\n36    6th grade unhealthy snack\n37    6th grade unhealthy snack\n38    6th grade unhealthy snack\n39    6th grade unhealthy snack\n40    6th grade unhealthy snack\n41    6th grade unhealthy snack\n42    6th grade unhealthy snack\n43    6th grade unhealthy snack\n44    6th grade unhealthy snack\n45    6th grade unhealthy snack\n46    6th grade unhealthy snack\n47    6th grade unhealthy snack\n48    6th grade unhealthy snack\n49    6th grade unhealthy snack\n50    6th grade unhealthy snack\n51    6th grade unhealthy snack\n52    6th grade unhealthy snack\n53    6th grade unhealthy snack\n54    6th grade unhealthy snack\n55    6th grade unhealthy snack\n56    6th grade unhealthy snack\n57    6th grade unhealthy snack\n58    6th grade unhealthy snack\n59    6th grade unhealthy snack\n60    6th grade unhealthy snack\n61    6th grade unhealthy snack\n62    6th grade unhealthy snack\n63    6th grade unhealthy snack\n64    6th grade unhealthy snack\n65    6th grade unhealthy snack\n66    6th grade unhealthy snack\n67    6th grade unhealthy snack\n68    6th grade unhealthy snack\n69    6th grade unhealthy snack\n70    6th grade unhealthy snack\n71    6th grade unhealthy snack\n72    6th grade unhealthy snack\n73    6th grade unhealthy snack\n74    6th grade unhealthy snack\n75    6th grade unhealthy snack\n76    6th grade unhealthy snack\n77    6th grade unhealthy snack\n78    6th grade unhealthy snack\n79    6th grade unhealthy snack\n80    6th grade unhealthy snack\n81    6th grade unhealthy snack\n82    6th grade unhealthy snack\n83    6th grade unhealthy snack\n84    6th grade unhealthy snack\n85    6th grade unhealthy snack\n86    6th grade unhealthy snack\n87    6th grade unhealthy snack\n88    6th grade unhealthy snack\n89    6th grade unhealthy snack\n90    6th grade unhealthy snack\n91    6th grade unhealthy snack\n92    6th grade unhealthy snack\n93    6th grade unhealthy snack\n94    6th grade unhealthy snack\n95    6th grade unhealthy snack\n96    6th grade unhealthy snack\n97    6th grade unhealthy snack\n98    6th grade unhealthy snack\n99    6th grade unhealthy snack\n100   6th grade unhealthy snack\n101   7th grade   healthy snack\n102   7th grade   healthy snack\n103   7th grade   healthy snack\n104   7th grade   healthy snack\n105   7th grade   healthy snack\n106   7th grade   healthy snack\n107   7th grade   healthy snack\n108   7th grade   healthy snack\n109   7th grade   healthy snack\n110   7th grade   healthy snack\n111   7th grade   healthy snack\n112   7th grade   healthy snack\n113   7th grade   healthy snack\n114   7th grade   healthy snack\n115   7th grade   healthy snack\n116   7th grade   healthy snack\n117   7th grade   healthy snack\n118   7th grade   healthy snack\n119   7th grade   healthy snack\n120   7th grade   healthy snack\n121   7th grade   healthy snack\n122   7th grade   healthy snack\n123   7th grade   healthy snack\n124   7th grade   healthy snack\n125   7th grade   healthy snack\n126   7th grade   healthy snack\n127   7th grade   healthy snack\n128   7th grade   healthy snack\n129   7th grade   healthy snack\n130   7th grade   healthy snack\n131   7th grade   healthy snack\n132   7th grade   healthy snack\n133   7th grade   healthy snack\n134   7th grade   healthy snack\n135   7th grade   healthy snack\n136   7th grade   healthy snack\n137   7th grade   healthy snack\n138   7th grade   healthy snack\n139   7th grade   healthy snack\n140   7th grade   healthy snack\n141   7th grade   healthy snack\n142   7th grade   healthy snack\n143   7th grade   healthy snack\n144   7th grade unhealthy snack\n145   7th grade unhealthy snack\n146   7th grade unhealthy snack\n147   7th grade unhealthy snack\n148   7th grade unhealthy snack\n149   7th grade unhealthy snack\n150   7th grade unhealthy snack\n151   7th grade unhealthy snack\n152   7th grade unhealthy snack\n153   7th grade unhealthy snack\n154   7th grade unhealthy snack\n155   7th grade unhealthy snack\n156   7th grade unhealthy snack\n157   7th grade unhealthy snack\n158   7th grade unhealthy snack\n159   7th grade unhealthy snack\n160   7th grade unhealthy snack\n161   7th grade unhealthy snack\n162   7th grade unhealthy snack\n163   7th grade unhealthy snack\n164   7th grade unhealthy snack\n165   7th grade unhealthy snack\n166   7th grade unhealthy snack\n167   7th grade unhealthy snack\n168   7th grade unhealthy snack\n169   7th grade unhealthy snack\n170   7th grade unhealthy snack\n171   7th grade unhealthy snack\n172   7th grade unhealthy snack\n173   7th grade unhealthy snack\n174   7th grade unhealthy snack\n175   7th grade unhealthy snack\n176   7th grade unhealthy snack\n177   7th grade unhealthy snack\n178   7th grade unhealthy snack\n179   7th grade unhealthy snack\n180   7th grade unhealthy snack\n181   7th grade unhealthy snack\n182   7th grade unhealthy snack\n183   7th grade unhealthy snack\n184   7th grade unhealthy snack\n185   7th grade unhealthy snack\n186   7th grade unhealthy snack\n187   7th grade unhealthy snack\n188   7th grade unhealthy snack\n189   7th grade unhealthy snack\n190   7th grade unhealthy snack\n191   7th grade unhealthy snack\n192   7th grade unhealthy snack\n193   7th grade unhealthy snack\n194   7th grade unhealthy snack\n195   7th grade unhealthy snack\n196   7th grade unhealthy snack\n197   7th grade unhealthy snack\n198   7th grade unhealthy snack\n199   7th grade unhealthy snack\n200   7th grade unhealthy snack\n201   8th grade   healthy snack\n202   8th grade   healthy snack\n203   8th grade   healthy snack\n204   8th grade   healthy snack\n205   8th grade   healthy snack\n206   8th grade   healthy snack\n207   8th grade   healthy snack\n208   8th grade   healthy snack\n209   8th grade   healthy snack\n210   8th grade   healthy snack\n211   8th grade   healthy snack\n212   8th grade   healthy snack\n213   8th grade   healthy snack\n214   8th grade   healthy snack\n215   8th grade   healthy snack\n216   8th grade   healthy snack\n217   8th grade   healthy snack\n218   8th grade   healthy snack\n219   8th grade   healthy snack\n220   8th grade   healthy snack\n221   8th grade   healthy snack\n222   8th grade   healthy snack\n223   8th grade   healthy snack\n224   8th grade   healthy snack\n225   8th grade   healthy snack\n226   8th grade   healthy snack\n227   8th grade   healthy snack\n228   8th grade   healthy snack\n229   8th grade   healthy snack\n230   8th grade   healthy snack\n231   8th grade   healthy snack\n232   8th grade   healthy snack\n233   8th grade   healthy snack\n234   8th grade   healthy snack\n235   8th grade   healthy snack\n236   8th grade   healthy snack\n237   8th grade   healthy snack\n238   8th grade   healthy snack\n239   8th grade   healthy snack\n240   8th grade   healthy snack\n241   8th grade   healthy snack\n242   8th grade   healthy snack\n243   8th grade   healthy snack\n244   8th grade   healthy snack\n245   8th grade   healthy snack\n246   8th grade   healthy snack\n247   8th grade   healthy snack\n248   8th grade   healthy snack\n249   8th grade   healthy snack\n250   8th grade   healthy snack\n251   8th grade   healthy snack\n252   8th grade unhealthy snack\n253   8th grade unhealthy snack\n254   8th grade unhealthy snack\n255   8th grade unhealthy snack\n256   8th grade unhealthy snack\n257   8th grade unhealthy snack\n258   8th grade unhealthy snack\n259   8th grade unhealthy snack\n260   8th grade unhealthy snack\n261   8th grade unhealthy snack\n262   8th grade unhealthy snack\n263   8th grade unhealthy snack\n264   8th grade unhealthy snack\n265   8th grade unhealthy snack\n266   8th grade unhealthy snack\n267   8th grade unhealthy snack\n268   8th grade unhealthy snack\n269   8th grade unhealthy snack\n270   8th grade unhealthy snack\n271   8th grade unhealthy snack\n272   8th grade unhealthy snack\n273   8th grade unhealthy snack\n274   8th grade unhealthy snack\n275   8th grade unhealthy snack\n276   8th grade unhealthy snack\n277   8th grade unhealthy snack\n278   8th grade unhealthy snack\n279   8th grade unhealthy snack\n280   8th grade unhealthy snack\n281   8th grade unhealthy snack\n282   8th grade unhealthy snack\n283   8th grade unhealthy snack\n284   8th grade unhealthy snack\n285   8th grade unhealthy snack\n286   8th grade unhealthy snack\n287   8th grade unhealthy snack\n288   8th grade unhealthy snack\n289   8th grade unhealthy snack\n290   8th grade unhealthy snack\n291   8th grade unhealthy snack\n292   8th grade unhealthy snack\n293   8th grade unhealthy snack\n294   8th grade unhealthy snack\n295   8th grade unhealthy snack\n296   8th grade unhealthy snack\n297   8th grade unhealthy snack\n298   8th grade unhealthy snack\n299   8th grade unhealthy snack\n300   8th grade unhealthy snack\n\n\nCode\nchisq.test(snack_data$snack,snack_data$grade_level,correct = FALSE)\n\n\n\n    Pearson's Chi-squared test\n\ndata:  snack_data$snack and snack_data$grade_level\nX-squared = 8.3383, df = 2, p-value = 0.01547\n\n\np value smaller than 0.05 means there is a relationship between grade and snack choice.\n\n\nQuestion7.\nPer-pupil costs (in thousands of dollars) for cyber charter school tuition for school districts in three areas are shown. Test the claim that there is a difference in means for the three areas, using an appropriate test. What is the null hypothesis? Which test should we use? What is the conclusion? Area 1 6.2 9.3 6.8 6.1 6.7 7.5 Area 2 7.5 8.2 8.5 8.2 7.0 9.3 Area 3 5.8 6.4 5.6 7.1 3.0 3.5\n\n\nCode\nArea &lt;- c(rep(\"Area1\", 6), rep(\"Area2\", 6), rep(\"Area3\", 6))\ncost &lt;- c(6.2, 9.3, 6.8, 6.1, 6.7, 7.5, 7.5, 8.2, 8.5, 8.2, 7.0, 9.3,\n          5.8, 6.4, 5.6, 7.1, 3.0, 3.5)\nArea_cost &lt;- data.frame(Area,cost)\nArea_cost\n\n\n    Area cost\n1  Area1  6.2\n2  Area1  9.3\n3  Area1  6.8\n4  Area1  6.1\n5  Area1  6.7\n6  Area1  7.5\n7  Area2  7.5\n8  Area2  8.2\n9  Area2  8.5\n10 Area2  8.2\n11 Area2  7.0\n12 Area2  9.3\n13 Area3  5.8\n14 Area3  6.4\n15 Area3  5.6\n16 Area3  7.1\n17 Area3  3.0\n18 Area3  3.5\n\n\nCode\nsummary(aov(cost~Area, data=Area_cost))\n\n\n            Df Sum Sq Mean Sq F value  Pr(&gt;F)   \nArea         2  25.66  12.832   8.176 0.00397 **\nResiduals   15  23.54   1.569                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "posts/HW2_DarronBunt.html",
    "href": "posts/HW2_DarronBunt.html",
    "title": "Homework 2",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\n\nWarning: package 'tidyverse' was built under R version 4.2.2\n\n\nWarning: package 'ggplot2' was built under R version 4.2.3\n\n\nWarning: package 'tibble' was built under R version 4.2.3\n\n\nWarning: package 'tidyr' was built under R version 4.2.2\n\n\nWarning: package 'readr' was built under R version 4.2.2\n\n\nWarning: package 'purrr' was built under R version 4.2.2\n\n\nWarning: package 'dplyr' was built under R version 4.2.3\n\n\nWarning: package 'stringr' was built under R version 4.2.2\n\n\nWarning: package 'forcats' was built under R version 4.2.2\n\n\nWarning: package 'lubridate' was built under R version 4.2.2\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.1     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\nQuestion 1\nThe time between the date a patient was recommended for heart surgery and the surgery date for cardiac patients in Ontario was collected by the Cardiac Care Network (“Wait Times Data Guide,” Ministry of Health and Long-Term Care, Ontario, Canada, 2006). The sample mean and sample standard deviation for wait times (in days) of patients for two cardiac procedures are given in the accompanying table. Assume that the sample is representative of the Ontario population\nConstruct the 90% confidence interval to estimate the actual mean wait time for each of the two procedures. Is the confidence interval narrower for angiography or bypass surgery?\n\n\nCode\n#Create the data frame that matches the table that was provided.\n\nWaitTimes &lt;- data.frame (\"Surgical Procedure\" = c(\"Bypass\", \"Angiography\"),\n                  \"Sample Size\" = c(539, 847),\n                  \"Mean Wait Time\" = c(19, 18),\n                  \"Standard Deviation\" = c(10,9)\n                  )\nWaitTimes\n\n\n  Surgical.Procedure Sample.Size Mean.Wait.Time Standard.Deviation\n1             Bypass         539             19                 10\n2        Angiography         847             18                  9\n\n\n\nPart 1\nConstruct the 90% confidence interval to estimate the actual mean wait time for the two procedures\nFormula for Confidence Interval = (X bar) +/- (t x s/sqrt(n))\n\n\nCode\n# For Bypass - \n\n#x-bar = mean wait time = 19\nsmean_bypass &lt;- 19 \n\n# standard deviation = 10\nssd_bypass &lt;- 10\n\n# sample size = 539\nn_bypass &lt;- 539\n\n# standard error = standard deviation / sqrt(sample size) =&gt; 10/sqrt(539)\nsterror_bypass &lt;- 10/sqrt(539) \nsterror_bypass\n\n\n[1] 0.4307305\n\n\nCode\n# confidence level = 0.9\ncl_bypass &lt;- 0.1 \n\n# calculate t score that corresponds to confidence level/sample size\nt_score_bypass &lt;- qt(1-cl_bypass/2, n_bypass-1)\nt_score_bypass\n\n\n[1] 1.647691\n\n\nCode\n#calculating the 90% confidence interval for bypass\n\nCI_bypass &lt;- c(smean_bypass - (t_score_bypass * sterror_bypass), smean_bypass + (t_score_bypass * sterror_bypass))\nCI_bypass\n\n\n[1] 18.29029 19.70971\n\n\nThe 90% Confidence Interval for Bypass is 18.3 to 19.7 days.\n\n\nCode\n# For Angiography - \n\n#x-bar = mean wait time = 18\nsmean_angio &lt;- 18 \n\n# standard deviation = 9\nssd_angio &lt;- 9\n\n# sample size = 847\nn_angio &lt;- 847\n\n# standard error = standard deviation / sqrt(sample size) =&gt; 9/sqrt(847)\nsterror_angio &lt;- 9/sqrt(847) \nsterror_angio\n\n\n[1] 0.3092437\n\n\nCode\n# confidence level = 0.9\ncl_angio &lt;- 0.1 \n\n# calculate t score that corresponds to confidence level/sample size\nt_score_angio &lt;- qt(1-cl_angio/2, n_angio-1)\nt_score_angio\n\n\n[1] 1.646657\n\n\nCode\n#Calculating the 90% confidence interval for angio\n\nCI_angio &lt;- c(smean_angio - (t_score_angio * sterror_angio), smean_angio + (t_score_angio * sterror_angio))\nCI_angio\n\n\n[1] 17.49078 18.50922\n\n\nThe 90% Confidence Interval for Angiography is 17.5 to 18.5 days.\n\n\nPart 2\nIs the confidence interval narrower for angiography or bypass surgery?\n90% CI for Bypass: 18.3 to 19.7 days - this is 1.4 days. 90% CI for Angiography: 17.5 to 18.5 days - this is 1 day.\nThe CI is narrower for Angiography.\n\n\n\nQuestion 2\nA survey of 1,031 adult Americans was carried out by the National Center for Public Policy. Assume that the sample is representative of adult Americans. Among those surveyed, 567 believed that college education is essential for success. Find the point estimate, p, of the proportion of all adult Americans who believe that a college education is essential for success.\nConstruct and interpret a 95% confidence interval for p.\n\nPart 1 -\nFind the point estimate (p) of the proportion of all adult Americans who believe that a college education is essential for success.\nThe point estimate (p) is the proportion observed in this sample and is equal to the number of respondents who believe a college education is essential for success (567) divided by the total number of respondents (1,031)\n\n\nCode\np_CollegeEd &lt;- 567/1031\np_CollegeEd\n\n\n[1] 0.5499515\n\n\np = 0.55 - approximately 55% of the sampled adult Americans believe that a college education is essential for success.\n\n\nPart 2 -\nConstruct and interpret a 95% confidence interval for p.\nWe learned about prop.test() in Tutorial 5 - the Z-Test of Proportions with Continuity Correction. Similar to a t-test, it is suited for tests about proportions.\n\n\nCode\n# to perform prop.test() we require three main arguments: x, n, and p\n\n#x = vector of counts of successes = 567\nx_CollegeEd &lt;- 567\n\n#n = vector of counts of trials = 1031\nn_CollegeEd &lt;- 1031\n\n#p = vector of probabilities of success = p_CollegeEd calculated above\n\nprop.test(x_CollegeEd, n_CollegeEd, p_CollegeEd)\n\n\n\n    1-sample proportions test without continuity correction\n\ndata:  x_CollegeEd out of n_CollegeEd, null probability p_CollegeEd\nX-squared = 6.9637e-30, df = 1, p-value = 1\nalternative hypothesis: true p is not equal to 0.5499515\n95 percent confidence interval:\n 0.5194543 0.5800778\nsample estimates:\n        p \n0.5499515 \n\n\nBased on the prop.test results, the 95% Confidence Interval for the proportion of all adult Americans who believe that a college education is essential for success is 0.52 - 0.58 (or 52% to 58%)\nWe can be 95% confident that the true population proportion falls within this range - if we were to take many random samples of the same size from the population and calculate the confidence intervals for each sample, 95% of these intervals would contain the true population proportion.\n\n\n\nQuestion 3\nSuppose that the financial aid office of UMass Amherst seeks to estimate the mean cost of textbooks per semester for students. The estimate will be useful if it is within $5 of the true population mean (i.e. they want the confidence interval to have a length of $10 or less).\nThe financial aid office is pretty sure that the amount spent on books varies widely, with most values between $30 and $200. They think that the population standard deviation is about a quarter of this range (in other words, you can assume they know the population standard deviation).\nAssuming the significance level to be 5%, what should be the size of the sample?\nIf: Margin of Error = z* (standard deviation / square root of n)\nThen we can rearrange the formula in order to solve for n: n = ((z*sd)/MoE)^2\n\n\nCode\n# find value for z-score  \nz_Amherst &lt;- qnorm(0.975)\nz_Amherst\n\n\n[1] 1.959964\n\n\nCode\n# find value for standard deviation\n\n# Most values betweeen $30-200 =&gt; range of 170\nrange_Amherst &lt;- 200-30\n\n# They think population standard deviation is about a quarter of this range =&gt; 170/4\nsd_Amherst &lt;- range_Amherst/4\nsd_Amherst\n\n\n[1] 42.5\n\n\nCode\n#Margin of Error = 5\nMOE_Amherst &lt;- 5\n\n# Solving for n\n# n = ((z*sd)/MoE)^2\n\nn_Amherst &lt;- ((z_Amherst*sd_Amherst)/MOE_Amherst)^2\nn_Amherst\n\n\n[1] 277.5454\n\n\nWe round up, so the sample size should be 278.\n\n\nQuestion 4\nAccording to a union agreement, the mean income for all senior-level workers in a large service company equals $500 per week. A representative of a women’s group decides to analyze whether the mean income μ for female employees matches this norm. For a random sample of nine female employees, ȳ = $410 and s = 90\nA. Test whether the mean income of female employees differs from $500 per week. Include assumptions, hypotheses, test statistic, and P-value. Interpret the result. B. Report the P-value for Ha: μ &lt; 500. Interpret. C. Report and interpret the P-value for Ha: μ &gt; 500. (Hint: The P-values for the two possible one-sided tests must sum to 1.\n\nPart A\nA. Test whether the mean income of female employees differs from $500 per week. Include assumptions, hypotheses, test statistic, and P-value. Interpret the result\nIn order to test this, we are going to need to do a one-sample t-test.\nAssumptions of a one-sample t-test: * normality: that the population distribution is normal * independence: that the observations in our sample are generated independently of one another.\nHypotheses:\nOur null hypotheisis is that the mean income of female employees does not differ from $500 per week.\nThe alternative hypothesis is that the mean income of female employees DOES differ from $500 per week.\nNow we need to calculate the test statistic (t-score)\nt = (m - μ) / (s / sqrt(n))\n\n\nCode\n# t = (m - μ) / (s / sqrt(n)), where:\n\n# m = sample mean (female employees) \nsamplemean_Union &lt;- 410\n\n# μ = population mean (reported for all employees)\npopulationmean_Union &lt;- 500\n\n# s = sample standard deviation (given - 90)\nsamplesd_Union &lt;- 90\n\n# n = sample size (given - 9 female employees)\nsamplen_Union &lt;- 9\n\nt_Union &lt;- (samplemean_Union - populationmean_Union) / ((samplesd_Union) / (sqrt(samplen_Union)))\nt_Union\n\n\n[1] -3\n\n\nThe t-score is -3.\nNow we need the P-value\nThe P-value is the probability of obtaining test results at least as extreme as the result actually observed, under the assumption that the null hypothesis is correct.\nTo find the p-value, we can use pt() in R. This requires the calculated t-statistic and the degrees of freedom. In a one-sample t-test, the degrees of freedom are n-1 (where n is the sample size).\nSo in this case, the degrees of freedom are 9-1, or 8.\n\n\nCode\n# find p-value\npvalueTwoTail_Union &lt;- 2*pt(q=t_Union, df=8, lower.tail=TRUE)\npvalueTwoTail_Union\n\n\n[1] 0.01707168\n\n\nThe p-value is 0.017\nInterpreting the result: The p-value is less than 0.05, so accordingly we can reject the hypothesis that the . This suggests that the true mean salary of female workers is not equal to $500.\n\n\nPart B\nB. Report the P-value for Ha: μ &lt; 500. Interpret.\nNull hypothesis is that mean income for female workers is greater than 500. The alternative hypothesis that the mean income for female workers is less than $500.\n\n\nCode\n# find lower tail of p-value\npvalueLowerTail_Union &lt;- pt(q=t_Union, df=8, lower.tail=TRUE)\npvalueLowerTail_Union\n\n\n[1] 0.008535841\n\n\nInterpreting the result: The p-value is less than 0.05, so accordingly we can reject the null hypothesis. This suggests that the true mean salary of female workers is less than $500.\n\n\nPart C\nC. Report and interpret the P-value for Ha: μ &gt; 500.\nNull hypothesis is that mean income for female workers is less than 500. The alternative hypothesis that the mean income for female workers is greater than $500.\n\n\nCode\n# find upper tail of p-value\npvalueUpperTail_Union &lt;- pt(q=t_Union, df=8, lower.tail=FALSE)\npvalueUpperTail_Union\n\n\n[1] 0.9914642\n\n\nInterpreting the result: The p-value is greater than 0.05, so we fail to reject the null hypothesis that the mean income for female workers is less than 500. This suggests that the true mean salary of female workers is less than $500.\n\n\n\nQuestion 5\n5. Jones and Smith separately conduct studies to test H0: μ = 500 against Ha: μ ≠ 500, each with n = 1000. Jones gets ȳ = 519.5, with se = 10.0. Smith gets ȳ = 519.7, with se = 10.0.\nA. Show that t = 1.95 and P-value = 0.051 for Jones. Show that t = 1.97 and P-value = 0.049 for Smith.\nB. Using α = 0.05, for each study indicate whether the result is “statistically significant.”\nC. Using this example, explain the misleading aspects of reporting the result of a test as “P ≤ 0.05” versus “P &gt; 0.05,” or as “reject H0” versus “Do not reject H0,” without reporting the actual P-value.\n\nPart A\nA. Show that t = 1.95 and P-value = 0.051 for Jones. Show that t = 1.97 and P-value = 0.049 for Smith.\nFirst we’ll confirm the t scores. The equation for calculating t score when you have the standard error is: t = (ȳ - μ) / se\n\n\nCode\n# t = (ȳ - μ) / se\n\n# ȳ = \ny_Jones &lt;- 519.5\ny_Smith &lt;- 519.7\n\n# μ = (from hypothesis)\npopulationmean_JonesSmith &lt;- 500\n\n# se = (given)\nse_Jones &lt;- 10.0\nse_Smith &lt;- 10.0\n\n# n - (given)\nn_JonesSmith &lt;- 1000\n\nt_Jones &lt;- (y_Jones - populationmean_JonesSmith) / se_Jones\nt_Jones\n\n\n[1] 1.95\n\n\nCode\nt_Smith &lt;- (y_Smith - populationmean_JonesSmith) / se_Smith\nt_Smith\n\n\n[1] 1.97\n\n\nThis confirms that the respective t scores for Jones and Smith are 1.95 and 1.97.\nNow we’ll calculate the p-values. Both t-scores are positive, so we are going to look at the upper tail.\n\n\nCode\n#find p-values\np_Jones &lt;- 2*pt(q=t_Jones, df=999, lower.tail=FALSE)\np_Jones\n\n\n[1] 0.05145555\n\n\nCode\np_Smith &lt;- 2*pt(q=t_Smith, df=999, lower.tail=FALSE)\np_Smith\n\n\n[1] 0.04911426\n\n\nThis confirms that the p-values for Jones and Smith are 0.051 and 0.049 respectively.\n\n\nPart B\nB. Using α = 0.05, for each study indicate whether the result is “statistically significant.”\nFor Jones’ study, the p-value of 0.051 is greater than α = 0.05, therefore Jones’ study is not statistically significant.\nFor Smith’s study, the p-value of 0.049 is less than α = 0.05, therefore Smith’s study is statistically significant.\n\n\nPart C\nC. Using this example, explain the misleading aspects of reporting the result of a test as “P ≤ 0.05” versus “P &gt; 0.05,” or as “reject H0” versus “Do not reject H0,” without reporting the actual P-value.\nIf only p ≤ 0.05 or p &gt; 0.05 was reported (or similarly, only whether or not we can reject the null hypothesis) and not the actual p-values, we lose a lot of context. In the case of these two studies, the difference in Jones’ and Smith’s p-values are 0.002 apart, but that made the difference between one result having statistical significance at the α = 0.05 level and the other not.\n\n\n\nQuestion 6\nA school nurse wants to determine whether age is a factor in whether children choose a healthy snack after school. She conducts a survey of 300 middle school students, with the results below. Test at α = 0.05 the claim that the proportion who choose a healthy snack differs by grade level.\nWhat is the null hypothesis? Which test should we use? What is the conclusion?\n\n\nCode\n# Replicate the data as a table\nHealthySnack &lt;- matrix(c(31, 43, 51, 69, 57, 49), nrow=2, byrow=TRUE)\ncolnames(HealthySnack) &lt;- c('Grade 6', 'Grade 7', 'Grade 8')\nrownames(HealthySnack) &lt;- c('Healthy', 'Unhealthy')\nHealthySnack &lt;- as.table(HealthySnack)\nHealthySnack\n\n\n          Grade 6 Grade 7 Grade 8\nHealthy        31      43      51\nUnhealthy      69      57      49\n\n\nWhat is the null hypothesis?\nWe are interested in the effect of grade level on snack choice.\nThe null hypothesis is that grade level does not impact whether students choose a healthy or unhealthy snack.\nThe alternative hypothesis is that grade level does impact whether students choose a healthy or unhealthy snack.\nWhich test should we use?\nWe should use the χ2 test of independence (or association), aka the chi-square test of independence/association, which is used to determine whether there is a significant association between two categorical variables.\nWhat is the conclusion?\nWe now need to compare the observed frequencies to the expected frequencies that would be observed if the variables were independent.\n\n\nCode\n# chi-square test to compare snack choice and grade level\nchisq.test(HealthySnack)\n\n\n\n    Pearson's Chi-squared test\n\ndata:  HealthySnack\nX-squared = 8.3383, df = 2, p-value = 0.01547\n\n\nWe are testing at α = 0.05 and the p-value is smaller than 0.05, which indicates that we can reject the null hypothesis that grade level does not impact snack choice; there is a relationship between grade level and choice of snack.\n\n\nQuestion 7\nPer-pupil costs (in thousands of dollars) for cyber charter school tuition for school districts in three areas are shown.\nTest the claim that there is a difference in means for the three areas, using an appropriate test. What is the null hypothesis? Which test should we use? What is the conclusion?\n\n\nCode\n# Replicating the data\nArea &lt;- c(rep(\"Area_1\", 6), rep(\"Area_2\", 6), rep(\"Area_3\", 6))\nCost &lt;- c(6.2, 9.3, 6.8, 6.1, 6.7, 7.5, 7.5, 8.2, 8.5, 8.2, 7.0, 9.3, 5.8, 6.4, 5.6, 7.1, 3.0, 3.5)\n\nCyberCharterCost &lt;- data.frame(Area,Cost)\nCyberCharterCost\n\n\n     Area Cost\n1  Area_1  6.2\n2  Area_1  9.3\n3  Area_1  6.8\n4  Area_1  6.1\n5  Area_1  6.7\n6  Area_1  7.5\n7  Area_2  7.5\n8  Area_2  8.2\n9  Area_2  8.5\n10 Area_2  8.2\n11 Area_2  7.0\n12 Area_2  9.3\n13 Area_3  5.8\n14 Area_3  6.4\n15 Area_3  5.6\n16 Area_3  7.1\n17 Area_3  3.0\n18 Area_3  3.5\n\n\nWhat is the null hypothesis?\nThe null hypothesis is that there is no difference in means for the three areas. The alternative hypothesis is that there is a difference in means for the three areas.\nWhich test should we use?\nWe should use ANOVA, specifically a one-way ANOVA. ANOVA is used to investigate differences in means, and a one-way ANOVA is used when we have several different groups of observations and are interested in finding out whethter those groups differ in terms of some outcome variable of interest.\nIn this case we are interested in whether Area 1, 2, and 3 differ in terms of per-pupil costs for cyber charter school tuition.\nTest the claim that there is a difference in means for the three areas\n\n\nCode\n# Running ANOVA in R uses aov()\n# formula =&gt; argument to specify the outcome variable and the grouping variable \n# data =&gt; the data frame that stores those variables\n# NOTE: Make sure the dependent variable is continuous and the independent variable is categorical.\n\nCyberCharterANOVA &lt;- aov(formula = Cost ~ Area, data = CyberCharterCost)\nsummary(CyberCharterANOVA)\n\n\n            Df Sum Sq Mean Sq F value  Pr(&gt;F)   \nArea         2  25.66  12.832   8.176 0.00397 **\nResiduals   15  23.54   1.569                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nWhat is the conclusion?\nThe p-value of 0.00397 is less than α = 0.05, which means that we reject the null hypothesis that there is no difference in the means for the three areas. There does indeed appear to be a difference in the per-pupil cost for cyber charter school tuition by area."
  },
  {
    "objectID": "posts/HW2_AkhileshKumarMeghwal.html",
    "href": "posts/HW2_AkhileshKumarMeghwal.html",
    "title": "Homework 2",
    "section": "",
    "text": "The time between the date a patient was recommended for heart surgery and the surgery date for cardiac patients in Ontario was collected by the Cardiac Care Network (“Wait Times Data Guide,” Ministry of Health and Long-Term Care, Ontario, Canada, 2006). The sample mean and sample standard deviation for wait times (in days) of patients for two cardiac procedures are given in the accompanying table. Assume that the sample is representative of the Ontario population\n\n\n\n\n\n\nSurgical Procedure\nSample Size\nMean Wait Time\nStandard Deviation\n\n\n\n\nBypass\n539\n19\n10\n\n\nAngiography\n847\n18\n9\n\n\n\n\n\n\nConstruct the 90% confidence interval to estimate the actual mean wait time for each of the two procedures. Is the confidence interval narrower for angiography or bypass surgery?\n\n\n\n\n\nCode\n# 90% confidence interval\n\nconfidence_level &lt;- 0.90\n\n# Tail area\n\ntail_area &lt;- (1-confidence_level)/2\n\n# t_score Calculation\n\nt_score &lt;- qt(p = 1-tail_area, df = sample_size-1)\n\n# Standard Error Calculation\n\nstandard_error &lt;- standard_deviation/sqrt(sample_size)\n\n# Confidence Interval Calculation for Bypass and Angiography\n\nconfidence_interval &lt;- c(mean_wait_time - t_score * standard_error,\n        mean_wait_time + t_score * standard_error)\n\nconfidence_interval\n\n\n[1] 18.29029 17.49078 19.70971 18.50922\n\n\n\n\n\nThe Cardiac Care Network collected data on the time between the date a patient was recommended for heart surgery and the surgery date for cardiac patients in Ontario. The sample mean and standard deviation of wait times (in days) for two cardiac procedures, bypass and angiography, were reported in a table.\nTo estimate the actual mean wait time for each of the two procedures, a 90% confidence interval was constructed using the t-distribution. The confidence interval for bypass surgery is between 18.29 and 19.71 days, while the confidence interval for angiography is between 17.49 and 18.51 days.\nThese results suggest that, with 90% confidence, the true mean wait time for bypass surgery is likely to fall between 18.29 and 19.71 days, and the true mean wait time for angiography is likely to fall between 17.49 and 18.51 days. Comparing the two confidence intervals, we can see that the interval for bypass surgery is slightly wider than the interval for angiography. This indicates that there is slightly more uncertainty in the estimate of the true mean wait time for bypass surgery than for angiography.\nOverall, the confidence intervals provide important information about the precision of the sample means as estimates of the true population means. The intervals convey the range of values within which the population mean is likely to fall with a certain level of confidence, and they can help healthcare providers and policymakers make informed decisions about resource allocation and patient care."
  },
  {
    "objectID": "posts/HW2_AkhileshKumarMeghwal.html#question-1",
    "href": "posts/HW2_AkhileshKumarMeghwal.html#question-1",
    "title": "Homework 2",
    "section": "",
    "text": "The time between the date a patient was recommended for heart surgery and the surgery date for cardiac patients in Ontario was collected by the Cardiac Care Network (“Wait Times Data Guide,” Ministry of Health and Long-Term Care, Ontario, Canada, 2006). The sample mean and sample standard deviation for wait times (in days) of patients for two cardiac procedures are given in the accompanying table. Assume that the sample is representative of the Ontario population\n\n\n\n\n\n\nSurgical Procedure\nSample Size\nMean Wait Time\nStandard Deviation\n\n\n\n\nBypass\n539\n19\n10\n\n\nAngiography\n847\n18\n9\n\n\n\n\n\n\nConstruct the 90% confidence interval to estimate the actual mean wait time for each of the two procedures. Is the confidence interval narrower for angiography or bypass surgery?\n\n\n\n\n\nCode\n# 90% confidence interval\n\nconfidence_level &lt;- 0.90\n\n# Tail area\n\ntail_area &lt;- (1-confidence_level)/2\n\n# t_score Calculation\n\nt_score &lt;- qt(p = 1-tail_area, df = sample_size-1)\n\n# Standard Error Calculation\n\nstandard_error &lt;- standard_deviation/sqrt(sample_size)\n\n# Confidence Interval Calculation for Bypass and Angiography\n\nconfidence_interval &lt;- c(mean_wait_time - t_score * standard_error,\n        mean_wait_time + t_score * standard_error)\n\nconfidence_interval\n\n\n[1] 18.29029 17.49078 19.70971 18.50922\n\n\n\n\n\nThe Cardiac Care Network collected data on the time between the date a patient was recommended for heart surgery and the surgery date for cardiac patients in Ontario. The sample mean and standard deviation of wait times (in days) for two cardiac procedures, bypass and angiography, were reported in a table.\nTo estimate the actual mean wait time for each of the two procedures, a 90% confidence interval was constructed using the t-distribution. The confidence interval for bypass surgery is between 18.29 and 19.71 days, while the confidence interval for angiography is between 17.49 and 18.51 days.\nThese results suggest that, with 90% confidence, the true mean wait time for bypass surgery is likely to fall between 18.29 and 19.71 days, and the true mean wait time for angiography is likely to fall between 17.49 and 18.51 days. Comparing the two confidence intervals, we can see that the interval for bypass surgery is slightly wider than the interval for angiography. This indicates that there is slightly more uncertainty in the estimate of the true mean wait time for bypass surgery than for angiography.\nOverall, the confidence intervals provide important information about the precision of the sample means as estimates of the true population means. The intervals convey the range of values within which the population mean is likely to fall with a certain level of confidence, and they can help healthcare providers and policymakers make informed decisions about resource allocation and patient care."
  },
  {
    "objectID": "posts/HW2_AkhileshKumarMeghwal.html#question-2",
    "href": "posts/HW2_AkhileshKumarMeghwal.html#question-2",
    "title": "Homework 2",
    "section": "Question 2",
    "text": "Question 2\nA survey of 1031 adult Americans was carried out by the National Center for Public Policy. Assume that the sample is representative of adult Americans. Among those surveyed, 567 believed that college education is essential for success. Find the point estimate, p, of the proportion of all adult Americans who believe that a college education is essential for success. Construct and interpret a 95% confidence interval for p.\n\n1-sample proportions test\n\n\nCode\nprop.test(567, 1031, conf.level = .95)\n\n\n\n    1-sample proportions test with continuity correction\n\ndata:  567 out of 1031, null probability 0.5\nX-squared = 10.091, df = 1, p-value = 0.00149\nalternative hypothesis: true p is not equal to 0.5\n95 percent confidence interval:\n 0.5189682 0.5805580\nsample estimates:\n        p \n0.5499515 \n\n\n\n\nConclusion\nThe National Center for Public Policy conducted a survey of 1031 adult Americans to determine the proportion of people who believe that a college education is essential for success. Among the 1031 adults surveyed, 567 believed that a college education is essential for success.\nUsing the prop.test() function in R with a 95% confidence level, the point estimate of the proportion of all adult Americans who believe that a college education is essential for success is 0.5499515, suggesting that 54.99% of adult Americans believe that a college education is essential for success. We can use this proportion as a point estimate of the true proportion of adult Americans who hold this belief.Additionally, the confidence interval at a 95% confidence level for this proportion is [0.5189682, 0.5805580].\nThe interpretation of this confidence interval is that if we were to repeat this survey many times and construct a confidence interval for each survey, approximately 95% of these intervals would contain the true population proportion. Therefore, we can conclude that with 95% confidence, the proportion of all adult Americans who believe that a college education is essential for success is somewhere between 0.5189682 and 0.5805580 and that a majority of adult Americans believe that a college education is essential for success, as the lower bound of the interval is above 0.5.\nSince the confidence interval does not include 0.5 (the null probability), we can conclude that the proportion of all adult Americans who believe that a college education is essential for success is significantly different from 0.5 at the 0.05 significance level."
  },
  {
    "objectID": "posts/HW2_AkhileshKumarMeghwal.html#question-3",
    "href": "posts/HW2_AkhileshKumarMeghwal.html#question-3",
    "title": "Homework 2",
    "section": "Question 3",
    "text": "Question 3\nSuppose that the financial aid office of UMass Amherst seeks to estimate the mean cost of textbooks per semester for students. The estimate will be useful if it is within $5 of the true population mean (i.e. they want the confidence interval to have a length of $10 or less). The financial aid office is pretty sure that the amount spent on books varies widely, with most values between $30 and $200. They think that the population standard deviation is about a quarter of this range (in other words, you can assume they know the population standard deviation). Assuming the significance level to be 5%, what should be the size of the sample\n\nSample Size Calculation\n\n\nCode\n# Define variables\nM_Error &lt;- 5\nstandard_deviation &lt;- (200-30)/4\nalpha &lt;- 0.05\nz_alpha &lt;- qnorm(p = 1-alpha/2, lower.tail = FALSE)\n\n\n# Calculate required sample size\n\nsample_size &lt;- ceiling(((z_alpha * standard_deviation) / M_Error)^2)\n\n\n# Required Sample Size, Round up to nearest integer\n\ncat(\"The required sample size is:\", sample_size)\n\n\nThe required sample size is: 278\n\n\n\n\nConclusion\nThe financial aid office of UMass Amherst wants to estimate the mean cost of textbooks per semester for their students. They want the estimate to be accurate within $5 of the true population mean, and the confidence interval should have a length of $10 or less. The office has some prior knowledge that the amount spent on textbooks varies widely, with most values between $30 and $200, and they assume that the population standard deviation is about a quarter of this range.\nTo determine the sample size needed for their estimation, they use a formula that takes into account the margin of error, confidence level, and estimated population standard deviation. The calculated sample size is 278, meaning that they would need to survey 278 students to estimate the mean cost of textbooks per semester within $5 with a 95% confidence interval.\nThis estimation is important for the financial aid office to determine the appropriate amount of financial assistance to provide to students for textbooks. If the estimate is inaccurate or the confidence interval is too wide, they risk providing inadequate or excessive support, which can affect the academic success and financial well-being of the students. Therefore, it is crucial for the financial aid office to carefully plan and conduct their survey to obtain reliable and useful estimates."
  },
  {
    "objectID": "posts/HW2_AkhileshKumarMeghwal.html#question-4",
    "href": "posts/HW2_AkhileshKumarMeghwal.html#question-4",
    "title": "Homework 2",
    "section": "Question 4",
    "text": "Question 4\nAccording to a union agreement, the mean income for all senior-level workers in a large service company equals $500 per week. A representative of a women’s group decides to analyze whether the mean income μ for female employees matches this norm. For a random sample of nine female employees, ȳ = $410 and s = 90\nA. Test whether the mean income of female employees differs from $500 per week. Include assumptions, hypotheses, test statistic, and P-value. Interpret the result. B. Report the P-value for Ha: μ &lt; 500. Interpret. C. Report and interpret the P-value for Ha: μ &gt; 500. (Hint: The P-values for the two possible one-sided tests must sum to 1.\n\nQuestion 4-A\nTwo assumptions for the analysis:\n\nSample is randomly selected, and\nPopulation follows a normal distribution.\n\nThe null hypothesis is that the mean income of female employees is equal to $500 per week, represented as H0: μ = 500.\nThe alternative hypothesis is that the mean income of female employees differs from $500 per week, represented as Ha: μ ≠ 500.\nWe will reject the null hypothesis if the p-value is less than or equal to 0.05, indicating that the observed sample mean of $410 is significantly different from the population mean of $500.\n\n\nTest Statistic and p value calculation\n\n\nCode\nsample_mean &lt;- 410 # sample mean\nmu &lt;- 500 # population mean\nstandard_deviation &lt;- 90 # standard deviation\nsample_size &lt;- 9 # sample size\n\n# Calculating test-statistic\n\nt_score &lt;- (sample_mean-mu)/(standard_deviation/sqrt(sample_size))\n\ncat(\"test-statistic:\", t_score, '\\n')\n\n\ntest-statistic: -3 \n\n\nCode\np_val &lt;- 2 * pt(t_score, df = sample_size - 1, lower.tail = TRUE)\n\ncat(\"p value :\", p_val)\n\n\np value : 0.01707168\n\n\n\n\nConclusion\nIn this analysis, we are investigating whether the mean income of female employees in a large service company differs from the norm of $500 per week, according to a union agreement. We have a sample of nine randomly selected female employees, with a sample mean of $410 and a sample standard deviation of $90.\nBased on our assumptions of random sampling and normal population distribution, we conduct a hypothesis test with a significance level of 0.05. Our null hypothesis is that the population mean of female employee income is equal to $500 per week, while our alternative hypothesis is that the mean income differs from $500 per week.\nOur test results in a test-statistic of -3 and a p-value of 0.01707168, which is less than the significance level of 0.05. Therefore, we reject the null hypothesis and conclude that the mean income of female employees in this service company is significantly different from $500 per week.\n\n\nQuestion 4-B\nTwo assumptions for the analysis:\n\nSample is randomly selected, and\nPopulation follows a normal distribution.\n\nThe null hypothesis is that the mean income of female employees is equal to $500 per week, represented as H0: mu = 500.\nThe alternative hypothesis is that the mean income of female employees is less than $500 per week, represented as Ha: mu &lt; 500.\nWe will reject the null hypothesis if the p-value is less than 0.05, indicating that the observed sample mean of $410 is significantly lower than the population mean of $500.\n\n\np-value calculation\n\n\nCode\np &lt;- pt(t_score, sample_size-1, lower.tail = TRUE)\np\n\n\n[1] 0.008535841\n\n\n\n\nConclusion\nPerformed one-tailed t-test to test the hypothesis. The p-value associated with this test statistic is 0.008535841.\nSince the p-value is less than the significance level of 0.05, we reject the null hypothesis and conclude that there is sufficient evidence to support the claim that the mean income of female employees is lower than $500 per week. This implies that female employees in this service company are paid less than the senior-level workers as per the union agreement.\n\n\nQuestion 4-C\nTwo assumptions for the analysis:\n\nSample is randomly selected, and\nPopulation follows a normal distribution.\n\nThe null hypothesis is that the mean income of female employees is equal to $500 per week, represented as H0: mu = 500.\nThe alternative hypothesis is that the mean income of female employees is greater than $500 per week, represented as Ha: mu &gt; 500.\nWe will reject the null hypothesis if the p-value is less than 0.05, indicating that the observed sample mean of $410 is significantly higher than the population mean of $500.\n\n\np-value calculation\n\n\nCode\np &lt;- pt(t_score, sample_size-1, lower.tail = FALSE)\np\n\n\n[1] 0.9914642\n\n\n\n\nConclusion\nThe p-value is 0.9914642, which is greater than the significance level of 0.05. Therefore, we fail to reject the null hypothesis and conclude that there is insufficient evidence to support the claim that the mean income of female employees is greater than $500 per week. In other words, we cannot conclude that female employees earn significantly more than the agreed-upon mean income for all senior-level workers in the company."
  },
  {
    "objectID": "posts/HW2_AkhileshKumarMeghwal.html#question-5",
    "href": "posts/HW2_AkhileshKumarMeghwal.html#question-5",
    "title": "Homework 2",
    "section": "Question 5",
    "text": "Question 5\nJones and Smith separately conduct studies to test H0: μ = 500 against Ha: μ ≠ 500, each with n = 1000. Jones gets ȳ = 519.5, with se = 10.0. Smith gets ȳ = 519.7, with se = 10.0.\nA. Show that t = 1.95 and P-value = 0.051 for Jones. Show that t = 1.97 and P-value = 0.049 for Smith. B. Using α = 0.05, for each study indicate whether the result is “statistically significant.” C. Using this example, explain the misleading aspects of reporting the result of a test as “P ≤ 0.05” versus “P &gt; 0.05,” or as “reject H0” versus “Do not reject H0,” without reporting the actual P-value.\n\nQuestion 5-A\nTwo assumptions for the analysis:\n\nSample is randomly selected, and\nPopulation follows a normal distribution.\n\nThe null hypothesis for both studies is that the population mean is equal to 500, represented as H0: μ = 500.\nThe alternative hypothesis for both studies is that the population mean is not equal to 500, represented as Ha: μ ≠ 500.\nWe will reject the null hypothesis at a p-value less than 0.05\n\n\nCalculating t-statistic and p-value for Jones\n\n\nCode\nsample_mean &lt;- 519.5\nmu &lt;- 500\nstandard_error &lt;- 10\nsample_size &lt;- 1000\n\nt_score_jones &lt;- (sample_mean-mu)/(standard_error)\nt_score_jones\n\n\n[1] 1.95\n\n\nCode\np &lt;- 2*pt(t_score_jones, sample_size-1, lower.tail = FALSE)\np\n\n\n[1] 0.05145555\n\n\n\n\nCalculating t-statistic and p-value for Smith\n\n\nCode\nsample_mean &lt;- 519.7\nmu &lt;- 500\nstandard_error &lt;- 10\nsample_size &lt;- 1000\n\nt_score_smith &lt;- (sample_mean-mu)/(standard_error)\nt_score_smith\n\n\n[1] 1.97\n\n\nCode\np &lt;- 2*pt(t_score_smith, sample_size-1, lower.tail = FALSE)\np\n\n\n[1] 0.04911426\n\n\n\n\nConclusion\nJones has a test-statistic of 1.95 and a p-value of 0.05145555, while Smith has a test-statistic of 1.97 and a p-value of 0.05145555.\n\n\nQuestion 5-B\n\n\nConclusion\nBased on the given information, the result is statistically significant for Smith, but not for Jones.\nFor Jones, the p-value (0.05145555) is greater than the significance level (α = 0.05), which means that we fail to reject the null hypothesis. This indicates that the result is not statistically significant for Jones.\nFor Smith, the p-value (0.04911426) is less than the significance level (α = 0.05), which means that we reject the null hypothesis. This indicates that the result is statistically significant for Smith.\n\n\nQuestion 5-C\n\n\nConclusion\nThe misleading aspect of reporting the result of a test as “P ≤ 0.05” versus “P &gt; 0.05” or as “reject H0” versus “do not reject H0” without reporting the actual P-value is that it can mask the degree of uncertainty in the results. In this example, if we only reported “P ≤ 0.05” for Smith’s study, we would conclude that the result is statistically significant. However, we would miss the fact that the P-value is very close to the significance level, indicating that the result may not be very strong or may be subject to random chance. Similarly, if we only reported “P &gt; 0.05” for Jones’s study, we would conclude that the result is not statistically significant, but we would miss the fact that the P-value is very close to the significance level, indicating that the result may be borderline significant. Therefore, it is important to report the actual P-value to provide a more accurate interpretation of the results.\nIf we fail to report the P-value and simply state whether the P-value is less than/equal to or greater than the defined significance level of the test, one cannot determine the strength of the conclusion. In the Jones/Smith example, reporting the results only as P ≤ 0.05 versus P &gt; 0.05 will lead to different conclusions about very similar results."
  },
  {
    "objectID": "posts/HW2_AkhileshKumarMeghwal.html#question-6",
    "href": "posts/HW2_AkhileshKumarMeghwal.html#question-6",
    "title": "Homework 2",
    "section": "Question 6",
    "text": "Question 6\nA school nurse wants to determine whether age is a factor in whether children choose a healthy snack after school. She conducts a survey of 300 middle school students, with the results below. Test at α = 0.05 the claim that the proportion who choose a healthy snack differs by grade level. What is the null hypothesis? Which test should we use? What is the conclusion?\n\nNull Hypothesis\nThe null hypothesis is that there is no association between grade level and the proportion of students who choose a healthy snack.\nThe alternative hypothesis is that there is an association between grade level and the proportion of students who choose a healthy snack.\n\n\nWhich test should we use? (Chi Square)?\nIn the case of the school nurse’s survey data on snack choices, a chi-square test of independence should be used because the data is categorical (healthy snack vs unhealthy snack) and the research question is about whether there is a relationship between snack choice and grade level. The chi-square test is used to determine whether there is a significant association between two categorical variables.\n\n\nChi-square Test\n\n\nCode\n# Create the contingency table\nsnack_table &lt;- matrix(c(31, 43, 51, 69, 57, 49), nrow = 2, byrow = TRUE)\nrownames(snack_table) &lt;- c(\"Healthy snack\", \"Unhealthy snack\")\ncolnames(snack_table) &lt;- c(\"6th grade\", \"7th grade\", \"8th grade\")\nsnack_table\n\n\n                6th grade 7th grade 8th grade\nHealthy snack          31        43        51\nUnhealthy snack        69        57        49\n\n\nCode\n# Conduct the chi-square test of independence\nchisq.test(snack_table)\n\n\n\n    Pearson's Chi-squared test\n\ndata:  snack_table\nX-squared = 8.3383, df = 2, p-value = 0.01547\n\n\n\n\nConclusion\nSince the p-value is less than the significance level of 0.05,\nThe output of the chisq.test function indicates that the chi-square test statistic is 8.3383 with 2 degrees of freedom, and the corresponding p-value is 0.01547. So we can reject the null hypothesis that there is no association between snack choice and grade level, and conclude that there is evidence of a significant association between two categorical variables: snack choice and grade level at a significance level of 0.05.\nBased on the results of the chi-square test of independence, it is found that there is a statistically significant association between snack choice and grade level among the surveyed students. Specifically, the data indicates that the proportion of students who choose healthy snacks differs significantly across grade levels. The contingency table and chi-square test showed that more 6th-grade students (31) chose healthy snacks compared to 7th-grade students (43) and 8th-grade students (51). On the other hand, more 8th-grade students (49) chose unhealthy snacks compared to 6th-grade students (69) and 7th-grade students (57).\nTherefore, we can conclude that grade level is a factor in whether children choose a healthy snack after school. The proportion of students who choose a healthy snack appears to differ across the three grade levels. Overall, these results suggest that the school may need to target their health promotion efforts towards the specific grade levels where unhealthy snack choices are more prevalent. The results may also inform further research into the factors that influence snack choices among middle school students."
  },
  {
    "objectID": "posts/HW2_AkhileshKumarMeghwal.html#question-7",
    "href": "posts/HW2_AkhileshKumarMeghwal.html#question-7",
    "title": "Homework 2",
    "section": "Question 7",
    "text": "Question 7\nPer-pupil costs (in thousands of dollars) for cyber charter school tuition for school districts in three areas are shown. Test the claim that there is a difference in means for the three areas, using an appropriate test. What is the null hypothesis? Which test should we use? What is the conclusion?\n\n\nCode\n# Define the data\ndata &lt;- data.frame(\n  Area = c(\"Area 1\", \"Area 2\", \"Area 3\"),\n  Value1 = c(6.2, 7.5, 5.8),\n  Value2 = c(9.3, 8.2, 6.4),\n  Value3 = c(6.8, 8.5, 5.6),\n  Value4 = c(6.1, 8.2, 7.1),\n  Value5 = c(6.7, 7.0, 3.0),\n  Value6 = c(7.5, 9.3, 3.5)\n)\n\ndata1=data\n# # Alternatively, you can set the row names separately\ncolnames(data1) &lt;- NULL\n# # Display table without boundaries/borders\nknitr::kable(data1, row.names = FALSE, col.names = NULL,\n             border = NA, digits = 1)%&gt;%\n  kable_styling(bootstrap_options = \"striped\", full_width = TRUE, position = 'center', font_size = 14, stripe_color='black')\n\n\n\n\n\nArea 1\n6.2\n9.3\n6.8\n6.1\n6.7\n7.5\n\n\nArea 2\n7.5\n8.2\n8.5\n8.2\n7.0\n9.3\n\n\nArea 3\n5.8\n6.4\n5.6\n7.1\n3.0\n3.5\n\n\n\n\n\n\n\n\nNull Hypothesis\nIn order to investigate if there is a difference in means for the per-pupil costs of cyber charter school tuition in three different areas, we can use Analysis of Variance (ANOVA) test. ANOVA is a statistical test that compares means of three or more groups.\nNull Hypothesis: The null hypothesis for the ANOVA test would state that there is no significant difference between the means of the per-pupil costs for cyber charter school tuition in the three areas. This can be expressed mathematically as H0: μ1 = μ2 = μ3, where μ1, μ2, and μ3 represent the means of the per-pupil costs for cyber charter school tuition in Area 1, Area 2, and Area 3, respectively.\nAlternative Hypothesis: On the other hand, the alternative hypothesis (H1) suggests that at least one mean is significantly different from the others.\n\n\nWhich test should we use? (Anova)\nTo test the claim that there is a difference in means for the three areas, we can use the analysis of variance (ANOVA) test.\nIn this case, since we are comparing means across three areas, we can use a one-way ANOVA. We will use the following R code to conduct the test:\nTo conduct the ANOVA test in R, we can use the aov function.\n\n\nAnova Test\n\n\nCode\n# Reshape the data to long format\ndata_long &lt;- gather(data, key = \"Variable\", value = \"Value\", -Area)\n\n# Conduct ANOVA test\nresult &lt;- aov(Value ~ Area, data = data_long)\n\n# Print the ANOVA table\nsummary(result)\n\n\n            Df Sum Sq Mean Sq F value  Pr(&gt;F)   \nArea         2  25.66  12.832   8.176 0.00397 **\nResiduals   15  23.54   1.569                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\nConclusion\nThe ANOVA table shows that the F value is 8.176 and the p-value is 0.00397. Since the p-value is less than the significance level of 0.05, we reject the null hypothesis and conclude that there is evidence of a difference in means for the three areas.\nTherefore, we can infer that the per-pupil costs for cyber charter school tuition differ among the three areas. Specifically, we found that the mean per-pupil costs for cyber charter school tuition in Area 1 (mean = 7.1) and Area 2 (mean = 8.1) were higher than the mean per-pupil costs in Area 3 (mean = 5.23). The ANOVA test supports the conclusion that there is a significant difference between the means of the per-pupil costs for cyber charter school tuition in the three areas."
  },
  {
    "objectID": "posts/AdithyaParupudi_hw3.html",
    "href": "posts/AdithyaParupudi_hw3.html",
    "title": "Homework 3",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(stats)\nlibrary(alr4)\nlibrary(smss)\n\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/AdithyaParupudi_hw3.html#a",
    "href": "posts/AdithyaParupudi_hw3.html#a",
    "title": "Homework 3",
    "section": "A",
    "text": "A\nThe predictor variable is ppgdp and the response variable is fertility."
  },
  {
    "objectID": "posts/AdithyaParupudi_hw3.html#b",
    "href": "posts/AdithyaParupudi_hw3.html#b",
    "title": "Homework 3",
    "section": "B",
    "text": "B\n\n\nCode\nUN11 %&gt;%\n  select(c(ppgdp, fertility)) %&gt;%\n  ggplot(aes(x = ppgdp, y = fertility)) +\n  geom_point() +\n  geom_smooth(method = lm)\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nThe graph shows an intense negative relationship between a country’s gross national product per person and fertility rate at the beginning, then appears to be little change in fertility in relationship to ppgdp moving beyond this point. A straight-line mean function does not seem to be an appropriate measure for summary of this graph."
  },
  {
    "objectID": "posts/AdithyaParupudi_hw3.html#c",
    "href": "posts/AdithyaParupudi_hw3.html#c",
    "title": "Homework 3",
    "section": "C",
    "text": "C\n\n\nCode\nUN11 %&gt;%\n  select(c(ppgdp, fertility)) %&gt;%\n  ggplot(aes(x = log(ppgdp), y = log(fertility))) +\n  geom_point() +\n  geom_smooth(method = lm)\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nThe relationship between the variables appears to be negative throughout the graph. The simple linear regression seems plausible to summarise this graph"
  },
  {
    "objectID": "posts/AdithyaParupudi_hw3.html#a-1",
    "href": "posts/AdithyaParupudi_hw3.html#a-1",
    "title": "Homework 3",
    "section": "A",
    "text": "A\n\n\nCode\nUN11$british &lt;- 1.33 * UN11$ppgdp\nsummary(lm(fertility ~ british, UN11))\n\n\n\nCall:\nlm(formula = fertility ~ british, data = UN11)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.9006 -0.8801 -0.3547  0.6749  3.7585 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  3.178e+00  1.048e-01  30.331  &lt; 2e-16 ***\nbritish     -2.407e-05  3.500e-06  -6.877  7.9e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.206 on 197 degrees of freedom\nMultiple R-squared:  0.1936,    Adjusted R-squared:  0.1895 \nF-statistic: 47.29 on 1 and 197 DF,  p-value: 7.903e-11\n\n\nCode\nsummary(lm(fertility ~ ppgdp, UN11))\n\n\n\nCall:\nlm(formula = fertility ~ ppgdp, data = UN11)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.9006 -0.8801 -0.3547  0.6749  3.7585 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  3.178e+00  1.048e-01  30.331  &lt; 2e-16 ***\nppgdp       -3.201e-05  4.655e-06  -6.877  7.9e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.206 on 197 degrees of freedom\nMultiple R-squared:  0.1936,    Adjusted R-squared:  0.1895 \nF-statistic: 47.29 on 1 and 197 DF,  p-value: 7.903e-11\n\n\nThe magnitude of the slope has reduced very slightly, the slope of the prediction equation changed."
  },
  {
    "objectID": "posts/AdithyaParupudi_hw3.html#b-1",
    "href": "posts/AdithyaParupudi_hw3.html#b-1",
    "title": "Homework 3",
    "section": "B",
    "text": "B\n\n\nCode\ncor(UN11$ppgdp, UN11$fertility)\n\n\n[1] -0.4399891\n\n\nCode\ncor(UN11$british, UN11$fertility)\n\n\n[1] -0.4399891\n\n\nThe correlation does not change after the conversion."
  },
  {
    "objectID": "posts/AdithyaParupudi_hw3.html#a-2",
    "href": "posts/AdithyaParupudi_hw3.html#a-2",
    "title": "Homework 3",
    "section": "A",
    "text": "A\n\n\nCode\nstudent.survey %&gt;%\n  select(c(pi, re)) %&gt;%\n  ggplot() + \n  geom_bar(aes(x = re, fill = pi)) +\n  xlab(\"Religiosity\") +\n  ylab(\"Political ideology\") \n\n\n\n\n\nReligiosity and conservatism seem to have a positive relationship.\n\n\nCode\nstudent.survey %&gt;%\n  select(c(tv, hi)) %&gt;%\n  ggplot(aes(x = tv, y = hi)) + \n  geom_point() +\n  geom_smooth(method=lm) +\n  xlab(\"Average Hours of TV watched per Week\") +\n  ylab(\"High School GPA\") \n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nHigh school GPA and TV-watching seem to have a negative relationship."
  },
  {
    "objectID": "posts/AdithyaParupudi_hw3.html#b-2",
    "href": "posts/AdithyaParupudi_hw3.html#b-2",
    "title": "Homework 3",
    "section": "B",
    "text": "B\n\n\nCode\nsummary(lm(data = student.survey, formula = as.numeric(pi) ~ as.numeric(re)))\n\n\n\nCall:\nlm(formula = as.numeric(pi) ~ as.numeric(re), data = student.survey)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.81243 -0.87160  0.09882  1.12840  3.09882 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      0.9308     0.4252   2.189   0.0327 *  \nas.numeric(re)   0.9704     0.1792   5.416 1.22e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.345 on 58 degrees of freedom\nMultiple R-squared:  0.3359,    Adjusted R-squared:  0.3244 \nF-statistic: 29.34 on 1 and 58 DF,  p-value: 1.221e-06\n\n\nAt a significance level of 0.01, there is a statistically significant association between religiosity and political ideology (as p-value &lt; .01). The correlation is moderate and positive, suggesting that as weekly church attendance increases, political ideology becomes more conservative leaning.\n\n\nCode\nsummary(lm(data = student.survey, formula = hi ~ tv))\n\n\n\nCall:\nlm(formula = hi ~ tv, data = student.survey)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.2583 -0.2456  0.0417  0.3368  0.7051 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  3.441353   0.085345  40.323   &lt;2e-16 ***\ntv          -0.018305   0.008658  -2.114   0.0388 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4467 on 58 degrees of freedom\nMultiple R-squared:  0.07156,   Adjusted R-squared:  0.05555 \nF-statistic: 4.471 on 1 and 58 DF,  p-value: 0.03879\n\n\nWith a slope of -0.018, there is a negative association between hours of tv watched per week and high school GPA, meaning that as hours of tv viewing increase, a student’s GPA tends to decrease. There is a statistically significant relationship between hours of tv viewed per week and GPA at a significance level of 0.05. However, the R-squared value is close to 0, which suggests that the regression model does not provide a strong prediction for the observed variables. This is not suprising after looking at the scatter plot with hours of tv watched and GPA, since there does not appear to be a linear trend in the data."
  },
  {
    "objectID": "posts/HW1_AkhileshKumarMeghwal.html",
    "href": "posts/HW1_AkhileshKumarMeghwal.html",
    "title": "Homework 1 - Akhilesh Kumar",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(ggplot2)\nlibrary(stats)\nlibrary(viridis)\n\nknitr::opts_chunk$set(echo = TRUE, warning = FALSE)"
  },
  {
    "objectID": "posts/HW1_AkhileshKumarMeghwal.html#reading-data",
    "href": "posts/HW1_AkhileshKumarMeghwal.html#reading-data",
    "title": "Homework 1 - Akhilesh Kumar",
    "section": "Reading Data",
    "text": "Reading Data\nFirst, let’s read in the data from the Excel file:\n\n\nCode\nlibrary(readxl)\ndf &lt;- read_excel(\"_data/LungCapData.xls\")\n\n\nThe distribution of LungCap looks as follows:\nThe LungCap dataset contains 725 observations and six variables, namely LungCap, Age, Height, Smoke, Gender, and Cesarean. LungCap is the primary variable of interest, measuring the lung capacity of participants. Age and Height provide demographic information, indicating the age and height of each participant.\nEach row in the dataset corresponds to a unique participant and provides their values for the six variables. Smoke is a categorical variable that denotes whether the participant smokes or not. Gender is another categorical variable that identifies the gender of each participant. Cesarean is a binary variable indicating whether the participant was born via cesarean section or not.\nThe LungCap dataset may be useful in exploring the relationships between lung capacity, demographic variables, and smoking status."
  },
  {
    "objectID": "posts/HW1_AkhileshKumarMeghwal.html#a-what-does-the-distribution-of-lungcap-look-like",
    "href": "posts/HW1_AkhileshKumarMeghwal.html#a-what-does-the-distribution-of-lungcap-look-like",
    "title": "Homework 1 - Akhilesh Kumar",
    "section": "1a: What does the distribution of LungCap look like?",
    "text": "1a: What does the distribution of LungCap look like?\nThe following plot displays the distribution of LungCap:\n\n\nCode\nlibrary(tidyverse)\nlibrary(ggplot2)\n\ndf %&gt;%\n  ggplot(aes(LungCap, ..density..)) +\n  geom_histogram(bins = 25, color = \"black\", fill = \"orange\") +\n  geom_density(color = \"darkblue\", linewidth = 1.25) +\n  theme_classic() + \n  theme(plot.title = element_text(hjust = 0.65, color = \"darkblue\")) +\n  theme(axis.text = element_text(face = \"bold\", color=\"darkblue\"), axis.title = element_text(face = \"bold\", size=12, color=\"darkblue\")) +\n  xlab(\"Lung Capacity\") +\n  ylab(\"Probability Density\") +\n  labs(title = \"Probability Distribution of LungCap\")\n\n\n\n\n\nThe plot displays the distribution of lung capacity measurements in a sample of individuals. The histogram and density plots indicate that the data is approximately normally distributed, with the majority of observations clustered near the mean. Specifically, the histogram shows a bell-shaped curve, with the highest frequency of observations centered around the mean value.\nThe density plot shows a smooth, symmetric curve, which closely follows the histogram and suggests that the data follows a normal distribution. These results are consistent with what we might expect, given that lung capacity is a physiological measure that is likely to vary around a central tendency. Taken together, these findings suggest that the sample data is well-behaved and that the majority of individuals in the sample have lung capacities that are close to the average value."
  },
  {
    "objectID": "posts/HW1_AkhileshKumarMeghwal.html#b-compare-the-probability-distribution-of-the-lungcap-with-respect-to-males-and-females",
    "href": "posts/HW1_AkhileshKumarMeghwal.html#b-compare-the-probability-distribution-of-the-lungcap-with-respect-to-males-and-females",
    "title": "Homework 1 - Akhilesh Kumar",
    "section": "1b: Compare the probability distribution of the LungCap with respect to Males and Females?",
    "text": "1b: Compare the probability distribution of the LungCap with respect to Males and Females?\n\n\nCode\ndf %&gt;%\n  ggplot(aes(y = dnorm(LungCap), x = Gender, color = Gender)) +\n  geom_boxplot(alpha = 0.5) +\n  scale_fill_manual(values = c(\"green\", \"orange\")) +\n  theme_classic() + \n  theme(plot.title = element_text(hjust = 0.5, color = \"darkblue\", face = \"bold\", size = 14),\n        axis.text = element_text(face = \"bold\", color=\"darkblue\"), \n        axis.title = element_text(face = \"bold\", size=12, color=\"darkblue\")) +\n  labs(title = \"LungCap Probability Distribution based on Gender\", y = \"Probability density\")\n\n\n\n\n\nThe probability density of LungCap for females is higher than for males, as evidenced by the higher median value and larger range of data in the female group compared to the male group. This suggests that gender may be a significant factor in determining LungCap values."
  },
  {
    "objectID": "posts/HW1_AkhileshKumarMeghwal.html#c-compare-the-mean-lung-capacities-for-smokers-and-non-smokers.-does-it-make-sense",
    "href": "posts/HW1_AkhileshKumarMeghwal.html#c-compare-the-mean-lung-capacities-for-smokers-and-non-smokers.-does-it-make-sense",
    "title": "Homework 1 - Akhilesh Kumar",
    "section": "1c: Compare the mean lung capacities for smokers and non-smokers. Does it make sense?",
    "text": "1c: Compare the mean lung capacities for smokers and non-smokers. Does it make sense?\n\n\nCode\nlibrary(knitr)\n\nMean_Lung_Capacity &lt;- df %&gt;%\n  group_by(Smoke) %&gt;%\n  summarise(mean = mean(LungCap))\n\n\nMean_Lung_Capacity\n\n\n\n\n  \n\n\n\nThe above table shows the lung capacity of individuals who smoke versus those who do not smoke. The table suggests that the mean lung capacity of those who smoke is greater than that of those who do not smoke. However, the statement argues that this result does not make sense and cannot be conclusively interpreted as a difference between smokers and non-smokers.\nThe output suggests that the lung capacity of an individual who smokes may be influenced by various biological factors unique to each person. Therefore, while smoking can be a contributing factor to reduced lung capacity, it cannot be the sole factor responsible for differences in lung capacity between smokers and non-smokers.\nThe output emphasizes that the difference in mean lung capacity between smokers and non-smokers is not a conclusive indicator of the effects of smoking on lung capacity. Other factors, such as age, gender and overall health, may also play a role in lung capacity. Therefore, it is important to consider these factors when drawing conclusions about the impact of smoking on lung capacity or other health outcomes."
  },
  {
    "objectID": "posts/HW1_AkhileshKumarMeghwal.html#d-examine-the-relationship-between-smoking-and-lung-capacity-within-age-groups-less-than-or-equal-to-13-14-to-15-16-to-17-and-greater-than-or-equal-to-18.",
    "href": "posts/HW1_AkhileshKumarMeghwal.html#d-examine-the-relationship-between-smoking-and-lung-capacity-within-age-groups-less-than-or-equal-to-13-14-to-15-16-to-17-and-greater-than-or-equal-to-18.",
    "title": "Homework 1 - Akhilesh Kumar",
    "section": "1d: Examine the relationship between Smoking and Lung Capacity within age groups: “less than or equal to 13”, “14 to 15”, “16 to 17”, and “greater than or equal to 18”.",
    "text": "1d: Examine the relationship between Smoking and Lung Capacity within age groups: “less than or equal to 13”, “14 to 15”, “16 to 17”, and “greater than or equal to 18”.\n\n\nCode\nLc &lt;- mutate(Lc, AgeGrp = case_when(\n    Age &lt;= 13 ~ \"less than or equal to 13\",\n    Age == 14 | Age == 15 ~ \"14 to 15\",\n    Age == 16 | Age == 17 ~ \"16 to 17\",\n    Age &gt;= 18 ~ \"greater than or equal to 18\"\n))\n\n\nError in mutate(Lc, AgeGrp = case_when(Age &lt;= 13 ~ \"less than or equal to 13\", : object 'Lc' not found\n\n\nCode\nLc %&gt;%\n  ggplot(aes(x = LungCap, fill = Smoke)) +\n  geom_histogram(bins = 25, color = \"black\") +\n  facet_wrap(vars(AgeGrp)) +\n  scale_fill_manual(values = c(\"darkgreen\", \"darkblue\")) +\n  theme_classic() + \n  labs(title = \"Relationship of LungCap and Smoke based on age categories\", y = \"Frequency\", x = \"Lung Capacity\")\n\n\nError in ggplot(., aes(x = LungCap, fill = Smoke)): object 'Lc' not found\n\n\nThe graph shows the frequency of lung capacity for non-smokers and smokers in different age categories, namely:\n\n“less than or equal to 13”,\n“14 to 15”,\n“16 to 17”, and\n“greater than or equal to 18”\n\nFrom the above plot, we can derive two important observations. Firstly, we observe that non-smokers generally have a higher lung capacity compared to smokers. This is indicated by the fact that the bars for non-smokers are consistently taller than the bars for smokers across all age groups. This observation is consistent with the known negative impact of smoking on lung function.\nSecondly, we observe that the number of smokers is relatively lower in the age group “less than or equal to 13”. This is shown by the relatively shorter bars for smokers in this age group. This observation could be due to several factors, such as lower access to cigarettes or lower rates of smoking initiation in younger age groups.\nAdditionally, we can also observe that there is a general trend of decreasing lung capacity as age increases, regardless of smoking status. This is indicated by the fact that the bars for all age groups are shorter compared to the bars for the previous age group. This trend is consistent with the natural decline in lung function that occurs with aging.\nIn summary, the above graph shows that non-smokers generally have a higher lung capacity compared to smokers, and that smoking rates are relatively lower in younger age groups. The graph also highlights the general trend of decreasing lung capacity as age increases, regardless of smoking status. These observations provide important insights into the relationship between lung capacity, smoking, and age, and can be useful for informing public health interventions and promoting healthy lifestyles."
  },
  {
    "objectID": "posts/HW1_AkhileshKumarMeghwal.html#e-compare-the-lung-capacities-for-smokers-and-non-smokers-within-each-age-group.-is-your-answer-different-from-the-one-in-part-c.-what-could-possibly-be-going-on-here",
    "href": "posts/HW1_AkhileshKumarMeghwal.html#e-compare-the-lung-capacities-for-smokers-and-non-smokers-within-each-age-group.-is-your-answer-different-from-the-one-in-part-c.-what-could-possibly-be-going-on-here",
    "title": "Homework 1 - Akhilesh Kumar",
    "section": "1e: Compare the lung capacities for smokers and non-smokers within each age group. Is your answer different from the one in part c. What could possibly be going on here?",
    "text": "1e: Compare the lung capacities for smokers and non-smokers within each age group. Is your answer different from the one in part c. What could possibly be going on here?\n\n\nCode\nLc %&gt;%\n  ggplot(aes(x = Age, y = LungCap, color = Smoke)) +\n  geom_line(linewidth = 1.25) +\n  scale_color_manual(values = c(\"darkgreen\", \"darkred\")) +\n  theme_classic() + \n  facet_wrap(vars(Smoke)) +\n  labs(title = \"Relationship of Lung Capacity and Smoking Status by Age\", y = \"Lung Capacity\", x = \"Age\") +\n  theme(plot.title = element_text(face = \"bold\", size = 14, hjust = 0.5),\n        axis.title.x = element_text(face = \"bold\", size = 12),\n        axis.title.y = element_text(face = \"bold\", size = 12),\n        axis.text.x = element_text(size = 10),\n        axis.text.y = element_text(size = 10),\n        legend.title = element_blank(),\n        legend.text = element_text(face = \"bold\", size = 10),\n        panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank(),\n        panel.background = element_blank())\n\n\nError in ggplot(., aes(x = Age, y = LungCap, color = Smoke)): object 'Lc' not found\n\n\n1c presents a the mean lung capacity of individuals who smoke versus those who do not smoke. The statement argues that this result cannot be conclusively interpreted as a difference between smokers and non-smokers, as other biological factors unique to each individual may also influence lung capacity.\nOn the other hand, 1e presents a line graph that compares lung capacity with age for smokers and non-smokers. The graph suggests that non-smokers generally have higher lung capacity compared to smokers, and that the difference in lung capacity between the two groups increases with age.\nIn summary, 1c presents a summary statistic that may be influenced by other factors, while 1e presents a visual representation of the relationship between smoking and lung capacity over time, which provides a more nuanced understanding of the impact of smoking on lung capacity."
  },
  {
    "objectID": "posts/HW1_AkhileshKumarMeghwal.html#question-2",
    "href": "posts/HW1_AkhileshKumarMeghwal.html#question-2",
    "title": "Homework 1 - Akhilesh Kumar",
    "section": "Question 2",
    "text": "Question 2"
  },
  {
    "objectID": "posts/HW1_AkhileshKumarMeghwal.html#reading-the-table",
    "href": "posts/HW1_AkhileshKumarMeghwal.html#reading-the-table",
    "title": "Homework 1 - Akhilesh Kumar",
    "section": "Reading the table",
    "text": "Reading the table\n\n\nCode\nPrior_convictions &lt;- c(0:4)\nInmate_count &lt;- c(128, 434, 160, 64, 24)\nPc &lt;- tibble(Prior_convictions, Inmate_count)\nPc\n\n\n\n\n  \n\n\n\n\n\nCode\nPc &lt;- mutate(Pc, Probability = Inmate_count/sum(Inmate_count))\nPc"
  },
  {
    "objectID": "posts/HW1_AkhileshKumarMeghwal.html#a-what-is-the-probability-that-a-randomly-selected-inmate-has-exactly-2-prior-convictions",
    "href": "posts/HW1_AkhileshKumarMeghwal.html#a-what-is-the-probability-that-a-randomly-selected-inmate-has-exactly-2-prior-convictions",
    "title": "Homework 1 - Akhilesh Kumar",
    "section": "2a: What is the probability that a randomly selected inmate has exactly 2 prior convictions?",
    "text": "2a: What is the probability that a randomly selected inmate has exactly 2 prior convictions?\n\n\nCode\nPc %&gt;%\n  filter(Prior_convitions == 2) %&gt;%\n  select(Probability)\n\n\nError in `filter()`:\n! Problem while computing `..1 = Prior_convitions == 2`.\nCaused by error in `mask$eval_all_filter()`:\n! object 'Prior_convitions' not found\n\n\nProbability that a randomly selected inmate has exactly 2 prior convictions is 0.1975309."
  },
  {
    "objectID": "posts/HW1_AkhileshKumarMeghwal.html#b-what-is-the-probability-that-a-randomly-selected-inmate-has-fewer-than-2-prior-convictions",
    "href": "posts/HW1_AkhileshKumarMeghwal.html#b-what-is-the-probability-that-a-randomly-selected-inmate-has-fewer-than-2-prior-convictions",
    "title": "Homework 1 - Akhilesh Kumar",
    "section": "2b: What is the probability that a randomly selected inmate has fewer than 2 prior convictions?",
    "text": "2b: What is the probability that a randomly selected inmate has fewer than 2 prior convictions?\n\n\nCode\ntemp &lt;- Pc %&gt;%\n  filter(Prior_convitions &lt; 2)\n\n\nError in `filter()`:\n! Problem while computing `..1 = Prior_convitions &lt; 2`.\nCaused by error in `mask$eval_all_filter()`:\n! object 'Prior_convitions' not found\n\n\nCode\nsum(temp$Probability)\n\n\nError in eval(expr, envir, enclos): object 'temp' not found\n\n\nProbability that a randomly selected inmate has fewer than 2 convictions is 0.6938272"
  },
  {
    "objectID": "posts/HW1_AkhileshKumarMeghwal.html#c-what-is-the-probability-that-a-randomly-selected-inmate-has-2-or-fewer-prior-convictions",
    "href": "posts/HW1_AkhileshKumarMeghwal.html#c-what-is-the-probability-that-a-randomly-selected-inmate-has-2-or-fewer-prior-convictions",
    "title": "Homework 1 - Akhilesh Kumar",
    "section": "2c: What is the probability that a randomly selected inmate has 2 or fewer prior convictions?",
    "text": "2c: What is the probability that a randomly selected inmate has 2 or fewer prior convictions?\n\n\nCode\ntemp &lt;- Pc %&gt;%\n  filter(Prior_convitions &lt;= 2)\n\n\nError in `filter()`:\n! Problem while computing `..1 = Prior_convitions &lt;= 2`.\nCaused by error in `mask$eval_all_filter()`:\n! object 'Prior_convitions' not found\n\n\nCode\nsum(temp$Probability)\n\n\nError in eval(expr, envir, enclos): object 'temp' not found\n\n\nProbability that a randomly selected inmate has 2 or fewer prior convictions: 0.891358"
  },
  {
    "objectID": "posts/HW1_AkhileshKumarMeghwal.html#d-what-is-the-probability-that-a-randomly-selected-inmate-has-more-than-2-prior-convictions",
    "href": "posts/HW1_AkhileshKumarMeghwal.html#d-what-is-the-probability-that-a-randomly-selected-inmate-has-more-than-2-prior-convictions",
    "title": "Homework 1 - Akhilesh Kumar",
    "section": "2d: What is the probability that a randomly selected inmate has more than 2 prior convictions?",
    "text": "2d: What is the probability that a randomly selected inmate has more than 2 prior convictions?\n\n\nCode\ntemp &lt;- Pc %&gt;%\n  filter(Prior_convitions &gt; 2)\n\n\nError in `filter()`:\n! Problem while computing `..1 = Prior_convitions &gt; 2`.\nCaused by error in `mask$eval_all_filter()`:\n! object 'Prior_convitions' not found\n\n\nCode\nsum(temp$Probability)\n\n\nError in eval(expr, envir, enclos): object 'temp' not found\n\n\nProbability that a randomly selected inmate has more than 2 prior convictions is 0.108642"
  },
  {
    "objectID": "posts/HW1_AkhileshKumarMeghwal.html#e-what-is-the-expected-value-for-the-number-of-prior-convictions",
    "href": "posts/HW1_AkhileshKumarMeghwal.html#e-what-is-the-expected-value-for-the-number-of-prior-convictions",
    "title": "Homework 1 - Akhilesh Kumar",
    "section": "2e: What is the expected value for the number of prior convictions?",
    "text": "2e: What is the expected value for the number of prior convictions?\n\n\nCode\nPc &lt;- mutate(Pc, Wm = Prior_convitions*Probability)\n\n\nError in `mutate()`:\n! Problem while computing `Wm = Prior_convitions * Probability`.\nCaused by error in `mask$eval_all_mutate()`:\n! object 'Prior_convitions' not found\n\n\nCode\ne &lt;- sum(Pc$Wm)\ne\n\n\n[1] 0\n\n\nExpected value for the number of prior convictions: 1.28642"
  },
  {
    "objectID": "posts/HW1_AkhileshKumarMeghwal.html#f-calculate-the-variance-and-the-standard-deviation-for-the-prior-convictions.",
    "href": "posts/HW1_AkhileshKumarMeghwal.html#f-calculate-the-variance-and-the-standard-deviation-for-the-prior-convictions.",
    "title": "Homework 1 - Akhilesh Kumar",
    "section": "2f: Calculate the variance and the standard deviation for the Prior Convictions.",
    "text": "2f: Calculate the variance and the standard deviation for the Prior Convictions.\nVariance for the Prior Convictions:\n\n\nCode\nv &lt;-sum(((Pc$Prior_convictions-e)^2)*Pc$Probability)\nv\n\n\n[1] 2.511111\n\n\nStandard Deviation for the Prior Convictions:\n\n\nCode\nsqrt(v)\n\n\n[1] 1.584649\n\n\nVariance and Standard Deviation for the Prior Convictions are 0.8562353 and 0.9253298 respectively."
  },
  {
    "objectID": "posts/Final_Project_Check2_GHTan.html",
    "href": "posts/Final_Project_Check2_GHTan.html",
    "title": "Final Project Check 2",
    "section": "",
    "text": "My final project will be a further investigation on digital devices in schools that I have submitted as the final project for DACSS 601. I still explore the data from the survey “Programme for International Student Assessment” in 2018. In this assignment, I will propose my hypothesis, and present the descriptive statistics with minor changes base on my last project.\nCode\nlibrary(tidyverse)\n\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.0      ✔ purrr   0.3.5 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.4.1 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\nCode\nlibrary(ggplot2)\nlibrary(dbplyr)\n\n\n\nAttaching package: 'dbplyr'\n\nThe following objects are masked from 'package:dplyr':\n\n    ident, sql\n\n\nCode\nlibrary(stargazer)\n\n\n\nPlease cite as: \n\n Hlavac, Marek (2022). stargazer: Well-Formatted Regression and Summary Statistics Tables.\n R package version 5.2.3. https://CRAN.R-project.org/package=stargazer \n\n\nCode\nlibrary(misty)\n\n\n|-------------------------------------|\n| misty 0.4.8 (2023-03-10)            |\n| Miscellaneous Functions T. Yanagida |\n|-------------------------------------|\n\n\nCode\npisa &lt;- read_csv('_data/CY07_MSU_SCH_QQQ.csv')\n\n\nNew names:\nRows: 21903 Columns: 198\n── Column specification\n──────────────────────────────────────────────────────── Delimiter: \",\" chr\n(8): CNT, CYC, NatCen, STRATUM, SUBNATIO, SC053D11TA, PRIVATESCH, VER_DAT dbl\n(189): ...1, CNTRYID, CNTSCHID, Region, OECD, ADMINMODE, LANGTEST, SC001... lgl\n(1): BOOKID\nℹ Use `spec()` to retrieve the full column specification for this data. ℹ\nSpecify the column types or set `show_col_types = FALSE` to quiet this message.\n• `` -&gt; `...1`"
  },
  {
    "objectID": "posts/Final_Project_Check2_GHTan.html#model-evaluation",
    "href": "posts/Final_Project_Check2_GHTan.html#model-evaluation",
    "title": "Final Project Check 2",
    "section": "model evaluation",
    "text": "model evaluation\n\n\nCode\nmodel_1 &lt;-lm(Career_Guidance~Urban+Public_or_Private+Digitals+Urban*Digitals, data = pisa_SC155)\nsummary(model_1)\n\n\n\nCall:\nlm(formula = Career_Guidance ~ Urban + Public_or_Private + Digitals + \n    Urban * Digitals, data = pisa_SC155)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.0225 -0.5833 -0.3219  0.5482  2.8554 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       0.788183   0.082837   9.515  &lt; 2e-16 ***\nUrban             0.005856   0.024580   0.238   0.8117    \nPublic_or_Private 0.134161   0.019114   7.019 2.31e-12 ***\nDigitals          0.161945   0.030173   5.367 8.09e-08 ***\nUrban:Digitals    0.015535   0.009013   1.724   0.0848 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9751 on 18558 degrees of freedom\n  (3340 observations deleted due to missingness)\nMultiple R-squared:  0.02977,   Adjusted R-squared:  0.02956 \nF-statistic: 142.3 on 4 and 18558 DF,  p-value: &lt; 2.2e-16\n\n\nCode\nmodel_2 &lt;-lm(Career_Guidance~Urban+Public_or_Private+Public_or_Private*Digitals, data = pisa_SC155)\nsummary(model_2)\n\n\n\nCall:\nlm(formula = Career_Guidance ~ Urban + Public_or_Private + Public_or_Private * \n    Digitals, data = pisa_SC155)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.9430 -0.5968 -0.3169  0.5416  2.8959 \n\nCoefficients:\n                            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                 0.472735   0.110481   4.279 1.89e-05 ***\nUrban                       0.047179   0.005769   8.178 3.06e-16 ***\nPublic_or_Private           0.297148   0.090982   3.266  0.00109 ** \nDigitals                    0.276078   0.038819   7.112 1.19e-12 ***\nPublic_or_Private:Digitals -0.056165   0.031141  -1.804  0.07131 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9751 on 18558 degrees of freedom\n  (3340 observations deleted due to missingness)\nMultiple R-squared:  0.02978,   Adjusted R-squared:  0.02957 \nF-statistic: 142.4 on 4 and 18558 DF,  p-value: &lt; 2.2e-16\n\n\nCode\nmodel_3 &lt;-lm(Career_Guidance~Urban+Public_or_Private+Digitals, data = pisa_SC155)\nsummary(model_3)\n\n\n\nCall:\nlm(formula = Career_Guidance ~ Urban + Public_or_Private + Digitals, \n    data = pisa_SC155)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.9883 -0.5860 -0.3205  0.5462  2.8801 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       0.660470   0.037034  17.834  &lt; 2e-16 ***\nUrban             0.047037   0.005768   8.154 3.74e-16 ***\nPublic_or_Private 0.136695   0.019058   7.173 7.64e-13 ***\nDigitals          0.209566   0.012125  17.284  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9751 on 18559 degrees of freedom\n  (3340 observations deleted due to missingness)\nMultiple R-squared:  0.02961,   Adjusted R-squared:  0.02946 \nF-statistic: 188.8 on 3 and 18559 DF,  p-value: &lt; 2.2e-16\n\n\nCode\nstargazer(model_1,model_2, model_3,  type = 'text')\n\n\n\n===========================================================================================================\n                                                         Dependent variable:                               \n                           --------------------------------------------------------------------------------\n                                                           Career_Guidance                                 \n                                      (1)                        (2)                        (3)            \n-----------------------------------------------------------------------------------------------------------\nUrban                                0.006                     0.047***                   0.047***         \n                                    (0.025)                    (0.006)                    (0.006)          \n                                                                                                           \nPublic_or_Private                   0.134***                   0.297***                   0.137***         \n                                    (0.019)                    (0.091)                    (0.019)          \n                                                                                                           \nDigitals                            0.162***                   0.276***                   0.210***         \n                                    (0.030)                    (0.039)                    (0.012)          \n                                                                                                           \nUrban:Digitals                       0.016*                                                                \n                                    (0.009)                                                                \n                                                                                                           \nPublic_or_Private:Digitals                                     -0.056*                                     \n                                                               (0.031)                                     \n                                                                                                           \nConstant                            0.788***                   0.473***                   0.660***         \n                                    (0.083)                    (0.110)                    (0.037)          \n                                                                                                           \n-----------------------------------------------------------------------------------------------------------\nObservations                         18,563                     18,563                     18,563          \nR2                                   0.030                      0.030                      0.030           \nAdjusted R2                          0.030                      0.030                      0.029           \nResidual Std. Error            0.975 (df = 18558)         0.975 (df = 18558)         0.975 (df = 18559)    \nF Statistic                142.347*** (df = 4; 18558) 142.420*** (df = 4; 18558) 188.786*** (df = 3; 18559)\n===========================================================================================================\nNote:                                                                           *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01\n\n\nAfter the model comparison, model_3 presents statistical significance on all independent variables. The career gudience depends on urban situations, school styles and the access to digital devices.\n\n\nCode\nfit&lt;-lm(Career_Guidance~Urban+Public_or_Private+Digitals, data = pisa_SC155)\npar(mfrow=c(2,3))\nplot(fit, which= 1:6)\n\n\n\n\n\nThe model diagnostic demonstrates that model_3 is the fittest model without significant errors."
  },
  {
    "objectID": "posts/Final_Project_Check2_GHTan.html#discussion-on-na",
    "href": "posts/Final_Project_Check2_GHTan.html#discussion-on-na",
    "title": "Final Project Check 2",
    "section": "Discussion on NA",
    "text": "Discussion on NA\n\n\nCode\nlibrary(naniar)\npisa_SC115_nonOECD &lt;- filter(pisa_SC155, OECD == \"0\")\npisa_SC115_OECD &lt;- filter(pisa_SC155, OECD == \"1\")\nvis_miss(pisa_SC155)\n\n\n\n\n\nCode\nvis_miss(pisa_SC115_nonOECD)\n\n\n\n\n\nCode\nvis_miss(pisa_SC115_OECD)\n\n\n\n\n\nThe graphics has suggested that OECD countries reported more NAs than non-OECD countries. This opens more space to further investigate why developed countries reported more NAs."
  },
  {
    "objectID": "posts/Kristin_Abijaoude_HW2.html",
    "href": "posts/Kristin_Abijaoude_HW2.html",
    "title": "Hw 2 by Kristin Abijaoude",
    "section": "",
    "text": "Code\n# load packages\npackages &lt;- c(\"readr\", \"ggplot2\", \"caret\", \"summarytools\", \"tidyverse\", \"dplyr\", \"stats\", \"pwr\")\nlapply(packages, require, character.only = TRUE)\n\n\nLoading required package: readr\n\n\nLoading required package: ggplot2\n\n\nLoading required package: caret\n\n\nLoading required package: lattice\n\n\nLoading required package: summarytools\n\n\nLoading required package: tidyverse\n\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ tibble  3.1.8     ✔ dplyr   1.1.0\n✔ tidyr   1.2.1     ✔ stringr 1.5.0\n✔ purrr   1.0.0     ✔ forcats 0.5.2\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n✖ purrr::lift()   masks caret::lift()\n✖ tibble::view()  masks summarytools::view()\nLoading required package: pwr\n\n\n[[1]]\n[1] TRUE\n\n[[2]]\n[1] TRUE\n\n[[3]]\n[1] TRUE\n\n[[4]]\n[1] TRUE\n\n[[5]]\n[1] TRUE\n\n[[6]]\n[1] TRUE\n\n[[7]]\n[1] TRUE\n\n[[8]]\n[1] TRUE"
  },
  {
    "objectID": "posts/Kristin_Abijaoude_HW2.html#a",
    "href": "posts/Kristin_Abijaoude_HW2.html#a",
    "title": "Hw 2 by Kristin Abijaoude",
    "section": "A",
    "text": "A\n\n\nCode\n# t test\nf_emp &lt;- 410\nincome &lt;- 500\ns &lt;- 90\nt &lt;- (f_emp - income) / (s / sqrt(9))\nt\n\n\n[1] -3"
  },
  {
    "objectID": "posts/Kristin_Abijaoude_HW2.html#b",
    "href": "posts/Kristin_Abijaoude_HW2.html#b",
    "title": "Hw 2 by Kristin Abijaoude",
    "section": "B",
    "text": "B\n\n\nCode\n# degree of freedom\ndf &lt;- 9 - 1 \n# df = 8\n\n# p-value\np_value &lt;- pt(t, df)\np_value\n\n\n[1] 0.008535841\n\n\nCode\n# significant level\nalpha &lt;- 0.05\n\n# to reject or not to reject\nif (p_value &lt; alpha/2 || p_value &gt; 1-alpha/2) {\n  cat(\"Reject the null hypothesis\")\n} else {\n  cat(\"Fail to reject the null hypothesis\")\n}\n\n\nReject the null hypothesis"
  },
  {
    "objectID": "posts/Kristin_Abijaoude_HW2.html#c",
    "href": "posts/Kristin_Abijaoude_HW2.html#c",
    "title": "Hw 2 by Kristin Abijaoude",
    "section": "C",
    "text": "C\n\n\nCode\n1-p_value\n\n\n[1] 0.9914642\n\n\nCode\n# fail to reject null hypothesis"
  },
  {
    "objectID": "posts/Kristin_Abijaoude_HW2.html#a-1",
    "href": "posts/Kristin_Abijaoude_HW2.html#a-1",
    "title": "Hw 2 by Kristin Abijaoude",
    "section": "A",
    "text": "A\n\n\nCode\n#  T values\nt_jones &lt;- (519.5 - 500) / 10 # sample mean = 519.5 - 500 for population mean / sample error of 10.0\nt_jones\n\n\n[1] 1.95\n\n\nCode\nt_smith &lt;- (519.7 - 500) / 10 # sample mean = 519.7 - 500 for population mean / sample error of 10.0\nt_smith\n\n\n[1] 1.97\n\n\n\n\nCode\n# p values\np_jones &lt;- 2 * pt(-abs(t_jones), df = 999)\np_jones\n\n\n[1] 0.05145555\n\n\nCode\np_smith &lt;- 2 * pt(-abs(t_smith), df = 999)\np_smith\n\n\n[1] 0.04911426"
  },
  {
    "objectID": "posts/Kristin_Abijaoude_HW2.html#b-1",
    "href": "posts/Kristin_Abijaoude_HW2.html#b-1",
    "title": "Hw 2 by Kristin Abijaoude",
    "section": "B",
    "text": "B\nSmith’s result is statistically significant, while Jones’ is not."
  },
  {
    "objectID": "posts/Kristin_Abijaoude_HW2.html#c-1",
    "href": "posts/Kristin_Abijaoude_HW2.html#c-1",
    "title": "Hw 2 by Kristin Abijaoude",
    "section": "C",
    "text": "C\nWhile the two results are close in variables, one of them is significant while the other is not. That’s why P-value is important in determining whether to reject or fail to reject the hypothesis."
  },
  {
    "objectID": "posts/HW2_JustineShakespeare.html",
    "href": "posts/HW2_JustineShakespeare.html",
    "title": "Homework 2",
    "section": "",
    "text": "The time between the date a patient was recommended for heart surgery and the surgery date for cardiac patients in Ontario was collected by the Cardiac Care Network (“Wait Times Data Guide,” Ministry of Health and Long-Term Care, Ontario, Canada, 2006). The sample mean and sample standard deviation for wait times (in days) of patients for two cardiac procedures are given in the accompanying table. Assume that the sample is representative of the Ontario population.\nConstruct the 90% confidence interval to estimate the actual mean wait time for each of the two procedures. Is the confidence interval narrower for angiography or bypass surgery?\nFirst we’ll calculate the confidence interval manually with the formula: CI = (X bar) ± (t × s/sqrt(n))\nWe’ll create objects in R to plug into this formula. We’ll start with the Bypass data.\n\n\nCode\nlibrary(tidyverse)\n\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.0      ✔ purrr   1.0.1 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.3.0      ✔ stringr 1.5.0 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\nCode\n# Bypass data\nBypassSample &lt;- 539\nBypassMean &lt;- 19\nBypassSD &lt;- 10\n\n# standard error\nBypassSEM &lt;- BypassSD/sqrt(BypassSample)\n\n# t-value\nConfidenceLevel &lt;- .90\nBYPtail &lt;- (1-ConfidenceLevel)/2\nBypass_t_score &lt;- qt(1-BYPtail, df=BypassSample-1)\n\n# plug everything back into the CI formula:\nCIBypass &lt;- c(BypassMean-Bypass_t_score*BypassSEM, \n              BypassMean+Bypass_t_score*BypassSEM)\n\nprint(CIBypass)\n\n\n[1] 18.29029 19.70971\n\n\nNow let’s do the same for the Angiography data.\n\n\nCode\nAngioSample &lt;- 847\nAngioMean &lt;- 18\nAngioSD &lt;- 9\n\n# standard error\nAngioSEM &lt;- AngioSD/sqrt(AngioSample)\n\n# t-value\nConfidenceLevel &lt;- .90\nAngtail &lt;- (1-ConfidenceLevel)/2\nAngio_t_score &lt;- qt(1-Angtail, df=AngioSample-1)\n\n# plug everything back in\n\nCIAngio &lt;- c(AngioMean-Angio_t_score*AngioSEM, \n             AngioMean+Angio_t_score*AngioSEM)\n\nprint(CIAngio)\n\n\n[1] 17.49078 18.50922\n\n\nNow we can compare the confidence intervals.\n\n\nCode\nabs(diff(CIAngio)) \n\n\n[1] 1.018436\n\n\nCode\nabs(diff(CIBypass))\n\n\n[1] 1.419421\n\n\nThe confidence interval for Angiography is narrower than for Bypass."
  },
  {
    "objectID": "posts/HW2_JustineShakespeare.html#question-1",
    "href": "posts/HW2_JustineShakespeare.html#question-1",
    "title": "Homework 2",
    "section": "",
    "text": "The time between the date a patient was recommended for heart surgery and the surgery date for cardiac patients in Ontario was collected by the Cardiac Care Network (“Wait Times Data Guide,” Ministry of Health and Long-Term Care, Ontario, Canada, 2006). The sample mean and sample standard deviation for wait times (in days) of patients for two cardiac procedures are given in the accompanying table. Assume that the sample is representative of the Ontario population.\nConstruct the 90% confidence interval to estimate the actual mean wait time for each of the two procedures. Is the confidence interval narrower for angiography or bypass surgery?\nFirst we’ll calculate the confidence interval manually with the formula: CI = (X bar) ± (t × s/sqrt(n))\nWe’ll create objects in R to plug into this formula. We’ll start with the Bypass data.\n\n\nCode\nlibrary(tidyverse)\n\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.0      ✔ purrr   1.0.1 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.3.0      ✔ stringr 1.5.0 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\nCode\n# Bypass data\nBypassSample &lt;- 539\nBypassMean &lt;- 19\nBypassSD &lt;- 10\n\n# standard error\nBypassSEM &lt;- BypassSD/sqrt(BypassSample)\n\n# t-value\nConfidenceLevel &lt;- .90\nBYPtail &lt;- (1-ConfidenceLevel)/2\nBypass_t_score &lt;- qt(1-BYPtail, df=BypassSample-1)\n\n# plug everything back into the CI formula:\nCIBypass &lt;- c(BypassMean-Bypass_t_score*BypassSEM, \n              BypassMean+Bypass_t_score*BypassSEM)\n\nprint(CIBypass)\n\n\n[1] 18.29029 19.70971\n\n\nNow let’s do the same for the Angiography data.\n\n\nCode\nAngioSample &lt;- 847\nAngioMean &lt;- 18\nAngioSD &lt;- 9\n\n# standard error\nAngioSEM &lt;- AngioSD/sqrt(AngioSample)\n\n# t-value\nConfidenceLevel &lt;- .90\nAngtail &lt;- (1-ConfidenceLevel)/2\nAngio_t_score &lt;- qt(1-Angtail, df=AngioSample-1)\n\n# plug everything back in\n\nCIAngio &lt;- c(AngioMean-Angio_t_score*AngioSEM, \n             AngioMean+Angio_t_score*AngioSEM)\n\nprint(CIAngio)\n\n\n[1] 17.49078 18.50922\n\n\nNow we can compare the confidence intervals.\n\n\nCode\nabs(diff(CIAngio)) \n\n\n[1] 1.018436\n\n\nCode\nabs(diff(CIBypass))\n\n\n[1] 1.419421\n\n\nThe confidence interval for Angiography is narrower than for Bypass."
  },
  {
    "objectID": "posts/HW2_JustineShakespeare.html#question-2",
    "href": "posts/HW2_JustineShakespeare.html#question-2",
    "title": "Homework 2",
    "section": "Question 2",
    "text": "Question 2\nA survey of 1031 adult Americans was carried out by the National Center for Public Policy. Assume that the sample is representative of adult Americans. Among those surveyed, 567 believed that college education is essential for success. Find the point estimate, p, of the proportion of all adult Americans who believe that a college education is essential for success. Construct and interpret a 95% confidence interval for p.\nWe can use the prop.test() function to find the point estimate and the confidence interval:\n\n\nCode\nprop.test(x=567, n=1031, conf.level = .95)\n\n\n\n    1-sample proportions test with continuity correction\n\ndata:  567 out of 1031, null probability 0.5\nX-squared = 10.091, df = 1, p-value = 0.00149\nalternative hypothesis: true p is not equal to 0.5\n95 percent confidence interval:\n 0.5189682 0.5805580\nsample estimates:\n        p \n0.5499515"
  },
  {
    "objectID": "posts/HW2_JustineShakespeare.html#question-3",
    "href": "posts/HW2_JustineShakespeare.html#question-3",
    "title": "Homework 2",
    "section": "Question 3",
    "text": "Question 3\nSuppose that the financial aid office of UMass Amherst seeks to estimate the mean cost of textbooks per semester for students. The estimate will be useful if it is within $5 of the true population mean (i.e. they want the confidence interval to have a length of $10 or less). The financial aid office is pretty sure that the amount spent on books varies widely, with most values between $30 and $200. They think that the population standard deviation is about a quarter of this range (in other words, you can assume they know the population standard deviation). Assuming the significance level to be 5%, what should be the size of the sample.\nWe have to solve for n here. Because we have the standard deviation, we’ll use the CI formula that uses the z-score: CI = (X bar) ± (z × s/sqrt(n))\nThis can be rewritten: n=((s*z)/5)^2\nUsing the data we have:\n\n\nCode\nQ3_s = (200-30)/4\nQ3_z = qnorm(.975)\n\n((Q3_s*Q3_z)/5)^2\n\n\n[1] 277.5454"
  },
  {
    "objectID": "posts/HW2_JustineShakespeare.html#question-4",
    "href": "posts/HW2_JustineShakespeare.html#question-4",
    "title": "Homework 2",
    "section": "Question 4",
    "text": "Question 4\nAccording to a union agreement, the mean income for all senior-level workers in a large service company equals 500 per week. A representative of a women’s group decides to analyze whether the mean income μ for female employees matches this norm. For a random sample of nine female employees, ȳ = $410 and s = 90\n\n4.A\nTest whether the mean income of female employees differs from $500 per week. Include assumptions, hypotheses, test statistic, and P-value. Interpret the result.\nTo solve this problem I followed the steps of hypothesis testing:\n1. Specify the null hypothesis and alternative hypothesis\n2. Set the significance level (alpha)\n3. Calculate the test statistic\n4. Compare test statistics to the critical value determined by alpha, or check if p-value is smaller than alpha\n5. Draw a conclusion\n\nOur null hypothesis is that the population mean is 500 dollars, our alternative hypothesis is that the population mean is NOT $500\nThe significance level (alpha) is .05, or 5%\nCalculate the test statistic:\n\n\n\nCode\nQ4_SD &lt;- 90\nQ4_ymean &lt;- 410\nQ4_umean &lt;- 500\nQ4_s_size &lt;- 9\nQ4_SEM &lt;- Q4_SD/sqrt(Q4_s_size)\n\nQ4_T_Statistic &lt;- (Q4_ymean - Q4_umean)/(Q4_SEM)\nQ4_T_Statistic\n\n\n[1] -3\n\n\n\nCalculate the critical value so that we can compare it with the test statistic.\n\n\n\nCode\nQ4_Crit_Value &lt;- qt(0.025, df=(Q4_s_size-1))\nQ4_Crit_Value\n\n\n[1] -2.306004\n\n\nThe critical value (on the lower tail end) is -2.3060. Because the test statistic is smaller than the critical value (and larger in absolute terms), we can conclude that we should reject the null hypothesis.\n\n(continued) calculate the p-value to check to see if it is smaller than alpha.\n\n\n\nCode\nQ4_p_value &lt;- pt(Q4_T_Statistic, df = (Q4_s_size-1), lower.tail = TRUE)*2\nQ4_p_value\n\n\n[1] 0.01707168\n\n\nThe p-value is 0.01707, which is smaller than our alpha (.05). 0.01707 &lt; 0.05 This again means that we can reject the null hypothesis.\n\nDraw a conclusion\n\nBecause the test statistic (-3) is in absolute terms larger than the critical values (2.30) AND because the p-value (0.01707) is less than the alpha levelof .05, we can reject the null hypothesis\nNote that when we calculate the confidence interval of these values we can also see that the estimated population mean here is outside of a 95% CI.\nTo calculate the confidence level we’ll first create some objects we’ll need in our formulas:\n\n\nCode\nConfLevel &lt;- 0.95\nQ4_tail_area &lt;- (1-ConfLevel)/2\nQ4_tscore &lt;- qt(p=1-Q4_tail_area, df = Q4_s_size-1)\n\n# Then, using this and some of the objects we created earlier we'll plug everything in: \nQ4_CI &lt;- c(Q4_ymean - (Q4_tscore * Q4_SEM), Q4_ymean + (Q4_tscore * Q4_SEM))\n\nprint(Q4_CI)\n\n\n[1] 340.8199 479.1801\n\n\nWe can see here that $500 is not within the 95% confidence interval, which is another reason we can reject the null hypothesis.\n\n\n4.B\nReport the P-value for Ha: μ &lt; 500. Interpret.\nTo calculate a one sided P-value you do not need to multiply the formula by 2 and you need to specify that we’re only looking at one side. An alternative hypothesis of μ &lt; 500 means the null hypothesis is μ &gt;= 500. We need to specify that we are looking at the lower tail.\n\n\nCode\nQ4_p_value1 &lt;- pt(Q4_T_Statistic, df = (Q4_s_size-1), lower.tail = TRUE)\nQ4_p_value1\n\n\n[1] 0.008535841\n\n\nThe p-value here is even lower than it was for the two sided test, it is 0.0085. We can definitely reject the null hypothesis that the true population mean is over $500.\n\n\n4.C\nReport and interpret the P-value for Ha: μ &gt; 500.\nTo calculate the P-value for Ha: μ &gt; 500 we run the same formula as before but we specify that we’re not interested in the lower tail.\n\n\nCode\nQ4_p_value2 &lt;- pt(Q4_T_Statistic, df = (Q4_s_size-1), lower.tail = FALSE)\nQ4_p_value2\n\n\n[1] 0.9914642\n\n\nThe p-value for this one sided test is quite large. It means that we should not reject the null hypothesis that the true population mean is less than $500.\nTo test these p-values we can add them up.\n\n\nCode\nQ4_p_value1 + Q4_p_value2\n\n\n[1] 1\n\n\nWe find that they add up to 1."
  },
  {
    "objectID": "posts/HW2_JustineShakespeare.html#question-5",
    "href": "posts/HW2_JustineShakespeare.html#question-5",
    "title": "Homework 2",
    "section": "Question 5",
    "text": "Question 5\nJones and Smith separately conduct studies to test H0: μ = 500 against Ha: μ ≠ 500, each with n = 1000. Jones gets ȳ = 519.5, with se = 10.0. Smith gets ȳ = 519.7, with se = 10.0.\n\n5.A\nShow that t = 1.95 and P-value = 0.051 for Jones. Show that t = 1.97 and P-value = 0.049 for Smith.\nFirst we can create some objects to represent the values for this question. Then we can use those to get the test statistic and the p-value for Jones and Smith.\n\n\nCode\nQ5_SEM &lt;- 10 # they have the same standard error\nJones_ymean &lt;- 519.5\nSmith_ymean &lt;- 519.7\nQ5_s_size &lt;- 1000 # they have the same sample size\n\nQ5_umean &lt;- 500\n\n# calculate the test statistic and p-value for Jones:\nJones_T_Statistic &lt;- (Jones_ymean - Q5_umean)/(Q5_SEM)\nJones_T_Statistic\n\n\n[1] 1.95\n\n\nCode\n# we find that Jones' test statistic is 1.95. We can use that to find the p-value:\nJones_p_value &lt;- pt(Jones_T_Statistic, df = (Q5_s_size-1), lower.tail = FALSE)*2\nJones_p_value\n\n\n[1] 0.05145555\n\n\nCode\n# Calculate test statistic and p-value for Smith:\nSmith_T_Statistic &lt;- (Smith_ymean - Q5_umean)/(Q5_SEM)\nSmith_T_Statistic\n\n\n[1] 1.97\n\n\nCode\n# we find that Smith's test statistic is 1.97. We can use that to find the p-value:\nSmith_p_value &lt;- pt(Smith_T_Statistic, df = (Q5_s_size-1), lower.tail = FALSE)*2\nSmith_p_value\n\n\n[1] 0.04911426\n\n\n\n\n5.B\nUsing α = 0.05, for each study indicate whether the result is “statistically significant.”\nAbove we found the p-value for both Jones and Smith, we can use that to show how those values compare to alpha set at 0.05.\n\n\nCode\nalpha &lt;- 0.05\n\nJones_p_value &lt; alpha\n\n\n[1] FALSE\n\n\nCode\nSmith_p_value &lt; alpha\n\n\n[1] TRUE\n\n\nThis shows that technically Jones’ p-value is not statistically significant because it is more than the alpha of 0.05, but Smith’s p-value is technically statistically significant because it is less than the alpha.\n\n\n5.C\nUsing this example, explain the misleading aspects of reporting the result of a test as “P ≤ 0.05” versus “P &gt; 0.05,” or as “reject H0” versus “Do not reject H0,” without reporting the actual P-value.\nThis question demonstrates how close two results can be and still technically not have the same significance level. In these cases it is important to include the p-value so that readers can put the results into context."
  },
  {
    "objectID": "posts/HW2_JustineShakespeare.html#question-6",
    "href": "posts/HW2_JustineShakespeare.html#question-6",
    "title": "Homework 2",
    "section": "Question 6",
    "text": "Question 6\nA school nurse wants to determine whether age is a factor in whether children choose a healthy snack after school. She conducts a survey of 300 middle school students, with the results below. Test at α = 0.05 the claim that the proportion who choose a healthy snack differs by grade level. What is the null hypothesis? Which test should we use? What is the conclusion?\nSince we’re dealing with categorical variables (grade and the choice of a healthy or unhealthy snack) we should use a chi-square test. First we’ll create the dataframe we’ll need to run the test in r.\n\n\nCode\nobs &lt;- matrix(c(31, 69, 43, 57, 51, 49), nrow=3, byrow = TRUE)\ndimnames(obs) &lt;- list(Grade = c(\"Grade_6\", \"Grade_7\", \"Grade_8\"),\n                      Snack = c(\"healthy\", \"unhealthy\"))\n\nobs\n\n\n         Snack\nGrade     healthy unhealthy\n  Grade_6      31        69\n  Grade_7      43        57\n  Grade_8      51        49\n\n\nThen we can run the chi-square test and print the result:\n\n\nCode\nChiResult &lt;- chisq.test(obs)\n\nprint(ChiResult)\n\n\n\n    Pearson's Chi-squared test\n\ndata:  obs\nX-squared = 8.3383, df = 2, p-value = 0.01547\n\n\nGiven the p-value &lt; 0.05, we can reject the null hypothesis that grade level does not have an effect on the choice of healthy snack. The relatively high X-squared value also indicates that there is a meaningful difference in the choices made by kids in different grade levels."
  },
  {
    "objectID": "posts/HW2_JustineShakespeare.html#question-7",
    "href": "posts/HW2_JustineShakespeare.html#question-7",
    "title": "Homework 2",
    "section": "Question 7",
    "text": "Question 7\nPer-pupil costs (in thousands of dollars) for cyber charter school tuition for school districts in three areas are shown. Test the claim that there is a difference in means for the three areas, using an appropriate test. What is the null hypothesis? Which test should we use? What is the conclusion?\nThe null hypothesis is that all three districts have equal average tuition. We can use the ANOVA test since we are comparing three means.\nFirst we’ll create the dataframe in r:\n\n\nCode\nArea1 &lt;- c(6.2, 9.3, 6.8, 6.1, 6.7, 7.5)  \nArea2 &lt;- c(7.5, 8.2, 8.5, 8.2, 7.0, 9.3)\nArea3 &lt;- c(5.8, 6.4, 5.6, 7.1, 3.0, 3.5)\n\nQ7df &lt;- data.frame(Area1, Area2, Area3)\n\nQ7df_PL &lt;- pivot_longer(Q7df, cols = c(Area1, Area2, Area3), names_to = \"district\", values_to = \"tuition\")\nQ7df_PL\n\n\n# A tibble: 18 × 2\n   district tuition\n   &lt;chr&gt;      &lt;dbl&gt;\n 1 Area1        6.2\n 2 Area2        7.5\n 3 Area3        5.8\n 4 Area1        9.3\n 5 Area2        8.2\n 6 Area3        6.4\n 7 Area1        6.8\n 8 Area2        8.5\n 9 Area3        5.6\n10 Area1        6.1\n11 Area2        8.2\n12 Area3        7.1\n13 Area1        6.7\n14 Area2        7  \n15 Area3        3  \n16 Area1        7.5\n17 Area2        9.3\n18 Area3        3.5\n\n\nNow that we have a dataframe with one continuous variable and one categorical variable, we can use the anova test function in r aov() to test our hypothesis. This type of hypothesis test is appropriate for this scenario because we want to compare the mean of three groups.\n\n\nCode\nQ7o &lt;- aov(tuition ~ district, data = Q7df_PL)\nsummary(Q7o)\n\n\n            Df Sum Sq Mean Sq F value  Pr(&gt;F)   \ndistrict     2  25.66  12.832   8.176 0.00397 **\nResiduals   15  23.54   1.569                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nGiven that the p value here is less than 0.01, we can reject the null hypothesis that the mean tuition is the same at all three schools."
  },
  {
    "objectID": "posts/asch_harwood_final_project_part1.html",
    "href": "posts/asch_harwood_final_project_part1.html",
    "title": "Estimating Per Capita Food Waste in the United States",
    "section": "",
    "text": "Code\nlibrary(\"dplyr\")\nlibrary(\"knitr\")\nlibrary(kableExtra)\nlibrary(xtable)\nlibrary(ggplot2)\nlibrary(GGally)\nlibrary(lme4)\nlibrary(car)\nlibrary(dplyr)\nlibrary(tidyverse)\nlibrary(stargazer)"
  },
  {
    "objectID": "posts/asch_harwood_final_project_part1.html#footnotes",
    "href": "posts/asch_harwood_final_project_part1.html#footnotes",
    "title": "Estimating Per Capita Food Waste in the United States",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI recognize that we were encouraged to work with pre-existing datasets. Nevertheless, I view this as an opportunity to prototype a project I believe might have real-world applicability in my professional life.↩︎"
  },
  {
    "objectID": "posts/AdithyaParupudi_hw1.html",
    "href": "posts/AdithyaParupudi_hw1.html",
    "title": "Homework1 - EDA of LungCap Data",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.1     ✔ purrr   0.3.4\n✔ tibble  3.1.8     ✔ dplyr   1.0.9\n✔ tidyr   1.2.1     ✔ stringr 1.4.1\n✔ readr   2.1.3     ✔ forcats 0.5.2\n\n\nWarning: package 'ggplot2' was built under R version 4.2.2\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\nCode\nlibrary(hrbrthemes)\n\n\nNOTE: Either Arial Narrow or Roboto Condensed fonts are required to use these themes.\n      Please use hrbrthemes::import_roboto_condensed() to install Roboto Condensed and\n      if Arial Narrow is not on your system, please see https://bit.ly/arialnarrow\n\n\nCode\nlibrary(viridis)\n\n\nWarning: package 'viridis' was built under R version 4.2.2\n\n\nLoading required package: viridisLite\n\n\nCode\nlibrary(readxl)"
  },
  {
    "objectID": "posts/AdithyaParupudi_hw1.html#question-1",
    "href": "posts/AdithyaParupudi_hw1.html#question-1",
    "title": "Homework1 - EDA of LungCap Data",
    "section": "Question 1",
    "text": "Question 1\n\n1a) The distribution of LungCap looks as follows:\n\n\nCode\nhist(df$LungCap)\n\n\n\n\n\nThe histogram suggests that the distribution is close to a normal distribution. Most of the observations are close to the mean. Very few observations are close to the margins (0 and 15).\n\n\n1b) Compare the probability distribution of the LungCap with respect to Males and Females? (Hint:make boxplots separated by gender using the boxplot() function)\n\n\nCode\n#boxplot code\n\ndf %&gt;%\n  ggplot( aes(x=Gender, y=LungCap, fill=Gender)) +\n    geom_boxplot() +\n    theme_ipsum() +\n    theme(\n      legend.position=\"none\",\n      plot.title = element_text(size=12)\n    ) +\n    ggtitle(\"Lungcap vs Gender\") +\n    xlab(\"Gender\") +\n  ylab(\"Lung Cap\")\n\n\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): font family not\nfound in Windows font database\n\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): font family not\nfound in Windows font database\n\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): font family not\nfound in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\n\n\n\n\n\nc) Compare the mean lung capacities for smokers and non-smokers. Does it make sense?\n\n\nCode\nmean_smoke &lt;- df %&gt;%\n  group_by(Smoke) %&gt;%\n  summarise(mean = mean(LungCap))\nmean_smoke\n\n\n# A tibble: 2 × 2\n  Smoke  mean\n  &lt;chr&gt; &lt;dbl&gt;\n1 no     7.77\n2 yes    8.65\n\n\nAccording to this mean, it doesn’t make sense that lung capacities of smokers is greater than that of non-smokers.\n\n\nd) Examine the relationship between Smoking and Lung Capacity within age groups: “less than or\nequal to 13”, “14 to 15”, “16 to 17”, and “greater than or equal to 18”.\n\n\nCode\ndf &lt;- mutate(df, AgeGrp = case_when(Age &lt;= 13 ~ \"less than or equal to 13\",\n                                    Age == 14 | Age == 15 ~ \"14 to 15\",\n                                    Age == 16 | Age == 17 ~ \"16 to 17\",\n                                    Age &gt;= 18 ~ \"greater than or equal to 18\"))\n\ndf %&gt;%\n  ggplot(aes(y = LungCap, color = Smoke)) +\n  geom_histogram(bins = 25) +\n  facet_wrap(vars(AgeGrp)) +\n  labs(title = \"Relationship of LungCap and Smoke based on Age\", y = \"Lung Capacity\", x = \"Frequency\")\n\n\n\n\n\n\n\ne) Compare the lung capacities for smokers and non-smokers within each age group. Is your answer different from the one in part c. What could possibly be going on here?\n\n\nCode\ndf %&gt;%\n  ggplot(aes(x = Age, y = LungCap, color = Smoke)) +\n  geom_line() +\n  theme_classic() + \n  facet_wrap(vars(Smoke)) +\n  labs(title = \"Relationship of LungCap and Smoke v/s Age\", y = \"Lung Capacity\", x = \"Age\")"
  },
  {
    "objectID": "posts/AdithyaParupudi_hw1.html#question-2",
    "href": "posts/AdithyaParupudi_hw1.html#question-2",
    "title": "Homework1 - EDA of LungCap Data",
    "section": "Question 2",
    "text": "Question 2\n\nReading the table\n\n\nCode\nPrior_convitions &lt;- c(0:4)\nInmate_count &lt;- c(128, 434, 160, 64, 24)\nPc &lt;- data_frame(Prior_convitions, Inmate_count)\n\n\nWarning: `data_frame()` was deprecated in tibble 1.1.0.\nℹ Please use `tibble()` instead.\n\n\nCode\nPc\n\n\n# A tibble: 5 × 2\n  Prior_convitions Inmate_count\n             &lt;int&gt;        &lt;dbl&gt;\n1                0          128\n2                1          434\n3                2          160\n4                3           64\n5                4           24\n\n\n\n\nCode\nPc &lt;- mutate(Pc, Probability = Inmate_count/sum(Inmate_count))\nPc\n\n\n# A tibble: 5 × 3\n  Prior_convitions Inmate_count Probability\n             &lt;int&gt;        &lt;dbl&gt;       &lt;dbl&gt;\n1                0          128      0.158 \n2                1          434      0.536 \n3                2          160      0.198 \n4                3           64      0.0790\n5                4           24      0.0296\n\n\n\n\n2a - Probability that a randomly selected inmate has exactly 2 prior convictions:\n\n\nCode\nPc %&gt;%\n  filter(Prior_convitions == 2) %&gt;%\n  select(Probability)\n\n\n# A tibble: 1 × 1\n  Probability\n        &lt;dbl&gt;\n1       0.198\n\n\n\n\n2b - Probability that a randomly selected inmate has fewer than 2 convictions:\n\n\nCode\ntemp &lt;- Pc %&gt;%\n  filter(Prior_convitions &lt; 2)\nsum(temp$Probability)\n\n\n[1] 0.6938272\n\n\n\n\n2c - Probability that a randomly selected inmate has 2 or fewer prior convictions:\n\n\nCode\ntemp &lt;- Pc %&gt;%\n  filter(Prior_convitions &lt;= 2)\nsum(temp$Probability)\n\n\n[1] 0.891358\n\n\n\n\n2d - Probability that a randomly selected inmate has more than 2 prior convictions:\n\n\nCode\ntemp &lt;- Pc %&gt;%\n  filter(Prior_convitions &gt; 2)\nsum(temp$Probability)\n\n\n[1] 0.108642\n\n\n\n\n2e - Expected value for the number of prior convictions:\n\n\nCode\nPc &lt;- mutate(Pc, Wm = Prior_convitions*Probability)\ne &lt;- sum(Pc$Wm)\ne\n\n\n[1] 1.28642\n\n\n\n\n2f - Variance for the Prior Convictions:\n\n\nCode\nv &lt;-sum(((Pc$Prior_convitions-e)^2)*Pc$Probability)\nv\n\n\n[1] 0.8562353\n\n\nstandard deviation for the Prior Convictions:\n\n\nCode\nsqrt(v)\n\n\n[1] 0.9253298"
  },
  {
    "objectID": "posts/HW1_solution_Pang.html",
    "href": "posts/HW1_solution_Pang.html",
    "title": "Homework 1 Solution",
    "section": "",
    "text": "First, let’s read in the data from the Excel file:\n\n\nCode\nlibrary(readxl)\nlibrary(dplyr, warn.conflicts = F)\nlibrary(magrittr)\ndf &lt;- read_excel(\"_data/LungCapData.xls\")\n\n\nThe distribution of LungCap looks as follows:\n\n\nCode\nhist(df$LungCap, xlab = 'Lung Capacity', main = '', freq = F)\n\n\n\n\n\nThe histogram suggests that the distribution is close to a normal distribution. Most of the observations are close to the mean. Very few observations are close to the margins (0 and 15).\n\n\n\n\n\nCode\nboxplot(LungCap ~ Gender, data = df)\n\n\n\n\n\nThe shape of the distribution is similar for males and females. The median, first quartile, third quartile lung capacity values all seem to be somewhat higher for males.\n\n\n\n\n\nCode\ndf %&gt;%\n  group_by(Smoke) %&gt;%\n  summarize(LungCap = mean(LungCap))\n\n\n# A tibble: 2 × 2\n  Smoke LungCap\n  &lt;chr&gt;   &lt;dbl&gt;\n1 no       7.77\n2 yes      8.65\n\n\nThe lung capacity for smokers seems to be higher than non-smokers. It goes against the common idea that smoking would hurt lung capacity.\n\n\n\n\nLess than or equal to 13\n\n\n\nCode\ndf %&gt;%\n  filter(Age &lt;= 13) %&gt;%\n  group_by(Smoke) %&gt;%\n  summarize(LungCap = mean(LungCap))\n\n\n# A tibble: 2 × 2\n  Smoke LungCap\n  &lt;chr&gt;   &lt;dbl&gt;\n1 no       6.36\n2 yes      7.20\n\n\n\n14 to 15\n\n\n\nCode\ndf %&gt;%\n  filter(Age == 14 | Age == 15) %&gt;%\n  group_by(Smoke) %&gt;%\n  summarize(LungCap = mean(LungCap))\n\n\n# A tibble: 2 × 2\n  Smoke LungCap\n  &lt;chr&gt;   &lt;dbl&gt;\n1 no       9.14\n2 yes      8.39\n\n\n\n16 to 17\n\n\n\nCode\ndf %&gt;%\n  filter(Age == 16 | Age == 17) %&gt;%\n  group_by(Smoke) %&gt;%\n  summarize(LungCap = mean(LungCap))\n\n\n# A tibble: 2 × 2\n  Smoke LungCap\n  &lt;chr&gt;   &lt;dbl&gt;\n1 no      10.5 \n2 yes      9.38\n\n\n\nGreater than or equal to 18\n\n\n\nCode\ndf %&gt;%\n  filter(Age &gt;= 18) %&gt;%\n  group_by(Smoke) %&gt;%\n  summarize(LungCap = mean(LungCap))\n\n\n# A tibble: 2 × 2\n  Smoke LungCap\n  &lt;chr&gt;   &lt;dbl&gt;\n1 no       11.1\n2 yes      10.5\n\n\n\n\n\nFor three out of the four groups, lung capacity if smaller for smokers. This makes another explanation plausible. Smoking is inversely related to lung capacity, but older people both smoke more and have more lung capacity. Thus, considering the relationship between smoking and lung capacity without looking at age makes the relationship look the opposite of what it is."
  },
  {
    "objectID": "posts/HW1_solution_Pang.html#a",
    "href": "posts/HW1_solution_Pang.html#a",
    "title": "Homework 1 Solution",
    "section": "",
    "text": "First, let’s read in the data from the Excel file:\n\n\nCode\nlibrary(readxl)\nlibrary(dplyr, warn.conflicts = F)\nlibrary(magrittr)\ndf &lt;- read_excel(\"_data/LungCapData.xls\")\n\n\nThe distribution of LungCap looks as follows:\n\n\nCode\nhist(df$LungCap, xlab = 'Lung Capacity', main = '', freq = F)\n\n\n\n\n\nThe histogram suggests that the distribution is close to a normal distribution. Most of the observations are close to the mean. Very few observations are close to the margins (0 and 15)."
  },
  {
    "objectID": "posts/HW1_solution_Pang.html#b",
    "href": "posts/HW1_solution_Pang.html#b",
    "title": "Homework 1 Solution",
    "section": "",
    "text": "Code\nboxplot(LungCap ~ Gender, data = df)\n\n\n\n\n\nThe shape of the distribution is similar for males and females. The median, first quartile, third quartile lung capacity values all seem to be somewhat higher for males."
  },
  {
    "objectID": "posts/HW1_solution_Pang.html#c",
    "href": "posts/HW1_solution_Pang.html#c",
    "title": "Homework 1 Solution",
    "section": "",
    "text": "Code\ndf %&gt;%\n  group_by(Smoke) %&gt;%\n  summarize(LungCap = mean(LungCap))\n\n\n# A tibble: 2 × 2\n  Smoke LungCap\n  &lt;chr&gt;   &lt;dbl&gt;\n1 no       7.77\n2 yes      8.65\n\n\nThe lung capacity for smokers seems to be higher than non-smokers. It goes against the common idea that smoking would hurt lung capacity."
  },
  {
    "objectID": "posts/HW1_solution_Pang.html#d",
    "href": "posts/HW1_solution_Pang.html#d",
    "title": "Homework 1 Solution",
    "section": "",
    "text": "Less than or equal to 13\n\n\n\nCode\ndf %&gt;%\n  filter(Age &lt;= 13) %&gt;%\n  group_by(Smoke) %&gt;%\n  summarize(LungCap = mean(LungCap))\n\n\n# A tibble: 2 × 2\n  Smoke LungCap\n  &lt;chr&gt;   &lt;dbl&gt;\n1 no       6.36\n2 yes      7.20\n\n\n\n14 to 15\n\n\n\nCode\ndf %&gt;%\n  filter(Age == 14 | Age == 15) %&gt;%\n  group_by(Smoke) %&gt;%\n  summarize(LungCap = mean(LungCap))\n\n\n# A tibble: 2 × 2\n  Smoke LungCap\n  &lt;chr&gt;   &lt;dbl&gt;\n1 no       9.14\n2 yes      8.39\n\n\n\n16 to 17\n\n\n\nCode\ndf %&gt;%\n  filter(Age == 16 | Age == 17) %&gt;%\n  group_by(Smoke) %&gt;%\n  summarize(LungCap = mean(LungCap))\n\n\n# A tibble: 2 × 2\n  Smoke LungCap\n  &lt;chr&gt;   &lt;dbl&gt;\n1 no      10.5 \n2 yes      9.38\n\n\n\nGreater than or equal to 18\n\n\n\nCode\ndf %&gt;%\n  filter(Age &gt;= 18) %&gt;%\n  group_by(Smoke) %&gt;%\n  summarize(LungCap = mean(LungCap))\n\n\n# A tibble: 2 × 2\n  Smoke LungCap\n  &lt;chr&gt;   &lt;dbl&gt;\n1 no       11.1\n2 yes      10.5"
  },
  {
    "objectID": "posts/HW1_solution_Pang.html#e",
    "href": "posts/HW1_solution_Pang.html#e",
    "title": "Homework 1 Solution",
    "section": "",
    "text": "For three out of the four groups, lung capacity if smaller for smokers. This makes another explanation plausible. Smoking is inversely related to lung capacity, but older people both smoke more and have more lung capacity. Thus, considering the relationship between smoking and lung capacity without looking at age makes the relationship look the opposite of what it is."
  },
  {
    "objectID": "posts/HW1_solution_Pang.html#a-1",
    "href": "posts/HW1_solution_Pang.html#a-1",
    "title": "Homework 1 Solution",
    "section": "a",
    "text": "a\n\n\nCode\ntb %&gt;%\n  filter(X == 2) %&gt;%\n  pull(Frequency) %&gt;%\n  divide_by(n)\n\n\n[1] 0.1975309"
  },
  {
    "objectID": "posts/HW1_solution_Pang.html#b-1",
    "href": "posts/HW1_solution_Pang.html#b-1",
    "title": "Homework 1 Solution",
    "section": "b",
    "text": "b\n\n\nCode\ntb %&gt;%\n  filter(X &lt; 2) %&gt;%\n  pull(Frequency) %&gt;%\n  sum() %&gt;%\n  divide_by(n)\n\n\n[1] 0.6938272"
  },
  {
    "objectID": "posts/HW1_solution_Pang.html#c-1",
    "href": "posts/HW1_solution_Pang.html#c-1",
    "title": "Homework 1 Solution",
    "section": "c",
    "text": "c\n\n\nCode\ntb %&gt;%\n  filter(X &lt;= 2) %&gt;%\n  pull(Frequency) %&gt;%\n  sum() %&gt;%\n  divide_by(n)\n\n\n[1] 0.891358"
  },
  {
    "objectID": "posts/HW1_solution_Pang.html#d-1",
    "href": "posts/HW1_solution_Pang.html#d-1",
    "title": "Homework 1 Solution",
    "section": "d",
    "text": "d\n\n\nCode\ntb %&gt;%\n  filter(X &gt; 2) %&gt;%\n  pull(Frequency) %&gt;%\n  sum() %&gt;%\n  divide_by(n)\n\n\n[1] 0.108642"
  },
  {
    "objectID": "posts/HW1_solution_Pang.html#e-1",
    "href": "posts/HW1_solution_Pang.html#e-1",
    "title": "Homework 1 Solution",
    "section": "e",
    "text": "e\nExpected number of prior convictions is just a weighted average of the number of prior convictions.\n\nMethod 1: Multiply every value with their frequency, then divide by total frequency i.e. (0 * 128 + 1 * 434 + 2 * 160 ……) / 810.\n\n\n\nCode\nsum(tb$X * tb$Frequency) / n\n\n\n[1] 1.28642\n\n\n\nMethod 2: Multiply every value with their probility, sum them up.\n\n\n\nCode\ntb %&gt;%\n  mutate(probability = Frequency / n) -&gt; tb\n\nprint(tb)\n\n\n# A tibble: 5 × 3\n      X Frequency probability\n  &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt;\n1     0       128      0.158 \n2     1       434      0.536 \n3     2       160      0.198 \n4     3        64      0.0790\n5     4        24      0.0296\n\n\n\n\nCode\nsum(tb$X * tb$probability)\n\n\n[1] 1.28642\n\n\n\nMethod 3: Recreate the whole sample (a vector that has 128 zeroes, 434 ones, 160 twos, ….) with a total length/size of 810. Take the mean.\n\n\n\nCode\nsample &lt;- c(rep(0, 128), rep(1, 434), rep(2, 160), rep(3, 64), rep(4, 24))\nmean(sample)\n\n\n[1] 1.28642"
  },
  {
    "objectID": "posts/HW1_solution_Pang.html#f",
    "href": "posts/HW1_solution_Pang.html#f",
    "title": "Homework 1 Solution",
    "section": "f",
    "text": "f\n\nMethod 1: Let’s start from the end: we have the sample, just call var() and sd()\n\n\n\nCode\ncat('Variance:', var(sample))\n\n\nVariance: 0.8572937\n\n\nCode\ncat('\\nStandard Deviation:', sd(sample))\n\n\n\nStandard Deviation: 0.9259016\n\n\nMethod 2: Manually apply the formula using weights.\nStandard deviation is square root of variance. So let’s calculate variance first. For that we need the mean. Let’s pull the expected value from the previous section:\n\n\nCode\nm &lt;- sum(tb$X * tb$Frequency) / n\n\n\nFor every observation, we’ll need the squared difference from mean (squared deviation from mean).\n\n\nCode\ntb %&gt;%\n  mutate(sq_deviation = (X - m)^2) -&gt; tb \nprint(tb)\n\n\n# A tibble: 5 × 4\n      X Frequency probability sq_deviation\n  &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;\n1     0       128      0.158        1.65  \n2     1       434      0.536        0.0820\n3     2       160      0.198        0.509 \n4     3        64      0.0790       2.94  \n5     4        24      0.0296       7.36  \n\n\nThen, we can now multiply them with probability.\n\n\nCode\nsum(tb$sq_deviation * tb$probability)\n\n\n[1] 0.8562353\n\n\nThis gives us the ‘population’ variance. If we wanted the ‘sample’ variance, what the var() function does, we could manually apply the Bessel’s correction:\n\n\nCode\nvariance &lt;- sum(tb$sq_deviation * tb$probability) * (n / (n-1))\nprint(variance)\n\n\n[1] 0.8572937\n\n\nStandard deviation is then just the square root:\n\n\nCode\nsqrt(variance)\n\n\n[1] 0.9259016\n\n\nThis replicated what we found directly using the sample."
  },
  {
    "objectID": "posts/Thrishul_Final_project_checkin_1.html#variable-descriptions",
    "href": "posts/Thrishul_Final_project_checkin_1.html#variable-descriptions",
    "title": "Final Project check in 1",
    "section": "VARIABLE DESCRIPTIONS:",
    "text": "VARIABLE DESCRIPTIONS:\n\ngender: specifies gender of the student(male/female)\nrace: specifies race of the student(group A,group B,group C)\nparental level of education: specifies highest educational qualification of any parent of each student\nlunch_type: standard/reduced,the type of lunch package selected for the student\ntest_prep: specifies if the test preparation course was completed by the student or not\nmath_score: specifies score in math(our target variable)\nreading_score: specifies score in reading\nwriting_score: specifies score in writing\n\nAll scores are taken out of 100.\n##Hypothesis\nThere is a significant correlation between the features available in the dataset, such as parental level of education, test preparation course, and lunch type, and the math score of students. By using various regression algorithms, it is possible to predict the math score of students with reasonable accuracy based on the available features. Furthermore, the use of regression algorithms can identify the most influential factors that contribute to student performance in Math, allowing educators and policymakers to take appropriate actions to improve student performance"
  },
  {
    "objectID": "posts/Thrishul_Final_project_checkin_1.html#importing-libraries",
    "href": "posts/Thrishul_Final_project_checkin_1.html#importing-libraries",
    "title": "Final Project check in 1",
    "section": "Importing Libraries",
    "text": "Importing Libraries\n\n\nCode\nset.seed(12345)\nlibrary(caret)\n\n\nWarning: package 'caret' was built under R version 4.2.3\n\n\nLoading required package: ggplot2\n\n\nLoading required package: lattice\n\n\nCode\nlibrary(Metrics)\n\n\nWarning: package 'Metrics' was built under R version 4.2.3\n\n\n\nAttaching package: 'Metrics'\n\n\nThe following objects are masked from 'package:caret':\n\n    precision, recall\n\n\nCode\n#The caret package provides a wide range of functions for training and evaluating machine learning models, while the Metrics package provides various metrics for evaluating model performance, including the R-squared score \n\nlibrary(glmnet)\n\n\nWarning: package 'glmnet' was built under R version 4.2.3\n\n\nLoading required package: Matrix\n\n\nLoaded glmnet 4.1-7\n\n\nCode\n#The glmnet package provides functions for fitting regularized regression models, including Ridge regression (glmnet function with alpha = 0) and Lasso regression (glmnet function with alpha = 1)\n\n#To perform cross-validation, the cv.glmnet function which performs k-fold cross-validation with a specified number of folds (nfolds)\n\n#Similarly, performing Lasso regression by setting alpha = 1 in the glmnet function\n\n\n\n\nCode\nlibrary(readr)\nStudentsPerformance &lt;- read_csv(\"_data/StudentsPerformance.csv\", show_col_types = FALSE)\nstr(StudentsPerformance)\n\n\nspc_tbl_ [1,000 × 8] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ gender                     : chr [1:1000] \"female\" \"female\" \"female\" \"male\" ...\n $ race/ethnicity             : chr [1:1000] \"group B\" \"group C\" \"group B\" \"group A\" ...\n $ parental level of education: chr [1:1000] \"bachelor's degree\" \"some college\" \"master's degree\" \"associate's degree\" ...\n $ lunch                      : chr [1:1000] \"standard\" \"standard\" \"standard\" \"free/reduced\" ...\n $ test preparation course    : chr [1:1000] \"none\" \"completed\" \"none\" \"none\" ...\n $ math score                 : num [1:1000] 72 69 90 47 76 71 88 40 64 38 ...\n $ reading score              : num [1:1000] 72 90 95 57 78 83 95 43 64 60 ...\n $ writing score              : num [1:1000] 74 88 93 44 75 78 92 39 67 50 ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   gender = col_character(),\n  ..   `race/ethnicity` = col_character(),\n  ..   `parental level of education` = col_character(),\n  ..   lunch = col_character(),\n  ..   `test preparation course` = col_character(),\n  ..   `math score` = col_double(),\n  ..   `reading score` = col_double(),\n  ..   `writing score` = col_double()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\n\n\n\nCode\nsummary(StudentsPerformance)\n\n\n    gender          race/ethnicity     parental level of education\n Length:1000        Length:1000        Length:1000                \n Class :character   Class :character   Class :character           \n Mode  :character   Mode  :character   Mode  :character           \n                                                                  \n                                                                  \n                                                                  \n    lunch           test preparation course   math score     reading score   \n Length:1000        Length:1000             Min.   :  0.00   Min.   : 17.00  \n Class :character   Class :character        1st Qu.: 57.00   1st Qu.: 59.00  \n Mode  :character   Mode  :character        Median : 66.00   Median : 70.00  \n                                            Mean   : 66.09   Mean   : 69.17  \n                                            3rd Qu.: 77.00   3rd Qu.: 79.00  \n                                            Max.   :100.00   Max.   :100.00  \n writing score   \n Min.   : 10.00  \n 1st Qu.: 57.75  \n Median : 69.00  \n Mean   : 68.05  \n 3rd Qu.: 79.00  \n Max.   :100.00"
  },
  {
    "objectID": "posts/Thrishul_Final_project_checkin_1.html#proposed-models",
    "href": "posts/Thrishul_Final_project_checkin_1.html#proposed-models",
    "title": "Final Project check in 1",
    "section": "Proposed Models",
    "text": "Proposed Models\nvarious regression algorithms such as Linear Regression, Polynomial Regression, Ridge Regression, Lasso Regression, and ElasticNet Regression will be trained on the preprocessed dataset. The performance of each algorithm will be evaluated using metrics such as Mean Squared Error, Root Mean Squared Error, and R-squared."
  },
  {
    "objectID": "posts/HW1_RahulSomu.html",
    "href": "posts/HW1_RahulSomu.html",
    "title": "Homework1",
    "section": "",
    "text": "First, let’s read in the data from the Excel file:\n\n\nCode\nlibrary(readxl)\nlibrary(dplyr)\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nCode\nlibrary(ggplot2)\ngetwd()\n\n\n[1] \"/Users/rahulsomu/Documents/DACSS_601/603_repo/posts\"\n\n\nCode\ndf &lt;- read_excel(\"_data/LungCapData.xls\")\n\n\nThe distribution of LungCap looks as follows:\n1a) The histogram suggests that the distribution is close to a normal distribution. Most of the observations are close to the mean. Very few observations are close to the margins (0 and 15).\n\n\nCode\nhist(df$LungCap)\n\n\n\n\n\n1b) Median lung capacity of male is greater than that of female.\n\n\nCode\nboxplot(LungCap ~ Gender, data = df, xlab = \"Gender\", ylab = \"Lung Capacity\",\n        main = \"Distribution of Lung Capacity by Gender\")\n\n\n\n\n\n1c) Logically mean lung capacity of non-smokers should be more than of smokers but with the data, it’s other way round\n\n\nCode\n# Calculate the mean lung capacity for smokers and non-smokers\nmean_lungcap_smokers &lt;- mean(df$LungCap[df$Smoke == \"yes\"])\nmean_lungcap_non_smokers &lt;- mean(df$LungCap[df$Smoke == \"no\"])\n\n# Print the mean lung capacities\ncat(\"Mean lung capacity for smokers:\", round(mean_lungcap_smokers, 2), \"\\n\")\n\n\nMean lung capacity for smokers: 8.65 \n\n\nCode\ncat(\"Mean lung capacity for non-smokers:\", round(mean_lungcap_non_smokers, 2), \"\\n\")\n\n\nMean lung capacity for non-smokers: 7.77 \n\n\n1d) The average lung capacity growth for the non-smokers is more that of the smokers. The lung capacity has been gradually increasing with age 1e) The discrepancy in 1c is due the data for the 13 or younger age where the average lung capacity for the non-smokers is less that of the smokers. Also there have been more data points for 13 or younger age group non-smokers which is affecting the mean of entire distribution.\n\n\nCode\n# Define the age groups\ndf &lt;- df %&gt;%\n  mutate(age_groups = cut(Age, c(0, 13, 15, 17, Inf), labels = c(\"&lt;= 13\", \"14-15\", \"16-17\", \"&gt;= 18\")))\n\n# Compare the probability distribution of lung capacity by gender\ndf %&gt;%\n  ggplot(aes(x = Gender, y = LungCap)) +\n  geom_boxplot() +\n  labs(x = \"Gender\", y = \"Lung Capacity\", \n       title = \"Lung Capacity by Gender\")\n\n\n\n\n\nCode\n# Compare the mean lung capacities for smokers and non-smokers\ndf %&gt;%\n  group_by(Smoke) %&gt;%\n  summarize(mean_lungcap = mean(LungCap)) %&gt;%\n  print()\n\n\n# A tibble: 2 × 2\n  Smoke mean_lungcap\n  &lt;chr&gt;        &lt;dbl&gt;\n1 no            7.77\n2 yes           8.65\n\n\nCode\n# Examine the relationship between smoking and lung capacity within age groups\ndf %&gt;%\n  filter(Smoke %in% c(\"yes\", \"no\")) %&gt;%\n  group_by(age_groups, Smoke) %&gt;%\n  summarize(mean_lungcap = mean(LungCap)) %&gt;%\n  print()\n\n\n`summarise()` has grouped output by 'age_groups'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 8 × 3\n# Groups:   age_groups [4]\n  age_groups Smoke mean_lungcap\n  &lt;fct&gt;      &lt;chr&gt;        &lt;dbl&gt;\n1 &lt;= 13      no            6.36\n2 &lt;= 13      yes           7.20\n3 14-15      no            9.14\n4 14-15      yes           8.39\n5 16-17      no           10.5 \n6 16-17      yes           9.38\n7 &gt;= 18      no           11.1 \n8 &gt;= 18      yes          10.5 \n\n\nCode\n# Compare the lung capacities for smokers and non-smokers within each age group\ndf %&gt;%\n  filter(Smoke %in% c(\"yes\", \"no\")) %&gt;%\n  ggplot(aes(x = age_groups, y = LungCap, fill = Smoke)) +\n  geom_boxplot() +\n  labs(x = \"Age Group\", y = \"Lung Capacity\", \n       title = \"Lung Capacity by Smoking Status and Age Group\")\n\n\n\n\n\n#Challange2"
  },
  {
    "objectID": "posts/HW1_RahulSomu.html#a",
    "href": "posts/HW1_RahulSomu.html#a",
    "title": "Homework1",
    "section": "",
    "text": "First, let’s read in the data from the Excel file:\n\n\nCode\nlibrary(readxl)\nlibrary(dplyr)\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nCode\nlibrary(ggplot2)\ngetwd()\n\n\n[1] \"/Users/rahulsomu/Documents/DACSS_601/603_repo/posts\"\n\n\nCode\ndf &lt;- read_excel(\"_data/LungCapData.xls\")\n\n\nThe distribution of LungCap looks as follows:\n1a) The histogram suggests that the distribution is close to a normal distribution. Most of the observations are close to the mean. Very few observations are close to the margins (0 and 15).\n\n\nCode\nhist(df$LungCap)\n\n\n\n\n\n1b) Median lung capacity of male is greater than that of female.\n\n\nCode\nboxplot(LungCap ~ Gender, data = df, xlab = \"Gender\", ylab = \"Lung Capacity\",\n        main = \"Distribution of Lung Capacity by Gender\")\n\n\n\n\n\n1c) Logically mean lung capacity of non-smokers should be more than of smokers but with the data, it’s other way round\n\n\nCode\n# Calculate the mean lung capacity for smokers and non-smokers\nmean_lungcap_smokers &lt;- mean(df$LungCap[df$Smoke == \"yes\"])\nmean_lungcap_non_smokers &lt;- mean(df$LungCap[df$Smoke == \"no\"])\n\n# Print the mean lung capacities\ncat(\"Mean lung capacity for smokers:\", round(mean_lungcap_smokers, 2), \"\\n\")\n\n\nMean lung capacity for smokers: 8.65 \n\n\nCode\ncat(\"Mean lung capacity for non-smokers:\", round(mean_lungcap_non_smokers, 2), \"\\n\")\n\n\nMean lung capacity for non-smokers: 7.77 \n\n\n1d) The average lung capacity growth for the non-smokers is more that of the smokers. The lung capacity has been gradually increasing with age 1e) The discrepancy in 1c is due the data for the 13 or younger age where the average lung capacity for the non-smokers is less that of the smokers. Also there have been more data points for 13 or younger age group non-smokers which is affecting the mean of entire distribution.\n\n\nCode\n# Define the age groups\ndf &lt;- df %&gt;%\n  mutate(age_groups = cut(Age, c(0, 13, 15, 17, Inf), labels = c(\"&lt;= 13\", \"14-15\", \"16-17\", \"&gt;= 18\")))\n\n# Compare the probability distribution of lung capacity by gender\ndf %&gt;%\n  ggplot(aes(x = Gender, y = LungCap)) +\n  geom_boxplot() +\n  labs(x = \"Gender\", y = \"Lung Capacity\", \n       title = \"Lung Capacity by Gender\")\n\n\n\n\n\nCode\n# Compare the mean lung capacities for smokers and non-smokers\ndf %&gt;%\n  group_by(Smoke) %&gt;%\n  summarize(mean_lungcap = mean(LungCap)) %&gt;%\n  print()\n\n\n# A tibble: 2 × 2\n  Smoke mean_lungcap\n  &lt;chr&gt;        &lt;dbl&gt;\n1 no            7.77\n2 yes           8.65\n\n\nCode\n# Examine the relationship between smoking and lung capacity within age groups\ndf %&gt;%\n  filter(Smoke %in% c(\"yes\", \"no\")) %&gt;%\n  group_by(age_groups, Smoke) %&gt;%\n  summarize(mean_lungcap = mean(LungCap)) %&gt;%\n  print()\n\n\n`summarise()` has grouped output by 'age_groups'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 8 × 3\n# Groups:   age_groups [4]\n  age_groups Smoke mean_lungcap\n  &lt;fct&gt;      &lt;chr&gt;        &lt;dbl&gt;\n1 &lt;= 13      no            6.36\n2 &lt;= 13      yes           7.20\n3 14-15      no            9.14\n4 14-15      yes           8.39\n5 16-17      no           10.5 \n6 16-17      yes           9.38\n7 &gt;= 18      no           11.1 \n8 &gt;= 18      yes          10.5 \n\n\nCode\n# Compare the lung capacities for smokers and non-smokers within each age group\ndf %&gt;%\n  filter(Smoke %in% c(\"yes\", \"no\")) %&gt;%\n  ggplot(aes(x = age_groups, y = LungCap, fill = Smoke)) +\n  geom_boxplot() +\n  labs(x = \"Age Group\", y = \"Lung Capacity\", \n       title = \"Lung Capacity by Smoking Status and Age Group\")\n\n\n\n\n\n#Challange2"
  },
  {
    "objectID": "posts/FelixBetancourt_Finalpart1 v2.html",
    "href": "posts/FelixBetancourt_Finalpart1 v2.html",
    "title": "Final Project - Check point 1",
    "section": "",
    "text": "Code\nknitr::opts_chunk$set(echo = TRUE, warning = FALSE)\n\n\n\nBurnout in the workplace.\nDACSS 603, Spring 2023\nBurnout is a pervasive issue in many professions, and its consequences can be significant for individuals and organizations alike. According to Maslach and Leiter (2016), burnout is characterized by emotional exhaustion, despersonalization, and reduced personal accomplishment. It is prevalent in a variety of fields, including healthcare (West et al., 2016), among other professions. Burnout can have serious consequences, including decreased job satisfaction, increased absenteeism, and turnover (West et al., 2016).\nThere is a growing body of research exploring the causes and consequences of burnout, as well as potential solutions. Some scholars have identified factors such as job demands, lack of control, and social support as contributing to burnout (Bakker & Demerouti, 2017).\nLee & Eissenstat (2017), for instance, affirm that psychological job demands and work-to-family conflict, as well as control over working hours/schedule, decision-making authority, and role clarity, have significant effects on burnout.\nThe aim of this research paper is to provide a comprehensive understanding of how type of business, and seniority level explain the level of burnout.\nNull hypothesis: The type of business, and seniority does not affect the burnout rate.\nAlternative hypotheses:\n\nWorking in Services (vs product type of business) predict significantly higher burnout rate, especially in Female workers.\nLess years of experience (lower seniority) is significantly related to higher burnout rate.\nThe numbers of work hours allocated affect the burnout rate significantly in WFH setup (vs on site).\n\n\n\nAbout the Data\n\n\nCode\n# Loading packages\n\nsuppressPackageStartupMessages(library(dplyr))\nsuppressPackageStartupMessages(library(tidyverse))\n\n# Reading the data.\n\nburn &lt;- read.csv(\"_data/burnout.csv\")\n\n\nThe dataset was obtained from:\nhttps://www.kaggle.com/datasets/blurredmachine/are-your-employees-burning-out\nLet’s check the strucutre of the dataset:\n\n\nCode\n#Structure\nstr(burn)\n\n\n'data.frame':   22750 obs. of  9 variables:\n $ Employee.ID         : chr  \"fffe32003000360033003200\" \"fffe3700360033003500\" \"fffe31003300320037003900\" \"fffe32003400380032003900\" ...\n $ Date.of.Joining     : chr  \"2008-09-30\" \"2008-11-30\" \"2008-03-10\" \"2008-11-03\" ...\n $ Gender              : chr  \"Female\" \"Male\" \"Female\" \"Male\" ...\n $ Company.Type        : chr  \"Service\" \"Service\" \"Product\" \"Service\" ...\n $ WFH.Setup.Available : chr  \"No\" \"Yes\" \"Yes\" \"Yes\" ...\n $ Designation         : num  2 1 2 1 3 2 3 2 3 3 ...\n $ Resource.Allocation : num  3 2 NA 1 7 4 6 4 6 6 ...\n $ Mental.Fatigue.Score: num  3.8 5 5.8 2.6 6.9 3.6 7.9 4.4 NA NA ...\n $ Burn.Rate           : num  0.16 0.36 0.49 0.2 0.52 0.29 0.62 0.33 0.56 0.67 ...\n\n\nThe dataset contain 9 variables and 22750 observations.\nFour of the variables are categorical and five are numeric (including one as date)\nLet’s see a Summary for each variable.\n\n\nCode\nsummary(burn)\n\n\n Employee.ID        Date.of.Joining       Gender          Company.Type      \n Length:22750       Length:22750       Length:22750       Length:22750      \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n                                                                            \n WFH.Setup.Available  Designation    Resource.Allocation Mental.Fatigue.Score\n Length:22750        Min.   :0.000   Min.   : 1.000      Min.   : 0.000      \n Class :character    1st Qu.:1.000   1st Qu.: 3.000      1st Qu.: 4.600      \n Mode  :character    Median :2.000   Median : 4.000      Median : 5.900      \n                     Mean   :2.179   Mean   : 4.481      Mean   : 5.728      \n                     3rd Qu.:3.000   3rd Qu.: 6.000      3rd Qu.: 7.100      \n                     Max.   :5.000   Max.   :10.000      Max.   :10.000      \n                                     NA's   :1381        NA's   :2117        \n   Burn.Rate    \n Min.   :0.000  \n 1st Qu.:0.310  \n Median :0.450  \n Mean   :0.452  \n 3rd Qu.:0.590  \n Max.   :1.000  \n NA's   :1124   \n\n\nAccording to the source of the data, here is an explanation of each variable:\nEmployee ID: The unique ID allocated for each employee.\nDate of Joining: The date-time when the employee has joined the organization.\nGender: The gender of the employee\nCompany Type: The type of company where the employee is working\nWFH Setup Available: Is the work from home facility available for the employee\nDesignation: The designation of the employee of work in the organization. In the range of [0.0, 5.0] bigger is higher designation.\nResource Allocation: The amount of resource allocated to the employee to work, ie. number of working hours.In the range of [1.0, 10.0] (higher means more resource)\nMental Fatigue Score: The level of fatigue mentally the employee is facing.In the range of [0.0, 10.0] where 0.0 means no fatigue and 10.0 means completely fatigue.\nBurn Rate: The value we need to predict for each employee telling the rate of Bur out while working.In the range of [0.0, 1.0] where the higher the value is more is the burn out.\n\n\nReferences:\nBakker, A. B., & Demerouti, E. (2017). Job demands-resources theory: Taking stock and looking forward. Journal of Occupational Health Psychology, 22(3), 273–285.\nLee, Y., Eissenstat, S. (2017). A longitudinal examination of the causes and effects of burnout based on the job demands-resources model. International Journal for Educational and Vocational Guidance, 18(3), 337–354.\nMaslach, C., & Leiter, M. P. (2016). Understanding the burnout experience: Recent research and its implications for psychiatry. World Psychiatry, 15(2), 103–111.\nWest, C. P., Dyrbye, L. N., Erwin, P., Shanafelt, T. D., (2016). Interventions to promote physician well-being and mitigate burnout: A systematic review and meta-analysis. The Lancet, 388(10057), 2272–228"
  },
  {
    "objectID": "posts/asch_harwood_hw2.html",
    "href": "posts/asch_harwood_hw2.html",
    "title": "Homework 2",
    "section": "",
    "text": "Code\nlibrary(dplyr)\nlibrary(ggplot2)\n\n\n\nQ1\n\n\nCode\n# create data frame\ndf &lt;- data.frame(\n  \"Surgical Procedure\" = c(\"Bypass\", \"Angiography\"),\n  \"Sample Size\" = c(539, 847),\n  \"Mean Wait Time\" = c(19, 18),\n  \"Standard Deviation\" = c(10, 9)\n)\n\n# bypass confidence interval\nmean &lt;- 19\nn &lt;- 539\nsd &lt;- 10\nci_level &lt;- .9\ntail_area &lt;- (1 - ci_level)/2\nt_score &lt;- qt(p=1-tail_area, df = n - 1)\nb_ci &lt;- c(mean - t_score * sd/sqrt(n), mean + t_score * sd/sqrt(n))\n\n# angiography\nmean &lt;- 18\nn &lt;- 847\nsd &lt;- 9\nci_level &lt;- .9\ntail_area &lt;- (1 - ci_level)/2\nt_score &lt;- qt(p=1-tail_area, df = n - 1)\na_ci &lt;- c(mean - t_score * sd/sqrt(n), mean + t_score * sd/sqrt(n))\n\n\ncat(\"Bypass 90% Confidence Interval: (\", b_ci[1], \", \", b_ci[2], \")\", sep = \"\")\n\n\nBypass 90% Confidence Interval: (18.29029, 19.70971)\n\n\nCode\ncat(\"\\n\")\n\n\nCode\ncat(\"\\n\")\n\n\nCode\ncat(\"Angiography 90% Confidence Interval: (\", a_ci[1], \", \", a_ci[2], \")\", sep = \"\")\n\n\nAngiography 90% Confidence Interval: (17.49078, 18.50922)\n\n\n\nInterpretation\nBypass confidence interval: 18.29029, 19.70971 Angiography confidence interval: 17.49078 18.50922\nThe angiography confidence level is smaller.\n\n\n\nQ2\n\n\nCode\nn &lt;- 1031 # num of americans survey\ncollege_essential &lt;- 567 # num americans believe college is essential\npoint_estimate &lt;- college_essential/n\nprop_results &lt;- prop.test(college_essential, n)\n\ncat(\"College is essential point estimate:\", point_estimate)\n\n\nCollege is essential point estimate: 0.5499515\n\n\nCode\ncat(\"\\n\")\n\n\nCode\ncat(\"\\n\")\n\n\nCode\ncat(\"College is essential 95% Confidence Interval: (\", prop_results$conf.int[1], \", \", prop_results$conf.int[2], \")\", sep = \"\")\n\n\nCollege is essential 95% Confidence Interval: (0.5189682, 0.580558)\n\n\n\n\nQ3\n\n\nCode\n# solve for n:  CI = x + (t * (sd / sqrt(n)))\n# plug in value in this equation: n = (sd * t / (CI - x))^2\n#calculate mean\nhigh_price &lt;- 200\nlow_price &lt;- 30\n\nsd &lt;- (high_price - low_price) * .25\nmean &lt;- (high_price + low_price)/2\nci_lower &lt;- mean - 5\nci_upper &lt;- mean + 5\nci_level &lt;- .95\nestimated_sample_size &lt;- (sd * ci_level / (ci_upper - mean))^2\n\ncat(\"The estimated sample size is: \", round(estimated_sample_size))\n\n\nThe estimated sample size is:  65\n\n\n\n\nQ4\n\n\nCode\n# goal: test whether mean female incole is inline with union agreement\nmean_weekly_income_predicted &lt;- 500\nn = 9\nmean_sampled = 410\nsd_sample = 90\n\n\n\nQ4_A\n\nApproach and Assumptions\n\nApproach 1: Construct confidence interval\nApproach 2: One-sample, two-tailed T-test\nAssumptions:\n\n\nSample is random and representative of underlying population\nData is normally distributed\nSample observations are independent\nPopulation standard deviation is not known\n\n\nHypothesis\n\n\nh0: female weekly salary is the same as union weekly ($410 == $500)\nh1: female weekly salary is different from the union weekly (higher/lower) ($410 != $500)\n\n\n\nCode\n#confidence interval calculation\nmean &lt;- 410\nunion_mean &lt;- 500\nn &lt;- 9\nsd &lt;- 90\nalpha &lt;- 0.05\nci_level &lt;- .95\ntail_area &lt;- (1 - ci_level)/2\nt_score &lt;- qt(p=1-tail_area, df = n - 1)\na_ci &lt;- c(mean - t_score * sd/sqrt(n), mean + t_score * sd/sqrt(n))\n\n#p-value calculation\n\n#compute t_staistics\nt_stat &lt;- (mean - union_mean)/(sd/sqrt(n))\n\n#computer degrees of freedom\ndf &lt;- n - 1\n\n#calculate lower/left tail because t statistics is negative\n#multipy by two for two-tailed test\np_val &lt;- 2*pt(t_stat, df)\n\ncat(\"Approach 1: Confidence Interval\")\n\n\nApproach 1: Confidence Interval\n\n\nCode\ncat(\"\\n\")\n\n\nCode\ncat(\"\\n\")\n\n\nCode\ncat(\"Female Salary 95% Confidence Interval: (\", a_ci[1], \", \", a_ci[2], \")\", sep = \"\")\n\n\nFemale Salary 95% Confidence Interval: (340.8199, 479.1801)\n\n\nCode\ncat(\"\\n\")\n\n\nCode\ncat(\"T score:\", t_score)\n\n\nT score: 2.306004\n\n\nCode\ncat(\"\\n\")\n\n\nCode\ncat(\"\\n\")\n\n\nCode\ncat(\"Interpretation\")\n\n\nInterpretation\n\n\nCode\ncat(\"\\n\")\n\n\nCode\ncat(\"\\n\")\n\n\nCode\ncat(\"The 95% confidence interval for the weekly female salary of $410 does not include $500, so we can reject the null hypothesis in favor of the alternative hypthosis that weekly female salary differs from $500 a week\")\n\n\nThe 95% confidence interval for the weekly female salary of $410 does not include $500, so we can reject the null hypothesis in favor of the alternative hypthosis that weekly female salary differs from $500 a week\n\n\nCode\ncat(\"Approach 2: Calculat P Value\")\n\n\nApproach 2: Calculat P Value\n\n\nCode\ncat(\"\\n\")\n\n\nCode\ncat(\"\\n\")\n\n\nCode\ncat(\"T statistic:\", t_stat)\n\n\nT statistic: -3\n\n\nCode\ncat(\"\\n\")\n\n\nCode\ncat(\"Two-tailed P Value\", p_val)\n\n\nTwo-tailed P Value 0.01707168\n\n\nCode\ncat(\"\\n\")\n\n\nCode\ncat(\"Is P value less than alpha?:\", p_val &lt; alpha)\n\n\nIs P value less than alpha?: TRUE\n\n\nCode\ncat(\"\\n\")\n\n\nCode\ncat(\"\\n\")\n\n\nCode\ncat(\"Interpretation\")\n\n\nInterpretation\n\n\nCode\ncat(\"\\n\")\n\n\nCode\ncat(\"\\n\")\n\n\nCode\ncat(\"We can reject the null hypothesis in favor of the alternative hypothosis that weekly female salary differs from $500 a week because the observed p value of\", p_val, \"is less than our designated signifiance level of\", alpha, \".\")\n\n\nWe can reject the null hypothesis in favor of the alternative hypothosis that weekly female salary differs from $500 a week because the observed p value of 0.01707168 is less than our designated signifiance level of 0.05 .\n\n\n\n\n\nQ4_B\n\n\nCode\n#p-value calculation\n\nmean &lt;- 410\nunion_mean &lt;- 500\nn &lt;- 9\nsd &lt;- 90\nalpha &lt;- 0.05\n\n#compute t_staistics\nt_stat &lt;- (mean - union_mean)/(sd/sqrt(n))\n\n#computer degrees of freedom\ndf &lt;- n - 1\n\np_val &lt;- pt(t_stat, df, lower.tail = TRUE)\n\n#calculate lower/left tail because t statistics is negative\n#multipy by two for two-tailed test\ncat(\"Lower Tail P Value:\", p_val)\n\n\nLower Tail P Value: 0.008535841\n\n\nCode\ncat(\"\\n\")\n\n\nCode\ncat(\"This lower tail p value can be used to test whether the observed weekly female salary is less than the union wage. In this case, if we were doing a one-tailed t-test with an alpha significance level of 0.05, we could reject the null hypothesis in favor of the altnernative that weekly female salary is less than the union wage with a p value of\", p_val,\".\")\n\n\nThis lower tail p value can be used to test whether the observed weekly female salary is less than the union wage. In this case, if we were doing a one-tailed t-test with an alpha significance level of 0.05, we could reject the null hypothesis in favor of the altnernative that weekly female salary is less than the union wage with a p value of 0.008535841 .\n\n\n\n\nQ4_C\n\n\nCode\nmean &lt;- 410\nunion_mean &lt;- 500\nn &lt;- 9\nsd &lt;- 90\nalpha &lt;- 0.05\n\n#compute t_staistics\nt_stat &lt;- (mean - union_mean)/(sd/sqrt(n))\n\n#computer degrees of freedom\ndf &lt;- n - 1\n\np_val &lt;- pt(t_stat, df, lower.tail = FALSE)\n\n#calculate lower/left tail because t statistics is negative\n#multipy by two for two-tailed test\ncat(\"Upper Tail P Value:\", p_val)\n\n\nUpper Tail P Value: 0.9914642\n\n\nCode\ncat(\"\\n\")\n\n\nCode\ncat(\"This upper tail p value can be used to test whether the observed weekly female salary is more than the union wage. In this case, if we were doing a one-tailed t-test with an alpha significance level of 0.05, we cannot reject the null hypothesis in favor of the altnernative that weekly female salary is less than the union wage with a p value of\", p_val,\".\")\n\n\nThis upper tail p value can be used to test whether the observed weekly female salary is more than the union wage. In this case, if we were doing a one-tailed t-test with an alpha significance level of 0.05, we cannot reject the null hypothesis in favor of the altnernative that weekly female salary is less than the union wage with a p value of 0.9914642 .\n\n\n\n\n\nQ5\n\n\nCode\nn &lt;- 1000 #sample size\ndf &lt;- n - 1 #degrees of freedom\nse &lt;- 10.0 #standard error\npop_mean &lt;- 500 #assumed pop mean\ny_jones &lt;- 519.5\ny_smith &lt;- 519.7\nt_jones &lt;- 1.95\nt_smith &lt;- 1.97\np_jones &lt;- 0.051\np_smith &lt;- 0.049\n\n# helper functions - computes t-stat\nt_stat_func &lt;- function(mean_1, test_mean, standard_error){\n  t_stat &lt;- (mean_1 - test_mean)/(standard_error)\n  return(t_stat)\n}\n\n#helper function - computers p-value\np_val_func &lt;- function(t_stat, df, tail_type){\n  if (t_stat &lt; 0){\n    p_val &lt;- pt(t_stat, df, lower.tail = TRUE)\n  } else{\n    p_val &lt;- pt(t_stat, df, lower.tail = FALSE)\n  }\n  if (tail_type==2){\n    p_val &lt;- p_val*2\n  }else{p_val}\n  \n  return(p_val)\n}\n\n\n\nQ5_A\n\n\nCode\n#compute t_staistics for jones\n\nt_stat_jones &lt;- t_stat_func(y_jones, pop_mean, se)\n\np_val_jones &lt;- p_val_func(t_stat_jones, df, 2)\n\nt_stat_smith &lt;- t_stat_func(y_smith, pop_mean, se)\n\np_val_smith &lt;- p_val_func(t_stat_smith, df, 2)\n\ncat(\"Jones\")\n\n\nJones\n\n\nCode\ncat(\"\\n\")\n\n\nCode\ncat(\"T statistic:\", t_stat_jones,\",\", \"P value:\", p_val_jones)\n\n\nT statistic: 1.95 , P value: 0.05145555\n\n\nCode\ncat(\"\\n\")\n\n\nCode\ncat(\"\\n\")\n\n\nCode\ncat(\"Smith\")\n\n\nSmith\n\n\nCode\ncat(\"\\n\")\n\n\nCode\ncat(\"T statistic:\", t_stat_smith,\",\", \"P value:\", p_val_smith)\n\n\nT statistic: 1.97 , P value: 0.04911426\n\n\n\n\nQ5_B\nWith an alpha of 0.05, Smith’s result is statistically significant, while Jones’s result is not.\n\n\nQ5_C\nThe chosen significant level (alpha = 0.05, for example) is typically a rule of thumb in the social sciences. In this case, a decision to round a value to decimal places would put both of these p-values at 0.05. Context matters as well. Depending on what is being measured in this example, the difference between 519.5 versus 519.7 might not be meaningful, or it could represent a huge real world difference. The implications/consequences of committing type 1 or type 2 error could be pertinent in the interpretation of these p_values as well. By reporting the actual p-values, readers are empowered to better evaluate the decisions of the researchers.\n\n\n\nQ6\n\nNull Hypothesis\nH0: There is no association between snack choice and grade level\n6th_grade_healthy_snack_31 = 7th_grade_healthy_snack_43 = 8th_grade_healthy_snack_51\n6th_grade_unhealthy_snack_69 = 7th_grade_healthy_snack_57 = 8th_grade_healthy_snack_49\n\n\nChi Squared Test\n\nWe will use a chi-squared test\n\n\n\nCode\nsnack_counts &lt;- matrix(\n  c(31, 43, 51, 69, 57, 49),\n  nrow = 2,\n  byrow = TRUE\n)\nrownames(snack_counts) &lt;- c(\"Healthy_Snack\", \"Unhealthy_Snack\")\ncolnames(snack_counts) &lt;- c(\"6th grade\", \"7th grade\", \"8th grade\")\nsnack_counts\n\n\n                6th grade 7th grade 8th grade\nHealthy_Snack          31        43        51\nUnhealthy_Snack        69        57        49\n\n\nCode\n# Perform the Chi-square test\nchi_square_test &lt;- chisq.test(snack_counts)\n\n# Print the test results\nprint(chi_square_test)\n\n\n\n    Pearson's Chi-squared test\n\ndata:  snack_counts\nX-squared = 8.3383, df = 2, p-value = 0.01547\n\n\n\n\nInterpretation\nWe can reject the null hypothesis that there is no association between snack choice and grade in favor of the alternative hypothesis that there is an association between snack choice and grade\n\n\nCode\n# Create a dataframe\nsnack_data &lt;- data.frame(\n  Grade_Level = c(\"6th grade\", \"7th grade\", \"8th grade\"),\n  Healthy_Snack = c(31, 43, 51),\n  Unhealthy_Snack = c(69, 57, 49)\n)\n\n# Calculate the proportions\nsnack_data$Total &lt;- snack_data$Healthy_Snack + snack_data$Unhealthy_Snack\nsnack_data$Healthy_Prop &lt;- snack_data$Healthy_Snack / snack_data$Total\nsnack_data$Unhealthy_Prop &lt;- snack_data$Unhealthy_Snack / snack_data$Total\n\n# Reshape the data to a long format\nsnack_data_long &lt;- tidyr::gather(snack_data, key = \"Snack_Type\", value = \"Proportion\", Healthy_Prop, Unhealthy_Prop)\n\n# Create a bar plot using ggplot2\nggplot(snack_data_long, aes(x = Grade_Level, y = Proportion, fill = Snack_Type)) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  labs(title = \"Proportion of Healthy and Unhealthy Snacks by Grade Level\",\n       x = \"Grade Level\",\n       y = \"Proportion\",\n       fill = \"Snack Type\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nQ7\n\n\nCode\n# Data from the table\ndata_matrix &lt;- matrix(\n  c(\n    6.2, 9.3, 6.8, 6.1, 6.7, 7.5,\n    7.5, 8.2, 8.5, 8.2, 7.0, 9.3,\n    5.8, 6.4, 5.6, 7.1, 3.0, 3.5\n  ),\n  nrow = 3,\n  byrow = TRUE\n)\n# Create a dataframe with Area and Value columns\ndata_df &lt;- data.frame(\n  Area = factor(rep(c(\"Area 1\", \"Area 2\", \"Area 3\"), each = 6)),\n  Value = as.vector(t(data_matrix))\n)\n\nmean_df &lt;- data_df %&gt;%\n  group_by(Area) %&gt;%\n  summarise(mean_val = mean(Value))\n\n\n\nAnswer\n\nNull Hypothesis\nH0: All groups have the same population mean\narea_1 = area_2 = area_3 is TRUE\nH1: At least one group has a different population mean\narea_1 = area_2 = area_3 is FALSE\n\n\nANOVA\n\nSince we are trying to determine if the observed differences in area means are statistically significant, we will use a one-way analysis of variance test\n\n\n\nCode\nanova_results &lt;- aov(Value ~ Area, data_df)\nsummary(anova_results)\n\n\n            Df Sum Sq Mean Sq F value  Pr(&gt;F)   \nArea         2  25.66  12.832   8.176 0.00397 **\nResiduals   15  23.54   1.569                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\nInterpretation\nWith a P value of 0.00397 we can reject the null hypothesis that there is no difference between the three group means in favor of the alternative hypothesis that there is indeed a difference."
  },
  {
    "objectID": "posts/HW2_LTucksmith.html",
    "href": "posts/HW2_LTucksmith.html",
    "title": "HW2",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tidyr)\nknitr::opts_chunk$set(echo = TRUE)\n\n\n\nThe time between the date a patient was recommended for heart surgery and the surgery date for cardiac patients in Ontario was collected by the Cardiac Care Network (“Wait Times Data Guide,” Ministry of Health and Long-Term Care, Ontario, Canada, 2006). The sample mean and sample standard deviation for wait times (in days) of patients for two cardiac procedures are given in the accompanying table. Assume that the sample is representative of the Ontario population\n\nConstruct the 90% confidence interval to estimate the actual mean wait time for each of the two procedures. Is the confidence interval narrower for angiography or bypass surgery?\n\n\nCode\nbp_num &lt;- 539\nbp_mean &lt;- 19\nbp_sd &lt;- 10\nbp_se &lt;- sqrt(bp_sd)\nag_num &lt;- 847\nag_mean &lt;- 18\nag_sd &lt;- 9\nag_se &lt;- sqrt(ag_sd)\n\nalpha = 0.1\ndegrees.freedom = bp_num - 1\nt.score = qt(p=alpha/2, df=degrees.freedom,lower.tail=F)\n\nmargin.error &lt;- t.score * bp_se\n\nlower.bound &lt;- bp_mean - margin.error\nupper.bound &lt;- bp_mean + margin.error\nbypass_confidenceInterval &lt;- print(c(lower.bound,upper.bound))\n\n\n[1] 13.78954 24.21046\n\n\nCode\ndegrees.freedom = ag_num - 1\nt.score = qt(p=alpha/2, df=degrees.freedom,lower.tail=F)\n\nmargin.error &lt;- t.score * ag_se\n\nlower.bound &lt;- ag_mean - margin.error\nupper.bound &lt;- ag_mean + margin.error\nangiography_confidenceInterval &lt;- print(c(lower.bound,upper.bound))\n\n\n[1] 13.06003 22.93997\n\n\nThe confidence interval is narrower for angiography surgery than it is for bypass surgery. The bypass surgery confidence interval spans from 13.78954 to 24.21046 days while the angiography surgery spans from 13.06003 to 22.93997 days. Since the bypass surgery confidence interval spans 10.42 days and the angiography surgery spans 9.88 days, the angiography surgery has a narrower confidence interval than the bypass surgery.\n\nA survey of 1031 adult Americans was carried out by the National Center for Public Policy. Assume that the sample is representative of adult Americans. Among those surveyed, 567 believed that college education is essential for success. Find the point estimate, p, of the proportion of all adult Americans who believe that a college education is essential for success. Construct and interpret a 95% confidence interval for p.\n\n\n\nCode\nn = 1031 \ncollege_y = 567\np = college_y/n\nprint(p)\n\n\n[1] 0.5499515\n\n\nCode\nmargin_error &lt;- qnorm(0.95)*sqrt(p*(1-p)/n)\nlowerbound &lt;- p - margin_error\nupperbound &lt;- p + margin_error\n\np_confidenceInterval &lt;- print(c(lowerbound,upperbound))\n\n\n[1] 0.5244662 0.5754368\n\n\n\nSuppose that the financial aid office of UMass Amherst seeks to estimate the mean cost of textbooks per semester for students. The estimate will be useful if it is within $5 of the true population mean (i.e. they want the confidence interval to have a length of $10 or less). The financial aid office is pretty sure that the amount spent on books varies widely, with most values between $30 and $200. They think that the population standard deviation is about a quarter of this range (in other words, you can assume they know the population standard deviation). Assuming the significance level to be 5%, what should be the size of the sample\n\n\n\nCode\nmargin_error_max = 5\nconfidence_interval_length = 10\nlower_bound_avg = 30\nupper_bound_avg = 200\nrange = upper_bound_avg - lower_bound_avg\n\nz = .95 +(1-.95)/2\nn = ((z - (range/4))/confidence_interval_length)^2\n\nprint(n)\n\n\n[1] 17.24326\n\n\n\nAccording to a union agreement, the mean income for all senior-level workers in a large service company equals $500 per week. A representative of a women’s group decides to analyze whether the mean income μ for female employees matches this norm. For a random sample of nine female employees, ȳ = $410 and s = 90\n\n\n\nCode\nincome_n &lt;- c(rnorm(9, mean = 410, sd = 90))\n\n\nA. Test whether the mean income of female employees differs from $500 per week. Include assumptions, hypotheses, test statistic, and P-value. Interpret the result.\nassumption: The mean income of female employees is $500 per week. null hypothesis: The mean income of female employees is $500 per week. alternative hypothesis: The mean income of female employees differs from $500 per week.\n\n\nCode\nincome_female &lt;- t.test(income_n, mu=500)\nprint(income_female)\n\n\n\n    One Sample t-test\n\ndata:  income_n\nt = -4.0036, df = 8, p-value = 0.00393\nalternative hypothesis: true mean is not equal to 500\n95 percent confidence interval:\n 303.2482 447.0648\nsample estimates:\nmean of x \n 375.1565 \n\n\nThe result is that with 95% certainty the mean female income falls between $322.6557 and $441.1327. Since the range falls below $500, we reject the null hypothesis that the mean female income is $500, and accept the alternative hypothesis that the true mean female income is not equal to $500.\nB. Report the P-value for Ha: μ &lt; 500. Interpret.\n\n\nCode\nincome_female &lt;- t.test(income_n, mu=500, alternative = 'less')\nprint(income_female)\n\n\n\n    One Sample t-test\n\ndata:  income_n\nt = -4.0036, df = 8, p-value = 0.001965\nalternative hypothesis: true mean is less than 500\n95 percent confidence interval:\n     -Inf 433.1429\nsample estimates:\nmean of x \n 375.1565 \n\n\nThe p_value is 0.0081. It is less than the 0.5 alpha so we reject the null hypothesis that that the mean female income is $500, and accept the alternative hypothesis that the true mean female income is less than $500.\nC. Report and interpret the P-value for Ha: μ &gt; 500.\n\n\nCode\nincome_female &lt;- t.test(income_n, mu=500, alternative = 'greater')\nprint(income_female)\n\n\n\n    One Sample t-test\n\ndata:  income_n\nt = -4.0036, df = 8, p-value = 0.998\nalternative hypothesis: true mean is greater than 500\n95 percent confidence interval:\n 317.1701      Inf\nsample estimates:\nmean of x \n 375.1565 \n\n\nThe p_value is 0.9919. It is not less than the 0.5 alpha so we fail to reject the null hypothesis that that the mean female income is $500.\n\nJones and Smith separately conduct studies to test H0: μ = 500 against Ha: μ ≠ 500, each with n = 1000. Jones gets ȳ = 519.5, with se = 10.0. Smith gets ȳ = 519.7, with se = 10.0.\n\n\n\nCode\nsd.js = 10*(sqrt(1000))\n\nincome_jones &lt;- c(rnorm(1000, mean = 519.5, sd = sd.js))\nincomes_smith &lt;- c(rnorm(1000, mean = 519.7, sd = sd.js))\n\n\nA. Show that t = 1.95 and P-value = 0.051 for Jones. Show that t = 1.97 and P-value = 0.049 for Smith.\n\n\nCode\njones_t &lt;- (519.5 - 500)/(sd.js/sqrt(1000))\njones_p &lt;- 2*(pt(-jones_t, 999))\n\nprint(jones_t)\n\n\n[1] 1.95\n\n\nCode\nprint(jones_p)\n\n\n[1] 0.05145555\n\n\nCode\nsmith_t &lt;- (519.7 - 500)/(sd.js/sqrt(1000))\nsmith_p &lt;- 2*(pt(-smith_t, 999))\n\nprint(smith_t)\n\n\n[1] 1.97\n\n\nCode\nprint(smith_p)\n\n\n[1] 0.04911426\n\n\nB. Using α = 0.05, for each study indicate whether the result is “statistically significant.”\nThe Jones study result is not statistically significant because their p-value is above the alpha. The Smith study result is statistically significant because the p-value is below the alpha.\nC. Using this example, explain the misleading aspects of reporting the result of a test as “P ≤ 0.05” versus “P &gt; 0.05,” or as “reject H0” versus “Do not reject H0,” without reporting the actual P-value.\nBecause in this case the p-value is so close to the alpha, it’s misleading to not report the actual p-value. If someone only read that a null hypothesis was not rejected and not that statistical significance criteria was almost met, they may falsely attribute the strength of confidence of which the null hypothesis was not rejected with. It’s important to report the p-value so that the strength of confidence is known.\n\nA school nurse wants to determine whether age is a factor in whether children choose a healthy snack after school. She conducts a survey of 300 middle school students, with the results below. Test at α = 0.05 the claim that the proportion who choose a healthy snack differs by grade level. What is the null hypothesis? Which test should we use? What is the conclusion?\n\nx = proportion of students who choose a healthy snack null hypothesis: X is not affected by grade level. alternative hypothesis: X is affected by grade level.\n\n\nCode\ngrade &lt;- c(6, 7, 8) \nhealthy_prop &lt;- c(31, 43, 51)\nunhealthy_prop &lt;- c(69, 57, 49)\nstudent_n &lt;- c(100, 100, 100)\n\nstudent_data &lt;- data.frame(grade, healthy_prop, unhealthy_prop)\n\nstudent_t &lt;- prop.test(student_data$healthy_prop, n=student_n)\nprint(student_t)\n\n\n\n    3-sample test for equality of proportions without continuity\n    correction\n\ndata:  student_data$healthy_prop out of student_n\nX-squared = 8.3383, df = 2, p-value = 0.01547\nalternative hypothesis: two.sided\nsample estimates:\nprop 1 prop 2 prop 3 \n  0.31   0.43   0.51 \n\n\nThe conclusion is that since the p-value of 0.01547 is below our 0.05 value we reject the null hypothesis that the proportion of students who choose a healthy snack is not affected by grade level in favor of the alternative hypothesis that the proportion of students who choose a healthy snack is affected by grade level.\n\nPer-pupil costs (in thousands of dollars) for cyber charter school tuition for school districts in three areas are shown. Test the claim that there is a difference in means for the three areas, using an appropriate test. What is the null hypothesis? Which test should we use? What is the conclusion?\n\nnull hypothesis: There is not a difference in mean per-pupil cost (in thousands of dollars) for cyber charter school tuition for school districts for the three areas. alternative hypothesis: There is a difference in mean per-pupil cost (in thousands of dollars) for cyber charter school tuition for school districts for the three areas.\n\n\nCode\nvalues &lt;- c(6.2,9.3,6.8,6.1,6.7,7.5,\n           7.5,8.2,8.5,8.2,7.0,9.3,\n           5.8,6.4,5.6,7.1,3.0,3.5)\narea &lt;- c(1,1,1,1,1,1,\n          2,2,2,2,2,2,\n          3,3,3,3,3,3)\n\ncost_data &lt;- data.frame(values, area)\n\nanova &lt;- aov(values~area, data = cost_data)\nsummary(anova)\n\n\n            Df Sum Sq Mean Sq F value Pr(&gt;F)  \narea         1  10.45  10.453   4.316 0.0542 .\nResiduals   16  38.75   2.422                 \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe conclusion is that since the p-value of 0.0542 is above our 0.05 value we fail reject the null hypothesis that there is not a difference in mean per-pupil cost (in thousands of dollars) for cyber charter school tuition for school districts for the three areas in favor of the alternative hypothesis that there is a difference in mean per-pupil cost (in thousands of dollars) for cyber charter school tuition for school districts for the three areas."
  },
  {
    "objectID": "posts/FelixBetancourt_Finalpart2_v2.html",
    "href": "posts/FelixBetancourt_Finalpart2_v2.html",
    "title": "Final Project - Check point 2",
    "section": "",
    "text": "Code\nknitr::opts_chunk$set(echo = TRUE, warning = FALSE)\n\n\n\nBurnout in the workplace - Research Question.\nBurnout is a pervasive issue in many professions, and its consequences can be significant for individuals and organizations alike. According to Maslach and Leiter (2016), burnout is characterized by emotional exhaustion, despersonalization, and reduced personal accomplishment. It is prevalent in a variety of fields, including healthcare (West et al., 2016), among other professions. Burnout can have serious consequences, including decreased job satisfaction, increased absenteeism, and turnover (West et al., 2016).\nThere is a growing body of research exploring the causes and consequences of burnout, as well as potential solutions. Some scholars have identified factors such as job demands, lack of control, and social support as contributing to burnout (Bakker & Demerouti, 2017).\nLee & Eissenstat (2017), for instance, affirm that psychological job demands and work-to-family conflict, as well as control over working hours/schedule, decision-making authority, and role clarity, have significant effects on burnout.\nThe aim of this research paper is to provide a comprehensive understanding of how type of business, and seniority level explain the level of burnout.\n\n\nHypothesis.\nNull Hypothesis: The type of business, and seniority does not affect the burnout rate.\nAlternative hypotheses:\n\nWorking in Services (vs product type of business) predict significantly higher burnout rate, especially in Female workers.\nLess years of experience (lower seniority) is significantly related to higher burnout rate.\nThe numbers of work hours allocated affect the burnout rate significantly in employees working from home setup (vs on site).\n\n\n\nData analysis\n\n\nCode\n# Loading packages\nsuppressPackageStartupMessages(library(dplyr))\nsuppressPackageStartupMessages(library(tidyverse))\nsuppressPackageStartupMessages(library(data.table))\nsuppressPackageStartupMessages(library(psych))\nsuppressPackageStartupMessages(library(scales))\nsuppressPackageStartupMessages(library(lattice))\nsuppressPackageStartupMessages(library(formattable))\nsuppressPackageStartupMessages(library(kableExtra))\nsuppressPackageStartupMessages(library(ggplot2))\n\n# Reading the data.\n\nburn &lt;- read.csv(\"_data/burnout.csv\")\n\n\nThe dataset was obtained from:\nhttps://www.kaggle.com/datasets/blurredmachine/are-your-employees-burning-out\nLet’s see the strucutre of the dataset:\n\n\nCode\n#Structure\nstr(burn)\n\n\n'data.frame':   22750 obs. of  9 variables:\n $ Employee.ID         : chr  \"fffe32003000360033003200\" \"fffe3700360033003500\" \"fffe31003300320037003900\" \"fffe32003400380032003900\" ...\n $ Date.of.Joining     : chr  \"2008-09-30\" \"2008-11-30\" \"2008-03-10\" \"2008-11-03\" ...\n $ Gender              : chr  \"Female\" \"Male\" \"Female\" \"Male\" ...\n $ Company.Type        : chr  \"Service\" \"Service\" \"Product\" \"Service\" ...\n $ WFH.Setup.Available : chr  \"No\" \"Yes\" \"Yes\" \"Yes\" ...\n $ Designation         : num  2 1 2 1 3 2 3 2 3 3 ...\n $ Resource.Allocation : num  3 2 NA 1 7 4 6 4 6 6 ...\n $ Mental.Fatigue.Score: num  3.8 5 5.8 2.6 6.9 3.6 7.9 4.4 NA NA ...\n $ Burn.Rate           : num  0.16 0.36 0.49 0.2 0.52 0.29 0.62 0.33 0.56 0.67 ...\n\n\nThe dataset contain 9 variables and 22750 observations.\nFour of the variables are categorical and five are numeric (including one as date).\nAccording to the source of the data, here is an definition of each variable:\nEmployee ID: The unique ID allocated for each employee.\nDate of Joining: The date-time when the employee has joined the organization.\nGender: The gender of the employee\nCompany Type: The type of company where the employee is working\nWFH Setup Available: Is the work from home facility available for the employee\nDesignation: The designation (designations refer to the expertise and qualifications a person must complete certain jobs) of the employee of work in the organization. In the range of [0.0, 5.0] bigger is higher designation.\nResource Allocation: The amount of resource allocated to the employee to work, ie. number of working hours.In the range of [1.0, 10.0] (higher means more resource)\nMental Fatigue Score: The level of fatigue mentally the employee is facing.In the range of [0.0, 10.0] where 0.0 means no fatigue and 10.0 means completely fatigue.\nBurnout Rate: The value we need to predict for each employee telling the rate of Bur out while working.In the range of [0.0, 1.0] where the higher the value is more is the Burnout.\nLet’s see a Summary for each variable.\nBut first let’s do some data wranglin:\n\nsimplify the name of the variables\ntransform the Date of hire into tenure in years\neliminate ID form the df as it is not relevant for the analysis\ncreate new variables for categorical variables as numeric to simplify analysis: Gender (0= Male, 1=Female), WFH (1=Yes, 0=No), Type of Business (0=Service, 1=Product)\n\n\n\nCode\n#simplify some variable names and transforming date\nburn &lt;- rename(burn, fatigue = Mental.Fatigue.Score, hours = Resource.Allocation, WFH = WFH.Setup.Available, doh = Date.of.Joining, id = Employee.ID, business = Company.Type, designation = Designation, burn.rate = Burn.Rate, gender = Gender)\n\n#transforming date of hire to tenure\nburn$doh &lt;- as.Date(burn$doh)\nburn$tenure &lt;- difftime(\"2023-4-20\", burn$doh, units = \"weeks\")\nburn$tenure &lt;- burn$tenure/52\nburn$tenure &lt;- as.numeric(burn$tenure)\n\n#eliminate ID as it's not relevant for the analysis. They are just the identification for each individual\nburn2 &lt;- burn %&gt;%\n  dplyr::select(-id)\n\n\nburn2 &lt;- burn2%&gt;%\n  mutate(gender.n = case_when(\n        gender == \"Male\" ~ 0,\n         gender == \"Female\" ~ 1,\n        )) %&gt;%\n  mutate(business.n = case_when(\n         business == \"Service\" ~ 0,\n         business == \"Product\" ~ 1,\n         )) %&gt;%\n  mutate(WFH.n = case_when(\n         WFH == \"Yes\" ~ 1,\n         WFH == \"No\" ~ 0,\n         ))\n\n\n\nDescriptives and exploratory visualization.\n\n\nCode\n#creating a df with only continuous variables\nburn3 &lt;- burn2 %&gt;%\n  dplyr::select(-gender, -doh, -WFH, -business)\n\ndescribe_data &lt;- describe(x=burn3) %&gt;% \n  dplyr::select(c(vars, n, mean, sd, median, min, max))\n\nkable(describe_data) %&gt;% \n  kable_styling(\"striped\")\n\n\n\n\n\n\nvars\nn\nmean\nsd\nmedian\nmin\nmax\n\n\n\n\ndesignation\n1\n22750\n2.1787253\n1.1351447\n2.00000\n0.00000\n5.00000\n\n\nhours\n2\n21369\n4.4813983\n2.0472111\n4.00000\n1.00000\n10.00000\n\n\nfatigue\n3\n20633\n5.7281879\n1.9208387\n5.90000\n0.00000\n10.00000\n\n\nburn.rate\n4\n21626\n0.4520055\n0.1982264\n0.45000\n0.00000\n1.00000\n\n\ntenure\n5\n22750\n14.8510224\n0.2894181\n14.84936\n14.34936\n15.35211\n\n\ngender.n\n6\n22750\n0.5234286\n0.4994618\n1.00000\n0.00000\n1.00000\n\n\nbusiness.n\n7\n22750\n0.3480000\n0.4763465\n0.00000\n0.00000\n1.00000\n\n\nWFH.n\n8\n22750\n0.5402198\n0.4983907\n1.00000\n0.00000\n1.00000\n\n\n\n\n\n\n\nHistograms for Burnout Rate, mental Fatigue, Work hours allocated, Designation Level and tenure.\n\n\nCode\nhist(burn2$burn.rate, freq = TRUE, col=\"#a1e9f0\", main = \"Histogram for Burnout Rate\", xlab = \"Burn Rate\", ylab = \"Frequency\")\n\n\n\n\n\nCode\nhist(burn2$fatigue, freq = TRUE, col=\"#a1e9f0\", main = \"Histogram for Mental Fatigue\", xlab = \"Values\", ylab = \"Frequency\")\n\n\n\n\n\nCode\nhist(burn2$hours, freq = TRUE, col=\"#a1e9f0\", main = \"Histogram of hours allocated\", xlab = \"Work Hours allocated\", ylab = \"Frequency\")\n\n\n\n\n\nCode\nhist(burn2$designation, freq = TRUE, col=\"#a1e9f0\", main = \"Histogram of Designation\", xlab = \"Designation Level\", ylab = \"Frequency\")\n\n\n\n\n\nCode\nhist(burn2$tenure, freq = TRUE, col=\"#a1e9f0\", main = \"Histogram of Tenure\", xlab = \"Tenure\", ylab = \"Frequency\")\n\n\n\n\n\nWe can note that Burnout Rate shows a distribution close to a normal distribution while a bit skewed to to the right. hours allocated seems also skewed to the right.\nIn the case of Mental Fatigue, the distribution seems also a bit skewed to the left.\nIn the case of tenure seems a very homogeneous distribution, with almost the same amount of employees hired within the same year and evenly distributed in that year.\nLet’s visualize Gender, Type of Business and Work setting (WFH):\n\n\nCode\nggplot(burn) +\n  geom_bar(aes(x=business, fill=gender),\n           position = \"dodge\") +\n  facet_wrap(~WFH)\n\n\n\n\n\nSeems that there are more females working from home than males regardless the business.\n\n\nExplore relationships\n\n\nCode\nbox2 &lt;- ggplot(burn2, aes(x=business, y=burn.rate, fill=gender)) + \n    geom_boxplot(alpha=0.7, outlier.shape = NA) +\n  labs(title=\"Box Plot - Burnout Rate by Gender, Type of business and Work From home (yes or no)\",\n        x =\"Type Of Business\", y = \"Burnout Rate\")+\n    facet_wrap(~WFH, scale=\"free\")\n\nbox2\n\n\n\n\n\nCode\nbox3 &lt;- ggplot(burn2, aes(x=business, y=hours, fill=gender)) + \n    geom_boxplot(alpha=0.7, outlier.shape = NA) +\n  labs(title=\"Box Plot - Hours allocated, Type of business and Work From home (yes or no)\",\n        x =\"Type Of Business\", y = \"Hours Allocated\")+\n    facet_wrap(~WFH, scale=\"free\")\n\nbox3\n\n\n\n\n\nIt looks like the Burnout rate is higher in the employees working on site compared to employees working from home, and males shows higher rate compared to females in both type of businesses but it seems a higher difference among employees working from home.\nOn the other hand, the numbers of hours allocated seems higher for employees working on site and particularly males regardless the type of business.\n\n\nCode\nscatter1 &lt;- ggplot(burn2, aes(designation,burn.rate))+\n  geom_point(aes(color=WFH))+\n  geom_smooth(method=\"lm\")+\n  labs(title=\"Scatter Plot - Burnout Rate vs Designation grouped by Gender and WFH setup\",\n        x =\"Designation\", y = \"Burnout Rate\")+\n  facet_wrap(~gender)\nscatter1\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nCode\nscatter2 &lt;- ggplot(burn2, aes(hours,burn.rate))+\n  geom_point(aes(color=WFH))+\n  geom_smooth(method=\"lm\")+\n  labs(title=\"Scatter Plot - Burnout Rate vs Hours allocated grouped by Gender and WFH setup\",\n        x =\"Hours allocation\", y = \"Burnout Rate\")+\n  facet_wrap(~gender)\nscatter2\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nCode\nscatter3 &lt;- ggplot(burn2, aes(fatigue,burn.rate))+\n  geom_point(aes(color=WFH))+\n  geom_smooth(method=\"gam\")+\n  labs(title=\"Scatter Plot - Burnout Rate vs Mental Fatigue grouped by Gender and WFH setup\",\n        x =\"Mental Fatigue\", y = \"Burnout Rate\")+\n  facet_wrap(~business)\nscatter3\n\n\n`geom_smooth()` using formula = 'y ~ s(x, bs = \"cs\")'\n\n\n\n\n\nCode\nscatter4 &lt;- ggplot(burn2, aes(hours,fatigue))+\n  geom_point(aes(color=WFH))+\n  geom_smooth(method=\"lm\")+\n  labs(title=\"Scatter Plot - Hours allocated vs Mental Fatigue grouped by Gender and WFH setup\",\n        x =\"Hours allocated\", y = \"Mental fatigue\")+\nfacet_wrap(~gender)\nscatter4\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nCode\nscatter5 &lt;- ggplot(burn2, aes(designation, hours))+\n  geom_point(aes(color=WFH))+\n  geom_smooth(method=\"lm\")+\n  labs(title=\"Scatter Plot - Hours allocated vs Designation grouped by Gender and WFH setup\",\n        x =\"Designation\", y = \"Hours allocated\")+\nfacet_wrap(~gender)\nscatter5\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nFrom the charts we can see that there is a positive relationship between:\n\nDesignation and burnout rate\nAmount of hours allocated and burnout rate\nMental fatigue and burnout rate\nHours allocated and Mental fatigue\n\nAll of those relationship exist regardless the gender and seems moderated by the work from home setup.\nSeems that working from home has a key role in the relationship of the variables.\nLastly the relationship between Burnout rate and Fatigue doesn’t look completely lineal, but let’s check the so I will create a new variable by doing a quadratic transformation of Fatigue, and using it later for the regression\nLet’s analyze these initial findings with a few statistics.\n\nfirst let’s see how is the relationship between Burnout Rate, type of business, WFH and gender:\n\n\n\nCode\n# Three-way ANOVA\n\naov.model1 &lt;- aov(burn.rate ~ business * gender * WFH, data = burn2)\n\nsummary(aov.model1)\n\n\n                       Df Sum Sq Mean Sq  F value   Pr(&gt;F)    \nbusiness                1    0.0    0.02    0.446 0.504133    \ngender                  1   20.4   20.40  584.657  &lt; 2e-16 ***\nWFH                     1   74.3   74.28 2128.566  &lt; 2e-16 ***\nbusiness:gender         1    0.0    0.02    0.546 0.459774    \nbusiness:WFH            1    0.1    0.09    2.465 0.116406    \ngender:WFH              1    0.4    0.45   12.861 0.000336 ***\nbusiness:gender:WFH     1    0.1    0.06    1.736 0.187602    \nResiduals           21618  754.4    0.03                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n1124 observations deleted due to missingness\n\n\nAs we can see in this three-way ANOVA, type of business seems not relevant to explain burnout rate as the F value is not significant, but gender and WFH are both significant and also are when combined.\nLet’s explore the relationship among continuous variables using a correlation matrix:\n\n\nCode\n# Cor Matrix\ncor_matrix2 &lt;- cor(burn2[, c(\"burn.rate\", \"hours\", \"designation\", \"fatigue\")], use = \"complete.obs\")\nround(cor_matrix2, 2)\n\n\n            burn.rate hours designation fatigue\nburn.rate        1.00  0.86        0.74    0.94\nhours            0.86  1.00        0.88    0.80\ndesignation      0.74  0.88        1.00    0.69\nfatigue          0.94  0.80        0.69    1.00\n\n\nThe first thing we have to notice is that Hours allocated shows strong correlation with almost all the rest fo the variables, this could represent multicollinearity. So I may need to exclude some variables the Regression analysis.\nIn this case seems that designation has a very strong positive correlation with hours allocated, seems that the company assign to more senior employees more work hours.\nBut to explain burnout seems more logic to think that number of work hours explain burnout, so I rather use hours and not designation in the model and avoid multicollinearity.\nAlso I’ll use backward Stepwise regression to keep only the relevant variables.\n\n\nCode\n# Fit a multiple regression model with predictor variables\nlm.model1 &lt;- lm(burn.rate ~ fatigue + gender.n + business.n + hours + WFH.n, data = burn2)\nsummary(lm.model1)\n\n\n\nCall:\nlm(formula = burn.rate ~ fatigue + gender.n + business.n + hours + \n    WFH.n, data = burn2)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.157101 -0.041106 -0.000602  0.040688  0.201499 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.0801515  0.0016618 -48.232  &lt; 2e-16 ***\nfatigue      0.0739451  0.0003566 207.364  &lt; 2e-16 ***\ngender.n    -0.0045718  0.0008294  -5.512 3.59e-08 ***\nbusiness.n  -0.0004356  0.0008608  -0.506    0.613    \nhours        0.0262183  0.0003355  78.151  &lt; 2e-16 ***\nWFH.n       -0.0117741  0.0008602 -13.688  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.05579 on 18584 degrees of freedom\n  (4160 observations deleted due to missingness)\nMultiple R-squared:  0.9205,    Adjusted R-squared:  0.9205 \nF-statistic: 4.303e+04 on 5 and 18584 DF,  p-value: &lt; 2.2e-16\n\n\nCode\n# Step Model - Forward\nstep_model1 &lt;- step(lm.model1, direction = \"backward\", criterion= \"AIC\")\n\n\nStart:  AIC=-107298.9\nburn.rate ~ fatigue + gender.n + business.n + hours + WFH.n\n\n             Df Sum of Sq     RSS     AIC\n- business.n  1     0.001  57.852 -107301\n&lt;none&gt;                     57.851 -107299\n- gender.n    1     0.095  57.946 -107270\n- WFH.n       1     0.583  58.434 -107114\n- hours       1    19.012  76.864 -102018\n- fatigue     1   133.857 191.708  -85028\n\nStep:  AIC=-107300.6\nburn.rate ~ fatigue + gender.n + hours + WFH.n\n\n           Df Sum of Sq     RSS     AIC\n&lt;none&gt;                   57.852 -107301\n- gender.n  1     0.094  57.946 -107272\n- WFH.n     1     0.583  58.435 -107116\n- hours     1    19.014  76.866 -102020\n- fatigue   1   133.857 191.709  -85030\n\n\nCode\nsummary(step_model1)\n\n\n\nCall:\nlm(formula = burn.rate ~ fatigue + gender.n + hours + WFH.n, \n    data = burn2)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.156953 -0.041084 -0.000499  0.040699  0.201211 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.0803079  0.0016327 -49.186  &lt; 2e-16 ***\nfatigue      0.0739452  0.0003566 207.369  &lt; 2e-16 ***\ngender.n    -0.0045675  0.0008293  -5.508 3.69e-08 ***\nhours        0.0262190  0.0003355  78.155  &lt; 2e-16 ***\nWFH.n       -0.0117739  0.0008602 -13.688  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.05579 on 18585 degrees of freedom\n  (4160 observations deleted due to missingness)\nMultiple R-squared:  0.9205,    Adjusted R-squared:  0.9205 \nF-statistic: 5.379e+04 on 4 and 18585 DF,  p-value: &lt; 2.2e-16\n\n\nType of business is not relevant to the model to explain Burnout. so it is excluded from the model.\nLet’s check assumptions\n\n\nCode\npar(mfrow = c(2,3))\nplot(step_model1, which = 1:6)\n\n\n\n\n\nDoesn’t seems there are assumptions violations in this model.\nHowever based on the scatter plot above about Burnout Rate and Fatigue, the relationship doesn’t seem very linear, therefore if we apply a polynomial regression by doing a quadratic transformation of fatigue the model would adjust a bit better.\n\n\nCode\nlm.model2 &lt;- lm(burn.rate ~ business.n + hours + gender.n + I(fatigue^2) + fatigue +  WFH.n, data = burn2)\n\n# Step Model - Forward\nstep_model2 &lt;- step(lm.model2, direction = \"backward\", criterion= \"AIC\")\n\n\nStart:  AIC=-108108.5\nburn.rate ~ business.n + hours + gender.n + I(fatigue^2) + fatigue + \n    WFH.n\n\n               Df Sum of Sq    RSS     AIC\n- business.n    1    0.0008 55.380 -108110\n&lt;none&gt;                      55.380 -108109\n- gender.n      1    0.0610 55.441 -108090\n- WFH.n         1    0.3857 55.765 -107981\n- I(fatigue^2)  1    2.4715 57.851 -107299\n- fatigue       1   10.8741 66.254 -104778\n- hours         1   15.8119 71.192 -103441\n\nStep:  AIC=-108110.3\nburn.rate ~ hours + gender.n + I(fatigue^2) + fatigue + WFH.n\n\n               Df Sum of Sq    RSS     AIC\n&lt;none&gt;                      55.380 -108110\n- gender.n      1    0.0609 55.441 -108092\n- WFH.n         1    0.3857 55.766 -107983\n- I(fatigue^2)  1    2.4715 57.852 -107301\n- fatigue       1   10.8741 66.255 -104780\n- hours         1   15.8129 71.193 -103443\n\n\nCode\nsummary(step_model2)\n\n\n\nCall:\nlm(formula = burn.rate ~ hours + gender.n + I(fatigue^2) + fatigue + \n    WFH.n, data = burn2)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.150292 -0.041127 -0.000951  0.039024  0.190195 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -2.834e-02  2.410e-03 -11.757  &lt; 2e-16 ***\nhours         2.437e-02  3.345e-04  72.844  &lt; 2e-16 ***\ngender.n     -3.671e-03  8.120e-04  -4.521 6.18e-06 ***\nI(fatigue^2)  2.277e-03  7.906e-05  28.799  &lt; 2e-16 ***\nfatigue       5.153e-02  8.530e-04  60.407  &lt; 2e-16 ***\nWFH.n        -9.613e-03  8.450e-04 -11.377  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.05459 on 18584 degrees of freedom\n  (4160 observations deleted due to missingness)\nMultiple R-squared:  0.9239,    Adjusted R-squared:  0.9239 \nF-statistic: 4.512e+04 on 5 and 18584 DF,  p-value: &lt; 2.2e-16\n\n\nCode\npar(mfrow = c(2,3))\nplot(step_model2, which = 1:6)\n\n\n\n\n\nThe model seems to fit better now. The new quadratic Fatigue is significant in the model, so I’ll include it.\nHours allocated, Gender, Fatigue and WFH or not seems to explain significantly the burnout in the employees included in the study. Being Hours allocated and fatigue the most relevant variables in the model to explain burnout (higher t value).\n\n\n\nConclusions\nWe can’t accept the null hypothesis, at least partially, as the type of business does not affect the burnout rate in employees, but the designation level certainly does.\nSpecifically there is a positive relationship between burnout and designation level.\nThis means that the Alternative hypothesis 2 can’t be accepted as it stated a negative relationship between those two variables.\nOn the other hand the alternative hypothesis 1 stated that being in Services and being female is related to higher burnout rate, and the results indicates that the type of business is not relevant to predict burnout, and being female is neither associated to higher burnout.\nHowever, gender explain in part the burnout. In particular, being female is associated with lower burnout rate compared to males employees.\nLastly, in relation to the alternative hypothesis 3, certainly the number of hours allocated are positive related to burnout, in particular, working on site is associated to more working hours for males and females compared to employees working from home. However within the group of employees working on site, males seems to have significantly more hours allocated than females.\nUltimately, seems that amount of working hours and mental fatigue are the most relevant variables to explain burnout.Gender and WFH setup seems relevant in the sense of how are the hours allocated. Being male working on site absorbing more work hours, therefore more burnout.\n\n\nReferences:\nBakker, A. B., & Demerouti, E. (2017). Job demands-resources theory: Taking stock and looking forward. Journal of Occupational Health Psychology, 22(3), 273–285.\nLee, Y., Eissenstat, S. (2017). A longitudinal examination of the causes and effects of burnout based on the job demands-resources model. International Journal for Educational and Vocational Guidance, 18(3), 337–354.\nMaslach, C., & Leiter, M. P. (2016). Understanding the burnout experience: Recent research and its implications for psychiatry. World Psychiatry, 15(2), 103–111.\nWest, C. P., Dyrbye, L. N., Erwin, P., Shanafelt, T. D., (2016). Interventions to promote physician well-being and mitigate burnout: A systematic review and meta-analysis. The Lancet, 388(10057), 2272–228"
  },
  {
    "objectID": "posts/dacss603hw1_LauraCollazo.html",
    "href": "posts/dacss603hw1_LauraCollazo.html",
    "title": "DACSS 603 Homework 1",
    "section": "",
    "text": "Code\nlibrary(readxl)\nlibrary(tidyverse)"
  },
  {
    "objectID": "posts/dacss603hw1_LauraCollazo.html#a",
    "href": "posts/dacss603hw1_LauraCollazo.html#a",
    "title": "DACSS 603 Homework 1",
    "section": "a",
    "text": "a\nThe distribution of LungCap looks as follows:\n\n\nCode\nhist(df$LungCap)\n\n\n\n\n\nThe histogram suggests that the distribution is close to a normal distribution. Most of the observations are close to the mean. Very few observations are close to the margins (0 and 15)."
  },
  {
    "objectID": "posts/dacss603hw1_LauraCollazo.html#b",
    "href": "posts/dacss603hw1_LauraCollazo.html#b",
    "title": "DACSS 603 Homework 1",
    "section": "b",
    "text": "b\nThe distribution of LungCap by Gender looks as follows:\n\n\nCode\nboxplot(df$LungCap ~ df$Gender)\n\n\n\n\n\nThe distribution shows that males, on average, have a higher lung capacity than females."
  },
  {
    "objectID": "posts/dacss603hw1_LauraCollazo.html#c",
    "href": "posts/dacss603hw1_LauraCollazo.html#c",
    "title": "DACSS 603 Homework 1",
    "section": "c",
    "text": "c\nThe mean lung capacities for smokers and non-smokers is shown below.\n\n\nCode\ndf %&gt;% \n  group_by(Smoke) %&gt;% \n  summarise(mean = mean(LungCap))\n\n\n# A tibble: 2 × 2\n  Smoke  mean\n  &lt;chr&gt; &lt;dbl&gt;\n1 no     7.77\n2 yes    8.65\n\n\nThese means are not what I would have expected to see. This shows smokers have a higher mean lung capacity than non-smokers."
  },
  {
    "objectID": "posts/dacss603hw1_LauraCollazo.html#d",
    "href": "posts/dacss603hw1_LauraCollazo.html#d",
    "title": "DACSS 603 Homework 1",
    "section": "d",
    "text": "d\nThe mean lung capacities for smokers and non-smokers by age group is shown below.\n\n\nCode\ndf_age_group &lt;- df %&gt;% \n  mutate(\n    age_group = case_when(\n      Age &lt; 14 ~ \"less than or equal to 13\",\n      Age == 14 ~ \"14 to 15\",\n      Age == 15 ~ \"14 to 15\",\n      Age == 16 ~ \"16 to 17\",\n      Age == 17 ~ \"16 to 17\",\n      Age &gt; 17 ~ \"greater than or equal to 18\")\n  ) \n\ndf_age_group %&gt;%\n  group_by(age_group, Smoke) %&gt;% \n  summarise(mean = mean(LungCap))\n\n\n`summarise()` has grouped output by 'age_group'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 8 × 3\n# Groups:   age_group [4]\n  age_group                   Smoke  mean\n  &lt;chr&gt;                       &lt;chr&gt; &lt;dbl&gt;\n1 14 to 15                    no     9.14\n2 14 to 15                    yes    8.39\n3 16 to 17                    no    10.5 \n4 16 to 17                    yes    9.38\n5 greater than or equal to 18 no    11.1 \n6 greater than or equal to 18 yes   10.5 \n7 less than or equal to 13    no     6.36\n8 less than or equal to 13    yes    7.20"
  },
  {
    "objectID": "posts/dacss603hw1_LauraCollazo.html#e",
    "href": "posts/dacss603hw1_LauraCollazo.html#e",
    "title": "DACSS 603 Homework 1",
    "section": "e",
    "text": "e\nThe distribution of LungCap by age_group and smoker looks as follows:\n\n\nCode\nggplot(df_age_group, aes(x=age_group, y=LungCap, color = Smoke)) +\n  geom_boxplot()\n\n\n\n\n\nThe mean lung capacity for non-smokers is higher than smokers for all age groups except for those age 13 or younger. This is interesting as it was observed earlier that overall smokers have a higher mean lung capacity than non-smokers. This led me to wonder if there are more individuals age 13 or younger in this sample than other age groups, and also what the count of smokers versus non smokers is for this sample.\nThe count by age group and smoker type is shown below.\n\n\nCode\ndf_age_group %&gt;% \n  group_by(age_group, Smoke) %&gt;% \n  summarise(total_count = n())\n\n\n`summarise()` has grouped output by 'age_group'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 8 × 3\n# Groups:   age_group [4]\n  age_group                   Smoke total_count\n  &lt;chr&gt;                       &lt;chr&gt;       &lt;int&gt;\n1 14 to 15                    no            105\n2 14 to 15                    yes            15\n3 16 to 17                    no             77\n4 16 to 17                    yes            20\n5 greater than or equal to 18 no             65\n6 greater than or equal to 18 yes            15\n7 less than or equal to 13    no            401\n8 less than or equal to 13    yes            27\n\n\nThe overall count by smoker type is shown below.\n\n\nCode\ndf %&gt;% \n  group_by(Smoke) %&gt;% \n  summarise(total_count = n())\n\n\n# A tibble: 2 × 2\n  Smoke total_count\n  &lt;chr&gt;       &lt;int&gt;\n1 no            648\n2 yes            77"
  },
  {
    "objectID": "posts/dacss603hw1_LauraCollazo.html#a-1",
    "href": "posts/dacss603hw1_LauraCollazo.html#a-1",
    "title": "DACSS 603 Homework 1",
    "section": "a",
    "text": "a\nThe probability that a randomly selected inmate has exactly 2 prior convictions is as follows:\n\n\nCode\ndbinom(x= 1, size= 1, prob= 160/810)\n\n\n[1] 0.1975309"
  },
  {
    "objectID": "posts/dacss603hw1_LauraCollazo.html#b-1",
    "href": "posts/dacss603hw1_LauraCollazo.html#b-1",
    "title": "DACSS 603 Homework 1",
    "section": "b",
    "text": "b\nThe probability that a randomly selected inmate has fewer than 2 prior convictions is as follows:\n\n\nCode\ndbinom(x= 1, size= 1, prob= sum(128/810 + 434/810))\n\n\n[1] 0.6938272"
  },
  {
    "objectID": "posts/dacss603hw1_LauraCollazo.html#c-1",
    "href": "posts/dacss603hw1_LauraCollazo.html#c-1",
    "title": "DACSS 603 Homework 1",
    "section": "c",
    "text": "c\nThe probability that a randomly selected inmate has 2 or fewer prior convictions is as follows:\n\n\nCode\ndbinom(x= 1, size= 1, prob= sum(128/810 + 434/810 + 160/810))\n\n\n[1] 0.891358"
  },
  {
    "objectID": "posts/dacss603hw1_LauraCollazo.html#d-1",
    "href": "posts/dacss603hw1_LauraCollazo.html#d-1",
    "title": "DACSS 603 Homework 1",
    "section": "d",
    "text": "d\nThe probability that a randomly selected inmate has more than 2 prior convictions is as follows:\n\n\nCode\ndbinom(x= 1, size= 1, prob= sum(64/810 + 24/810))\n\n\n[1] 0.108642"
  },
  {
    "objectID": "posts/dacss603hw1_LauraCollazo.html#e-1",
    "href": "posts/dacss603hw1_LauraCollazo.html#e-1",
    "title": "DACSS 603 Homework 1",
    "section": "e",
    "text": "e\nThe expected value for the number of prior convictions is as follows:\n\n\nCode\nev &lt;- sum(convictions$number_convictions * convictions$prop)\n\nprint(ev)\n\n\n[1] 1.28642"
  },
  {
    "objectID": "posts/dacss603hw1_LauraCollazo.html#f",
    "href": "posts/dacss603hw1_LauraCollazo.html#f",
    "title": "DACSS 603 Homework 1",
    "section": "f",
    "text": "f\nThe variance for prior convictions is as follows:\n\n\nCode\nvar &lt;- sum((convictions$number - ev) ^ 2 * convictions$prop)\n\nprint(var)\n\n\n[1] 0.8562353\n\n\nThe standard deviation for prior convictions is as follows:\n\n\nCode\nsd &lt;- sqrt(var)\n\nprint(sd)\n\n\n[1] 0.9253298"
  },
  {
    "objectID": "posts/HW1_DarronBunt.html",
    "href": "posts/HW1_DarronBunt.html",
    "title": "Homework - 1",
    "section": "",
    "text": "Use the LungCapData to answer the following questions. (Hint: Using dplyr, especially group_by() and summarize() can help you answer the following questions relatively efficiently.)\n\n\nWhat does the distribution of LungCap look like? (Hint: Plot a histogram with probability density on the y axis)\nFirst, let’s read in the data from the Excel file:\n\n\nCode\nLungCapData &lt;- read_excel(\"_data/LungCapData.xls\")\n\n\nThe distribution of LungCap looks as follows:\n\n\nCode\nhist(LungCapData$LungCap)\n\n\n\n\n\nThe histogram suggests that the distribution is close to a normal distribution. Most of the observations are close to the mean, while very few observations are close to the margins (0 and 15).\n\n\n\nCompare the probability distribution of the LungCap with respect to Males and Females? (Hint: make boxplots separated by gender using the boxplot() function)\n\n\nCode\n# Create a boxplot comparing LungCap for males and females\nboxplot(LungCap~Gender, data=LungCapData)\n\n\n\n\n\nThe boxplot suggests that the median for lung capacity in males is slightly higher than that of females. The IQR for lung capacity in males is also slightly higher than that of females. The minimum value for females is lower than that of males, as is the maximum. All of this suggests that males are more likely to have a greater lung capacity than females.\n\n\n\nCompare the mean lung capacities for smokers and non-smokers. Does it make sense?\n\n\nCode\n# Calculate mean lung capacity for smokers and non-smokers\nLungCapData %&gt;%\n  group_by(Smoke) %&gt;%\n  summarise_at(vars(LungCap),\n               list(LCap = mean))\n\n\n# A tibble: 2 × 2\n  Smoke  LCap\n  &lt;chr&gt; &lt;dbl&gt;\n1 no     7.77\n2 yes    8.65\n\n\nThis suggests that the mean lung capacity for smokers is greater than that of non-smokers. This is not what I would have expected given the (negative) impact that smoking has on the lungs.\n\n\n\nExamine the relationship between Smoking and Lung Capacity within age groups: “less than or equal to 13”, “14 to 15”, “16 to 17”, and “greater than or equal to 18”.\n\n\nCode\n# Create a new variable, AgeGroup, using the parameters outlined above\nAgeSmokeLC &lt;- LungCapData %&gt;%\n mutate(\n   AgeGroup = case_when(\n     Age &lt;= 13 ~ \"13 and Under\",\n     Age == 14 | Age == 15 ~ \"14-15\",\n     Age == 16 | Age == 17 ~ \"16-17\", \n     Age &gt;= 18 ~ \"18 and Over\")) \n\n# Calculate the mean lung capacity for smokers and non-smokers in each age group  \n  AgeSmokeLCMean &lt;- AgeSmokeLC %&gt;%\n    group_by(AgeGroup, Smoke) %&gt;%\n  summarise_at(vars(LungCap),\n               list(MeanLungCap = mean)) %&gt;%\n  arrange(desc(MeanLungCap))\nAgeSmokeLCMean\n\n\n# A tibble: 8 × 3\n# Groups:   AgeGroup [4]\n  AgeGroup     Smoke MeanLungCap\n  &lt;chr&gt;        &lt;chr&gt;       &lt;dbl&gt;\n1 18 and Over  no          11.1 \n2 18 and Over  yes         10.5 \n3 16-17        no          10.5 \n4 16-17        yes          9.38\n5 14-15        no           9.14\n6 14-15        yes          8.39\n7 13 and Under yes          7.20\n8 13 and Under no           6.36\n\n\nThis suggests that lung capacity increases with age; the mean lung capacity for each subsequent age category is greater than the one before it. Individuals aged 13 and under who smoke have a greater mean lung capacity than those who do not; however, at ages 14-15, 16-17, and 18+, non-smokers have a greater lung capacity than their similarly aged smoking counterparts.\n\n\n\nCompare the lung capacities for smokers and non-smokers within each age group. Is your answer different from the one in part c. What could possibly be going on here?\nFrom the age group data above, we can ascertain that in the majority of age groups (three of the four), non-smokers have greater mean lung capacity than smokers, and yet when we calculated the mean lung capacity solely for smokers/non-smokers (ie. not accounting for age), smokers had greater mean lung capacity. The mean lung capacities for smokers/non-smokers (again, not accounting for age) are also lower than one might expect considering that when we do account for age, most of the represented groups have a higher mean lung capacity. This would suggest that something is skewing our results.\nOne way to gain insight into this is to examine how many respondents fell into each age/smoking status category.\n\n\nCode\n# Count number of responses in each age group who are smokers/non-smokers\nAgeSmokeLC %&gt;%\n  count(AgeGroup, Smoke)\n\n\n# A tibble: 8 × 3\n  AgeGroup     Smoke     n\n  &lt;chr&gt;        &lt;chr&gt; &lt;int&gt;\n1 13 and Under no      401\n2 13 and Under yes      27\n3 14-15        no      105\n4 14-15        yes      15\n5 16-17        no       77\n6 16-17        yes      20\n7 18 and Over  no       65\n8 18 and Over  yes      15\n\n\nThere are more non-smoking individuals aged 13 and Under in this sample than all other categories combined. Accordingly, data from this group can (and has) skewed the overall non-smoking mean."
  },
  {
    "objectID": "posts/HW1_DarronBunt.html#a",
    "href": "posts/HW1_DarronBunt.html#a",
    "title": "Homework - 1",
    "section": "",
    "text": "What does the distribution of LungCap look like? (Hint: Plot a histogram with probability density on the y axis)\nFirst, let’s read in the data from the Excel file:\n\n\nCode\nLungCapData &lt;- read_excel(\"_data/LungCapData.xls\")\n\n\nThe distribution of LungCap looks as follows:\n\n\nCode\nhist(LungCapData$LungCap)\n\n\n\n\n\nThe histogram suggests that the distribution is close to a normal distribution. Most of the observations are close to the mean, while very few observations are close to the margins (0 and 15)."
  },
  {
    "objectID": "posts/HW1_DarronBunt.html#b",
    "href": "posts/HW1_DarronBunt.html#b",
    "title": "Homework - 1",
    "section": "",
    "text": "Compare the probability distribution of the LungCap with respect to Males and Females? (Hint: make boxplots separated by gender using the boxplot() function)\n\n\nCode\n# Create a boxplot comparing LungCap for males and females\nboxplot(LungCap~Gender, data=LungCapData)\n\n\n\n\n\nThe boxplot suggests that the median for lung capacity in males is slightly higher than that of females. The IQR for lung capacity in males is also slightly higher than that of females. The minimum value for females is lower than that of males, as is the maximum. All of this suggests that males are more likely to have a greater lung capacity than females."
  },
  {
    "objectID": "posts/HW1_DarronBunt.html#c",
    "href": "posts/HW1_DarronBunt.html#c",
    "title": "Homework - 1",
    "section": "",
    "text": "Compare the mean lung capacities for smokers and non-smokers. Does it make sense?\n\n\nCode\n# Calculate mean lung capacity for smokers and non-smokers\nLungCapData %&gt;%\n  group_by(Smoke) %&gt;%\n  summarise_at(vars(LungCap),\n               list(LCap = mean))\n\n\n# A tibble: 2 × 2\n  Smoke  LCap\n  &lt;chr&gt; &lt;dbl&gt;\n1 no     7.77\n2 yes    8.65\n\n\nThis suggests that the mean lung capacity for smokers is greater than that of non-smokers. This is not what I would have expected given the (negative) impact that smoking has on the lungs."
  },
  {
    "objectID": "posts/HW1_DarronBunt.html#d",
    "href": "posts/HW1_DarronBunt.html#d",
    "title": "Homework - 1",
    "section": "",
    "text": "Examine the relationship between Smoking and Lung Capacity within age groups: “less than or equal to 13”, “14 to 15”, “16 to 17”, and “greater than or equal to 18”.\n\n\nCode\n# Create a new variable, AgeGroup, using the parameters outlined above\nAgeSmokeLC &lt;- LungCapData %&gt;%\n mutate(\n   AgeGroup = case_when(\n     Age &lt;= 13 ~ \"13 and Under\",\n     Age == 14 | Age == 15 ~ \"14-15\",\n     Age == 16 | Age == 17 ~ \"16-17\", \n     Age &gt;= 18 ~ \"18 and Over\")) \n\n# Calculate the mean lung capacity for smokers and non-smokers in each age group  \n  AgeSmokeLCMean &lt;- AgeSmokeLC %&gt;%\n    group_by(AgeGroup, Smoke) %&gt;%\n  summarise_at(vars(LungCap),\n               list(MeanLungCap = mean)) %&gt;%\n  arrange(desc(MeanLungCap))\nAgeSmokeLCMean\n\n\n# A tibble: 8 × 3\n# Groups:   AgeGroup [4]\n  AgeGroup     Smoke MeanLungCap\n  &lt;chr&gt;        &lt;chr&gt;       &lt;dbl&gt;\n1 18 and Over  no          11.1 \n2 18 and Over  yes         10.5 \n3 16-17        no          10.5 \n4 16-17        yes          9.38\n5 14-15        no           9.14\n6 14-15        yes          8.39\n7 13 and Under yes          7.20\n8 13 and Under no           6.36\n\n\nThis suggests that lung capacity increases with age; the mean lung capacity for each subsequent age category is greater than the one before it. Individuals aged 13 and under who smoke have a greater mean lung capacity than those who do not; however, at ages 14-15, 16-17, and 18+, non-smokers have a greater lung capacity than their similarly aged smoking counterparts."
  },
  {
    "objectID": "posts/HW1_DarronBunt.html#e",
    "href": "posts/HW1_DarronBunt.html#e",
    "title": "Homework - 1",
    "section": "",
    "text": "Compare the lung capacities for smokers and non-smokers within each age group. Is your answer different from the one in part c. What could possibly be going on here?\nFrom the age group data above, we can ascertain that in the majority of age groups (three of the four), non-smokers have greater mean lung capacity than smokers, and yet when we calculated the mean lung capacity solely for smokers/non-smokers (ie. not accounting for age), smokers had greater mean lung capacity. The mean lung capacities for smokers/non-smokers (again, not accounting for age) are also lower than one might expect considering that when we do account for age, most of the represented groups have a higher mean lung capacity. This would suggest that something is skewing our results.\nOne way to gain insight into this is to examine how many respondents fell into each age/smoking status category.\n\n\nCode\n# Count number of responses in each age group who are smokers/non-smokers\nAgeSmokeLC %&gt;%\n  count(AgeGroup, Smoke)\n\n\n# A tibble: 8 × 3\n  AgeGroup     Smoke     n\n  &lt;chr&gt;        &lt;chr&gt; &lt;int&gt;\n1 13 and Under no      401\n2 13 and Under yes      27\n3 14-15        no      105\n4 14-15        yes      15\n5 16-17        no       77\n6 16-17        yes      20\n7 18 and Over  no       65\n8 18 and Over  yes      15\n\n\nThere are more non-smoking individuals aged 13 and Under in this sample than all other categories combined. Accordingly, data from this group can (and has) skewed the overall non-smoking mean."
  },
  {
    "objectID": "posts/HW1_DarronBunt.html#a-1",
    "href": "posts/HW1_DarronBunt.html#a-1",
    "title": "Homework - 1",
    "section": "A",
    "text": "A\nWhat is the probability that a randomly selected inmate has exactly 2 prior convictions?\nTo answer this question, we are going to need to know the total number of inmates.\n\n\nCode\n#Find total number of inmates\nsum(PriorConvictions$Inmates)\n\n\n[1] 810\n\n\nTo calculate this probability we are going calculate the binomial distribution. For this, we require three main arguments: x - the number specifying the outcomes you’re trying to calculate (for this question, x=1) size - the size of the experiment (for this question, size=1) prob - the probability of success for any one trial in the experiment (for this question, prob = the total number of inmates with two convictions / the total number of inmates, or 160/810)\n\n\nCode\n# Calculate probability of randomly selecting an inmate with two prior convictions\ndbinom(x=1, size=1, prob=(160/810))\n\n\n[1] 0.1975309\n\n\nThe probability of randomly selecting an inmate with two prior convictions is roughly 19.8%."
  },
  {
    "objectID": "posts/HW1_DarronBunt.html#b-1",
    "href": "posts/HW1_DarronBunt.html#b-1",
    "title": "Homework - 1",
    "section": "B",
    "text": "B\nWhat is the probability that a randomly selected inmate has fewer than 2 prior convictions?\nHaving fewer than two prior convictions would mean that the inmate could either have zero or one prior convictions.\nx = 1 size = 1 prob = ((the total number of inmates with no prior convictions + inmates with 1 conviction) / the total number of inmates, or ((128 + 434)/810), or 562/810.\n\n\nCode\n# Calculate probability of randomly selecting an inmate with less than two prior convictions\ndbinom(x=1, size=1, prob=(562/810))\n\n\n[1] 0.6938272\n\n\nThe probability of randomly selecting an inmate with fewer than two prior convictions is roughly 69.4%."
  },
  {
    "objectID": "posts/HW1_DarronBunt.html#c-1",
    "href": "posts/HW1_DarronBunt.html#c-1",
    "title": "Homework - 1",
    "section": "C",
    "text": "C\nWhat is the probability that a randomly selected inmate has 2 or fewer prior convictions?\nHaving two or fewer prior convictions would mean that the inmate could either have zero, one, or two prior convictions.\nx = 1 size = 1 prob = ((the total number of inmates with no prior convictions + inmates with 1 conviction + inmates with 2 convictions) / the total number of inmates, or ((128 + 434 + 160)/810), or 722/810.\n\n\nCode\n# Calculate probability of randomly selecting an inmate with two or fewer prior convictions\ndbinom(x=1, size=1, prob=(722/810))\n\n\n[1] 0.891358\n\n\nThe probability of randomly selecting an inmate with two or fewer prior convictions is roughly 89.1%."
  },
  {
    "objectID": "posts/HW1_DarronBunt.html#d-1",
    "href": "posts/HW1_DarronBunt.html#d-1",
    "title": "Homework - 1",
    "section": "D",
    "text": "D\nWhat is the probability that a randomly selected inmate has more than 2 prior convictions?\nHaving more than two prior convictions would mean that the inmate could either have three or four prior convictions.\nx = 1 size = 1 prob = ((the total number of inmates with 3 convictions + inmates with 4 convictions) / the total number of inmates, or ((64+24)/810), or 88/810.\n\n\nCode\n# Calculate probability of randomly selecting an inmate with more than two prior convictions\ndbinom(x=1, size=1, prob=(88/810))\n\n\n[1] 0.108642\n\n\nThe probability of randomly selecting an inmate with more than two prior convictions is roughly 10.9%."
  },
  {
    "objectID": "posts/HW1_DarronBunt.html#e-1",
    "href": "posts/HW1_DarronBunt.html#e-1",
    "title": "Homework - 1",
    "section": "E",
    "text": "E\nWhat is the expected value for the number of prior convictions? (The expected value of a discrete random variable X, symbolized as E(X), is often referred to as the long-term average or mean)\nIn order to calculate the expected value for the number of prior convictions, we will need to use the proportional breakdown of the number of inmates with each number of convictions.\n\n\nCode\n# Calculate proportion of inmates with 0,1,2,3,4 convictions\nPriorConvictionsProp &lt;- PriorConvictions %&gt;%\n  mutate(Proportion = Inmates/810)\nPriorConvictionsProp\n\n\n  PriorConv Inmates Proportion\n1         0     128 0.15802469\n2         1     434 0.53580247\n3         2     160 0.19753086\n4         3      64 0.07901235\n5         4      24 0.02962963\n\n\n\n\nCode\n# Calculate the expected value for prior number of convictions\nExpValue &lt;- sum(PriorConvictionsProp$PriorConv*PriorConvictionsProp$Proportion)\nExpValue\n\n\n[1] 1.28642\n\n\nThe expected value is 1.29 prior convictions."
  },
  {
    "objectID": "posts/HW1_DarronBunt.html#f",
    "href": "posts/HW1_DarronBunt.html#f",
    "title": "Homework - 1",
    "section": "F",
    "text": "F\nCalculate the variance and the standard deviation for the Prior Convictions.\n\n\nCode\n# Calculate the variance for prior convictions\nPriorConvVar &lt;- sum((PriorConvictionsProp$PriorConv - ExpValue) ^ 2 * PriorConvictionsProp$Proportion)\nPriorConvVar\n\n\n[1] 0.8562353\n\n\n\n\nCode\n# Calculate the standard deviation for prior convictions\nPriorConvSTD &lt;- sqrt(PriorConvVar)\nPriorConvSTD\n\n\n[1] 0.9253298"
  },
  {
    "objectID": "posts/asch_harwood_hw_4.html",
    "href": "posts/asch_harwood_hw_4.html",
    "title": "Homework 4",
    "section": "",
    "text": "Code\nlibrary(\"dplyr\")\nlibrary(\"knitr\")\nlibrary(kableExtra)\nlibrary(xtable)\nlibrary(ggplot2)\nlibrary(GGally)\nlibrary(lme4)\nlibrary(car)\nlibrary(dplyr)\nlibrary(alr4)\nlibrary(stargazer)\nlibrary(smss)\n\n\n\nQuestion 1\n\nA\n\n\nCode\nx1 &lt;-  1240 #house size\nx2 &lt;- 18000 #lot size\ny_observed &lt;- 145000\n\n\n\n\nCode\n# find predicted selling price\ny_hat &lt;- (-10536) + (53.8*x1) + (2.84*x2)\nresidual &lt;- y_observed - y_hat\ncat('Predict sale price: $',  y_hat)\n\n\nPredict sale price: $ 107296\n\n\nCode\ncat('\\n')\n\n\nCode\ncat('Residual: $', residual)\n\n\nResidual: $ 37704\n\n\nCode\ncat('\\n')\n\n\nCode\ncat('The predicted sale price of', y_hat, 'dollars is', residual, 'dollars less than the actual observed house price of', y_observed, '.')\n\n\nThe predicted sale price of 107296 dollars is 37704 dollars less than the actual observed house price of 145000 .\n\n\n\n\nB\nHolding the lot size fixed, each unit increase in square footage increases the house value by 53.8 dollars, which is the coefficient.\n\n\nC\nThe lot size would need to increase by 19.94 square feet to increase the home value by 53.8 dollars.\n\n\n\nQuestion 2\n\n\nCode\ndata(salary)\n\n\n\nA\nWe cannot reject the null hypothesis that there is a difference between the mean male and female salary at the 0.05 significance level given the observed p-value of 0.09. The 95 percent confidence interval supports this conclusion because it contains zero.\n\n\nCode\nggplot(data = salary, aes(x=salary, fill=sex)) + \n  geom_histogram(bins=5)\n\n\n\n\n\n\n\nCode\nmale_salaries &lt;- salary[salary$sex == \"Male\",]$salary\nfemale_salaries &lt;- salary[salary$sex == \"Female\",]$salary\nt.test(male_salaries, female_salaries)\n\n\n\n    Welch Two Sample t-test\n\ndata:  male_salaries and female_salaries\nt = 1.7744, df = 21.591, p-value = 0.09009\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -567.8539 7247.1471\nsample estimates:\nmean of x mean of y \n 24696.79  21357.14 \n\n\n\n\nB\n\n\nCode\ndata(salary)\nfit &lt;- lm(salary ~ rank + sex + year + ysdeg + degree, data = salary)\nsex_ci &lt;- confint(fit)[\"sexFemale\", ]\ncat('sexFemale confidence interval: ', sex_ci)\n\n\nsexFemale confidence interval:  -697.8183 3030.565\n\n\n\n\nC\nIntercept: The intercept of $15,746.05 refers to when all coefficients are zero, which can be interpreted as the ‘base’ salary. In this case, our base reference professor is a male, assistant professor with zero years of experience and 0 years since graduation.\nrankAssoc: Holding all else equal, associate professors make $5,292.36 more than assistant professors. This finding is statistically significant, which means we can reject the null hypothesis that there is no different in salary between associate and assistant professors.\nrankProf: Holding all else equal, professors make $11,118.76 more than assistant professors. This finding is statistically significant, which means we can reject the null hypothesis that there is no different in salary between professors and assistant professors.\nsexFemale: Holding all else equal, female professors make $1,166.37 more than male professors. This finding is NOT statistically significant, which means we cannot reject the null hypothesis that there is no difference in salary between male and female professors.\nyear: Holding all else equal, for each year increase in years in current position, salary increases by $476.31. This finding is statistically significant, which means we can reject the null hypothesis that an increase in years does not increase salary.\nysdeg: Holding all else equal, for each year increase in years since degree, salary decreases by $124.57. This finding is NOT statistically significant, which means we cannot reject the null hypothesis that a change in years in position does not have a corresponding change in salary.\ndegreePhD: Holding all else equal, professors with PhD’s make $1,388 more than those with masters degrees. This is NOT statistically significant.\n\n\nCode\nsummary(fit)\n\n\n\nCall:\nlm(formula = salary ~ rank + sex + year + ysdeg + degree, data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4045.2 -1094.7  -361.5   813.2  9193.1 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 15746.05     800.18  19.678  &lt; 2e-16 ***\nrankAssoc    5292.36    1145.40   4.621 3.22e-05 ***\nrankProf    11118.76    1351.77   8.225 1.62e-10 ***\nsexFemale    1166.37     925.57   1.260    0.214    \nyear          476.31      94.91   5.018 8.65e-06 ***\nysdeg        -124.57      77.49  -1.608    0.115    \ndegreePhD    1388.61    1018.75   1.363    0.180    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2398 on 45 degrees of freedom\nMultiple R-squared:  0.855, Adjusted R-squared:  0.8357 \nF-statistic: 44.24 on 6 and 45 DF,  p-value: &lt; 2.2e-16\n\n\n\n\nD\nrankAsst: Holding all else equal, associate professors make $11,118.76 less than a professor. This finding is statistically significant.\nrankAssoc: Holding all else equal, assistant professors make $5,826.40 less than a professor. This finding is statistically significant.\n\n\nCode\nsalary$rank &lt;- relevel(salary$rank, ref=\"Prof\")\nfit &lt;- lm(salary ~ rank + sex + year + ysdeg + degree, data = salary)\nsummary(fit)\n\n\n\nCall:\nlm(formula = salary ~ rank + sex + year + ysdeg + degree, data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4045.2 -1094.7  -361.5   813.2  9193.1 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  26864.81    1375.29  19.534  &lt; 2e-16 ***\nrankAsst    -11118.76    1351.77  -8.225 1.62e-10 ***\nrankAssoc    -5826.40    1012.93  -5.752 7.28e-07 ***\nsexFemale     1166.37     925.57   1.260    0.214    \nyear           476.31      94.91   5.018 8.65e-06 ***\nysdeg         -124.57      77.49  -1.608    0.115    \ndegreePhD     1388.61    1018.75   1.363    0.180    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2398 on 45 degrees of freedom\nMultiple R-squared:  0.855, Adjusted R-squared:  0.8357 \nF-statistic: 44.24 on 6 and 45 DF,  p-value: &lt; 2.2e-16\n\n\n\n\nE\nBy excluding rank:\n\nthe model as a whole is less ‘useful’. R-squared has gone down from 86 to 63, as has the adjusted r-squared, even though we have simplified the model. This means the model without rank has less ‘explanatory’ power. We also see an increase in the residual standard error.\nsexFemale in the new model is now correlated with a decreased salary of $1,286.54 compared to the original model’s increase of $1,166.37. Sex is still not statistically significant.\nThere is a slight decrease in the coefficient for year.\nIn the new model, an increase in one year in number of years since degree is associated with a $339.40 increase in salary, compared to the $124.57 reduction from the previous model. This finding is statistically significant.\nIn the new model, having a phd is associated with a decline in salary by $3,299.35 compared to the $1,388.61 bump in the original model.\n\n\n\nCode\nfit_no_rank &lt;- lm(salary ~ sex + year + ysdeg + degree, data = salary)\n\n\n\n\nCode\n# Create a side-by-side table of the models using stargazer\nstargazer(fit, fit_no_rank,type = \"text\",\n          title = \"Professor Salary Regression\",\n          align = TRUE,\n          column.labels = c(\"M1:AllVari\", \"M2:NoRank\"),\n          ci = TRUE, # Show confidence intervals\n          digits = 2) \n\n\n\nProfessor Salary Regression\n==================================================================\n                                 Dependent variable:              \n                    ----------------------------------------------\n                                        salary                    \n                          M1:AllVari              M2:NoRank       \n                              (1)                    (2)          \n------------------------------------------------------------------\nrankAsst                 -11,118.76***                            \n                    (-13,768.19, -8,469.34)                       \n                                                                  \nrankAssoc                -5,826.40***                             \n                    (-7,811.72, -3,841.09)                        \n                                                                  \nsexFemale                  1,166.37               -1,286.54       \n                      (-647.71, 2,980.45)   (-3,860.15, 1,287.06) \n                                                                  \nyear                       476.31***               351.97**       \n                       (290.28, 662.34)        (72.71, 631.23)    \n                                                                  \nysdeg                       -124.57               339.40***       \n                       (-276.44, 27.30)        (181.38, 497.41)   \n                                                                  \ndegreePhD                  1,388.61              -3,299.35**      \n                      (-608.09, 3,385.32)    (-5,852.24, -746.46) \n                                                                  \nConstant                 26,864.81***            17,183.57***     \n                    (24,169.30, 29,560.33)  (14,933.65, 19,433.50)\n                                                                  \n------------------------------------------------------------------\nObservations                  52                      52          \nR2                           0.86                    0.63         \nAdjusted R2                  0.84                    0.60         \nResidual Std. Error   2,398.42 (df = 45)      3,743.50 (df = 47)  \nF Statistic          44.24*** (df = 6; 45)  20.11*** (df = 4; 47) \n==================================================================\nNote:                                  *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01\n\n\n\n\nF\nTo prevent multicollinearity, I excluded ys_deg from the model. A pair plot visual inspection suggests that ys_deg is correlated with years. Without a visual inspection, we already know it is correlated with dean_selected because dean_selected is derived from ys_deg. We want to remove multicollinearity because it can influence other coefficients in the model, possibly obscuring the true relationship between the independent and dependent variable.\nThere is evidence to suggest that the dean is preferentially rewarding staff he has hired. Holding all else equal, those hired by the dean earn $2,160 more than those who were not. This number is statistically significant.\n\n\nCode\n#add dean selection\nsalary$dean_selected &lt;- ifelse(salary$ysdeg&lt;=15, 1, 0)\n\n#dropping ysdeg b/c correlated with dean_selected\nx_no_y &lt;- subset(salary, select = -c(salary, ysdeg))\n#pairs(x_no_y)\n\n# not including ys_deg b/c correlation with dean_selected and year\nfit_dean &lt;- lm(salary ~ degree + rank + sex + dean_selected + year, data = salary)\nsummary(fit_dean)\n\n\n\nCall:\nlm(formula = salary ~ degree + rank + sex + dean_selected + year, \n    data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3403.3 -1387.0  -167.0   528.2  9233.8 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    24425.32    1107.52  22.054  &lt; 2e-16 ***\ndegreePhD        818.93     797.48   1.027   0.3100    \nrankAsst      -11096.95    1191.00  -9.317 4.54e-12 ***\nrankAssoc      -6124.28    1028.58  -5.954 3.65e-07 ***\nsexFemale        907.14     840.54   1.079   0.2862    \ndean_selected   2163.46    1072.04   2.018   0.0496 *  \nyear             434.85      78.89   5.512 1.65e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2362 on 45 degrees of freedom\nMultiple R-squared:  0.8594,    Adjusted R-squared:  0.8407 \nF-statistic: 45.86 on 6 and 45 DF,  p-value: &lt; 2.2e-16\n\n\n\n\nCode\nvif(fit_dean)\n\n\n                  GVIF Df GVIF^(1/(2*Df))\ndegree        1.341872  1        1.158392\nrank          2.964200  2        1.312130\nsex           1.295820  1        1.138341\ndean_selected 2.678486  1        1.636608\nyear          1.726209  1        1.313853\n\n\n\n\n\nQuestion 3\n\n\nCode\ndata(\"house.selling.price\")\nhouse.selling.price$New &lt;- as.factor(house.selling.price$New)\n\n\n\nA\nSize: Holding all else equal, a one square foot increase in house size adds $116 dollars to its sale price. This finding is statistically significant.\nNew: Holding all else equal, a new house sells for $57,736 more than an ‘old’ house. This finding is also statistically significant.\n\n\nCode\nfit &lt;- lm(Price ~ Size + New, data = house.selling.price)\nsummary(fit)\n\n\n\nCall:\nlm(formula = Price ~ Size + New, data = house.selling.price)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-205102  -34374   -5778   18929  163866 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -40230.867  14696.140  -2.738  0.00737 ** \nSize           116.132      8.795  13.204  &lt; 2e-16 ***\nNew1         57736.283  18653.041   3.095  0.00257 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 53880 on 97 degrees of freedom\nMultiple R-squared:  0.7226,    Adjusted R-squared:  0.7169 \nF-statistic: 126.3 on 2 and 97 DF,  p-value: &lt; 2.2e-16\n\n\n\n\nB\nCombined Model\nprice = -40230.867 + 116.132*x_size + 57736.283*x_new\nIn this model, our price prediction is based on our intercept to -40,230.867 plus 116.132 multiplied by the house square footage plus $57,736 if the house is new.\nNew House Model\nprice_new_house = 116.132*x_size + 17505.42\nOld House Model\nprice_old_house = 116.132*x_size - 40230.867\n\n\nCode\n-40230.867 + 57736.283\n\n\n[1] 17505.42\n\n\n\n\nC\nFind the predicted selling price for a home of 3000 square feet that is (i) new, (ii) not new.\n\n\nCode\n# house size\nx_size &lt;- 3000\n\n# new model\nprice_new_house &lt;- 116.132*x_size + 17505.42\n\n#old house model\nprice_old_house = 116.132*x_size - 40230.867\n\ncat('New house estimate: ', price_new_house)\n\n\nNew house estimate:  365901.4\n\n\nCode\ncat('\\n')\n\n\nCode\ncat('Old house estimate: ', price_old_house)\n\n\nOld house estimate:  308165.1\n\n\n\n\nD\n\n\nCode\nfit &lt;- lm(Price ~ Size + New + Size*New, data = house.selling.price)\nsummary(fit)\n\n\n\nCall:\nlm(formula = Price ~ Size + New + Size * New, data = house.selling.price)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-175748  -28979   -6260   14693  192519 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -22227.808  15521.110  -1.432  0.15536    \nSize           104.438      9.424  11.082  &lt; 2e-16 ***\nNew1        -78527.502  51007.642  -1.540  0.12697    \nSize:New1       61.916     21.686   2.855  0.00527 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 52000 on 96 degrees of freedom\nMultiple R-squared:  0.7443,    Adjusted R-squared:  0.7363 \nF-statistic: 93.15 on 3 and 96 DF,  p-value: &lt; 2.2e-16\n\n\n\n\nE\nNew1: Coefficient of $-78,537. Compared to old houses, new houses with zero square feet would sell for $78,527, which is not possible. This result, however, is NOT statistically significant.\nSize:New1: For every unit increase in size, new homes sale price will increase by 61 dollars more than an old home. This coefficient is statistically significant.\n\n\nF\n\n\nCode\nnew_house &lt;- data.frame(Size = 3000, New = 1)\nnew_house$New &lt;- as.factor(new_house$New)\nnew_house_predicted_price &lt;- as.numeric(predict(fit, newdata = new_house))\n\n\nold_house &lt;- data.frame(Size = 3000, New = 0)\nold_house$New &lt;- as.factor(old_house$New)\nold_house_predicted_price &lt;- as.numeric(predict(fit, newdata = old_house))\n\ncat('New house predicted price: ', new_house_predicted_price)\n\n\nNew house predicted price:  398307.5\n\n\nCode\ncat('\\n')\n\n\nCode\ncat('Old house predicted price: ', old_house_predicted_price)\n\n\nOld house predicted price:  291087.4\n\n\n\n\nG\nGiven the use of our interaction term, which says that as homes increase in square footage, new homes will increase by $61 per square foot more than old homes. Therefore as a result, as houses get larger, the price difference between new and old homes will also get larger.\n\n\nCode\nnew_house &lt;- data.frame(Size = 1500, New = 1)\nnew_house$New &lt;- as.factor(new_house$New)\nnew_house_predicted_price &lt;- as.numeric(predict(fit, newdata = new_house))\n\n\nold_house &lt;- data.frame(Size = 1500, New = 0)\nold_house$New &lt;- as.factor(old_house$New)\nold_house_predicted_price &lt;- as.numeric(predict(fit, newdata = old_house))\n\ncat('New house predicted price: ', new_house_predicted_price)\n\n\nNew house predicted price:  148776.1\n\n\nCode\ncat('\\n')\n\n\nCode\ncat('Old house predicted price: ', old_house_predicted_price)\n\n\nOld house predicted price:  134429.8\n\n\n\n\nH\nI prefer the model with the interaction term. While only slightly stronger in terms of its R-Squared, the model and the interaction term are simple enough to be easily interpreted. I also prefer it because ‘New’ by itself is not statistically significant. I feel the New variable is not ‘specific’ enough about what it refers to and thus not actually that useful in understanding home prices. What constitutes a new house? What happens if there is an old house that has been remodeled or has an addition?\n\n\nCode\nfit_interaction &lt;- lm(Price ~ Size + New + Size*New, data = house.selling.price)\nfit_no_interaction &lt;- lm(Price ~ Size + New, data = house.selling.price)\n\nstargazer(fit_interaction, fit_no_interaction,type = \"text\",\n          title = \"New vs Old Home Prices\",\n          align = TRUE,\n          column.labels = c(\"M1:wInteraction\", \"M2:NoInteraction\"),\n          ci = TRUE, # Show confidence intervals\n          digits = 2) \n\n\n\nNew vs Old Home Prices\n=====================================================================\n                                   Dependent variable:               \n                    -------------------------------------------------\n                                          Price                      \n                        M1:wInteraction          M2:NoInteraction    \n                              (1)                      (2)           \n---------------------------------------------------------------------\nSize                       104.44***                116.13***        \n                        (85.97, 122.91)          (98.89, 133.37)     \n                                                                     \nNew1                       -78,527.50              57,736.28***      \n                    (-178,500.60, 21,445.64)  (21,176.99, 94,295.57) \n                                                                     \nSize:New1                   61.92***                                 \n                        (19.41, 104.42)                              \n                                                                     \nConstant                   -22,227.81             -40,230.87***      \n                     (-52,648.62, 8,193.01)  (-69,034.77, -11,426.96)\n                                                                     \n---------------------------------------------------------------------\nObservations                  100                      100           \nR2                            0.74                     0.72          \nAdjusted R2                   0.74                     0.72          \nResidual Std. Error   51,998.11 (df = 96)      53,880.95 (df = 97)   \nF Statistic          93.15*** (df = 3; 96)    126.34*** (df = 2; 97) \n=====================================================================\nNote:                                     *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01"
  },
  {
    "objectID": "posts/HW1_ChristineBrydges.html",
    "href": "posts/HW1_ChristineBrydges.html",
    "title": "Homework 1",
    "section": "",
    "text": "First, let’s read in the data from the Excel file:\n\n\nCode\nlibrary(readxl)\ndf &lt;- read_excel(\"_data/LungCapData.xls\")\n\n\nThe distribution of LungCap looks as follows:\n\n\nCode\nhist(df$LungCap)\n\n\n\n\n\nThe histogram suggests that the distribution is close to a normal distribution. Most of the observations are close to the mean. Very few observations are close to the margins (0 and 15).\n\n\n\nThe distribution and mean of the probability density of Lung Capacity can be shown in the box plots below, separated by genders ‘Male’ and ‘Female’\n\n\nCode\n# Create a Box Plot with Lung Capacity on y-axis, grouped by gender\nboxplot(LungCap~ Gender, data = df)\n\n# Add a title\ntitle(\"Lung Capacity: Males vs. Females\")\n\n\n\n\n\nThe boxplot suggests that there is no significant difference between lung capacity of females and males, as the error bars of each boxplot significantly overlap. Additionally, the mean of the probability density of lung capacity appear to be close between female and male constituents.\n\n\n\n\n\nCode\n# Create a Box Plot with Lung Capacity on y-axis, grouped by populations that smoke or do not smoke \nboxplot(LungCap~ Smoke, data = df)\n\n# Add a title\ntitle(\"Lung Capacity: Smokers vs. Non-Smokers\")\n\n\n\n\n\nThe boxplot suggests that smokers have a higher lung capacity than non-smokers, which is counter-intuitive.\n#c Next, we will explore the differences of lung capacity of non-smokers and smokers, broken down by age group, as shown in the boxplot below. The green color is used to call out groups that do smoke while the blue color is used to call out groups that do not smoke.\n\n\nCode\n#Group respondents into specific Age Groups \nAgeGroups &lt;- cut(df$Age, breaks=c(0,13,15,17,19), labels=c('&lt;13','14-15','16-17','&gt;=18'))\nlevels(AgeGroups)\n\n\n[1] \"&lt;13\"   \"14-15\" \"16-17\" \"&gt;=18\" \n\n\nCode\n#Create a stratified box plot based on two factors: Smoking vs. Nonsmoking and Age groups\nboxplot(df$LungCap~df$Smoke*AgeGroups, ylab=\"LungCap\", main=\"LungCap vs. Smoke, by AgeGroup\", las = 2, col=c(4,3))\n\n\n\n\n\nFrom these boxplots, you can see that when you separate the smoking and nonsmoking groups by age, the smokers have a higher lung capacity than non-smokers. This is because age is a confounding variable, with young people gaining lung capacity as they have bigger bodies, and with older peope generally smoking more than younger people . Once you take out age from consideration and compare “apples to apples” people of the same age but differences in whether they smoke or not, you can see that smoking DOES have a negative effect on lung capacity."
  },
  {
    "objectID": "posts/HW1_ChristineBrydges.html#a",
    "href": "posts/HW1_ChristineBrydges.html#a",
    "title": "Homework 1",
    "section": "",
    "text": "First, let’s read in the data from the Excel file:\n\n\nCode\nlibrary(readxl)\ndf &lt;- read_excel(\"_data/LungCapData.xls\")\n\n\nThe distribution of LungCap looks as follows:\n\n\nCode\nhist(df$LungCap)\n\n\n\n\n\nThe histogram suggests that the distribution is close to a normal distribution. Most of the observations are close to the mean. Very few observations are close to the margins (0 and 15)."
  },
  {
    "objectID": "posts/HW1_ChristineBrydges.html#b",
    "href": "posts/HW1_ChristineBrydges.html#b",
    "title": "Homework 1",
    "section": "",
    "text": "The distribution and mean of the probability density of Lung Capacity can be shown in the box plots below, separated by genders ‘Male’ and ‘Female’\n\n\nCode\n# Create a Box Plot with Lung Capacity on y-axis, grouped by gender\nboxplot(LungCap~ Gender, data = df)\n\n# Add a title\ntitle(\"Lung Capacity: Males vs. Females\")\n\n\n\n\n\nThe boxplot suggests that there is no significant difference between lung capacity of females and males, as the error bars of each boxplot significantly overlap. Additionally, the mean of the probability density of lung capacity appear to be close between female and male constituents."
  },
  {
    "objectID": "posts/HW1_ChristineBrydges.html#c",
    "href": "posts/HW1_ChristineBrydges.html#c",
    "title": "Homework 1",
    "section": "",
    "text": "Code\n# Create a Box Plot with Lung Capacity on y-axis, grouped by populations that smoke or do not smoke \nboxplot(LungCap~ Smoke, data = df)\n\n# Add a title\ntitle(\"Lung Capacity: Smokers vs. Non-Smokers\")\n\n\n\n\n\nThe boxplot suggests that smokers have a higher lung capacity than non-smokers, which is counter-intuitive.\n#c Next, we will explore the differences of lung capacity of non-smokers and smokers, broken down by age group, as shown in the boxplot below. The green color is used to call out groups that do smoke while the blue color is used to call out groups that do not smoke.\n\n\nCode\n#Group respondents into specific Age Groups \nAgeGroups &lt;- cut(df$Age, breaks=c(0,13,15,17,19), labels=c('&lt;13','14-15','16-17','&gt;=18'))\nlevels(AgeGroups)\n\n\n[1] \"&lt;13\"   \"14-15\" \"16-17\" \"&gt;=18\" \n\n\nCode\n#Create a stratified box plot based on two factors: Smoking vs. Nonsmoking and Age groups\nboxplot(df$LungCap~df$Smoke*AgeGroups, ylab=\"LungCap\", main=\"LungCap vs. Smoke, by AgeGroup\", las = 2, col=c(4,3))\n\n\n\n\n\nFrom these boxplots, you can see that when you separate the smoking and nonsmoking groups by age, the smokers have a higher lung capacity than non-smokers. This is because age is a confounding variable, with young people gaining lung capacity as they have bigger bodies, and with older peope generally smoking more than younger people . Once you take out age from consideration and compare “apples to apples” people of the same age but differences in whether they smoke or not, you can see that smoking DOES have a negative effect on lung capacity."
  },
  {
    "objectID": "posts/HW1_ChristineBrydges.html#a.-here-we-will-explore-the-probability-that-a-randomly-selected-inmate-has-exactly-2-prior-convictions-based-on-a-dataset-given-to-us.",
    "href": "posts/HW1_ChristineBrydges.html#a.-here-we-will-explore-the-probability-that-a-randomly-selected-inmate-has-exactly-2-prior-convictions-based-on-a-dataset-given-to-us.",
    "title": "Homework 1",
    "section": "a. Here, we will explore the probability that a randomly selected inmate has exactly 2 prior convictions, based on a dataset given to us.",
    "text": "a. Here, we will explore the probability that a randomly selected inmate has exactly 2 prior convictions, based on a dataset given to us.\nSince the data set is not continuous or binomial, we will use basic probability functions to find probabilities.\n\n\nCode\n#Calculate the probability of a prisoner having exactly 2 convictions (events/total possible events)\nprobability = (150/810) * 100\nprint(probability)\n\n\n[1] 18.51852\n\n\nWe can see that the probability of a prisoner having exactly 2 convictions is 18.5%."
  },
  {
    "objectID": "posts/HW1_ChristineBrydges.html#b.",
    "href": "posts/HW1_ChristineBrydges.html#b.",
    "title": "Homework 1",
    "section": "b.",
    "text": "b.\nNext, we’ll look at the probability that a randomly selected inmate has fewer than 2 prior convictions.\n\n\nCode\n# Calculate the probability of a prisoner having 0, or 1 convictions (fewer than 2 prior convictions) (events/total possible events)\nlessthan2convictions &lt;- 128 + 434 \nprobabilitylessthan2 &lt;- (lessthan2convictions/810) * 100\nprint(probabilitylessthan2)\n\n\n[1] 69.38272\n\n\nWe can see that the probability of a prisoner having less than 2 convictions is 69.4%."
  },
  {
    "objectID": "posts/HW1_ChristineBrydges.html#c.",
    "href": "posts/HW1_ChristineBrydges.html#c.",
    "title": "Homework 1",
    "section": "c. ",
    "text": "c. \nNext, we’ll look at the probability that a randomly selected inmate has 2 or less prior convictions.\n\n\nCode\n# Calculate the probability of a prisoner having 0, 1, or 2 convictions ( 2 or fewer prior convictions) (events/total possible events)\ntwoorlessconvictions &lt;- 128 + 434 + 160\nprobability2orless &lt;- (twoorlessconvictions/810) * 100\nprint(probability2orless)\n\n\n[1] 89.1358\n\n\nWe can see that the probability of a prisoner having 2 or less convictions is 89.1%."
  },
  {
    "objectID": "posts/HW1_ChristineBrydges.html#d.",
    "href": "posts/HW1_ChristineBrydges.html#d.",
    "title": "Homework 1",
    "section": "d. ",
    "text": "d. \nNext, we’ll look at the probability that a randomly selected inmate has more than 2 prior convictions.\n\n\nCode\n#Calculate the probability of a prisoner having more than 2 convictions ( 3 or 4 prior convictions) (events/total possible events)\nmorethan2convictions &lt;- 64 + 24\nprobabilitymorethan2 &lt;- (morethan2convictions/810) * 100\nprint(probabilitymorethan2)\n\n\n[1] 10.8642\n\n\nWe can see that the probability of a prisoner having more than 2 convictions is 10.9%."
  },
  {
    "objectID": "posts/HW1_ChristineBrydges.html#d.-1",
    "href": "posts/HW1_ChristineBrydges.html#d.-1",
    "title": "Homework 1",
    "section": "d. ",
    "text": "d. \nHere, we’ll calculate the expected value for the number of prior convictions.\n\n\nCode\n#define values\nx &lt;- c(0,1,2,3,4)\n\n#define probabilities\nfrequency &lt;- c(128/810, 434/810, 160/810, 64/810, 24/810)\n\n#calculate expected value\nsum(x * frequency)\n\n\n[1] 1.28642\n\n\nThe expected value is 1.29 prior convictions."
  },
  {
    "objectID": "posts/HW1_ChristineBrydges.html#f.",
    "href": "posts/HW1_ChristineBrydges.html#f.",
    "title": "Homework 1",
    "section": "f. ",
    "text": "f. \nIn this final section, we’ll calculate the variance and standard deviation for the prior convictions.\n\n\nCode\n# calculate variance of frequencies\nfrequency &lt;- c(128, 434, 160, 64, 24)\nvar(frequency)\n\n\n[1] 25948\n\n\nThe variance is 5948.\n\n\nCode\n# calculate standard deviation of prior convictions \nfrequency &lt;- c(128, 434, 160, 64, 24)\nsd(frequency)\n\n\n[1] 161.0838\n\n\nThe standard deviation is 161.1."
  },
  {
    "objectID": "posts/FinalProjectPart1_HannahRosenbaum.html",
    "href": "posts/FinalProjectPart1_HannahRosenbaum.html",
    "title": "Final Project Part 1",
    "section": "",
    "text": "Fifty years after the passage of the Title IX Amendment, collegial sports equity has shown relatively minimal change. Allocations of sports budgets often highlight pay discrepancies in participants being “spent [on] $4,285 per men’s participant versus $2,588 per women’s participant.” (Feinberg, D., & Hunzinger, E) With these vast differences in individual spending by gender, we see this phenomenon only heightened in the NCAA with women’s basketball. Women’s basketball not only fares having lower budgets from the NCAA but also, per an ESPN report, “is underpaying the NCAA for the tournament rights for 29 championships causing the association to lose out on substantial and crucial revenue... denoting that the current budget of $81 million to $112 million multiples more than what the network currently gives.” (Zimmbalist) Thus, there is not only a discrepancy in budget allocations among the participants by gender but also amongst large broadcast networks.\nSignificant systemic issues occur within the gendered branding of ‘March Madness.’ This can be seen with differentiated treatment of male versus female brackets due to the lack of general awareness of when the women’s bracket games even occur. Largely the inequity of the ‘March Madness’ tournament derives from a differentiation from the NCAA in “distribution agreements, corporate sponsorships, distribution of revenue, organizational structure and culture all to prioritize Division I men’s basketball over everything else... to perpetuate gender inequities.” (Blinder) Likewise, this institutional creation of a high investment in TV rights for men’s basketball and minimal airtime for the women’s bracket has led to smaller budgeting and fewer avenues to earn revenue. This has led women’s teams to be “starved of a starring role in the national discourse.” (Blinder) Thus, it creates a circular effect in women’s basketball, deriding fewer resources even within facilities at the NCAA tournament in 2021 and in general awareness of TV times.\nI am primarily interested in discussing sports equity in women’s basketball due to my own personal experience at UF of wanting to watch NCAA basketball for women but having no general knowledge of when women play. I believe that the discussion of equity in sports for women is essential because of the common dismissal of watching women’s sports as a pastime."
  },
  {
    "objectID": "posts/FinalProjectPart1_HannahRosenbaum.html#background",
    "href": "posts/FinalProjectPart1_HannahRosenbaum.html#background",
    "title": "Final Project Part 1",
    "section": "",
    "text": "Fifty years after the passage of the Title IX Amendment, collegial sports equity has shown relatively minimal change. Allocations of sports budgets often highlight pay discrepancies in participants being “spent [on] $4,285 per men’s participant versus $2,588 per women’s participant.” (Feinberg, D., & Hunzinger, E) With these vast differences in individual spending by gender, we see this phenomenon only heightened in the NCAA with women’s basketball. Women’s basketball not only fares having lower budgets from the NCAA but also, per an ESPN report, “is underpaying the NCAA for the tournament rights for 29 championships causing the association to lose out on substantial and crucial revenue... denoting that the current budget of $81 million to $112 million multiples more than what the network currently gives.” (Zimmbalist) Thus, there is not only a discrepancy in budget allocations among the participants by gender but also amongst large broadcast networks.\nSignificant systemic issues occur within the gendered branding of ‘March Madness.’ This can be seen with differentiated treatment of male versus female brackets due to the lack of general awareness of when the women’s bracket games even occur. Largely the inequity of the ‘March Madness’ tournament derives from a differentiation from the NCAA in “distribution agreements, corporate sponsorships, distribution of revenue, organizational structure and culture all to prioritize Division I men’s basketball over everything else... to perpetuate gender inequities.” (Blinder) Likewise, this institutional creation of a high investment in TV rights for men’s basketball and minimal airtime for the women’s bracket has led to smaller budgeting and fewer avenues to earn revenue. This has led women’s teams to be “starved of a starring role in the national discourse.” (Blinder) Thus, it creates a circular effect in women’s basketball, deriding fewer resources even within facilities at the NCAA tournament in 2021 and in general awareness of TV times.\nI am primarily interested in discussing sports equity in women’s basketball due to my own personal experience at UF of wanting to watch NCAA basketball for women but having no general knowledge of when women play. I believe that the discussion of equity in sports for women is essential because of the common dismissal of watching women’s sports as a pastime."
  },
  {
    "objectID": "posts/FinalProjectPart1_HannahRosenbaum.html#section",
    "href": "posts/FinalProjectPart1_HannahRosenbaum.html#section",
    "title": "Final Project Part 1",
    "section": "",
    "text": "Research Questions\n1. Is there a relationship between a higher percentage of female students in post-secondary education and a portion of female athletes at those institutions?\n2. In NCAA sports, is there a relationship between expenditure on university sports programs and the percentage of females in university sports programs?\n3. In NCAA sports, is there a relationship between revenue from university sports programs and the percentage of females in university sports programs?"
  },
  {
    "objectID": "posts/FinalProjectPart1_HannahRosenbaum.html#hypothesis-testing",
    "href": "posts/FinalProjectPart1_HannahRosenbaum.html#hypothesis-testing",
    "title": "Final Project Part 1",
    "section": "Hypothesis Testing",
    "text": "Hypothesis Testing\n1. Ho: There is no relationship between a higher percentage of female students in post-secondary education and the percentage of female athletes.\nHa: There is a relationship between a higher percentage of female students in post-secondary education and percentage of female athletes.\n2. Ho: There is no relationship between expenditure on university sports programs and the percentage of females in university sports programs.\nHa: There is a relationship between expenditure on university sports programs and the percentage of females in university sports programs.\n3. Ho: There is no relationship between revenue from university sports programs and the percentage of females in sports programs.\nHa: There is a relationship between revenue from university sports programs and the percentage of females in sports programs."
  },
  {
    "objectID": "posts/FinalProjectPart1_HannahRosenbaum.html#descriptive-statistics",
    "href": "posts/FinalProjectPart1_HannahRosenbaum.html#descriptive-statistics",
    "title": "Final Project Part 1",
    "section": "Descriptive Statistics",
    "text": "Descriptive Statistics\nThe Equity in Athletics Disclosures Act requires the full financial disclosure of total expenditures, revenue, staffing, and recruiting efforts by men’s and women’s athletic programs (Mock, J.T.). Data provided by the Equity in Sports project is from all postsecondary programs that receive government funding from Title IV funding and is an online database of funding expenses from 2015-2019.\nThere are 132,327 rows and a total of 28 columns.\n\nRead in Sports Equity data-set\n\n\nCode\nsports &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-03-29/sports.csv')\n\n\n`curl` package not installed, falling back to using `url()`\nRows: 132327 Columns: 28\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (8): institution_name, city_txt, state_cd, zip_text, classification_nam...\ndbl (20): year, unitid, classification_code, ef_male_count, ef_female_count,...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\nCode\nlibrary(dplyr)\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\n\n\nCreate data-frames: Critical dimensions, Attendance specific, Basketball specific\n\n\nCode\ndata &lt;- as.data.frame(sports[, c(\"year\", \"institution_name\", \"sports\", \"ef_male_count\", \"ef_female_count\", \"sum_partic_men\", \"sum_partic_women\", 'rev_men', 'rev_women', 'exp_men', 'exp_women')])\nattendance_data &lt;- sports[,c(\"institution_name\", \"sports\", \"ef_male_count\", \"ef_female_count\", \"sum_partic_men\", \"sum_partic_women\")] %&gt;% group_by(institution_name, sports) %&gt;% summarise(across(everything(), mean), .groups='drop')\nbasketball &lt;- filter(sports, sports=='Basketball')\n\n\n\n\nScatter plots comparing Expenditures against Revenue by Gender\n\n\nCode\nplot(data$exp_men, data$rev_men)\n\n\n\n\n\n\n\nCode\nplot(data$exp_women, data$rev_women)\n\n\n\n\n\n\n\nDescriptive Statistics\n\n\nCode\nglimpse(data)\n\n\nRows: 132,327\nColumns: 11\n$ year             &lt;dbl&gt; 2015, 2015, 2015, 2015, 2015, 2015, 2015, 2015, 2015,…\n$ institution_name &lt;chr&gt; \"Alabama A & M University\", \"Alabama A & M University…\n$ sports           &lt;chr&gt; \"Baseball\", \"Basketball\", \"All Track Combined\", \"Foot…\n$ ef_male_count    &lt;dbl&gt; 1923, 1923, 1923, 1923, 1923, 1923, 1923, 1923, 1923,…\n$ ef_female_count  &lt;dbl&gt; 2300, 2300, 2300, 2300, 2300, 2300, 2300, 2300, 2300,…\n$ sum_partic_men   &lt;dbl&gt; 31, 19, 61, 99, 9, 0, 0, 7, 0, 0, 32, 13, 0, 10, 2, 3…\n$ sum_partic_women &lt;dbl&gt; 0, 16, 46, 0, 0, 21, 25, 10, 16, 9, 0, 20, 68, 7, 10,…\n$ rev_men          &lt;dbl&gt; 345592, 1211095, 183333, 2808949, 78270, NA, NA, 7827…\n$ rev_women        &lt;dbl&gt; NA, 748833, 315574, NA, NA, 410717, 298164, 131145, 3…\n$ exp_men          &lt;dbl&gt; 397818, 817868, 246949, 3059353, 83913, NA, NA, 99612…\n$ exp_women        &lt;dbl&gt; NA, 742460, 251184, NA, NA, 432648, 340259, 113886, 3…\n\n\n\n\nCode\nsummary(data)\n\n\n      year      institution_name      sports          ef_male_count  \n Min.   :2015   Length:132327      Length:132327      Min.   :    0  \n 1st Qu.:2016   Class :character   Class :character   1st Qu.:  513  \n Median :2018   Mode  :character   Mode  :character   Median :  986  \n Mean   :2018                                         Mean   : 2126  \n 3rd Qu.:2019                                         3rd Qu.: 2385  \n Max.   :2019                                         Max.   :35954  \n                                                                     \n ef_female_count sum_partic_men   sum_partic_women    rev_men         \n Min.   :    0   Min.   :  0.00   Min.   :  0.00   Min.   :       65  \n 1st Qu.:  652   1st Qu.:  0.00   1st Qu.:  0.00   1st Qu.:    63428  \n Median : 1248   Median :  0.00   Median :  6.00   Median :   158126  \n Mean   : 2496   Mean   : 14.49   Mean   : 10.86   Mean   :   809011  \n 3rd Qu.: 2860   3rd Qu.: 20.00   3rd Qu.: 17.00   3rd Qu.:   400604  \n Max.   :30325   Max.   :331.00   Max.   :327.00   Max.   :156147208  \n                                                   NA's   :70462      \n   rev_women           exp_men           exp_women      \n Min.   :       0   Min.   :      65   Min.   :     65  \n 1st Qu.:   58746   1st Qu.:   63062   1st Qu.:  59301  \n Median :  138318   Median :  159666   Median : 141800  \n Mean   :  279346   Mean   :  662386   Mean   : 331594  \n 3rd Qu.:  331120   3rd Qu.:  424025   3rd Qu.: 361860  \n Max.   :21440365   Max.   :69718059   Max.   :9485162  \n NA's   :63444      NA's   :70462      NA's   :63442    \n\n\n\n\nScatter plots comparing Institution Attendance against Participation by Gender\n\n\nCode\nplot(attendance_data$ef_male_count, attendance_data$sum_partic_men)\n\n\n\n\n\n\n\nCode\nplot(attendance_data$ef_female_count, attendance_data$sum_partic_women)\n\n\n\n\n\n\n\nScatter plots comparing basketball Participation against Expenditures by Gender\n\n\nCode\nplot(basketball$sum_partic_men, basketball$exp_men)\n\n\n\n\n\n\n\nCode\nplot(basketball$sum_partic_women, basketball$exp_women)\n\n\n\n\n\nFor the dataset, I could extrapolate my variables of interest as seen here: &lt;https://github.com/rfordatascience/tidytuesday/blob/master/data/2022/2022-03-29/readme.md&gt;.\nMy critical variables of interest are the following items:\n\nyear: Period year\ninstitution name: School name\nsports: Sport name\nef_male_count: Total male population\nef_female_count: Total female population\nsum_partic_men: Total male participation\nsum_partic_women: Total female participation\nrev_men: Revenue in USD for men\nrev_women: Revenue in USD for women\nexp_men: Expenditures in USD for men\nexp_women: Expenditures in USD for women"
  },
  {
    "objectID": "posts/FinalProjectPart1_HannahRosenbaum.html#references",
    "href": "posts/FinalProjectPart1_HannahRosenbaum.html#references",
    "title": "Final Project Part 1",
    "section": "References",
    "text": "References\n\nBlinder, A. (2021, August 3). Report: N.C.A.A. Prioritized Men’s Basketball ‘Over Everything Else.’ The New York Times. Retrieved April 12, 2023, from https://www.nytimes.com/2021/08/03/sports/ncaabasketball/ncaa-gender-equity-investigation.html?partner=slack&smid=sl-share.\nFeinberg, D., & Hunzinger, E. (2021, October 26). Second NCAA Gender Equity Report Shows Spending Disparities. US News. Retrieved April 11, 2023, from https://www.usnews.com/news/sports/articles/2021-10-26/second-ncaa-gender-equity-report-shows-spending-disparities#:~:text=The%20NCAA%20spent%20%244%2C285%20per,championships%20than%20for%20the%20women%27s.\nMock, J. T. (2022, March 29). rfordatascience/tidytuesday. GitHub. Retrieved April 10, 2023, from https://github.com/rfordatascience/tidytuesday/blob/master/data/2022/2022-03-29/readme.md.\nZimbalist, A. (2022, October 12). Female Athletes Are Undervalued, In Both Money And Media Terms. Forbes. Retrieved April 12, 2023, from https://www.forbes.com/sites/andrewzimbalist/2019/04/10/female-athletes-are-undervalued-in-both-money-and-media-terms/?sh=5006015513ed."
  },
  {
    "objectID": "posts/HW3_JustineShakespeare.html",
    "href": "posts/HW3_JustineShakespeare.html",
    "title": "Homework 3",
    "section": "",
    "text": "Before we dive into the homework let’s load the data.\nCode\nlibrary(alr4)\nlibrary(smss)\nlibrary(tidyverse)"
  },
  {
    "objectID": "posts/HW3_JustineShakespeare.html#a",
    "href": "posts/HW3_JustineShakespeare.html#a",
    "title": "Homework 3",
    "section": "a",
    "text": "a\nIdentify the predictor and the response.\nFirst we’ll load the data and take a look.\n\n\nCode\ndata(UN11)\nglimpse(UN11)\n\n\nRows: 199\nColumns: 6\n$ region    &lt;fct&gt; Asia, Europe, Africa, Africa, Caribbean, Latin Amer, Asia, C…\n$ group     &lt;fct&gt; other, other, africa, africa, other, other, other, other, oe…\n$ fertility &lt;dbl&gt; 5.968, 1.525, 2.142, 5.135, 2.000, 2.172, 1.735, 1.671, 1.94…\n$ ppgdp     &lt;dbl&gt; 499.0, 3677.2, 4473.0, 4321.9, 13750.1, 9162.1, 3030.7, 2285…\n$ lifeExpF  &lt;dbl&gt; 49.49, 80.40, 75.00, 53.17, 81.10, 79.89, 77.33, 77.75, 84.2…\n$ pctUrban  &lt;dbl&gt; 23, 53, 67, 59, 100, 93, 64, 47, 89, 68, 52, 84, 89, 29, 45,…\n\n\nFertility is the response variable and ppgdp is the predictor variable."
  },
  {
    "objectID": "posts/HW3_JustineShakespeare.html#b",
    "href": "posts/HW3_JustineShakespeare.html#b",
    "title": "Homework 3",
    "section": "b",
    "text": "b\nDraw the scatterplot of fertility on the vertical axis versus ppgdp on the horizontal axis and summarize the information in this graph. Does a straight-line mean function seem to be plausible for a summary of this graph?\nWe’ll use ggplot() to draw the scatterplot.\n\n\nCode\nggplot(UN11, aes(ppgdp, fertility)) + geom_point() +\n  labs(title = \"Fertility by GDP per Capita\", x = \"GDP per capita\", \n       y = \"Fertility (birth rate per 1000 females)\")\n\n\n\n\n\nThis graph shows that fertility levels can vary widely for countries with low gdp per capita, but as the gdp per capita gets larger, the fertility levels are relatively low. Given the shape, it does not seem like a straight-line mean function will be appropriate as a summary of this graph."
  },
  {
    "objectID": "posts/HW3_JustineShakespeare.html#c",
    "href": "posts/HW3_JustineShakespeare.html#c",
    "title": "Homework 3",
    "section": "c",
    "text": "c\nDraw the scatterplot of log(fertility) versus log(ppgdp) using natural logarithms. Does the simple linear regression model seem plausible for a summary of this graph? If you use a different base of logarithms, the shape of the graph won’t change, but the values on the axes will change.\nWe’ll use the log() function to get the natural logarithms of both variables.\n\n\nCode\nUN11$log_fertility &lt;- log(UN11$fertility)\nUN11$log_ppgdp &lt;- log(UN11$ppgdp)\n\nggplot(UN11, aes(log_ppgdp, log_fertility)) + geom_point() +\n  labs(title = \"Fertility by GDP per Capita: Logarithm Edition\", \n       x = \"Log of GDP per capita\", \n       y = \"Log of Fertility (birth rate per 1000 females)\") + \n  geom_smooth(method = \"lm\")\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nThis graph does lend itself well to using a linear regression model. We’ve added an OLS line here using the geom_smooth() function."
  },
  {
    "objectID": "posts/HW3_JustineShakespeare.html#a-1",
    "href": "posts/HW3_JustineShakespeare.html#a-1",
    "title": "Homework 3",
    "section": "a",
    "text": "a\nHow, if at all, does the slope of the prediction equation change?\nTo answer this question we’ll run a linear regression for both x_US and y, and x_UK and y and compare the coefficients.\n\n\nCode\n## US dollars\nLR_US &lt;- lm(formula = y ~ x_US, data = df)\nsummary(LR_US)$coefficients\n\n\n            Estimate Std. Error   t value     Pr(&gt;|t|)\n(Intercept) 3.150123 2.59153772  1.215542 2.270796e-01\nx_US        1.949919 0.05204747 37.464236 6.901293e-60\n\n\nCode\n## UK pounds sterling\nLR_UK &lt;- lm(formula = y ~ x_UK, data = df)\nsummary(LR_UK)$coefficients\n\n\n            Estimate Std. Error   t value     Pr(&gt;|t|)\n(Intercept) 3.150123 2.59153772  1.215542 2.270796e-01\nx_UK        2.593392 0.06922314 37.464236 6.901293e-60\n\n\nThe coefficients are different, indicating that the slope did change with the different currency. The signs are the same (both slopes are positive and indicate a positive relationship), but the slope of the regression with pounds sterling is steeper."
  },
  {
    "objectID": "posts/HW3_JustineShakespeare.html#b-1",
    "href": "posts/HW3_JustineShakespeare.html#b-1",
    "title": "Homework 3",
    "section": "b",
    "text": "b\nHow, if at all, does the correlation change?\nWe’ll run the cor() function to compare the correlations.\n\n\nCode\ncor(df$x_US, df$y, method = \"pearson\")\n\n\n[1] 0.9668169\n\n\nCode\ncor(df$x_UK, df$y, method = \"pearson\")\n\n\n[1] 0.9668169\n\n\nThe correlations appears to be the same for pounds sterling and US dollars."
  },
  {
    "objectID": "posts/HW3_JustineShakespeare.html#a-2",
    "href": "posts/HW3_JustineShakespeare.html#a-2",
    "title": "Homework 3",
    "section": "a",
    "text": "a\nGraphically portray how the explanatory variable relates to the outcome variable in each of the two cases.\nWe can explore this relationship and our finding with a visual depiction of the data.\nThe scatterplot of this data is a little awkward because of the nature of the data (that it was originally ordinal variables).\n\n\nCode\nggplot(student.survey_i, aes(religiosity, pol_ideo)) + \n  geom_point() + \n  geom_smooth(method = \"lm\") +\n  labs(title = \"Political Ideology and Religiosity\", x = \"Religiosity\", y = \"Political Ideology\")\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nLet’s see what it looks like in a bar chart. We’ll use the original variables so that R recognizes them as ordinal.\n\n\nCode\nggplot(student.survey_i, aes(re, fill = pi)) + \n  geom_bar() +\n  labs(title = \"Political Ideology and Religiosity\", x = \"How often respondents attend religious services\", y = \"Count\", fill = \"Political Ideology\") +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\nThis chart illustrates the relationship a little better. You can see that pretty much all of the respondents who identified as “very conservative” also reported attending religious services “every week”, which is the most frequent choice available."
  },
  {
    "objectID": "posts/HW3_JustineShakespeare.html#b-2",
    "href": "posts/HW3_JustineShakespeare.html#b-2",
    "title": "Homework 3",
    "section": "b",
    "text": "b\nSummarize and interpret results of inferential analyses.\nWe can run the linear regression with the numeric variables we made earlier.\n\n\nCode\npol_reg &lt;- lm(pol_ideo ~ religiosity, data = student.survey_i)\nsummary(pol_reg)\n\n\n\nCall:\nlm(formula = pol_ideo ~ religiosity, data = student.survey_i)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.81243 -0.87160  0.09882  1.12840  3.09882 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   0.9308     0.4252   2.189   0.0327 *  \nreligiosity   0.9704     0.1792   5.416 1.22e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.345 on 58 degrees of freedom\nMultiple R-squared:  0.3359,    Adjusted R-squared:  0.3244 \nF-statistic: 29.34 on 1 and 58 DF,  p-value: 1.221e-06\n\n\nThe results of this regression indicate that there is a positive and significant relationship between religiosity and political ideology (p-value 1.22e-06 &lt;0.001), with people who report being more conservative also reporting being more likely to attend religious services often. The R-squared value is also fairly high (0.3359), indicating that the model is a relatively good fit.\nNow let’s take a look at the second set of variables and explore their relationship. (ii) y = high school GPA and x = hours of TV watching."
  },
  {
    "objectID": "posts/HW3_JustineShakespeare.html#a-3",
    "href": "posts/HW3_JustineShakespeare.html#a-3",
    "title": "Homework 3",
    "section": "a",
    "text": "a\nGraphically portray how the explanatory variable relates to the outcome variable in each of the two cases.\nSince both of these variables are continuous this relationship lends itself well to being visualized with a scatterplot.\n\n\nCode\nggplot(student.survey_i, aes(tv, hi)) + \n  geom_point() + \n  labs(title = \"High school GPA and hours of of TV watched per week\", x = \"Hours of TV watched per week\", y = \"High school GPA\")\n\n\n\n\n\nThis visualization indicates there is a negative relationship between the number of hours of tv watched per week and high school GPA."
  },
  {
    "objectID": "posts/HW3_JustineShakespeare.html#b-3",
    "href": "posts/HW3_JustineShakespeare.html#b-3",
    "title": "Homework 3",
    "section": "b",
    "text": "b\nSummarize and interpret results of inferential analyses.\n\n\nCode\ngpa_tv &lt;- lm(hi ~ tv, data = student.survey_i)\nsummary(gpa_tv)\n\n\n\nCall:\nlm(formula = hi ~ tv, data = student.survey_i)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.2583 -0.2456  0.0417  0.3368  0.7051 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  3.441353   0.085345  40.323   &lt;2e-16 ***\ntv          -0.018305   0.008658  -2.114   0.0388 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4467 on 58 degrees of freedom\nMultiple R-squared:  0.07156,   Adjusted R-squared:  0.05555 \nF-statistic: 4.471 on 1 and 58 DF,  p-value: 0.03879\n\n\nThis linear regression model indicates that the relationship between these variables is negative, as we could see from the scatterplot above. It is also significant (p-vlue 0.0388 &lt; 0.05). The R-squared is not as large as the previous relationship we explored, indicating the model is not as good of a fit."
  },
  {
    "objectID": "posts/Hw3_thrishul.html",
    "href": "posts/Hw3_thrishul.html",
    "title": "Homework - 2",
    "section": "",
    "text": "Code\nlibrary(alr4)\n\n\nWarning: package 'alr4' was built under R version 4.2.3\n\n\nLoading required package: car\n\n\nWarning: package 'car' was built under R version 4.2.3\n\n\nLoading required package: carData\n\n\nWarning: package 'carData' was built under R version 4.2.3\n\n\nLoading required package: effects\n\n\nWarning: package 'effects' was built under R version 4.2.3\n\n\nlattice theme set by effectsTheme()\nSee ?effectsTheme for details.\n\n\nCode\nlibrary(smss)\n\n\nWarning: package 'smss' was built under R version 4.2.3\n\n\nCode\nlibrary(ggplot2)\nlibrary(stargazer)\n\n\n\nPlease cite as: \n\n\n Hlavac, Marek (2022). stargazer: Well-Formatted Regression and Summary Statistics Tables.\n\n\n R package version 5.2.3. https://CRAN.R-project.org/package=stargazer \n\n\n\n\n\n\nCode\ndata(UN11)"
  },
  {
    "objectID": "posts/Hw3_thrishul.html#load-data",
    "href": "posts/Hw3_thrishul.html#load-data",
    "title": "Homework - 2",
    "section": "",
    "text": "Code\ndata(UN11)"
  },
  {
    "objectID": "posts/Hw3_thrishul.html#a",
    "href": "posts/Hw3_thrishul.html#a",
    "title": "Homework - 2",
    "section": "(a)",
    "text": "(a)\nThe independent variable is the Gross Domestic Product (GDP) per person, commonly referred to as GDP per capita. This variable serves as the predictor in the analysis. The dependent variable, or the response, is the birth rate per 1000 women, also known as fertility."
  },
  {
    "objectID": "posts/Hw3_thrishul.html#b",
    "href": "posts/Hw3_thrishul.html#b",
    "title": "Homework - 2",
    "section": "(b)",
    "text": "(b)\n\n\nCode\nggplot(data = UN11, aes(x = ppgdp, y = fertility)) +\n  geom_point()\n\n\n\n\n\nTo adequately model the relationship between the two variables, a linear regression line would not be suitable as the data displays an L-shaped structure (or the left half of a U-shape). This implies that a more complex model would be needed to capture the true nature of the relationship between the variables."
  },
  {
    "objectID": "posts/Hw3_thrishul.html#c",
    "href": "posts/Hw3_thrishul.html#c",
    "title": "Homework - 2",
    "section": "(c)",
    "text": "(c)\n\n\nCode\nggplot(data = UN11, aes(x = log(ppgdp), y = log(fertility))) +\n  geom_point()\n\n\n\n\n\nGiven the nature of the data, a simple linear regression model appears to be appropriate. It is feasible to visualize a negatively-sloped straight line that can be fitted to the points in the scatter plot, which can serve as a preliminary model for the relationship between the two variables."
  },
  {
    "objectID": "posts/Hw3_thrishul.html#a-1",
    "href": "posts/Hw3_thrishul.html#a-1",
    "title": "Homework - 2",
    "section": "(a)",
    "text": "(a)\nTo convert the values of the response variable from US dollars (USD) to British pounds (GBP), it is essential to take into account that the numerical value of the response variable will be reduced by a factor of 1.33. As a result, the slope of the regression line would also need to be adjusted by dividing it by 1.33, in order to ensure that the model accurately reflects the relationship between the variables under the new currency."
  },
  {
    "objectID": "posts/Hw3_thrishul.html#b-1",
    "href": "posts/Hw3_thrishul.html#b-1",
    "title": "Homework - 2",
    "section": "(b)",
    "text": "(b)\nThe correlation coefficient between the two variables is a standardized measure that is not affected by the unit of measurement. Therefore, converting the values from US dollars to British pounds will not have an impact on the correlation coefficient between the two variables. The correlation will remain the same as before the conversion, as it is solely based on the strength and direction of the relationship between the variables, regardless of the unit of measurement used."
  },
  {
    "objectID": "posts/Hw3_thrishul.html#a-2",
    "href": "posts/Hw3_thrishul.html#a-2",
    "title": "Homework - 2",
    "section": "(a)",
    "text": "(a)\nA potential method of visually depicting the correlation between religiosity and political ideology is to use a bar graph. In this representation, as the bars move towards the right side of the graph, indicating higher levels of religiosity, lighter colors start to appear, indicating a greater degree of conservatism in political ideology. It is worth noting that there are alternative visualizations that could be utilized to depict this relationship, but this is one possible approach that could be used to highlight the connection between these two variables.\n\n\nCode\ndata(student.survey)\nggplot(data = student.survey, aes(x = re, fill = pi)) + geom_bar(position = \"fill\")\n\n\n\n\n\nA scatter plot is a useful tool to visually demonstrate the correlation between two variables, and can be used to depict the relationship between high school GPA and hours of watching TV in this case. By plotting the hours of watching TV on the x-axis and the high school GPA on the y-axis, a scatter plot can be generated that will enable us to observe the pattern of the relationship between these two variables. Therefore, a scatter plot is a suitable choice to represent the connection between high school GPA and hours of watching TV.\n\n\nCode\nggplot(data = student.survey, aes(x = tv, y = hi)) +\n  geom_point()"
  },
  {
    "objectID": "posts/Hw3_thrishul.html#b-2",
    "href": "posts/Hw3_thrishul.html#b-2",
    "title": "Homework - 2",
    "section": "(b)",
    "text": "(b)\nHandling ordinal variables in linear regression can be a challenging task. However, in this particular case, we can simplify the process by converting ordinal variables such as political ideology and religiosity into numerical values, and subsequently include them in our analysis. It is worth noting that this is a common approach for dealing with ordinal variables in regression analysis. The high school GPA and hours of TV variables are already continuous, and thus do not require any additional conversion\n\n\nCode\nm1 &lt;- lm(as.numeric(pi) ~ as.numeric(re), \n         data = student.survey)\nm2 &lt;- lm(hi ~ tv, data = student.survey)\nstargazer(m1, m2, type = 'text', dep.var.labels = c('Pol. Ideology', 'HS GPA'), covariate.labels = c('Religiosity', 'Hours of TV'))\n\n\n\n==========================================================\n                                  Dependent variable:     \n                              ----------------------------\n                               Pol. Ideology     HS GPA   \n                                    (1)            (2)    \n----------------------------------------------------------\nReligiosity                       0.970***                \n                                  (0.179)                 \n                                                          \nHours of TV                                     -0.018**  \n                                                 (0.009)  \n                                                          \nConstant                          0.931**       3.441***  \n                                  (0.425)        (0.085)  \n                                                          \n----------------------------------------------------------\nObservations                         60            60     \nR2                                 0.336          0.072   \nAdjusted R2                        0.324          0.056   \nResidual Std. Error (df = 58)      1.345          0.447   \nF Statistic (df = 1; 58)         29.336***       4.471**  \n==========================================================\nNote:                          *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01\n\n\nThe statistical analysis has revealed that there is a significant positive correlation (at the 0.01 significance level) between religiosity and conservatism. In other words, as the level of religiosity increases, conservatism tends to increase as well.\nFurthermore, the analysis has also uncovered a significant negative correlation (at the 0.05 significance level) between hours of TV and high school GPA. Specifically, it has been determined that each additional hour of TV watched per week is associated with a decline of 0.018 in high school GPA. This finding suggests that watching excessive amounts of TV may have a detrimental effect on academic performance in high school."
  },
  {
    "objectID": "posts/HW2_RahulSomu.html",
    "href": "posts/HW2_RahulSomu.html",
    "title": "Homework2",
    "section": "",
    "text": "Question 1\nBelow code chunk calculated the confidence interval for bypass surgery as [18.80009, 19.19991], and the confidence interval for angiography as [17.76757, 18.23243]. Based on the results, we can conclude confidence interval for bypass surgery is slightly narrower than that of angiography, which implies that the estimate of the mean wait time for bypass surgery is slightly more precise than that of angiography.\n\n\nCode\n# Create dataframe\ndf &lt;- data.frame(\n  procedure = c(\"Bypass\", \"Angiography\"),\n  sample_size = c(539, 847),\n  sample_mean = c(19, 18),\n  sample_sd = c(10, 9)\n)\n\n# confidence level\nconf_level &lt;- 0.9\n\n#degrees of freedom for each procedure\ndf$df &lt;- df$sample_size - 1\n\n#critical value for the confidence interval\nt_critical &lt;- qt(1 - (1 - conf_level) / 2, df$df)\n\n#standard error of the mean for each procedure\ndf$sem &lt;- df$sample_sd / sqrt(df$sample_size)\n\n# confidence intervals for each procedure\ndf$ci &lt;- apply(df[, c(\"sample_mean\", \"sem\", \"df\")], 1, function(x) {\n  x[1] + c(-1, 1) * t_critical * x[2] * sqrt(x[3] + 1) / sqrt(x[3])\n})\n\n# Print the confidence intervals\ncat(\"Confidence intervals:\\n\")\n\n\nConfidence intervals:\n\n\nCode\nprint(df$ci)\n\n\n         [,1]     [,2]\n[1,] 18.28963 17.49016\n[2,] 19.70992 18.50952\n\n\n#Question 2\nBelow results suggest that we are 95% confident that the true proportion of adult Americans who believe that the college education is essential for success lies between 0.5189 and 0.5808.\n\n\nCode\nn &lt;- 1031\np_hat &lt;- 567/1031\nz &lt;- qnorm(1-0.05/2)\n\nCI &lt;- p_hat + z*sqrt(p_hat*(1-p_hat)/n) * c(-1, 1)\nCI\n\n\n[1] 0.5195839 0.5803191\n\n\n#Question 3\n\n\nCode\nn = ((1.959964)^2 * (42.5)^2) / (5)^2\nn\n\n\n[1] 277.5454\n\n\n#Question 4\n\n\nCode\n# Part A\nn &lt;- 9\nybar &lt;- 410\ns &lt;- 90\nmu0 &lt;- 500\nalpha &lt;- 0.05\nse &lt;- s / sqrt(n)\nt &lt;- (ybar - mu0) / se\np_value &lt;- 2 * pt(-abs(t), df = n - 1)\n\n# Report results\ncat(\"Test statistic:\", round(t, 2), \"\\n\")\n\n\nTest statistic: -3 \n\n\nCode\ncat(\"P-value:\", p_value, \"\\n\")\n\n\nP-value: 0.01707168 \n\n\nCode\nif(p_value &lt; alpha) {\n  cat(\"Reject null hypothesis; the mean income of female employees differs from $500 per week.\\n\")\n} else {\n  cat(\"Fail to reject null hypothesis. \\n\")\n}\n\n\nReject null hypothesis; the mean income of female employees differs from $500 per week.\n\n\nCode\n# Part B\np_value_lt &lt;- pt(t, df = n - 1)\ncat(\"P-value (Ha: μ &lt; 500):\", p_value_lt, \"\\n\")\n\n\nP-value (Ha: μ &lt; 500): 0.008535841 \n\n\nCode\nif(p_value_lt &lt; alpha) {\n  cat(\"the mean income of female employees is less than $500 per week.\\n\")\n} else {\n  cat(\"Fail to reject null hypothesis. \\n\")\n}\n\n\nthe mean income of female employees is less than $500 per week.\n\n\nCode\n# Part C\np_value_gt &lt;- pt(-t, df = n - 1)\ncat(\"P-value (Ha: μ &gt; 500):\", p_value_gt, \"\\n\")\n\n\nP-value (Ha: μ &gt; 500): 0.9914642 \n\n\nCode\nif(p_value_gt &lt; alpha) {\n  cat(\"the mean income of female employees is greater than $500 per week.\\n\")\n} else {\n  cat(\"Fail to reject null hypothesis\\n\")\n}\n\n\nFail to reject null hypothesis\n\n\n#QUESTION 5\n\n\nCode\n# Jones' study\njones &lt;- data.frame(y_bar = 519.5, se = 10.0, n = 1000)\njones$t &lt;- (jones$y_bar - 500) / jones$se\njones$p_value &lt;- 2 * pt(-abs(jones$t), df = jones$n - 1)\njones$t\n\n\n[1] 1.95\n\n\nCode\njones$p_value\n\n\n[1] 0.05145555\n\n\nCode\n# Smith's study\nsmith &lt;- data.frame(y_bar = 519.7, se = 10.0, n = 1000)\nsmith$t &lt;- (smith$y_bar - 500) / smith$se\nsmith$p_value &lt;- 2 * pt(-abs(smith$t), df = smith$n - 1)\nsmith$t\n\n\n[1] 1.97\n\n\nCode\nsmith$p_value \n\n\n[1] 0.04911426\n\n\nCode\n# Significance testing\nalpha &lt;- 0.05\nif (jones$p_value &lt; alpha) {\n  cat(\"Jones' study is statistically significant\\n\")\n} else {\n  cat(\"Jones' study is not statistically significant\\n\")\n}\n\n\nJones' study is not statistically significant\n\n\nCode\nif (smith$p_value &lt; alpha) {\n  cat(\"Smith's study is statistically significant\\n\")\n} else {\n  cat(\"Smith's study is not statistically significant\\n\")\n}\n\n\nSmith's study is statistically significant\n\n\n#Question 6\n\n\nCode\n# Create contingency table\nsnack_table &lt;- matrix(c(31, 43, 51, 69, 57, 49), nrow = 3, byrow = TRUE)\n\n# Perform chi-squared test\nchisq.test(snack_table)\n\n\n\n    Pearson's Chi-squared test\n\ndata:  snack_table\nX-squared = 3.656, df = 2, p-value = 0.1607\n\n\n#Question 7\n\n\nCode\n# Create a data frame\narea &lt;- c(rep(\"Area 1\", 6), rep(\"Area 2\", 6), rep(\"Area 3\", 6))\ncost &lt;- c(6.2, 9.3, 6.8, 6.1, 6.7, 7.5, 7.5, 8.2, 8.5, 8.2, 7.0, 9.3, 5.8, 6.4, 5.6, 7.1, 3.0, 3.5)\ncost_data &lt;- data.frame(area, cost)\n\n# Perform one-way ANOVA\nmodel &lt;- aov(cost ~ area, data = cost_data)\n\n# Print ANOVA table summary\nsummary(model)\n\n\n            Df Sum Sq Mean Sq F value  Pr(&gt;F)   \narea         2  25.66  12.832   8.176 0.00397 **\nResiduals   15  23.54   1.569                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "posts/dacss603hw2_LauraCollazo.html",
    "href": "posts/dacss603hw2_LauraCollazo.html",
    "title": "DACSS 603 Homework 2",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(kableExtra)\nlibrary(readxl)\nlibrary(lsr)"
  },
  {
    "objectID": "posts/dacss603hw2_LauraCollazo.html#a",
    "href": "posts/dacss603hw2_LauraCollazo.html#a",
    "title": "DACSS 603 Homework 2",
    "section": "A",
    "text": "A\nThe below shows the results of a one-sample t-test which tests whether the mean income of female employees ($410) differs from the mean income of all senior-level workers ($500).\nOne-sample t-tests assume that the population distribution is normal and also that the observations in the sample are generated independently of one another.\n\n\nCode\nset.seed(27)\n\nrnorm_fixed &lt;- function(n, mean, sd) {\n  as.vector(mean + sd * scale(rnorm(n)))\n}\n\ndata4 &lt;- rnorm_fixed(9, 410, 90)\n\noneSampleTTest(x=data4, mu=500)\n\n\n\n   One sample t-test \n\nData variable:   data4 \n\nDescriptive statistics: \n              data4\n   mean     410.000\n   std dev.  90.000\n\nHypotheses: \n   null:        population mean equals 500 \n   alternative: population mean not equal to 500 \n\nTest results: \n   t-statistic:  -3 \n   degrees of freedom:  8 \n   p-value:  0.017 \n\nOther information: \n   two-sided 95% confidence interval:  [340.82, 479.18] \n   estimated effect size (Cohen's d):  1 \n\n\nThe results of this T-test is statistically significant as the p-value is below .05. This means we can reject the null hypothesis."
  },
  {
    "objectID": "posts/dacss603hw2_LauraCollazo.html#b",
    "href": "posts/dacss603hw2_LauraCollazo.html#b",
    "title": "DACSS 603 Homework 2",
    "section": "B",
    "text": "B\nThe p-value when the mean is less than 500 is below.\n\n\nCode\nt.test(data4, mu = 500, alternative = \"less\")\n\n\n\n    One Sample t-test\n\ndata:  data4\nt = -3, df = 8, p-value = 0.008536\nalternative hypothesis: true mean is less than 500\n95 percent confidence interval:\n     -Inf 465.7864\nsample estimates:\nmean of x \n      410 \n\n\nThe results of this t-test are significant as the p-value is below .05. This means we can reject the null hypothesis that the true mean is not less than 500."
  },
  {
    "objectID": "posts/dacss603hw2_LauraCollazo.html#c",
    "href": "posts/dacss603hw2_LauraCollazo.html#c",
    "title": "DACSS 603 Homework 2",
    "section": "C",
    "text": "C\nThe p-value when the mean is greater than 500 is below.\n\n\nCode\nt.test(data4, mu = 500, alternative = \"greater\")\n\n\n\n    One Sample t-test\n\ndata:  data4\nt = -3, df = 8, p-value = 0.9915\nalternative hypothesis: true mean is greater than 500\n95 percent confidence interval:\n 354.2136      Inf\nsample estimates:\nmean of x \n      410 \n\n\nThe results of this test are not significant as the p-value is greater than .05. This means we cannot reject the null hypothesis that the true mean is not greater than 500."
  },
  {
    "objectID": "posts/dacss603hw2_LauraCollazo.html#a-1",
    "href": "posts/dacss603hw2_LauraCollazo.html#a-1",
    "title": "DACSS 603 Homework 2",
    "section": "A",
    "text": "A\nTh first step for this question is to find the standard deviation. It is shown below.\n\n\nCode\n# find the standard deviation (se = sd/sqrt(n))\n\nn &lt;- 1000\n\nse &lt;- 10\n\nsd &lt;- se * sqrt(n)\n\nsd\n\n\n[1] 316.2278\n\n\nThe below shows the t and p-value for Jones.\n\n\nCode\nrnorm_fixed &lt;- function(n, mean, sd) {\n  as.vector(mean + sd * scale(rnorm(n)))\n}\n\ndata5_j &lt;- rnorm_fixed(1000, 519.5, sd)\n\nt_test_j &lt;- oneSampleTTest(x=data5_j, mu=500)\n\nt_test_j\n\n\n\n   One sample t-test \n\nData variable:   data5_j \n\nDescriptive statistics: \n            data5_j\n   mean     519.500\n   std dev. 316.228\n\nHypotheses: \n   null:        population mean equals 500 \n   alternative: population mean not equal to 500 \n\nTest results: \n   t-statistic:  1.95 \n   degrees of freedom:  999 \n   p-value:  0.051 \n\nOther information: \n   two-sided 95% confidence interval:  [499.877, 539.123] \n   estimated effect size (Cohen's d):  0.062 \n\n\nThe below shows the t and p-value for Smith.\n\n\nCode\ndata5_s &lt;- rnorm_fixed(1000, 519.7, sd)\n\nt_test_s &lt;- oneSampleTTest(x=data5_s, mu=500)\n\nt_test_s\n\n\n\n   One sample t-test \n\nData variable:   data5_s \n\nDescriptive statistics: \n            data5_s\n   mean     519.700\n   std dev. 316.228\n\nHypotheses: \n   null:        population mean equals 500 \n   alternative: population mean not equal to 500 \n\nTest results: \n   t-statistic:  1.97 \n   degrees of freedom:  999 \n   p-value:  0.049 \n\nOther information: \n   two-sided 95% confidence interval:  [500.077, 539.323] \n   estimated effect size (Cohen's d):  0.062"
  },
  {
    "objectID": "posts/dacss603hw2_LauraCollazo.html#b-1",
    "href": "posts/dacss603hw2_LauraCollazo.html#b-1",
    "title": "DACSS 603 Homework 2",
    "section": "B",
    "text": "B\nThe above indicates the results of Jones’ study (p = .051) are not statistically significant and the results of Smith’s study are statistically significant (p = .049) when α = 0.05."
  },
  {
    "objectID": "posts/dacss603hw2_LauraCollazo.html#c-1",
    "href": "posts/dacss603hw2_LauraCollazo.html#c-1",
    "title": "DACSS 603 Homework 2",
    "section": "C",
    "text": "C\nI am not yet certain of how to describe the misleading aspects of reporting the result of a test as “P is &lt; or = to 0.05” versus “P is &gt; 0.” Perhaps this has to do with the importance of saying that the null hypothesis is rejected if p is statistically significant rather than saying we accept the null hypothesis as true if p is not statistically significant as we should always assume the null hypothesis is true. I think this question is asking for something different than this, though. I look forward to seeing the answer to this question so I can learn from it!"
  },
  {
    "objectID": "posts/Final_Project_Check-In_Saisrinivas.html",
    "href": "posts/Final_Project_Check-In_Saisrinivas.html",
    "title": "603_Project_Check_In_Saisrinivas_Ambatipudi",
    "section": "",
    "text": "The blood transfusion dataset contain 748 samples with 5 input features: Input Features: • Recency (number of months since the last donation) • Frequency (total number of donations) • Monetary (total blood donated in c.c.) • Time (number of months since the first donation) • Age (age of the donor)\nSource: https://www.kaggle.com/datasets/ninalabiba/blood-transfusion-dataset\n\nBD &lt;- read_csv(\"C:/UMass/DACSS_603/603_Spring_2023/posts/_data/transfusion_saisrinivas.csv\")\n\nRows: 748 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (5): Recency (months), Frequency (times), Monetary (c.c. blood), Time (m...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nBD\n\n# A tibble: 748 × 5\n   `Recency (months)` `Frequency (times)` Monetary (c.c. blood…¹ Time …² wheth…³\n                &lt;dbl&gt;               &lt;dbl&gt;                  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n 1                  2                  50                  12500      98       1\n 2                  0                  13                   3250      28       1\n 3                  1                  16                   4000      35       1\n 4                  2                  20                   5000      45       1\n 5                  1                  24                   6000      77       0\n 6                  4                   4                   1000       4       0\n 7                  2                   7                   1750      14       1\n 8                  1                  12                   3000      35       0\n 9                  2                   9                   2250      22       1\n10                  5                  46                  11500      98       1\n# … with 738 more rows, and abbreviated variable names\n#   ¹​`Monetary (c.c. blood)`, ²​`Time (months)`,\n#   ³​`whether he/she donated blood in March 2007`"
  },
  {
    "objectID": "posts/Final_Project_Check-In_Saisrinivas.html#data-description",
    "href": "posts/Final_Project_Check-In_Saisrinivas.html#data-description",
    "title": "603_Project_Check_In_Saisrinivas_Ambatipudi",
    "section": "",
    "text": "The blood transfusion dataset contain 748 samples with 5 input features: Input Features: • Recency (number of months since the last donation) • Frequency (total number of donations) • Monetary (total blood donated in c.c.) • Time (number of months since the first donation) • Age (age of the donor)\nSource: https://www.kaggle.com/datasets/ninalabiba/blood-transfusion-dataset\n\nBD &lt;- read_csv(\"C:/UMass/DACSS_603/603_Spring_2023/posts/_data/transfusion_saisrinivas.csv\")\n\nRows: 748 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (5): Recency (months), Frequency (times), Monetary (c.c. blood), Time (m...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nBD\n\n# A tibble: 748 × 5\n   `Recency (months)` `Frequency (times)` Monetary (c.c. blood…¹ Time …² wheth…³\n                &lt;dbl&gt;               &lt;dbl&gt;                  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n 1                  2                  50                  12500      98       1\n 2                  0                  13                   3250      28       1\n 3                  1                  16                   4000      35       1\n 4                  2                  20                   5000      45       1\n 5                  1                  24                   6000      77       0\n 6                  4                   4                   1000       4       0\n 7                  2                   7                   1750      14       1\n 8                  1                  12                   3000      35       0\n 9                  2                   9                   2250      22       1\n10                  5                  46                  11500      98       1\n# … with 738 more rows, and abbreviated variable names\n#   ¹​`Monetary (c.c. blood)`, ²​`Time (months)`,\n#   ³​`whether he/she donated blood in March 2007`"
  },
  {
    "objectID": "posts/Final_Project_Check-In_Saisrinivas.html#research-questions",
    "href": "posts/Final_Project_Check-In_Saisrinivas.html#research-questions",
    "title": "603_Project_Check_In_Saisrinivas_Ambatipudi",
    "section": "Research Questions:",
    "text": "Research Questions:\nBlood Donation Prediction Frequency:\nThe aim of this study is to develop linear regression, logistic regression machine learning model, to accurately predict whether a blood donor is likely to donate in the future. The results of this study could be useful in developing targeted strategies for donor recruitment and retention, ultimately improving the availability and accessibility of blood donations.\nIdentify Factors that Affect Blood Donation:\nHow do donation behaviors vary across different regions, and what factors may contribute to these variations? Using hypothesis testing, this research project aims to compare the donation patterns of donors from different regions based on demographic and donation-related variables, such as age, gender, donation frequency, and time since last donation. The findings can provide insights into the regional variations in blood donation behaviors and inform targeted strategies to address these differences, potentially leading to increased donation rates and more efficient allocation of resources for blood donation organizations. The study will visualize the results using plots and charts to identify any significant patterns or trends that could help healthcare and blood donation organizations to develop effective strategies to increase blood donation rates.\nSegmentation of Blood Donors:\nUse clustering techniques to segment blood donors based on their demographic and donation history characteristics. Explore methods such as K-means clustering or hierarchical clustering to identify groups of donors with similar characteristics. Examine the differences between these groups and explore their donation patterns over time.\nBuilding a Donor Retention Strategy:\nUse the insights gained from the previous projects to develop a donor retention strategy for the blood donation center. Identify factors that are associated with donor churn (i.e., donors who stop donating blood), and develop a plan to mitigate these factors. This can help healthcare and blood donation organizations to develop strategies to retain donors over the long term."
  },
  {
    "objectID": "posts/Final_Project_Check-In_Saisrinivas.html#summary-of-the-data",
    "href": "posts/Final_Project_Check-In_Saisrinivas.html#summary-of-the-data",
    "title": "603_Project_Check_In_Saisrinivas_Ambatipudi",
    "section": "Summary of the data",
    "text": "Summary of the data\n\nsummary(BD)\n\n Recency (months) Frequency (times) Monetary (c.c. blood) Time (months)  \n Min.   : 0.000   Min.   : 1.000    Min.   :  250         Min.   : 2.00  \n 1st Qu.: 2.750   1st Qu.: 2.000    1st Qu.:  500         1st Qu.:16.00  \n Median : 7.000   Median : 4.000    Median : 1000         Median :28.00  \n Mean   : 9.507   Mean   : 5.515    Mean   : 1379         Mean   :34.28  \n 3rd Qu.:14.000   3rd Qu.: 7.000    3rd Qu.: 1750         3rd Qu.:50.00  \n Max.   :74.000   Max.   :50.000    Max.   :12500         Max.   :98.00  \n whether he/she donated blood in March 2007\n Min.   :0.000                             \n 1st Qu.:0.000                             \n Median :0.000                             \n Mean   :0.238                             \n 3rd Qu.:0.000                             \n Max.   :1.000"
  },
  {
    "objectID": "posts/Final_Project_Check-In_Saisrinivas.html#descrpition-of-the-variables",
    "href": "posts/Final_Project_Check-In_Saisrinivas.html#descrpition-of-the-variables",
    "title": "603_Project_Check_In_Saisrinivas_Ambatipudi",
    "section": "Descrpition of the Variables:",
    "text": "Descrpition of the Variables:\nThis summary function presents a statistical description of a dataset related to blood donation, with five variables: Recency, Frequency, Monetary, Time, and whether the individual donated blood in March 2007. Here’s a breakdown of each variable:\nRecency (months): This variable represents the number of months since the last blood donation. The minimum value is 0 months, indicating that some individuals donated blood very recently. The mean is 9.507 months, suggesting that, on average, people donated blood around 9.5 months ago. The maximum value is 74 months, which means the longest gap between donations is 74 months.\nFrequency (times): This variable shows the total number of times an individual has donated blood. The minimum value is 1, meaning that at least one person has only donated blood once. The mean is 5.515 times, indicating that people, on average, have donated blood about 5.5 times. The maximum value is 50 times, showing that some individuals have donated blood quite frequently.\nMonetary (c.c. blood): This variable represents the total volume of blood donated by an individual, measured in cubic centimeters (c.c.). The minimum value is 250 c.c., which corresponds to the minimum single donation volume. The mean is 1379 c.c., suggesting that, on average, individuals have donated around 1.379 liters of blood. The maximum value is 12,500 c.c., indicating that the highest total volume donated by a person is 12.5 liters.\nTime (months): This variable measures the length of time an individual has been donating blood. The minimum value is 2 months, suggesting that some individuals are relatively new to blood donation. The mean is 34.28 months, indicating that, on average, people have been donating blood for about 34.3 months. The maximum value is 98 months, showing that some individuals have been donating blood for a long time.\nWhether he/she donated blood in March 2007: This is a binary variable that indicates if an individual donated blood in March 2007. The mean is 0.238, which means that about 23.8% of the individuals in the dataset donated blood in that specific month.\nThe summary function provides an overview of the dataset’s key statistics, such as minimum, 1st quartile, median, mean, 3rd quartile, and maximum values for each variable. This information helps to understand the distribution, central tendency, and spread of the data."
  },
  {
    "objectID": "posts/Final_Project_Check-In_Saisrinivas.html#expected-contribution",
    "href": "posts/Final_Project_Check-In_Saisrinivas.html#expected-contribution",
    "title": "603_Project_Check_In_Saisrinivas_Ambatipudi",
    "section": "Expected Contribution:",
    "text": "Expected Contribution:\n\nAkhilesh Kumar: Will work on the Segmentation of Blood Donors and Building a Donor Retention Strategy\nSai Srinivas: Will work on the Blood Donation Prediction Frequency and Identify Factors that Affect Blood Donation"
  },
  {
    "objectID": "posts/Final_Project_Check-In_Saisrinivas.html#references",
    "href": "posts/Final_Project_Check-In_Saisrinivas.html#references",
    "title": "603_Project_Check_In_Saisrinivas_Ambatipudi",
    "section": "References:",
    "text": "References:\nKaggle: https://www.kaggle.com/datasets/ninalabiba/blood-transfusion-dataset\nOriginal Owner and Donor: Prof. I-Cheng Yeh, Department of Information Management, Chung-Hua University, Hsin Chu, Taiwan 30067, R.O.C., e-mail:icyeh ‘@’ chu.edu.tw, TEL:886-3-5186511, Date Donated: October 3, 2008"
  },
  {
    "objectID": "posts/AlexisGamez_Project_Analysis.html",
    "href": "posts/AlexisGamez_Project_Analysis.html",
    "title": "Project Analysis",
    "section": "",
    "text": "Code\nlibrary(plyr)\nlibrary(tidyverse)\nlibrary(readr)\nlibrary(summarytools)\nlibrary(psych)\nlibrary(lattice)\nlibrary(FSA)\nlibrary(kableExtra)\n\nknitr::opts_chunk$set(echo = T)\nCode\n# reading in our data set\nVideo_Game_Sales &lt;- read_csv(\"_data/final_project/Video_Game_Sales_as_of_Jan_2017.csv\")\nhead(Video_Game_Sales)\n\n\n# A tibble: 6 × 15\n  Name       Platform Year_of_Release Genre Publisher NA_Sales EU_Sales JP_Sales\n  &lt;chr&gt;      &lt;chr&gt;              &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;        &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 Wii Sports Wii                 2006 Spor… Nintendo      41.4    29.0      3.77\n2 Super Mar… NES                 1985 Plat… Nintendo      29.1     3.58     6.81\n3 Mario Kar… Wii                 2008 Raci… Nintendo      15.7    12.8      3.79\n4 Wii Sport… Wii                 2009 Spor… Nintendo      15.6    11.0      3.28\n5 Pokemon R… G                   1996 Role… Nintendo      11.3     8.89    10.2 \n6 Tetris     G                   1989 Puzz… Nintendo      23.2     2.26     4.22\n# … with 7 more variables: Other_Sales &lt;dbl&gt;, Global_Sales &lt;dbl&gt;,\n#   Critic_Score &lt;dbl&gt;, Critic_Count &lt;dbl&gt;, User_Score &lt;dbl&gt;, User_Count &lt;dbl&gt;,\n#   Rating &lt;chr&gt;"
  },
  {
    "objectID": "posts/AlexisGamez_Project_Analysis.html#explanatory-variables",
    "href": "posts/AlexisGamez_Project_Analysis.html#explanatory-variables",
    "title": "Project Analysis",
    "section": "Explanatory Variables",
    "text": "Explanatory Variables\n\nGenre\nPlatform/Manufacturer"
  },
  {
    "objectID": "posts/AlexisGamez_Project_Analysis.html#response-varibles",
    "href": "posts/AlexisGamez_Project_Analysis.html#response-varibles",
    "title": "Project Analysis",
    "section": "Response Varibles",
    "text": "Response Varibles\n\nGlobal Sales\nNA Sales\nJPN Sales\nEU Sales\nOther Sales"
  },
  {
    "objectID": "posts/AlexisGamez_Project_Analysis.html#control-varibles",
    "href": "posts/AlexisGamez_Project_Analysis.html#control-varibles",
    "title": "Project Analysis",
    "section": "Control Varibles",
    "text": "Control Varibles\n\nPublisher\nYear of Release\nName\nCritic Score\nCritic Count\nUser Score\nUser Count\nRating"
  },
  {
    "objectID": "posts/AlexisGamez_Project_Analysis.html#anova",
    "href": "posts/AlexisGamez_Project_Analysis.html#anova",
    "title": "Project Analysis",
    "section": "ANOVA",
    "text": "ANOVA\nIn this section we’ll be testing the explanatory variables, Genre & Manufacturer, against the main response variable Global Sales. We’re also going to be testing a control variable Rating against sales to see if there is any significance. It’s important to note that all 3 variables we will be testing are categorical, where as Global Sales is continuous.\nFirst up is the Manufacturer variable.\n\n\nCode\nM_aov &lt;- aov(Global_Sales ~ Manufacturer, data = VGS3)\n\nsummary(M_aov)\n\n\n                Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nManufacturer     4    194   48.53   17.99 9.83e-15 ***\nResiduals    10232  27599    2.70                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nHere we see that our Pr(&gt;F) value is very small, allowing us to reject the null at a significance level of 0.001. This means the Manufacturer means are significantly different.\n\n\nCode\nggplot(VGS3, mapping = aes(x=Manufacturer, y=Global_Sales, color=Manufacturer))+\n    geom_boxplot() +\n    labs(title = \"Distribution of Global Sales per Manufacturer\", y = \"Global Sales (millions)\") \n\n\n\n\n\nCode\nlimit &lt;- c(0, 1)\n\nggplot(VGS3, mapping = aes(x=Manufacturer, y=Global_Sales, color=Manufacturer))+\n    geom_boxplot() +\n    labs(title = \"Distribution of Global Sales per Manufacturer\", y = \"Total Worlwide Sales (millions)\") +\n    scale_y_continuous(breaks = seq(from =0, to = 1, by = .25),\n                       limits = limit)\n\n\n\n\n\nNext we’ll be testing whether the population means among the category Genre are significantly different as well.\n\n\nCode\nG_aov &lt;- aov(Global_Sales ~ Genre, data = VGS3)\n\nsummary(G_aov)\n\n\n               Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nGenre          11    212  19.245   7.134 3.27e-12 ***\nResiduals   10225  27581   2.697                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nOnce again, we receive an extremely small F-value telling us that the Genre means are significantly different.\n\n\nCode\nggplot(VGS3, mapping = aes(x=Genre, y=Global_Sales, color=Genre))+\n    geom_boxplot() +\n    labs(title = \"Distribution of Global Sales per Genre\", y = \"Global Sales (millions)\") \n\n\n\n\n\nCode\nggplot(VGS3, mapping = aes(x=Genre, y=Global_Sales, color=Genre))+\n    geom_boxplot() +\n    labs(title = \"Distribution of Global Sales per Genre\", y = \"Total Worlwide Sales (millions)\") +\n    scale_y_continuous(breaks = seq(from =0, to = 1, by = .25),\n                       limits = limit)\n\n\n\n\n\nFinally, the last test will decided whether the Rating means are significantly different as well.\n\n\nCode\nR_aov &lt;- aov(Global_Sales ~ Rating, data = VGS3)\n\nsummary(R_aov)\n\n\n               Df Sum Sq Mean Sq F value Pr(&gt;F)    \nRating          3    245   81.60   30.31 &lt;2e-16 ***\nResiduals   10233  27548    2.69                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nAnd again it looks like we receive another small F-value telling us that the Rating means are significantly different.\n\n\nCode\nggplot(VGS3, mapping = aes(x=Rating, y=Global_Sales, color=Rating))+\n    geom_boxplot() +\n    labs(title = \"Distribution of Global Sales per Rating\", y = \"Global Sales (millions)\") \n\n\n\n\n\nCode\nggplot(VGS3, mapping = aes(x=Rating, y=Global_Sales, color=Rating))+\n    geom_boxplot() +\n    labs(title = \"Distribution of Global Sales per Rating\", y = \"Total Worlwide Sales (millions)\") +\n    scale_y_continuous(breaks = seq(from =0, to = 1, by = .25),\n                       limits = limit)"
  },
  {
    "objectID": "posts/AlexisGamez_Project_Analysis.html#chi-square-test",
    "href": "posts/AlexisGamez_Project_Analysis.html#chi-square-test",
    "title": "Project Analysis",
    "section": "Chi-Square Test",
    "text": "Chi-Square Test\nThe point of a Chi-Square test is to determine whether there is any association between two categorical variables that may or may not be independent. As such, I’ll be testing my explanatory variables against each other and, individually against the control variable Rating. All variables tested must be categorical in the case of Chi-Square tests.\nFirst, I’ll test Genre against the control.\n\n\nCode\nchisq.test(VGS3$Genre, VGS3$Rating, correct = F)\n\n\n\n    Pearson's Chi-squared test\n\ndata:  VGS3$Genre and VGS3$Rating\nX-squared = 5263.6, df = 33, p-value &lt; 2.2e-16\n\n\nHere we receive a very low p-value, telling me that I can reject the null and that Genre is not independent of Rating.\n\n\nCode\ntable1 &lt;- data.frame(with(VGS3, table(Genre,Rating)))\n\nggplot(table1, aes(x=Genre,y=Freq, fill=Rating))+\n  geom_bar(stat=\"identity\",position=\"dodge\")+\n  scale_fill_discrete(name = \"Rating\",labels = Rating_List)\n\n\n\n\n\nNext, I’ll be testing Manufacturer against Rating.\n\n\nCode\nchisq.test(VGS3$Manufacturer, VGS3$Rating, correct = F)\n\n\n\n    Pearson's Chi-squared test\n\ndata:  VGS3$Manufacturer and VGS3$Rating\nX-squared = 1691.1, df = 12, p-value &lt; 2.2e-16\n\n\nYet again, I received a low p-value and I know that these 2 variables are also not independent of each other\n\n\nCode\ntable2 &lt;- data.frame(with(VGS3, table(Manufacturer,Rating)))\n\nggplot(table2, aes(x=Manufacturer,y=Freq, fill=Rating))+\n  geom_bar(stat=\"identity\",position=\"dodge\")+\n  scale_fill_discrete(name = \"Rating\",labels = Rating_List)\n\n\n\n\n\nLastly, I will be testing our explanatory variables against each other\n\n\nCode\nchisq.test(VGS3$Genre, VGS3$Manufacturer, correct = F)\n\n\n\n    Pearson's Chi-squared test\n\ndata:  VGS3$Genre and VGS3$Manufacturer\nX-squared = 1955.3, df = 44, p-value &lt; 2.2e-16\n\n\nEven with my last test, I again receive a small p-value telling me the results are, yet again, statistically significant.\n\n\nCode\ntable3 &lt;- data.frame(with(VGS3, table(Genre,Manufacturer)))\n\nggplot(table3, aes(x=Genre,y=Freq, fill=Manufacturer))+\n  geom_bar(stat=\"identity\",position=\"dodge\")+\n  scale_fill_discrete(name = \"Rating\",labels = Manuf_List)"
  },
  {
    "objectID": "posts/HW1_SaisrinivasAmbatipudi.html",
    "href": "posts/HW1_SaisrinivasAmbatipudi.html",
    "title": "Homework 1",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(ggplot2)\nlibrary(stats)\n\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/HW1_SaisrinivasAmbatipudi.html#question-1",
    "href": "posts/HW1_SaisrinivasAmbatipudi.html#question-1",
    "title": "Homework 1",
    "section": "Question 1",
    "text": "Question 1"
  },
  {
    "objectID": "posts/HW1_SaisrinivasAmbatipudi.html#reading-data",
    "href": "posts/HW1_SaisrinivasAmbatipudi.html#reading-data",
    "title": "Homework 1",
    "section": "Reading data",
    "text": "Reading data\n\n\nCode\nLc &lt;- read_excel(\"C:/UMass/DACSS_603/603_Spring_2023/posts/_data/LungCapData.xls\")\nLc\n\n\n\n\n  \n\n\n\nThe data consists of 725 rows and 6 columns. It determines the lung capacity of the based on their age, height and different characteristics. The main key classification that I can see is if they smoke or not."
  },
  {
    "objectID": "posts/HW1_SaisrinivasAmbatipudi.html#a",
    "href": "posts/HW1_SaisrinivasAmbatipudi.html#a",
    "title": "Homework 1",
    "section": "1a",
    "text": "1a\nThe distribution of LungCap looks as follows:\n\n\nCode\nLc %&gt;%\n  ggplot(aes(LungCap, ..density..)) +\n  geom_histogram(bins= 25, color = \"orange\") +\n  geom_density(color = \"darkblue\") +\n  theme_classic() + \n  labs(title = \"Probability distribution of LungCap\", x = \"Lung Capcity\", y = \"Probability density\")\n\n\nWarning: The dot-dot notation (`..density..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(density)` instead.\n\n\n\n\n\nThe histogram and density plots show that it is pretty close to a normal distribution. Most of the observations are close to the mean."
  },
  {
    "objectID": "posts/HW1_SaisrinivasAmbatipudi.html#b",
    "href": "posts/HW1_SaisrinivasAmbatipudi.html#b",
    "title": "Homework 1",
    "section": "1b",
    "text": "1b\nThe distribution of LungCap on basis of gender looks as follows:\n\n\nCode\nLc %&gt;%\n  ggplot(aes(y = dnorm(LungCap), color = Gender)) +\n  geom_boxplot() +\n  theme_classic() + \n  labs(title = \"Probability distribution of LungCap based on gender\", y = \"Probability density\")\n\n\n\n\n\nThe box plot shows that the probability density of the male is lesser than the female."
  },
  {
    "objectID": "posts/HW1_SaisrinivasAmbatipudi.html#c",
    "href": "posts/HW1_SaisrinivasAmbatipudi.html#c",
    "title": "Homework 1",
    "section": "1c",
    "text": "1c\nComparison of mean lung capacities between smokers and non-smokers:\n\n\nCode\nMean_smoke &lt;- Lc %&gt;%\n  group_by(Smoke) %&gt;%\n  summarise(mean = mean(LungCap))\nMean_smoke\n\n\n\n\n  \n\n\n\nFrom the above table, we see that the mean lung capacity of those who smoke is greater than those who don’t smoke, but it doesn’t make sense. It also depends on the biological factors of the person who smoke, so we can’t conclude it."
  },
  {
    "objectID": "posts/HW1_SaisrinivasAmbatipudi.html#d",
    "href": "posts/HW1_SaisrinivasAmbatipudi.html#d",
    "title": "Homework 1",
    "section": "1d",
    "text": "1d\nRelationship between Smoke and Lung capacity on basis of given age categories:\n\n\nCode\nLc &lt;- mutate(Lc, AgeGrp = case_when(Age &lt;= 13 ~ \"less than or equal to 13\",\n                                    Age == 14 | Age == 15 ~ \"14 to 15\",\n                                    Age == 16 | Age == 17 ~ \"16 to 17\",\n                                    Age &gt;= 18 ~ \"greater than or equal to 18\"))\n\nLc %&gt;%\n  ggplot(aes(y = LungCap, color = Smoke)) +\n  geom_histogram(bins = 25) +\n  facet_wrap(vars(AgeGrp)) +\n  theme_classic() + \n  labs(title = \"Relationship of LungCap and Smoke based on age categories\", y = \"Lung Capacity\", x = \"Frequency\")\n\n\n\n\n\nFrom the above plot, we can derive two important observations: 1. The lung capacity of non smokers is more than smokers. 2. The people who smoke are less in age group of “less than or equal to 13”. So as the result as age increases the lung capacity decreases."
  },
  {
    "objectID": "posts/HW1_SaisrinivasAmbatipudi.html#e",
    "href": "posts/HW1_SaisrinivasAmbatipudi.html#e",
    "title": "Homework 1",
    "section": "1e",
    "text": "1e\nRelationship between Smoke and Lung capacity on basis of age:\n\n\nCode\nLc %&gt;%\n  ggplot(aes(x = Age, y = LungCap, color = Smoke)) +\n  geom_line() +\n  theme_classic() + \n  facet_wrap(vars(Smoke)) +\n  labs(title = \"Relationship of LungCap and Smoke based on age\", y = \"Lung Capacity\", x = \"Age\")\n\n\n\n\n\nForm the above data we can compare 1d and 1e and can say the results are pretty similar. Only 10 and above age group smoke."
  },
  {
    "objectID": "posts/HW1_SaisrinivasAmbatipudi.html#f",
    "href": "posts/HW1_SaisrinivasAmbatipudi.html#f",
    "title": "Homework 1",
    "section": "1f",
    "text": "1f\nCalculating the correlation and covariance between Lung Capacity and Age:\n\n\nCode\nCovariance &lt;- cov(Lc$LungCap, Lc$Age)\nCorrelation &lt;- cor(Lc$LungCap, Lc$Age)\nCovariance\n\n\n[1] 8.738289\n\n\nCode\nCorrelation\n\n\n[1] 0.8196749\n\n\nWe can observe from the comparison that the covariance is positive and it indicates that there is a direct relationship between age and lung capacity. And the correlation is also positive, so they move in same direction. We can say from these results that as the age increases, the lung capacity also increases that is they are directly proportional to each other."
  },
  {
    "objectID": "posts/HW1_SaisrinivasAmbatipudi.html#question-2",
    "href": "posts/HW1_SaisrinivasAmbatipudi.html#question-2",
    "title": "Homework 1",
    "section": "Question 2",
    "text": "Question 2"
  },
  {
    "objectID": "posts/HW1_SaisrinivasAmbatipudi.html#reading-the-table",
    "href": "posts/HW1_SaisrinivasAmbatipudi.html#reading-the-table",
    "title": "Homework 1",
    "section": "Reading the table",
    "text": "Reading the table\n\n\nCode\nPrior_convitions &lt;- c(0:4)\nInmate_count &lt;- c(128, 434, 160, 64, 24)\nPc &lt;- data_frame(Prior_convitions, Inmate_count)\n\n\nWarning: `data_frame()` was deprecated in tibble 1.1.0.\nℹ Please use `tibble()` instead.\n\n\nCode\nPc\n\n\n\n\n  \n\n\n\n\n\nCode\nPc &lt;- mutate(Pc, Probability = Inmate_count/sum(Inmate_count))\nPc"
  },
  {
    "objectID": "posts/HW1_SaisrinivasAmbatipudi.html#a-1",
    "href": "posts/HW1_SaisrinivasAmbatipudi.html#a-1",
    "title": "Homework 1",
    "section": "2a",
    "text": "2a\nProbability that a randomly selected inmate has exactly 2 prior convictions:\n\n\nCode\nPc %&gt;%\n  filter(Prior_convitions == 2) %&gt;%\n  select(Probability)"
  },
  {
    "objectID": "posts/HW1_SaisrinivasAmbatipudi.html#b-1",
    "href": "posts/HW1_SaisrinivasAmbatipudi.html#b-1",
    "title": "Homework 1",
    "section": "2b",
    "text": "2b\nProbability that a randomly selected inmate has fewer than 2 convictions:\n\n\nCode\ntemp &lt;- Pc %&gt;%\n  filter(Prior_convitions &lt; 2)\nsum(temp$Probability)\n\n\n[1] 0.6938272"
  },
  {
    "objectID": "posts/HW1_SaisrinivasAmbatipudi.html#c-1",
    "href": "posts/HW1_SaisrinivasAmbatipudi.html#c-1",
    "title": "Homework 1",
    "section": "2c",
    "text": "2c\nProbability that a randomly selected inmate has 2 or fewer prior convictions:\n\n\nCode\ntemp &lt;- Pc %&gt;%\n  filter(Prior_convitions &lt;= 2)\nsum(temp$Probability)\n\n\n[1] 0.891358"
  },
  {
    "objectID": "posts/HW1_SaisrinivasAmbatipudi.html#d-1",
    "href": "posts/HW1_SaisrinivasAmbatipudi.html#d-1",
    "title": "Homework 1",
    "section": "2d",
    "text": "2d\nProbability that a randomly selected inmate has more than 2 prior convictions:\n\n\nCode\ntemp &lt;- Pc %&gt;%\n  filter(Prior_convitions &gt; 2)\nsum(temp$Probability)\n\n\n[1] 0.108642"
  },
  {
    "objectID": "posts/HW1_SaisrinivasAmbatipudi.html#e-1",
    "href": "posts/HW1_SaisrinivasAmbatipudi.html#e-1",
    "title": "Homework 1",
    "section": "2e",
    "text": "2e\nExpected value for the number of prior convictions:\n\n\nCode\nPc &lt;- mutate(Pc, Wm = Prior_convitions*Probability)\ne &lt;- sum(Pc$Wm)\ne\n\n\n[1] 1.28642"
  },
  {
    "objectID": "posts/HW1_SaisrinivasAmbatipudi.html#f-1",
    "href": "posts/HW1_SaisrinivasAmbatipudi.html#f-1",
    "title": "Homework 1",
    "section": "2f",
    "text": "2f\nVariance for the Prior Convictions:\n\n\nCode\nv &lt;-sum(((Pc$Prior_convitions-e)^2)*Pc$Probability)\nv\n\n\n[1] 0.8562353\n\n\nstandard deviation for the Prior Convictions:\n\n\nCode\nsqrt(v)\n\n\n[1] 0.9253298"
  },
  {
    "objectID": "posts/603_Final_Project_Check_In_2_Saisrinivas_Ambatipudi.html",
    "href": "posts/603_Final_Project_Check_In_2_Saisrinivas_Ambatipudi.html",
    "title": "603_Final_Project_Check_In_2_Saisrinivas_Ambatipudi",
    "section": "",
    "text": "The blood transfusion dataset contain 748 samples with 5 input features: Input Features: • Recency (number of months since the last donation) • Frequency (total number of donations) • Monetary (total blood donated in c.c.) • Time (number of months since the first donation) • Age (age of the donor)\nSource: https://www.kaggle.com/datasets/ninalabiba/blood-transfusion-dataset\nThis summary function presents a statistical description of a dataset related to blood donation, with five variables: Recency, Frequency, Monetary, Time, and whether the individual donated blood in March 2007. Here’s a breakdown of each variable:\nRecency (months): This variable represents the number of months since the last blood donation. The minimum value is 0 months, indicating that some individuals donated blood very recently. The mean is 9.507 months, suggesting that, on average, people donated blood around 9.5 months ago. The maximum value is 74 months, which means the longest gap between donations is 74 months.\nFrequency (times): This variable shows the total number of times an individual has donated blood. The minimum value is 1, meaning that at least one person has only donated blood once. The mean is 5.515 times, indicating that people, on average, have donated blood about 5.5 times. The maximum value is 50 times, showing that some individuals have donated blood quite frequently.\nMonetary (c.c. blood): This variable represents the total volume of blood donated by an individual, measured in cubic centimeters (c.c.). The minimum value is 250 c.c., which corresponds to the minimum single donation volume. The mean is 1379 c.c., suggesting that, on average, individuals have donated around 1.379 liters of blood. The maximum value is 12,500 c.c., indicating that the highest total volume donated by a person is 12.5 liters.\nTime (months): This variable measures the length of time an individual has been donating blood. The minimum value is 2 months, suggesting that some individuals are relatively new to blood donation. The mean is 34.28 months, indicating that, on average, people have been donating blood for about 34.3 months. The maximum value is 98 months, showing that some individuals have been donating blood for a long time.\nWhether he/she donated blood in March 2007: This is a binary variable that indicates if an individual donated blood in March 2007. The mean is 0.238, which means that about 23.8% of the individuals in the dataset donated blood in that specific month.\nThe summary function provides an overview of the dataset’s key statistics, such as minimum, 1st quartile, median, mean, 3rd quartile, and maximum values for each variable. This information helps to understand the distribution, central tendency, and spread of the data.\n\nBD &lt;- read_csv(\"C:/UMass/DACSS_603/603_Spring_2023/posts/_data/transfusion_saisrinivas.csv\")\n\nRows: 748 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (5): Recency (months), Frequency (times), Monetary (c.c. blood), Time (m...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndata(BD)\n\nWarning in data(BD): data set 'BD' not found\n\nBD\n\n# A tibble: 748 × 5\n   `Recency (months)` `Frequency (times)` Monetary (c.c. blood…¹ Time …² wheth…³\n                &lt;dbl&gt;               &lt;dbl&gt;                  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n 1                  2                  50                  12500      98       1\n 2                  0                  13                   3250      28       1\n 3                  1                  16                   4000      35       1\n 4                  2                  20                   5000      45       1\n 5                  1                  24                   6000      77       0\n 6                  4                   4                   1000       4       0\n 7                  2                   7                   1750      14       1\n 8                  1                  12                   3000      35       0\n 9                  2                   9                   2250      22       1\n10                  5                  46                  11500      98       1\n# … with 738 more rows, and abbreviated variable names\n#   ¹​`Monetary (c.c. blood)`, ²​`Time (months)`,\n#   ³​`whether he/she donated blood in March 2007`\n\n\n\nsummary(BD)\n\n Recency (months) Frequency (times) Monetary (c.c. blood) Time (months)  \n Min.   : 0.000   Min.   : 1.000    Min.   :  250         Min.   : 2.00  \n 1st Qu.: 2.750   1st Qu.: 2.000    1st Qu.:  500         1st Qu.:16.00  \n Median : 7.000   Median : 4.000    Median : 1000         Median :28.00  \n Mean   : 9.507   Mean   : 5.515    Mean   : 1379         Mean   :34.28  \n 3rd Qu.:14.000   3rd Qu.: 7.000    3rd Qu.: 1750         3rd Qu.:50.00  \n Max.   :74.000   Max.   :50.000    Max.   :12500         Max.   :98.00  \n whether he/she donated blood in March 2007\n Min.   :0.000                             \n 1st Qu.:0.000                             \n Median :0.000                             \n Mean   :0.238                             \n 3rd Qu.:0.000                             \n Max.   :1.000"
  },
  {
    "objectID": "posts/603_Final_Project_Check_In_2_Saisrinivas_Ambatipudi.html#data-description",
    "href": "posts/603_Final_Project_Check_In_2_Saisrinivas_Ambatipudi.html#data-description",
    "title": "603_Final_Project_Check_In_2_Saisrinivas_Ambatipudi",
    "section": "",
    "text": "The blood transfusion dataset contain 748 samples with 5 input features: Input Features: • Recency (number of months since the last donation) • Frequency (total number of donations) • Monetary (total blood donated in c.c.) • Time (number of months since the first donation) • Age (age of the donor)\nSource: https://www.kaggle.com/datasets/ninalabiba/blood-transfusion-dataset\nThis summary function presents a statistical description of a dataset related to blood donation, with five variables: Recency, Frequency, Monetary, Time, and whether the individual donated blood in March 2007. Here’s a breakdown of each variable:\nRecency (months): This variable represents the number of months since the last blood donation. The minimum value is 0 months, indicating that some individuals donated blood very recently. The mean is 9.507 months, suggesting that, on average, people donated blood around 9.5 months ago. The maximum value is 74 months, which means the longest gap between donations is 74 months.\nFrequency (times): This variable shows the total number of times an individual has donated blood. The minimum value is 1, meaning that at least one person has only donated blood once. The mean is 5.515 times, indicating that people, on average, have donated blood about 5.5 times. The maximum value is 50 times, showing that some individuals have donated blood quite frequently.\nMonetary (c.c. blood): This variable represents the total volume of blood donated by an individual, measured in cubic centimeters (c.c.). The minimum value is 250 c.c., which corresponds to the minimum single donation volume. The mean is 1379 c.c., suggesting that, on average, individuals have donated around 1.379 liters of blood. The maximum value is 12,500 c.c., indicating that the highest total volume donated by a person is 12.5 liters.\nTime (months): This variable measures the length of time an individual has been donating blood. The minimum value is 2 months, suggesting that some individuals are relatively new to blood donation. The mean is 34.28 months, indicating that, on average, people have been donating blood for about 34.3 months. The maximum value is 98 months, showing that some individuals have been donating blood for a long time.\nWhether he/she donated blood in March 2007: This is a binary variable that indicates if an individual donated blood in March 2007. The mean is 0.238, which means that about 23.8% of the individuals in the dataset donated blood in that specific month.\nThe summary function provides an overview of the dataset’s key statistics, such as minimum, 1st quartile, median, mean, 3rd quartile, and maximum values for each variable. This information helps to understand the distribution, central tendency, and spread of the data.\n\nBD &lt;- read_csv(\"C:/UMass/DACSS_603/603_Spring_2023/posts/_data/transfusion_saisrinivas.csv\")\n\nRows: 748 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (5): Recency (months), Frequency (times), Monetary (c.c. blood), Time (m...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndata(BD)\n\nWarning in data(BD): data set 'BD' not found\n\nBD\n\n# A tibble: 748 × 5\n   `Recency (months)` `Frequency (times)` Monetary (c.c. blood…¹ Time …² wheth…³\n                &lt;dbl&gt;               &lt;dbl&gt;                  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n 1                  2                  50                  12500      98       1\n 2                  0                  13                   3250      28       1\n 3                  1                  16                   4000      35       1\n 4                  2                  20                   5000      45       1\n 5                  1                  24                   6000      77       0\n 6                  4                   4                   1000       4       0\n 7                  2                   7                   1750      14       1\n 8                  1                  12                   3000      35       0\n 9                  2                   9                   2250      22       1\n10                  5                  46                  11500      98       1\n# … with 738 more rows, and abbreviated variable names\n#   ¹​`Monetary (c.c. blood)`, ²​`Time (months)`,\n#   ³​`whether he/she donated blood in March 2007`\n\n\n\nsummary(BD)\n\n Recency (months) Frequency (times) Monetary (c.c. blood) Time (months)  \n Min.   : 0.000   Min.   : 1.000    Min.   :  250         Min.   : 2.00  \n 1st Qu.: 2.750   1st Qu.: 2.000    1st Qu.:  500         1st Qu.:16.00  \n Median : 7.000   Median : 4.000    Median : 1000         Median :28.00  \n Mean   : 9.507   Mean   : 5.515    Mean   : 1379         Mean   :34.28  \n 3rd Qu.:14.000   3rd Qu.: 7.000    3rd Qu.: 1750         3rd Qu.:50.00  \n Max.   :74.000   Max.   :50.000    Max.   :12500         Max.   :98.00  \n whether he/she donated blood in March 2007\n Min.   :0.000                             \n 1st Qu.:0.000                             \n Median :0.000                             \n Mean   :0.238                             \n 3rd Qu.:0.000                             \n Max.   :1.000"
  },
  {
    "objectID": "posts/603_Final_Project_Check_In_2_Saisrinivas_Ambatipudi.html#eda",
    "href": "posts/603_Final_Project_Check_In_2_Saisrinivas_Ambatipudi.html#eda",
    "title": "603_Final_Project_Check_In_2_Saisrinivas_Ambatipudi",
    "section": "EDA:",
    "text": "EDA:\nExploratory Data Analysis would be performed along with graphs, including the variables (Dependent and Independent Variables) in the Blood Transfusion Dataset (Recency (months), Frequency (times), Monetary (c.c. blood), Time (months), whether he/she donated blood in March 2007)"
  },
  {
    "objectID": "posts/603_Final_Project_Check_In_2_Saisrinivas_Ambatipudi.html#detailed-research-statement",
    "href": "posts/603_Final_Project_Check_In_2_Saisrinivas_Ambatipudi.html#detailed-research-statement",
    "title": "603_Final_Project_Check_In_2_Saisrinivas_Ambatipudi",
    "section": "Detailed Research Statement:",
    "text": "Detailed Research Statement:\n\nHypothesis 1: There is a significant difference in the past frequency of blood donations between donors who have donated in March 2007 and those who have not.\n\nConfounding variables recency of blood donations, Monetary Value of blood donations, Time (months) of blood donations\n\nHypothesis 2: There is a significant difference in the Recency of blood donations between donors who have donated in March 2007 and those who have not.\n\nConfounding variables: Past frequency of blood donations, Monetary Value of blood donations, Time (months) of blood donations\n\nHypothesis 3: There is a significant difference in the Monetary Value of blood donations between donors who have donated in March 2007 and those who have not.\n\nConfounding variables: Past frequency of blood donations, Recency of blood donations, Time (months) of blood donations\n\nHypothesis 4: There is a significant difference in the Time (months) of blood donations between donors who have donated in March 2007 and those who have not.\n\nConfounding variables: Past frequency of blood donations, Recency of blood donations, Monetary Value of blood donations\n\nHypothesis 5: Donors who have donated blood more frequently in the past (i.e. a higher average number of donations per month) are more likely to donate again in March 2007.\nHypothesis 6: Donors who have donated higher average Monetary (c.c. blood) (per month) are more likely to donate again in March 2007.\nHypothesis 7: Donors who have donated blood more recently (Recency (months)) and with higher frequency (Frequency (times)) are more likely to donate again in March 2007.\nHypothesis 8: Donors who have donated blood more recently (Recency (months)) and with higher blood donation in past (Monetary (c.c. blood)) are more likely to donate again in March 2007.\nHypothesis 9: Donors who have donated blood more recently (Recency (months)) and with Time (months) are more likely to donate again in March 2007."
  },
  {
    "objectID": "posts/603_Final_Project_Check_In_2_Saisrinivas_Ambatipudi.html#demographic-data",
    "href": "posts/603_Final_Project_Check_In_2_Saisrinivas_Ambatipudi.html#demographic-data",
    "title": "603_Final_Project_Check_In_2_Saisrinivas_Ambatipudi",
    "section": "Demographic Data:",
    "text": "Demographic Data:\nWe have considered removing demographic data from the scope, as we couldn’t find any relevant data resources on internet. So confounding variables are limited to the variable available in the blood transfusion dataset."
  },
  {
    "objectID": "posts/603_Final_Project_Check_In_2_Saisrinivas_Ambatipudi.html#graphs",
    "href": "posts/603_Final_Project_Check_In_2_Saisrinivas_Ambatipudi.html#graphs",
    "title": "603_Final_Project_Check_In_2_Saisrinivas_Ambatipudi",
    "section": "Graphs:",
    "text": "Graphs:\nWherever necessary bar graph, scatter plot, correlation, regression, logistic regression graphs would be included for the hypothesis mentioned above."
  },
  {
    "objectID": "posts/603_Final_Project_Check_In_2_Saisrinivas_Ambatipudi.html#building-a-donor-retention-strategy",
    "href": "posts/603_Final_Project_Check_In_2_Saisrinivas_Ambatipudi.html#building-a-donor-retention-strategy",
    "title": "603_Final_Project_Check_In_2_Saisrinivas_Ambatipudi",
    "section": "Building a Donor Retention Strategy:",
    "text": "Building a Donor Retention Strategy:\nUse the insights gained to develop a donor retention strategy for the blood donation center. Identify factors that are associated with donor churn (i.e., donors who stop donating blood), and develop a plan to mitigate these factors. This can help healthcare and blood donation organizations to develop strategies to retain donors over the long term."
  },
  {
    "objectID": "posts/603_Final_Project_Check_In_2_Saisrinivas_Ambatipudi.html#expected-contribution",
    "href": "posts/603_Final_Project_Check_In_2_Saisrinivas_Ambatipudi.html#expected-contribution",
    "title": "603_Final_Project_Check_In_2_Saisrinivas_Ambatipudi",
    "section": "Expected Contribution:",
    "text": "Expected Contribution:\nSai Srinivas: Exploratory Analysis, Research Question 1-4\nAkhilesh Kumar: and Research Question 6-9, Donor Retention Strategy"
  },
  {
    "objectID": "posts/603_Final_Project_Check_In_2_Saisrinivas_Ambatipudi.html#references",
    "href": "posts/603_Final_Project_Check_In_2_Saisrinivas_Ambatipudi.html#references",
    "title": "603_Final_Project_Check_In_2_Saisrinivas_Ambatipudi",
    "section": "References:",
    "text": "References:\nKaggle: https://www.kaggle.com/datasets/ninalabiba/blood-transfusion-dataset\nOriginal Owner and Donor: Prof. I-Cheng Yeh, Department of Information Management, Chung-Hua University, Hsin Chu, Taiwan 30067, R.O.C., e-mail:icyeh ‘@’ chu.edu.tw, TEL:886-3-5186511, Date Donated: October 3, 2008"
  },
  {
    "objectID": "posts/Rowley_Homework_2.html",
    "href": "posts/Rowley_Homework_2.html",
    "title": "Homework 2",
    "section": "",
    "text": "Code\n# load libraries\n\nlibrary(dplyr)\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nCode\nlibrary(magrittr)\nlibrary(ggplot2)\nlibrary(markdown)\nlibrary(ggtext)\n\n\nWarning: package 'ggtext' was built under R version 4.2.2\n\n\nCode\nlibrary(readxl)\n\n\nWarning: package 'readxl' was built under R version 4.2.2\n\n\n\nQuestion 1\nThe time between the date a patient was recommended for heart surgery and the surgery date\nfor cardiac patients in Ontario was collected by the Cardiac Care Network (“Wait Times Data\nGuide,” Ministry of Health and Long-Term Care, Ontario, Canada, 2006). The sample mean\nand sample standard deviation for wait times (in days) of patients for two cardiac procedures\nare given in the accompanying table. Assume that the sample is representative of the Ontario\npopulation. Construct the 90% confidence interval to estimate the actual mean wait time for each of the two procedures. Is the confidence interval narrower for angiography or bypass surgery?\n\n\nCode\n# bypass: sample size = 539, mean wait time = 19, SD = 10\n\nb_size=539\nb_mean=19\nb_sd=10\nb_ci=0.9\n\n# calculate standard error:\nbypass_se = b_sd/sqrt(b_size)\n\n# specify confidence interval:\nbypass_tail &lt;- (1-b_ci)/2\nprint(bypass_tail)\n\n\n[1] 0.05\n\n\nCode\n# calculate t-value:\nb_t_value &lt;-  qt(p=1-bypass_tail, df=b_size-1)\nprint(b_t_value)\n\n\n[1] 1.647691\n\n\nCode\n# calculate confidence intervals:\nb_conf_int &lt;- c(b_mean-b_t_value*bypass_se, b_mean+b_t_value*bypass_se)\nprint(b_conf_int)\n\n\n[1] 18.29029 19.70971\n\n\nThe 90% confidence interval for wait-time for bypass surgery is 18.3–19.7 days.\n\n\nCode\n# angiography: sample size = 847, mean wait time = 18, SD = 9\n\na_size=847\na_mean=18\na_sd=9\na_ci=0.9\n\n# calculate standard error:\nangio_se = a_sd/sqrt(a_size)\n\n# specify confidence interval:\nangio_tail &lt;- (1-a_ci)/2\nprint(angio_tail)\n\n\n[1] 0.05\n\n\nCode\n# calculate t-value:\na_t_value &lt;-  qt(p=1-angio_tail, df=a_size-1)\nprint(a_t_value)\n\n\n[1] 1.646657\n\n\nCode\n# calculate confidence intervals:\na_conf_int &lt;- c(a_mean-a_t_value*angio_se, a_mean+a_t_value*angio_se)\nprint(a_conf_int)\n\n\n[1] 17.49078 18.50922\n\n\nThe 90% confidence interval for wait-time for angiography surgery is 17.5–18.5 days.\n\n\nCode\n# calculate size of confidence interval for bypass:\n\nbypass_ci_size=19.70971-18.29029\nprint(bypass_ci_size)\n\n\n[1] 1.41942\n\n\n\n\nCode\n# calculate size of confidence interval for angiography:\n\nangio_ci_size=18.50922-17.49078\nprint(angio_ci_size)\n\n\n[1] 1.01844\n\n\nThe confidence level is narrower for angiography surgery (1.01844) than it is for bypass surgery (1.41942).\n\n\nQuestion 2\nA survey of 1031 adult Americans was carried out by the National Center for Public\nPolicy. Assume that the sample is representative of adult Americans. Among those surveyed, 567\nbelieved that college education is essential for success. Find the point estimate, p, of the\nproportion of all adult Americans who believe that a college education is essential for success.\nConstruct and interpret a 95% confidence interval for p.\n\n\nCode\n# binomial test - compares a sample proportion to a hypothesized proportion\n\ntotal_pop = 1031\nsurvey_pop = 567\n\nbinom.test(survey_pop, total_pop)\n\n\n\n    Exact binomial test\n\ndata:  survey_pop and total_pop\nnumber of successes = 567, number of trials = 1031, p-value = 0.001478\nalternative hypothesis: true probability of success is not equal to 0.5\n95 percent confidence interval:\n 0.5189927 0.5806243\nsample estimates:\nprobability of success \n             0.5499515 \n\n\nThe proportion of American adults who believe that a college education is essential for success, or p, is 0.55%, which falls between the 95% confidence interval of 52%-58%.\n\n\nQuestion 3\nSuppose that the financial aid office of UMass Amherst seeks to estimate the mean cost\nof textbooks per semester for students. The estimate will be useful if it is within $5 of the true\npopulation mean (i.e. they want the confidence interval to have a length of $10 or less). The\nfinancial aid office is pretty sure that the amount spent on books varies widely, with most values\nbetween $30 and $200. They think that the population standard deviation is about a quarter of\nthis range (in other words, you can assume they know the population standard deviation). Assuming the significance level to be 5%, what should be the size of the sample?\n\n\nCode\n# find sample size using [error=z*SD/sqrt of n]\n\n# range in cost of books:\nrange=200-30\n\n# population SD is 1/4 of range:\npopulation_sd = range/4\nprint(population_sd)\n\n\n[1] 42.5\n\n\nCode\n# If 95% of the area lies between −z and z, then 5% of the area must lie outside of this range. Since normal curves are symmetric, half of this amount (2.5%) must lie before −z. Then the area under the curve before z must be: 0.025+0.95=0.975. The number z is the 97.5th percentile of the standard normal distribution:\nz=qnorm(.975)\n\n# estimate is within $5 of true population mean:\nn=((z*population_sd)/5)^2\nprint(n)\n\n\n[1] 277.5454\n\n\nThe sample size should be 278.\n\n\nQuestion 4\nAccording to a union agreement, the mean income for all senior-level workers in a large service company equals $500 per week. A representative of a women’s group decides to analyze whether the mean income ‘u’ for female employees matches this norm. For a random sample of nine female employees, y=$410 and s=90.\nA. Test whether the mean income of female employees differs from $500 per week. Include assumptions, hypotheses, test statistics, and P-value. Interpret the result.\nB. Report the P-value for Ha: u&lt;500. Interpret.\nC. Report and interpret the P-value for Ha: u&gt;500. (Hint: the P-values for the two possible one-sided tests must sum to 1).\n\nA)\n\n\nCode\n# mean income = $500/week\n# mean income female: y=$410, s=90\n\n# A)\n# test statistics: describes how far your observed data is from the null hypothesis of no relationship between variables or no difference among sample groups (support or reject null)\n# formula: x_bar is sample mean, mu is hypothesized population mean, sd is sample standard deviation, and n is sample size.\n# formula: (sample mean-population mean)/(standard deviation/sqrt(sample size))\n\ntest_stat &lt;- function(x_bar, mu, sd, n){return((x_bar-mu)/(sd/sqrt(n)))}\n\nsample_mean=410\npop_mean=500\nsd=90\nsample_size=9\n\n# find t-value:\nt_stat &lt;- test_stat(sample_mean, pop_mean, sd, sample_size)\nprint(t_stat)\n\n\n[1] -3\n\n\n\n\nCode\n# t-value is negative, so find lower tail\n# degree of freedom = n-1\n# find p-value:\nlow_p_value &lt;- pt(q=t_stat, df=8, lower.tail=TRUE)\nprint(low_p_value)\n\n\n[1] 0.008535841\n\n\nCode\n# find p-value for two-tailed t-test:\nlow_p_value &lt;- 2*pt(t_stat, 8)\nprint(low_p_value)\n\n\n[1] 0.01707168\n\n\nMy assumption is that the mean income of female employees will differ from the mean of all senior-level workers ($500/week). After running test statistics, we see that the t-stat is -3, meaning that females’ average pay is 3 standard deviations from the population mean. We also see that the p-value is 0.02, which means that we can reject the null hypothesis, which assumes that there is no significant difference in pay across genders (i.e., females make $500/week on average).\n\n\nB)\n\n\nCode\n# B\n# calculate p-value for LT alternative hypothesis (Ha: u&lt;500):\n\np_Ha = pt(q=t_stat, df=8, lower.tail=TRUE)\np_Ha\n\n\n[1] 0.008535841\n\n\nThe p-value for the lower-tail alternative hypothesis is 0.009, which supports our previous assertion that we can reject the null hypothesis and accept the alternative. This indicates that u≠500. In other words, females do not make $500/week.\n\n\nC)\n\n\nCode\n# C\n# calculate p-value for UT alternative hypothesis 2 (Ha: u&gt;500):\n\np_Ha = pt(q=t_stat, df=8, lower.tail=FALSE)\np_Ha\n\n\n[1] 0.9914642\n\n\nThe p-value for the upper-tail alternative hypothesis is 0.991, which indicates that ‘u’ is not greater than 500. In other words, we now know that, on average, females make less than $500/week.\n\n\n\nQuestion 5\nJones and Smith separately conduct studies to test H0: μ = 500 against Ha: μ ≠ 500, each with n = 1000. Jones gets ȳ = 519.5, with se = 10.0. Smith gets ȳ = 519.7, with se = 10.0.\nA. Show that t = 1.95 and P-value = 0.051 for Jones. Show that t = 1.97 and P-value = 0.049 for Smith.\nB. Using α = 0.05, for each study indicate whether the result is “statistically significant.”\nC. Using this example, explain the misleading aspects of reporting the result of a test as “P ≤0.05” versus “P &gt; 0.05,” or as “reject H0” versus “Do not reject H0,” without reporting the actual P-value.\n\nA)\n\n\nCode\n# H0: u=500\n# Ha: u≠500\n# n=1000\n# Jones: y=519.5, se=10.0\n# Smith : y=519.7, se=10.0\n\n# Jones: t=1.95\n\nt_jones = ((519.5 - 500)/ 10)\ncat(\"t value for Jones:\", t_jones, '\\n')\n\n\nt value for Jones: 1.95 \n\n\n\n\nCode\n# Jones: p-value=0.0515\n\ncat('p value for Jones:', round(2*pt(t_jones, df = 999, lower.tail=FALSE), 4), '\\n')\n\n\np value for Jones: 0.0515 \n\n\n\n\nCode\n# Smith: t=1.97\n\nt_smith = ((519.7 - 500)/ 10)\ncat(\"t value for Smith:\", t_smith, '\\n')\n\n\nt value for Smith: 1.97 \n\n\n\n\nCode\n# Smith: p-value=0.049\n\ncat('p value for Smith:', round(2*pt(t_smith, df = 999, lower.tail=FALSE), 4), '\\n')\n\n\np value for Smith: 0.0491 \n\n\n\n\nB)\nAssuming that 0.05 indicates statistical significance, we can see that while Smith’s study shows statistical significance with a p-value of 0.0491 (&lt;0.05), Jones’s does not (p-value=0.0515, p-value&gt;0.05).\n\n\nC)\nBased on this example, we can see why it is misleading to report a p-value as significant solely using “p ≤0.05,” “reject H0,” or “cannot reject H0” as indicators. After calculating each study’s respective p-value, we can see that the two values are very close, and, if we were to round each to the nearest hundredth, both would be equal to 0.05. So, it is important to provide the p-values themselves so as to make the distinction between degrees of significance. In other words, although Jones’s study showed statistical significance and Smith’s did not, the differences in the two p-values was marginal, so it is important to indicate by how slim a margin both were greater/less than the significance level (0.05) so as to not overestimate statistical significance.\n\n\n\nQuestion 6\nA school nurse wants to determine whether age is a factor in whether children choose a\nhealthy snack after school. She conducts a survey of 300 middle school students, with the results\nbelow. Test at α = 0.05 the claim that the proportion who choose a healthy snack differs by grade\nlevel. What is the null hypothesis? Which test should we use? What is the conclusion?\n\n\nCode\n# 6th: healthy = 31, unhealthy = 69\n# 7th: healthy = 43, unhealthy = 57\n# 8th: healthy = 51, unhealthy = 49\n# n=300, each grade has 100 survey participants\n\n# create dataframe:\ngrade &lt;- c(rep(\"6th\", 100), rep(\"7th\", 100), rep(\"8th\", 100))\nsnack &lt;- c(rep(\"healthy\", 31), rep(\"unhealthy\", 69), rep(\"healthy\", 43),\n           rep(\"unhealthy\", 57), rep(\"healthy\", 51), rep(\"unhealthy\", 49))\n\nsurvey_data &lt;- data.frame(grade, snack)\nhead(survey_data)\n\n\n  grade   snack\n1   6th healthy\n2   6th healthy\n3   6th healthy\n4   6th healthy\n5   6th healthy\n6   6th healthy\n\n\nCode\n# transform dataframe into table:\ntable(survey_data$snack,survey_data$grade)\n\n\n           \n            6th 7th 8th\n  healthy    31  43  51\n  unhealthy  69  57  49\n\n\nCode\n# conduct chi-squared test:\nchisq.test(survey_data$snack,survey_data$grade,correct = FALSE)\n\n\n\n    Pearson's Chi-squared test\n\ndata:  survey_data$snack and survey_data$grade\nX-squared = 8.3383, df = 2, p-value = 0.01547\n\n\nThe null hypothesis is that age is not a factor in whether children choose a healthy snack after school. To test this hypothesis (two categorical variables), we should use Pearson’s Chi-squared test. After running this test, we can see that the p-value is 0.015, which indicates statistical significance. Thus, we can reject the null hypothesis and conclude that age is a factor in whether children choose a healthy snack after school.\n\n\nQuestion 7\nPer-pupil costs (in thousands of dollars) for cyber charter school tuition for school\ndistricts in three areas are shown. Test the claim that there is a difference in means for the three\nareas, using an appropriate test. What is the null hypothesis? Which test should we use? What is\nthe conclusion?\n\n\nCode\n# area 1: 6.2, 9.3, 6.8, 6.1, 6.7, 7.5\n# area 2: 7.5, 8.2, 8.5, 8.2, 7.0, 9.3\n# area 3: 5.8, 6.4, 5.6, 7.1, 3.0, 3.5\n\n# create dataframe:\narea &lt;- c(rep(\"area_1\", 6), rep(\"area_2\", 6), rep(\"area_3\", 6))\ncost &lt;- c(6.2, 9.3, 6.8, 6.1, 6.7, 7.5, 7.5, 8.2, 8.5, 8.2, 7.0, 9.3,\n          5.8, 6.4, 5.6, 7.1, 3.0, 3.5)\narea_cost &lt;- data.frame(area,cost)\nhead(area_cost)\n\n\n    area cost\n1 area_1  6.2\n2 area_1  9.3\n3 area_1  6.8\n4 area_1  6.1\n5 area_1  6.7\n6 area_1  7.5\n\n\nCode\n# one-way ANOVA test:\none.way &lt;- aov(cost ~ area, data = area_cost)\nsummary(one.way)\n\n\n            Df Sum Sq Mean Sq F value  Pr(&gt;F)   \narea         2  25.66  12.832   8.176 0.00397 **\nResiduals   15  23.54   1.569                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe null hypothesis is that area has an effect on cost per pupil. Because we are testing the means of more than one group with one independent variable, we should use the one-way ANOVA test. After running the ANOVA test, we can see that the p-value (0.00397) is statistically significant. Thus, we can reject the null hypothesis and conclude that area does have an effect on cost per pupil."
  },
  {
    "objectID": "posts/HW3_Guanhua_Tan.html",
    "href": "posts/HW3_Guanhua_Tan.html",
    "title": "Homework 3",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.0      ✔ purrr   0.3.5 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.4.1 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\nCode\nlibrary(smss)\nlibrary(alr4)\n\n\nLoading required package: car\nLoading required package: carData\n\nAttaching package: 'car'\n\nThe following object is masked from 'package:dplyr':\n\n    recode\n\nThe following object is masked from 'package:purrr':\n\n    some\n\nLoading required package: effects\nlattice theme set by effectsTheme()\nSee ?effectsTheme for details.\n\n\nCode\ndata(UN11)\n\n\n\nQuestion 1\n\n\nCode\nglimpse(UN11)\n\n\nRows: 199\nColumns: 6\n$ region    &lt;fct&gt; Asia, Europe, Africa, Africa, Caribbean, Latin Amer, Asia, C…\n$ group     &lt;fct&gt; other, other, africa, africa, other, other, other, other, oe…\n$ fertility &lt;dbl&gt; 5.968, 1.525, 2.142, 5.135, 2.000, 2.172, 1.735, 1.671, 1.94…\n$ ppgdp     &lt;dbl&gt; 499.0, 3677.2, 4473.0, 4321.9, 13750.1, 9162.1, 3030.7, 2285…\n$ lifeExpF  &lt;dbl&gt; 49.49, 80.40, 75.00, 53.17, 81.10, 79.89, 77.33, 77.75, 84.2…\n$ pctUrban  &lt;dbl&gt; 23, 53, 67, 59, 100, 93, 64, 47, 89, 68, 52, 84, 89, 29, 45,…\n\n\nCode\nsummary(UN11)\n\n\n        region      group       fertility         ppgdp         \n Africa    :53   oecd  : 31   Min.   :1.134   Min.   :   114.8  \n Asia      :50   other :115   1st Qu.:1.754   1st Qu.:  1283.0  \n Europe    :39   africa: 53   Median :2.262   Median :  4684.5  \n Latin Amer:20                Mean   :2.761   Mean   : 13012.0  \n Caribbean :17                3rd Qu.:3.545   3rd Qu.: 15520.5  \n Oceania   :17                Max.   :6.925   Max.   :105095.4  \n (Other)   : 3                                                  \n    lifeExpF        pctUrban     \n Min.   :48.11   Min.   : 11.00  \n 1st Qu.:65.66   1st Qu.: 39.00  \n Median :75.89   Median : 59.00  \n Mean   :72.29   Mean   : 57.93  \n 3rd Qu.:79.58   3rd Qu.: 75.00  \n Max.   :87.12   Max.   :100.00  \n                                 \n\n\n\nthe variable ppgdp is the predictor and the variable fertility is the response.\n\n\n\n\nCode\n# scatterplot\nggplot(UN11, aes(x=ppgdp, y=fertility))+\n  geom_point()+\n  geom_smooth(method = 'lm')\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nWhen ppgdp is lower than 25000, fertility surges. when ppgdp is greater than 25000, fertility maintains stable. I don’t believe a stright-line mean function would be plausible for a summary of this graph.\n\n\n\n\n\nCode\n# scatterplot log(data)\nggplot(UN11, aes(x=log(ppgdp), y=fertility))+\n  geom_point()+\n  geom_smooth(method = 'lm')\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nYes, the simple line regression model seem plausible for a summary of this graphic.\n\n\nQuestion 2\n\n\nCode\nggplot(UN11, aes(x=log(ppgdp), y=fertility))+\n  geom_point()+\n  geom_smooth(method=\"lm\")\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nCode\ncor(UN11$ppgdp,UN11$fertility)\n\n\n[1] -0.4399891\n\n\n\n\nCode\nUN11$income.pound=UN11$ppgdp*1.33\nggplot(UN11, aes(x=log(income.pound), y=fertility))+\n  geom_point()+\n  geom_smooth(method = 'lm')\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nCode\ncor.test(UN11$income.pound, UN11$fertility)\n\n\n\n    Pearson's product-moment correlation\n\ndata:  UN11$income.pound and UN11$fertility\nt = -6.877, df = 197, p-value = 7.903e-11\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.5456842 -0.3205140\nsample estimates:\n       cor \n-0.4399891 \n\n\n\nthe slopes of the prediction equation maintain the same.\nthe correlation doesn’t change.\n\n\n\nQuestion 3\n\n\nCode\ndata(\"water\")\nsummary(water)\n\n\n      Year          APMAM            APSAB           APSLAKE     \n Min.   :1948   Min.   : 2.700   Min.   : 1.450   Min.   : 1.77  \n 1st Qu.:1958   1st Qu.: 4.975   1st Qu.: 3.390   1st Qu.: 3.36  \n Median :1969   Median : 7.080   Median : 4.460   Median : 4.62  \n Mean   :1969   Mean   : 7.323   Mean   : 4.652   Mean   : 4.93  \n 3rd Qu.:1980   3rd Qu.: 9.115   3rd Qu.: 5.685   3rd Qu.: 5.83  \n Max.   :1990   Max.   :18.080   Max.   :11.960   Max.   :13.02  \n     OPBPC             OPRC           OPSLAKE           BSAAM       \n Min.   : 4.050   Min.   : 4.350   Min.   : 4.600   Min.   : 41785  \n 1st Qu.: 7.975   1st Qu.: 7.875   1st Qu.: 8.705   1st Qu.: 59857  \n Median : 9.550   Median :11.110   Median :12.140   Median : 69177  \n Mean   :12.836   Mean   :12.002   Mean   :13.522   Mean   : 77756  \n 3rd Qu.:16.545   3rd Qu.:14.975   3rd Qu.:16.920   3rd Qu.: 92206  \n Max.   :43.370   Max.   :24.850   Max.   :33.070   Max.   :146345  \n\n\nCode\npairs(~APMAM+APSAB+APSLAKE+OPBPC+OPRC+OPSLAKE+BSAAM, data=water)\n\n\n\n\n\nThe sctterplot martrix clearly demonstrates that there are positive correlations between any two sites.\n\n\nQuestion 4\n\n\nCode\ndata(\"Rateprof\")\nsummary(Rateprof)\n\n\n    gender       numYears        numRaters       numCourses     pepper   \n female:159   Min.   : 1.000   Min.   :10.00   Min.   : 1.000   no :320  \n male  :207   1st Qu.: 6.000   1st Qu.:15.00   1st Qu.: 3.000   yes: 46  \n              Median :10.000   Median :24.00   Median : 4.000            \n              Mean   : 8.347   Mean   :28.58   Mean   : 4.251            \n              3rd Qu.:11.000   3rd Qu.:37.00   3rd Qu.: 5.000            \n              Max.   :11.000   Max.   :86.00   Max.   :12.000            \n                                                                         \n    discipline          dept        quality       helpfulness   \n Hum     :134   English   : 49   Min.   :1.409   Min.   :1.364  \n SocSci  : 66   Math      : 34   1st Qu.:2.936   1st Qu.:3.069  \n STEM    :103   Biology   : 20   Median :3.612   Median :3.662  \n Pre-prof: 63   Chemistry : 20   Mean   :3.575   Mean   :3.631  \n                Psychology: 20   3rd Qu.:4.250   3rd Qu.:4.351  \n                Spanish   : 20   Max.   :4.981   Max.   :5.000  \n                (Other)   :203                                  \n    clarity         easiness     raterInterest     sdQuality      \n Min.   :1.333   Min.   :1.391   Min.   :1.098   Min.   :0.09623  \n 1st Qu.:2.871   1st Qu.:2.548   1st Qu.:2.934   1st Qu.:0.87508  \n Median :3.600   Median :3.148   Median :3.305   Median :1.15037  \n Mean   :3.525   Mean   :3.135   Mean   :3.310   Mean   :1.05610  \n 3rd Qu.:4.214   3rd Qu.:3.692   3rd Qu.:3.692   3rd Qu.:1.28730  \n Max.   :5.000   Max.   :4.900   Max.   :4.909   Max.   :1.67739  \n                                                                  \n sdHelpfulness      sdClarity        sdEasiness     sdRaterInterest \n Min.   :0.0000   Min.   :0.0000   Min.   :0.3162   Min.   :0.3015  \n 1st Qu.:0.9902   1st Qu.:0.9085   1st Qu.:0.9045   1st Qu.:1.0848  \n Median :1.2860   Median :1.1712   Median :1.0247   Median :1.2167  \n Mean   :1.1719   Mean   :1.0970   Mean   :1.0196   Mean   :1.1965  \n 3rd Qu.:1.4365   3rd Qu.:1.3328   3rd Qu.:1.1485   3rd Qu.:1.3326  \n Max.   :1.8091   Max.   :1.8091   Max.   :1.6293   Max.   :1.7246  \n                                                                    \n\n\nCode\npairs(~quality+helpfulness+clarity+easiness+raterInterest, data=Rateprof)\n\n\n\n\n\nThe sccatterplot matrix suggests that there are positive correlations between quality and helpfulness, helpfulness and clarity, quality and clarity. It also indicates that there is no strong correlation between the rest of them.\n\n\nQuestion 5\n\n\nCode\ndata(\"student.survey\")\nsummary(student.survey)\n\n\n      subj       ge           ag              hi              co       \n Min.   : 1.00   f:31   Min.   :22.00   Min.   :2.000   Min.   :2.600  \n 1st Qu.:15.75   m:29   1st Qu.:24.00   1st Qu.:3.000   1st Qu.:3.175  \n Median :30.50          Median :26.50   Median :3.350   Median :3.500  \n Mean   :30.50          Mean   :29.17   Mean   :3.308   Mean   :3.453  \n 3rd Qu.:45.25          3rd Qu.:31.00   3rd Qu.:3.625   3rd Qu.:3.725  \n Max.   :60.00          Max.   :71.00   Max.   :4.000   Max.   :4.000  \n                                                                       \n       dh             dr               tv               sp        \n Min.   :   0   Min.   : 0.200   Min.   : 0.000   Min.   : 0.000  \n 1st Qu.: 205   1st Qu.: 1.450   1st Qu.: 3.000   1st Qu.: 3.000  \n Median : 640   Median : 2.000   Median : 6.000   Median : 5.000  \n Mean   :1232   Mean   : 3.818   Mean   : 7.267   Mean   : 5.483  \n 3rd Qu.:1350   3rd Qu.: 5.000   3rd Qu.:10.000   3rd Qu.: 7.000  \n Max.   :8000   Max.   :20.000   Max.   :37.000   Max.   :16.000  \n                                                                  \n       ne               ah             ve          pa    \n Min.   : 0.000   Min.   : 0.000   Mode :logical   d:21  \n 1st Qu.: 2.000   1st Qu.: 0.000   FALSE:60        i:24  \n Median : 3.000   Median : 0.500                   r:15  \n Mean   : 4.083   Mean   : 1.433                         \n 3rd Qu.: 5.250   3rd Qu.: 2.000                         \n Max.   :14.000   Max.   :11.000                         \n                                                         \n                     pi                re         ab              aa         \n very liberal         : 8   never       :15   Mode :logical   Mode :logical  \n liberal              :24   occasionally:29   FALSE:60        FALSE:59       \n slightly liberal     : 6   most weeks  : 7                   NA's :1        \n moderate             :10   every week  : 9                                  \n slightly conservative: 6                                                    \n conservative         : 4                                                    \n very conservative    : 2                                                    \n     ld         \n Mode :logical  \n FALSE:44       \n NA's :16       \n                \n                \n                \n                \n\n\nCode\n# (i)\nlm_ideology_religiosity &lt;-lm(as.numeric(pi)~as.numeric(re), data=student.survey)\nsummary(lm_ideology_religiosity)\n\n\n\nCall:\nlm(formula = as.numeric(pi) ~ as.numeric(re), data = student.survey)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.81243 -0.87160  0.09882  1.12840  3.09882 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      0.9308     0.4252   2.189   0.0327 *  \nas.numeric(re)   0.9704     0.1792   5.416 1.22e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.345 on 58 degrees of freedom\nMultiple R-squared:  0.3359,    Adjusted R-squared:  0.3244 \nF-statistic: 29.34 on 1 and 58 DF,  p-value: 1.221e-06\n\n\n\n\nCode\n# ii\nlm_hi_tv &lt;- lm(hi~tv, data=student.survey)\nsummary(lm_hi_tv)\n\n\n\nCall:\nlm(formula = hi ~ tv, data = student.survey)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.2583 -0.2456  0.0417  0.3368  0.7051 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  3.441353   0.085345  40.323   &lt;2e-16 ***\ntv          -0.018305   0.008658  -2.114   0.0388 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4467 on 58 degrees of freedom\nMultiple R-squared:  0.07156,   Adjusted R-squared:  0.05555 \nF-statistic: 4.471 on 1 and 58 DF,  p-value: 0.03879\n\n\n\n\nCode\nggplot(student.survey, aes(x=re, fill=pi))+\n  geom_bar(position=\"fill\")\n\n\n\n\n\nCode\nggplot(student.survey, aes(x=tv, y=log(hi)))+\n   geom_smooth(method=\"lm\")\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nFor the first regression analysis, the people who are very conservative come to church every week. By contrast, the people who are liberal or very liberal come to church rarely. It indicates the close relationship between political ideology and religion.\nFor the second regression analysis, the graphic demonstrates that students who spent more time watching TV achieve lower gpa. In other words, there is a negative association between gpa and hours of watching TV."
  },
  {
    "objectID": "posts/HW2_solutions_Pang.html",
    "href": "posts/HW2_solutions_Pang.html",
    "title": "Homework 2",
    "section": "",
    "text": "Please check your answers against the solutions."
  },
  {
    "objectID": "posts/HW2_solutions_Pang.html#question-1",
    "href": "posts/HW2_solutions_Pang.html#question-1",
    "title": "Homework 2",
    "section": "Question 1",
    "text": "Question 1\n\nbypass_n = 539\nangio_n = 847\n\nbypass_sample_mean = 19\nangio_sample_mean = 18\n\nbypass_sample_sd = 10\nangio_sample_sd = 9\n\nbypass_se = bypass_sample_sd/sqrt(bypass_n)\nangio_se = angio_sample_sd/sqrt(angio_n)\n\nbypass_me = qt(0.95, df = bypass_n - 1)*bypass_se\nangio_me = qt(0.95, df = angio_n - 1)*angio_se\n\nThe confidence intervals:\n\nprint(bypass_sample_mean + c(-bypass_me, bypass_me))\n\n[1] 18.29029 19.70971\n\nprint(angio_sample_mean + c(-angio_me, angio_me))\n\n[1] 17.49078 18.50922\n\n\nThe size of the confidence intervals, which is twice the margin of error:\n\n2 * bypass_me\n\n[1] 1.419421\n\n2 * angio_me\n\n[1] 1.018436\n\n\nThe confidence interval for angiography is narrower."
  },
  {
    "objectID": "posts/HW2_solutions_Pang.html#question-2",
    "href": "posts/HW2_solutions_Pang.html#question-2",
    "title": "Homework 2",
    "section": "Question 2",
    "text": "Question 2\none-step solution:\n\nn = 1031\nk = 567\nprop.test(k, n)\n\n\n    1-sample proportions test with continuity correction\n\ndata:  k out of n, null probability 0.5\nX-squared = 10.091, df = 1, p-value = 0.00149\nalternative hypothesis: true p is not equal to 0.5\n95 percent confidence interval:\n 0.5189682 0.5805580\nsample estimates:\n        p \n0.5499515 \n\n\nAlternatively:\n\np_hat &lt;- k/n # point estimate\nse = sqrt((p_hat*(1-p_hat))/n) # standard error\ne = qnorm(0.975)*se # margin of error\np_hat + c(-e, e) # confidence interval \n\n[1] 0.5195839 0.5803191\n\n\nAlternatively, we can use the exact binomial test. In large samples like the one we have, the results should essentially be the same as prop.test().\n\nbinom.test(k, n)\n\n\n    Exact binomial test\n\ndata:  k and n\nnumber of successes = 567, number of trials = 1031, p-value = 0.001478\nalternative hypothesis: true probability of success is not equal to 0.5\n95 percent confidence interval:\n 0.5189927 0.5806243\nsample estimates:\nprobability of success \n             0.5499515"
  },
  {
    "objectID": "posts/HW2_solutions_Pang.html#question-3",
    "href": "posts/HW2_solutions_Pang.html#question-3",
    "title": "Homework 2",
    "section": "Question 3",
    "text": "Question 3\n\nrange = 200-30\npopulation_sd = range/4\n\nRemember:\n\\[CI_{95} = \\bar x \\pm z \\frac{s}{\\sqrt n}\\] (We can use \\(z\\) because we assume population standard deviation is known.)\nWe want the number \\(n\\) that ensures:\n\\[ z \\frac{s}{\\sqrt n} = 5 \\] \\[ zs = 5 \\sqrt n\\] \\[ \\frac{zs}{5} = \\sqrt n\\] \\[  (\\frac{zs}{5})^2 = n\\]\nIn our case:\n\nz = qnorm(.975)\ns = population_sd\nn = ((z *s) / 5)^2\nprint(n)\n\n[1] 277.5454\n\n\nRounding up, we need a sample of 278."
  },
  {
    "objectID": "posts/HW2_solutions_Pang.html#question-4",
    "href": "posts/HW2_solutions_Pang.html#question-4",
    "title": "Homework 2",
    "section": "Question 4",
    "text": "Question 4\nWe can write a function to find the t-statistic, and then do all the tests in a, b, and c using that.\n\\[t = \\frac{\\bar x - \\mu}{s / \\sqrt n}\\]\nwhere \\(\\bar x\\) is them sample mean, \\(\\mu\\) is the hypothesizes population mean, \\(s\\) is the sample standard deviation, and \\(n\\) is the sample size.\nWriting this in R:\n\nget_t_stat &lt;- function(x_bar, mu, sd, n){\n  return((x_bar - mu) / (sd / sqrt(n)))\n}\n\nFind the t-statistic:\n\nt_stat &lt;- get_t_stat(x_bar = 410, mu = 500, sd = 90, n = 9)\n\n\nA\nTwo-tailed test\n\nn = 9\npval_two_tail = 2*pt(t_stat, df = n-1)\npval_two_tail\n\n[1] 0.01707168\n\n\nWe can reject the hypothesis that population mean is 500.\n\n\nB\n\npval_lower_tail = pt(t_stat, df = n-1)\npval_lower_tail\n\n[1] 0.008535841\n\n\nWe can reject the hypothesis that population mean is greater than 500.\n\n\nC\n\npval_upper_tail = pt(t_stat, df = n-1, lower.tail=FALSE)\npval_upper_tail\n\n[1] 0.9914642\n\n\nWe fail to reject the hypothesis that population mean is less than 500.\nAlternatively for C, we could just subtract the answer in B from 1:\n\n1 - pval_lower_tail\n\n[1] 0.9914642"
  },
  {
    "objectID": "posts/HW2_solutions_Pang.html#question-5",
    "href": "posts/HW2_solutions_Pang.html#question-5",
    "title": "Homework 2",
    "section": "Question 5",
    "text": "Question 5\n\nt_jones = ((519.5 - 500)/ 10)\nt_smith = ((519.7 - 500)/ 10)\ncat(\"t value for Jones:\", t_jones, '\\n')\n\nt value for Jones: 1.95 \n\ncat(\"t value for Smith:\", t_smith, '\\n')\n\nt value for Smith: 1.97 \n\ncat('p value for Jones:', round(2*pt(t_jones, df = 999, lower.tail=FALSE), 4), '\\n')\n\np value for Jones: 0.0515 \n\ncat('p value for Smith:', round(2*pt(t_smith, df = 999, lower.tail=FALSE), 4), '\\n')\n\np value for Smith: 0.0491 \n\n\nAt 0.05 level Smith’s result is statistically significant but Jones’s is not. The result show the arbitrariness of the 0.05 demarcation line and the importance of reporting actual p-values to better make sense of results."
  },
  {
    "objectID": "posts/HW2_solutions_Pang.html#question-6",
    "href": "posts/HW2_solutions_Pang.html#question-6",
    "title": "Homework 2",
    "section": "Question 6:",
    "text": "Question 6:\n\n# Creating the dataframe\ngrade_level &lt;- c(rep(\"6th grade\", 100), rep(\"7th grade\", 100), rep(\"8th grade\", 100))\nsnack &lt;- c(rep(\"healthy snack\", 31), rep(\"unhealthy snack\", 69), rep(\"healthy snack\", 43),\n           rep(\"unhealthy snack\", 57), rep(\"healthy snack\", 51), rep(\"unhealthy snack\", 49))\nsnack_data &lt;- data.frame(grade_level, snack)\n\nWe are conducting a Chi-square test in this question since we are testing the association between two categorical variables.\n\ntable(snack_data$snack,snack_data$grade_level)\n\n                 \n                  6th grade 7th grade 8th grade\n  healthy snack          31        43        51\n  unhealthy snack        69        57        49\n\nchisq.test(snack_data$snack,snack_data$grade_level,correct = FALSE)\n\n\n    Pearson's Chi-squared test\n\ndata:  snack_data$snack and snack_data$grade_level\nX-squared = 8.3383, df = 2, p-value = 0.01547\n\n\nA p-value smaller than 0.05 indicates that there is a relationship between grade level and the choice of snack."
  },
  {
    "objectID": "posts/HW2_solutions_Pang.html#question-7",
    "href": "posts/HW2_solutions_Pang.html#question-7",
    "title": "Homework 2",
    "section": "Question 7:",
    "text": "Question 7:\n\n# Creating the dataframe\nArea &lt;- c(rep(\"Area1\", 6), rep(\"Area2\", 6), rep(\"Area3\", 6))\ncost &lt;- c(6.2, 9.3, 6.8, 6.1, 6.7, 7.5, 7.5, 8.2, 8.5, 8.2, 7.0, 9.3,\n          5.8, 6.4, 5.6, 7.1, 3.0, 3.5)\nArea_cost &lt;- data.frame(Area,cost)\n\nSince we are comparing the means of more than two groups, we are using the ANOVA test in this question.\n\none.way &lt;- aov(cost ~ Area, data = Area_cost)\nsummary(one.way)\n\n            Df Sum Sq Mean Sq F value  Pr(&gt;F)   \nArea         2  25.66  12.832   8.176 0.00397 **\nResiduals   15  23.54   1.569                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe small p-value tells us that the three areas have a difference in means."
  },
  {
    "objectID": "posts/Kristin_Abijaoude_HW4.html",
    "href": "posts/Kristin_Abijaoude_HW4.html",
    "title": "Kristin Abijaoude_HW4",
    "section": "",
    "text": "Question 1\nŷ = −10,536 + 53.8x1 + 2.84x2\ny = selling price of home (in dollars) x1 = size of home (in square feet) x2 = lot size (in square feet)\nA particular home of 1240 square feet on a lot of 18,000 square feet sold for $145,000. Find the predicted selling price and the residual, and interpret.\n\n\nCode\nprice = -10536 + (53.8 * 18000) + (2.84 * 1240)\nprint(price)\n\n\n[1] 961385.6"
  },
  {
    "objectID": "posts/abigailbalint_finalpart2v2.html",
    "href": "posts/abigailbalint_finalpart2v2.html",
    "title": "Final Project Part 2",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(dplyr)\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/abigailbalint_finalpart2v2.html#description-of-data",
    "href": "posts/abigailbalint_finalpart2v2.html#description-of-data",
    "title": "Final Project Part 2",
    "section": "Description of data",
    "text": "Description of data\nThe dataset I am using comes from Kaggle https://www.kaggle.com/datasets/kaggle/kaggle-survey-2018 and is a survey titled “2018 Kaggle Machine Learning & Data Science Survey” conducted by Kaggle to capture the current state of machine learning and data science usage, mainly at the enterprise and academic level. The dataset contains survey responses from almost 24,000 respondents from varying backgrounds. The survey contains 50 questions, including 9 demographic questions and 41 questions around machine learning and data science. The questions range from platforms and products used, and tools and methodology, barriers to entry, and more. It also asks respondents about their employee experience working in these fields. I believe that the wide array of types of questions used make this dataset a good fit for research, as there are binary and categorical variables to explore but also some that ask for explicit numeric values like what percentage of their work falls to different tasks. Having several different types of questions provide opportunities for multiple types of models to be performed.\nThis survey was also run in 2017, 2019, and 2020 on Kaggle as part of an annual competition where users could submit code and analysis using this public data. However, I decided to use the 2018 dataset as my focus because certain questions that I think would be really interesting to analyze were omitted in later years/the survey was shortened overall. This survey was hosted by Kaggle, open to anyone in the industry, for one week in October 2018.\nReading in the dataset –\n\n\nCode\nfinal &lt;- read_csv(\"_data/final_project_data.csv\")\n\n\nWarning: One or more parsing issues, see `problems()` for details\n\n\nRows: 23304 Columns: 409\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (347): Q1, Q2, Q3, Q4, Q5, Q6, Q7, Q7_RCSTUD, Q7_RCTECH, Q8, Q9, Q10, Q1...\ndbl  (58): Time from Start to Finish (seconds), Q1_OTHER_TEXT, Q6_OTHER_TEXT...\nlgl   (4): Q28_Part_22, Q29_Part_16, Q38_Part_19, Q38_Part_20\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nCode\nhead(final,10)\n\n\n# A tibble: 10 × 409\n   Time from…¹ Q1    Q1_OT…² Q2    Q3    Q4    Q5    Q6    Q6_OT…³ Q7    Q7_RC…⁴\n         &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;  \n 1         959 Male       -1 35-39 Chile Doct… Info… Other       1 Acad… Non-st…\n 2        1512 Male       -1 18-21 India Bach… Comp… Stud…      -1 Acad… Non-st…\n 3         848 Male       -1 22-24 Denm… Bach… Engi… Stud…      -1 Acad… Non-st…\n 4        2309 Male       -1 30-34 China Doct… Other Rese…      -1 Acad… Non-st…\n 5        1487 Male       -1 18-21 China Bach… Comp… Stud…      -1 Acad… Non-st…\n 6         790 Male       -1 25-29 China Mast… Engi… Stud…      -1 Acad… Non-st…\n 7        2986 Male       -1 40-44 India Doct… Comp… Other       7 Acad… Non-st…\n 8        1255 Male       -1 18-21 India Some… Engi… Stud…      -1 Acad… Non-st…\n 9         825 Male       -1 25-29 India Bach… Comp… Data…      -1 Acad… Non-st…\n10        1315 Male       -1 18-21 India Bach… Engi… Stud…      -1 Acad… Non-st…\n# … with 398 more variables: Q7_RCSTUDY &lt;dbl&gt;, Q7_RCSTUDN &lt;dbl&gt;,\n#   Q7_RCSTUD2 &lt;dbl&gt;, Q7_RCTECH &lt;chr&gt;, Q7_RCTECH2 &lt;dbl&gt;, Q7_OTHER_TEXT &lt;dbl&gt;,\n#   Q8 &lt;chr&gt;, Q8RC &lt;dbl&gt;, Q9 &lt;chr&gt;, Q10 &lt;chr&gt;, Q11_Part_1 &lt;chr&gt;,\n#   Q11_Part_1RC &lt;dbl&gt;, Q11_Part_1RC2 &lt;chr&gt;, Q11_Part_2 &lt;chr&gt;,\n#   Q11_Part_3 &lt;chr&gt;, Q11_Part_4 &lt;chr&gt;, Q11_Part_5 &lt;chr&gt;, Q11_Part_6 &lt;chr&gt;,\n#   Q11_Part_7 &lt;chr&gt;, Q11_OTHER_TEXT &lt;dbl&gt;, Q12_MULTIPLE_CHOICE &lt;chr&gt;,\n#   Q12_Part_1_TEXT &lt;dbl&gt;, Q12_Part_2_TEXT &lt;dbl&gt;, Q12_Part_3_TEXT &lt;dbl&gt;, …"
  },
  {
    "objectID": "posts/abigailbalint_finalpart2v2.html#research-question",
    "href": "posts/abigailbalint_finalpart2v2.html#research-question",
    "title": "Final Project Part 2",
    "section": "Research Question",
    "text": "Research Question\nUpon doing a cursory search around this data, I see some high level executive-summary style research published about this data set, but I wasn’t able to find anything focused on more specific research questions. It was more demographic data of the state of ML and Data Science. I think there is the opportunity to speak more specifically about the state of machine learning and data science, and look deeper at what tools students and employees are using versus what their time is devoted to.\nBased on feedback from my part 1 of the final, I am narrowing down my area of focus. There is a huge amount of data within this dataset, but as this research is focused on performing regression modeling, I think the best fit for my main research question is as follows:\nHow does an individual’s background, experience, and time in their field/role impact what their day to day looks like?\nI plan to use questions like “During a typical data science project at work or school, approximately what proportion of your time is devoted to the following?” to get exact numbers that I can correlate against demos and more general usage of tools and platforms to see if there is any connection between the work one does and the tools they use.\nI am interested in this dataset because a lot of research in my career is in the machine learning space, so I am always interested in contextualizing the employee experience in these areas so that I can better understand the subject of some of my survey research. I also do more general employee engagement research in my career and I think this final is a great opportunity to try my hand at some of the correlations I would like to run at my job now but have never been able to because I don’t have any prior stats knowledge."
  },
  {
    "objectID": "posts/abigailbalint_finalpart2v2.html#hypothesis",
    "href": "posts/abigailbalint_finalpart2v2.html#hypothesis",
    "title": "Final Project Part 2",
    "section": "Hypothesis",
    "text": "Hypothesis\nThe hypothesis I would like to test is:\nThose with more years of experience in machine learning/coding will spend a larger proportion of their time doing analyzing and decision making, as opposed to those newer to the job will spend more time cleaning data and doing actual coding.\nThe below variables are the ones I plan to consider using:\nIndependent variable: Q24 How long have you been writing code to analyze data?\nDependent variable: Q34_Part_6 During a typical data science project at work or school, approximately what proportion of your time is devoted to the following? (Answers must add up to 100%) - Finding insights in the data and communicating with stakeholders\nThe below variables could be used as confounding variables to the independent variable, or as alternative dependent variables to test that could give me opposite results to test essentially the second half of my hypothesis statement:\n\nConfounding variable 1 - Q8 How many years of experience do you have in your current role?\nConfounding variable 2 - Q25 For how many years have you used machine learning methods (at work or in school)?\nOther alternative dependent - Q11_1 Select any activities that make up an important part of your role at work: (Select all that apply) - Analyze and understand data to influence product or business decisions\nOther alternative dependent - Q23 Approximately what percent of your time at work or school is spent actively coding?\nOther alternative dependent - Q34_Part_2 During a typical data science project at work or school, approximately what proportion of your time is devoted to the following? - Cleaning data"
  },
  {
    "objectID": "posts/abigailbalint_finalpart2v2.html#descriptive-statistics",
    "href": "posts/abigailbalint_finalpart2v2.html#descriptive-statistics",
    "title": "Final Project Part 2",
    "section": "Descriptive Statistics",
    "text": "Descriptive Statistics\nI described my dataset at the top of this as well as discussed variables of interest in the Research Question section, but here is a little bit of exploratory code to give a general feel for what the data looks like:\nI can see the data contains mostly younger males, but because of the sample size can really work with lots of demographic combinations.\n\n\nCode\nggplot(final, aes(x = Q1)) +\n  geom_bar() +\n   labs(x=\"Gender\")\n\n\n\n\n\n\n\nCode\nggplot(final, aes(x = Q2)) +\n  geom_bar() +\n  labs(x=\"Age\") +\n  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust=1))\n\n\n\n\n\nThere is also a range of coding experience in the dataset.\n\n\nCode\nggplot(final, aes(x = Q24)) +\n  geom_bar() +\n   labs(x=\"Years of coding experience\") +\n  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust=1))\n\n\n\n\n\nThe data is split between students, tech industry employees, and other industry employees.\n\n\nCode\nggplot(final, aes(x = Q7)) +\n  geom_bar() +\n   labs(x=\"Industry\") +\n  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust=1))\n\n\n\n\n\nHere, I made a scatterplot showing where the amount of training from work meets the amount of time spent finding insights instead of cleaning data, coding, etc. I expected this to be much higher for those who received most or all of their training from work, but that isn’t the case.\n\n\nCode\nggplot(final, aes(x = Q35_Part_3, y=Q34_Part_6)) +\n  geom_point() +\n   labs(x=\"Percentage of machine learning training from work\", y=\"Percentage of project time spent finding insights\")\n\n\nWarning: Removed 7559 rows containing missing values (geom_point)."
  },
  {
    "objectID": "posts/AlexisGamez_HW1.html",
    "href": "posts/AlexisGamez_HW1.html",
    "title": "Blog Post #1",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(readxl)\n\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/AlexisGamez_HW1.html#a",
    "href": "posts/AlexisGamez_HW1.html#a",
    "title": "Blog Post #1",
    "section": "a)",
    "text": "a)\nFirst, let’s read in the data from the Excel file:\n\n\nCode\ngetwd()\n\n\n[1] \"C:/Users/Leshiii/Desktop/DACSS Master's/DACSS 603/603_Spring_2023/posts\"\n\n\nCode\ndf &lt;- read_excel(\"_data/LungCapData.xls\")\n\n\nThe distribution of LungCap looks as follows:\n\n\nCode\nhist(df$LungCap, main = \"Lung Capacity Distribution\", xlab = \"Lung Capacity\", ylab = \"Probability Density\", prob = TRUE)\n\n\n\n\n\nThe histogram suggests that the distribution is close to a normal distribution. Most of the observations are close to the mean. Very few observations are close to the margins (0 and 15)."
  },
  {
    "objectID": "posts/AlexisGamez_HW1.html#b",
    "href": "posts/AlexisGamez_HW1.html#b",
    "title": "Blog Post #1",
    "section": "b)",
    "text": "b)\nProvided below is a box plot of the probability distributions of the Lung Capacity data for the male and female genders.\n\n\nCode\nboxplot(df$LungCap ~ df$Gender, \n        ylab = \"Gender\",\n        xlab = \"Lung Capacity\",\n        horizontal = TRUE,\n        col = \"maroon\")"
  },
  {
    "objectID": "posts/AlexisGamez_HW1.html#c",
    "href": "posts/AlexisGamez_HW1.html#c",
    "title": "Blog Post #1",
    "section": "c)",
    "text": "c)\nI don’t believe the data provided below make much sense. I would argue that it is much more likely for smokers to have a smaller lung capacity than those that do not smoke. I suspect that something could be going on with our sample.\n\n\nCode\nboxplot(df$LungCap ~ df$Smoke, \n        ylab = \"Smoking Preference\",\n        xlab = \"Lung Capacity\",\n        horizontal = TRUE,\n        col = \"bisque\")\n\n\n\n\n\nCode\nno_smoke &lt;- subset(df, Smoke == \"no\")\nyes_smoke &lt;- subset(df, Smoke == \"yes\")\nmean(no_smoke$LungCap)\n\n\n[1] 7.770188\n\n\nCode\nmean(yes_smoke$LungCap)\n\n\n[1] 8.645455"
  },
  {
    "objectID": "posts/AlexisGamez_HW1.html#d",
    "href": "posts/AlexisGamez_HW1.html#d",
    "title": "Blog Post #1",
    "section": "d)",
    "text": "d)\nComparing the charts below, we can see that as the participant ages, the mean and ranges of Lung Capacity increases. This could be due to natural maturation, with Lungs growing as children grow to adolescents, stalling circa 18 years old. However, I suspect there might be more to it, particularly our sample of smokers being so small compared to that of non-smokers.\n\n\nCode\nno_smoke_mean &lt;- mean(no_smoke$LungCap)\n`13_under` &lt;- subset(yes_smoke, Age &lt;= 13)\n\n`14_to_15` &lt;- subset(yes_smoke, subset = Age == 14 | Age == 15)\n\n`16_to_17` &lt;- subset(yes_smoke, subset = Age == 16 | Age == 17)\n\n`18_over` &lt;- subset(yes_smoke, Age &gt;= 18)\n\nhist(`13_under`$LungCap, main = \"Smoker Lung Capacity Distribution by Age\", xlab = \"Lung Capacity\", ylab = \"Probability Density\", prob = TRUE)\nlegend(\"topright\", legend=c(\"13 & Under\",\"14 to 15\", \"16 to 17\", \"18 & Over\"), col=c(\"gray\", rgb(0,0,1,0.5), \n     rgb(0,1,0,0.5), rgb(1,0,0,0.5)), pt.cex=2, pch=15)\n\n\n\n\n\nCode\nhist(`14_to_15`$LungCap, prob = TRUE, col = rgb(0,0,1,0.5), main = \"Smoker Lung Capacity Distribution by Age\", xlab = \"Lung Capacity\", ylab = \"Probability Density\")\nlegend(\"topright\", legend=c(\"13 & Under\",\"14 to 15\", \"16 to 17\", \"18 & Over\"), col=c(\"gray\", rgb(0,0,1,0.5), \n     rgb(0,1,0,0.5), rgb(1,0,0,0.5)), pt.cex=2, pch=15)\n\n\n\n\n\nCode\nhist(`16_to_17`$LungCap, prob = TRUE, col = rgb(0,1,0,0.5), main = \"Smoker Lung Capacity Distribution by Age\", xlab = \"Lung Capacity\", ylab = \"Probability Density\")\nlegend(\"topright\", legend=c(\"13 & Under\",\"14 to 15\", \"16 to 17\", \"18 & Over\"), col=c(\"gray\", rgb(0,0,1,0.5), \n     rgb(0,1,0,0.5), rgb(1,0,0,0.5)), pt.cex=2, pch=15)\n\n\n\n\n\nCode\nhist(`18_over`$LungCap, prob = TRUE, col = rgb(1,0,0,0.5), main = \"Smoker Lung Capacity Distribution by Age\", xlab = \"Lung Capacity\", ylab = \"Probability Density\")\nlegend(\"topright\", legend=c(\"13 & Under\",\"14 to 15\", \"16 to 17\", \"18 & Over\"), col=c(\"gray\", rgb(0,0,1,0.5), \n     rgb(0,1,0,0.5), rgb(1,0,0,0.5)), pt.cex=2, pch=15)"
  },
  {
    "objectID": "posts/AlexisGamez_HW1.html#e",
    "href": "posts/AlexisGamez_HW1.html#e",
    "title": "Blog Post #1",
    "section": "e)",
    "text": "e)\n\n\nCode\ndf_Agegroup &lt;- df %&gt;% \n  mutate(\nAge_group = dplyr::case_when(\n      Age &lt;= 13            ~ \"0-13\",\n      Age &gt; 13 & Age &lt;= 15 ~ \"14-15\",\n      Age &gt; 15 & Age &lt;= 17 ~ \"16-17\",\n      Age &gt;= 18             ~ \"18+\"))\n\nggplot(data = df_Agegroup, aes(x=Age_group, y=LungCap)) + \n  geom_boxplot(aes(fill=Smoke)) +\n  labs(x=\"Age Group\",y=\"Lung Capacity\",title=\"Lung Capacity of Smokers vs Non Smokers by Age Group\")\n\n\n\n\n\nLooking at the box plot above, its seems as though the ranges of data for the latter 3 age group divisions are naturally larger for non-smokers than for smokers. Additionally, the means for said divisions under non-smokers are also higher! What seems to be skewing the data is the large quantity of participants equal to 13 years of age or younger. I believe this is why our data hasn’t made much sense until now. There might be other factors in play here, like respondent bias, but I believe the sample size here is the main influence."
  },
  {
    "objectID": "posts/AlexisGamez_HW1.html#a-1",
    "href": "posts/AlexisGamez_HW1.html#a-1",
    "title": "Blog Post #1",
    "section": "a)",
    "text": "a)\nReading in our data.\n\n\nCode\nConvictions &lt;- seq(0,4)\nFreq &lt;- c(128, 434, 160, 64, 24)/810\n\n\nThe probability of that a randomly selected inmate has exactly 2 prior convictions is:\n\n\nCode\ndbinom(x=1, size = 1, prob = 160/810)\n\n\n[1] 0.1975309"
  },
  {
    "objectID": "posts/AlexisGamez_HW1.html#b-1",
    "href": "posts/AlexisGamez_HW1.html#b-1",
    "title": "Blog Post #1",
    "section": "b)",
    "text": "b)\nThe probability that a randomly selected inmate has fewer than 2 prior convictions is:\n\n\nCode\ndbinom(x = 1, size = 1, prob = (128+434)/810)\n\n\n[1] 0.6938272"
  },
  {
    "objectID": "posts/AlexisGamez_HW1.html#c-1",
    "href": "posts/AlexisGamez_HW1.html#c-1",
    "title": "Blog Post #1",
    "section": "c)",
    "text": "c)\nThe probability that a randomly selected inmate has 2 or fewer prior convictions is:\n\n\nCode\ndbinom(x = 1, size = 1, prob = (128+434+160)/810)\n\n\n[1] 0.891358"
  },
  {
    "objectID": "posts/AlexisGamez_HW1.html#d-1",
    "href": "posts/AlexisGamez_HW1.html#d-1",
    "title": "Blog Post #1",
    "section": "d)",
    "text": "d)\nThe probability that a randomly selected inmate has more than 2 prior convictions is:\n\n\nCode\ndbinom(x = 1, size = 1, prob = (64+24)/810)\n\n\n[1] 0.108642"
  },
  {
    "objectID": "posts/AlexisGamez_HW1.html#e-1",
    "href": "posts/AlexisGamez_HW1.html#e-1",
    "title": "Blog Post #1",
    "section": "e)",
    "text": "e)\nThe expected value for the number of prior convictions is:\n\n\nCode\nExpected_v &lt;- sum(Freq*Convictions)\nExpected_v\n\n\n[1] 1.28642"
  },
  {
    "objectID": "posts/AlexisGamez_HW1.html#f",
    "href": "posts/AlexisGamez_HW1.html#f",
    "title": "Blog Post #1",
    "section": "f)",
    "text": "f)\nThe variance for prior convictions is:\n\n\nCode\nVariance &lt;- sum((Convictions-Expected_v)^2*Freq)\nVariance\n\n\n[1] 0.8562353\n\n\nThe standard deviation for prior convictions is:\n\n\nCode\nSD &lt;- sqrt(Variance)\nSD\n\n\n[1] 0.9253298\n\n\nPlotting our data further validates our calculations as all values we’ve presented seem to coincide with their respective points on the plot.\n\n\nCode\nConv_data &lt;- tibble(\n  x= Convictions,\n  y= Freq)\n\nggplot(Conv_data, aes(x,y))+\n  geom_line()+\n  geom_vline(xintercept = 2, col=\"red\",size=1)+\n  geom_text(x=2.15,y=.245,label=\"c = 2\")+\n  labs(x=\"# of Prior Convictions\",y=\"Probability\",title=\"Probability Distribution of Prisoner # of Prior Convictions\")\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead."
  },
  {
    "objectID": "posts/HW_2_Diana_Rinker.html",
    "href": "posts/HW_2_Diana_Rinker.html",
    "title": "Homework 2",
    "section": "",
    "text": "DACSS 603, spring 2023\n\n\nHomework 2, Diana Rinker.\nLoading necessary libraries:\n\n\nCode\nlibrary(readxl)\nlibrary(dplyr)\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nCode\nlibrary(ggplot2)\nlibrary(tidyverse)\n\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2\n──\n\n\n✔ tibble  3.1.8     ✔ purrr   1.0.1\n✔ tidyr   1.3.0     ✔ stringr 1.5.0\n✔ readr   2.1.4     ✔ forcats 1.0.0\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\n\n\nQuestion 1\nThe time between the date a patient was recommended for heart surgery and the surgery date for cardiac patients in Ontario was collected by the Cardiac Care Network (“Wait Times Data Guide,” Ministry of Health and Long-Term Care, Ontario, Canada, 2006). The sample mean and sample standard deviation for wait times (in days) of patients for two cardiac procedures are given in the accompanying table. Assume that the sample is representative of the Ontario population.\n\n\nCode\n`Surgical Procedure` &lt;- c(\"Bypass\", \"Angiography\" )\n`Sample Size` &lt;- c(539,847 )\n`Mean Wait Time` &lt;- c(19, 18)\n`Standard Deviation` &lt;- c(10, 9)\ndata&lt;- data.frame(`Surgical Procedure`,`Sample Size`, `Mean Wait Time`, `Standard Deviation` )\nknitr::kable(data )\n\n\n\n\n\nSurgical.Procedure\nSample.Size\nMean.Wait.Time\nStandard.Deviation\n\n\n\n\nBypass\n539\n19\n10\n\n\nAngiography\n847\n18\n9\n\n\n\n\n\nConstruct the 90% confidence interval to estimate the actual mean wait time for each of the two procedures. Is the confidence interval narrower for angiography or bypass surgery?\nThe formula for the confidence interval of a sample is\n\\[\nCI =\\bar{X} \\pm  t \\cdot \\frac{s}{\\sqrt n}\n\\]\n\n\nCode\n#calculating degree of freedom for each sample: \ndata &lt;- data%&gt;%\n  mutate(confidence_level = 0.9,\n         tail_area = (1-confidence_level)/2,\n         t_score = qt(p = 1-tail_area, df = `Sample.Size` -1),\n         CI.low  = `Mean Wait Time` - t_score * `Standard.Deviation` / sqrt(`Sample.Size`), \n         CI.high= `Mean Wait Time` + t_score * `Standard.Deviation` / sqrt(`Sample.Size`),\n         CI.width = abs(CI.high - CI.low)\n         )\ndata&lt;- data %&gt;%\nselect (Surgical.Procedure, CI.low,CI.high , CI.width, t_score, everything() )\nknitr::kable(data ) \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSurgical.Procedure\nCI.low\nCI.high\nCI.width\nt_score\nSample.Size\nMean.Wait.Time\nStandard.Deviation\nconfidence_level\ntail_area\n\n\n\n\nBypass\n18.29029\n19.70971\n1.419421\n1.647691\n539\n19\n10\n0.9\n0.05\n\n\nAngiography\n17.49078\n18.50922\n1.018436\n1.646657\n847\n18\n9\n0.9\n0.05\n\n\n\n\n\nFrom the table above I can see that the width of confidence interval for Angiography is smaller than for Bypass.\nAlternatvie way to calculate the interval is by simulating the data based on given parameters and running one-sided t.test() :\n\n\nCode\nAngiography.data &lt;- rnorm(n=847, mean =18 , sd = 9)\n# Calculate the sample mean\nmean.q1 &lt;- mean(Angiography.data )\n# Adjust the sample to have a mean of exactly 410\nAngiography.data &lt;- Angiography.data + 18 - mean.q1\n\nt.test(Angiography.data, alternative = \"two.sided\", conf.level = 0.9) \n\n\n\n    One Sample t-test\n\ndata:  Angiography.data\nt = 59.128, df = 846, p-value &lt; 2.2e-16\nalternative hypothesis: true mean is not equal to 0\n90 percent confidence interval:\n 17.49872 18.50128\nsample estimates:\nmean of x \n       18 \n\n\n\n\nCode\nBypass.data &lt;- rnorm(n=539, mean =19 , sd = 10)\n# Calculate the sample mean\nmean.bypass.q1 &lt;- mean(Bypass.data )\n# Adjust the sample to have a mean of exactly 410\nBypass.data  &lt;- Bypass.data  + 19 - mean.bypass.q1\n\nt.test(Bypass.data , alternative = \"two.sided\", conf.level = 0.9) \n\n\n\n    One Sample t-test\n\ndata:  Bypass.data\nt = 45.732, df = 538, p-value &lt; 2.2e-16\nalternative hypothesis: true mean is not equal to 0\n90 percent confidence interval:\n 18.31544 19.68456\nsample estimates:\nmean of x \n       19 \n\n\n\n\nQuestion 2\nA survey of 1031 adult Americans was carried out by the National Center for Public Policy. Assume that the sample is representative of adult Americans. Among those surveyed, 567 believed that college education is essential for success. Find the point estimate, p, of the proportion of all adult Americans who believe that a college education is essential for success. Construct and interpret a 95% confidence interval for p.\nI will assume, that the survey responses are within binomial distribution ( i.e. respondents had an option to agree or disagree that college education is essential for success).\nPoint estimate for this sample is 567/1031\n\n\nCode\npoint.estimate &lt;- 567/1031\n\n\nConfidence interval = point estimate ± margin of error. To calculate a confidence interval for the p, I will usr the following formula:\n\\[\nCI =\\bar{p} \\pm Zscore \\cdot SE\n\\]\n\n\nCode\nSample.proportion &lt;- 567/1031\nSample.Size &lt;- 1031\ndata2&lt;- data.frame(Sample.proportion,Sample.Size  )\ndata2&lt;- data2%&gt;%\n  mutate(SE =(sqrt(Sample.proportion*(1-Sample.proportion)/Sample.Size)), # for binomial distribution\n        z_score = 1.96, # for 95% cofidence interval  \n       CI.low  = Sample.proportion - z_score * SE,\n         CI.high= Sample.proportion + z_score * SE\n        )\nknitr::kable(data2 ) \n\n\n\n\n\n\n\n\n\n\n\n\n\nSample.proportion\nSample.Size\nSE\nz_score\nCI.low\nCI.high\n\n\n\n\n0.5499515\n1031\n0.015494\n1.96\n0.5195833\n0.5803197\n\n\n\n\n\nThis confidence interval can be interpreted as “The people who agreethat college education is essential for success is between 52% and 58%from general American population”.\nI could also calculate confidence interval by generating a sample data set and using prob.test() function:\n\n\nCode\n#generating data table: \n  # Generating sample for grade 6:\n  successes &lt;- 0\n  p &lt;- 567/1031\n  n&lt;-1031\n  # repeat until the desired number of successes is obtained\n  while (successes != 567) {\n          responses &lt;- rbinom(n=1031, 1, p=p)\n  # count the number of successes\n          successes &lt;- sum(responses)\n  }\nrespondent.n &lt;-seq(1:1031)\ndataset.q2 &lt;-data.frame( respondent.n , responses)\n  \nprop.test(x = sum(dataset.q2$responses), n=n,  p =p )\n\n\n\n    1-sample proportions test without continuity correction\n\ndata:  sum(dataset.q2$responses) out of n, null probability p\nX-squared = 6.9637e-30, df = 1, p-value = 1\nalternative hypothesis: true p is not equal to 0.5499515\n95 percent confidence interval:\n 0.5194543 0.5800778\nsample estimates:\n        p \n0.5499515 \n\n\nThis way of calculating CI provided me with the same interval as the first one.\n\n\nQuestion 3\nSuppose that the financial aid office of UMass Amherst seeks to estimate the mean cost of textbooks per semester for students. The estimate will be useful if it is within $5 of the true population mean (i.e. they want the confidence interval to have a length of $10 or less). The financial aid office is pretty sure that the amount spent on books varies widely, with most values between $30 and $200. They think that the population standard deviation is about a quarter of this range (in other words, you can assume they know the population standard deviation). Assuming the significance level to be 5%, what should be the size of the sample?\nSince I am given standard deviation of population, I can use the following formula for CI for confidence level 95%:\n\\[\nCI =\\bar{X} \\pm  1.96 \\cdot \\frac{\\sigma}{\\sqrt n}  \n\\] The range of CI will be\n\\[\nCI range = 10=(\\bar{X} +  1.96 \\cdot \\frac{\\sigma}{\\sqrt n})  -(\\bar{X} -  1.96 \\cdot \\frac{\\sigma}{\\sqrt n})   \n\\]\n\\[\n  2\\cdot (1.96 \\cdot \\frac{\\sigma}{\\sqrt n})  = 10\n\\]\n\\[\n  {\\sqrt n}  = \\frac{1.96\\sigma}{5}\n\\]\n\\[\nn = (\\frac{1.96 \\cdot  42.5}{5})^2\n\\]\n\n\nCode\nSD.population &lt;- (200-30)/4\nn&lt;- ((1.96*SD.population)/5)^2\nknitr::kable(n) \n\n\n\n\n\nx\n\n\n\n\n277.5556\n\n\n\n\n\nTo calculate confidence interval width of $10, financial aid office should collect a sample size of 278 students.\n\n\nQuestion 4\nAccording to a union agreement, the mean income for all senior-level workers in a large service company equals $500 per week. A representative of a women’s group decides to analyze whether the mean income μ for female employees matches this norm. For a random sample of nine female employees, ȳ = $410 and s = 90 A. Test whether the mean income of female employees differs from $500 per week. Include assumptions, hypotheses, test statistic, and P-value. Interpret the result. B. Report the P-value for Ha: μ &lt; 500. Interpret. C. Report and interpret the P-value for Ha: μ &gt; 500. (Hint: The P-values for the two possible one-sided tests must sum to 1.\n\nA. Two-sided t.test:\nTest whether the mean income of female employees differs from $500 per week. Include assumptions, hypotheses, test statistic, and P-value. Interpret the result.\nAssumption: The sample is representative of all senior-level female workers in that company.\nHo: mean of female income is close to the company policy income of $500. H1: mean of female income is not the same as the company policy income of $500.\nTo test this hypothesis, I will used one-sample t.test, which will compare mean and cinficance interval of a sample to a policy level of $500.\nT.test() function in R uses t-statistics to evaluate small sample size (9).\n\n\nCode\n# Generate a sample with a mean close to 410\nfemale.sample &lt;- rnorm(n=9, mean=410, sd=90)\n# Calculate the sample mean\nsample.mean &lt;- mean(female.sample)\n# Adjust the sample to have a mean of exactly 410\nfemale.sample &lt;- female.sample + 410 - sample.mean\na.ttest.sample&lt;-t.test(female.sample, alternative = \"two.sided\", mu=500)\na.ttest.sample\n\n\n\n    One Sample t-test\n\ndata:  female.sample\nt = -2.2966, df = 8, p-value = 0.05074\nalternative hypothesis: true mean is not equal to 500\n95 percent confidence interval:\n 319.6305 500.3695\nsample estimates:\nmean of x \n      410 \n\n\nThe output shows 95% confidence interval (325 - 494) with the p-value under 0.05. This interval does not include policy value of $500, meaning that the true female income mean at the company is very unlikely to be 500. Therefore I reject Ho, and Accept H1.\n\n\nB. One-sided, “less”\nReport the P-value for Ha: μ &lt; 500. Interpret.\nHo: mean of female income is no less than the company policy income of $500. H1: mean of female income is lower than the company policy income of $500.\n\n\nCode\nb.ttest.sample&lt;-t.test(female.sample, alternative = \"less\", mu=500)\nb.ttest.sample\n\n\n\n    One Sample t-test\n\ndata:  female.sample\nt = -2.2966, df = 8, p-value = 0.02537\nalternative hypothesis: true mean is less than 500\n95 percent confidence interval:\n     -Inf 482.8735\nsample estimates:\nmean of x \n      410 \n\n\nThe confidence interval for this hypothesis is even further from 500, that for two-sided test: (-inf, 455) which does not include 500. I accept alternative hypothesis that true female income is lower than 500. P-value for one-sided test is twice lower than for two-sided , because we are only estimating one direction from the mean.\n\n\nC. One-sided, “greater”.\nReport and interpret the P-value for Ha: μ &gt; 500.\nHo: mean of female income is not greater than the company policy income of $500. H1: mean of female income is greater than the company policy income of $500.\n\n\nCode\nc.ttest.sample&lt;-t.test(female.sample, alternative = \"greater\", mu=500)\nc.ttest.sample\n\n\n\n    One Sample t-test\n\ndata:  female.sample\nt = -2.2966, df = 8, p-value = 0.9746\nalternative hypothesis: true mean is greater than 500\n95 percent confidence interval:\n 337.1265      Inf\nsample estimates:\nmean of x \n      410 \n\n\nThe confidence interval for this hypothesis includes 500, which suppports H0 that the female income is not greater than company policy. Additionally, p-value is greater than 0.05 which also supports Ho.  income is less or greater than 500) equals to 1.\n\n\n\n\nQuestion 5\nJones and Smith separately conduct studies to test H0: μ = 500 against Ha: μ ≠ 500, each with n = 1000. Jones gets ȳ = 519.5, with se = 10.0. Smith gets ȳ = 519.7, with se = 10.0.\nA. Show that t = 1.95 and P-value = 0.051 for Jones. Show that t = 1.97 and P-value = 0.049 for Smith.\nB. Using α = 0.05, for each study indicate whether the result is “statistically significant.”\nC. Using this example, explain the misleading aspects of reporting the result of a test as “P ≤ 0.05” versus “P &gt; 0.05,” or as “reject H0” versus “Do not reject H0,” without reporting the actual P-value.\n\n*A. Calculating t-statistics for each sample:\nShow that t = 1.95 and P-value = 0.051 for Jones. Show that t = 1.97 and P-value = 0.049 for Smith.\nTo do that, we will create a summary table containing both samples/ parameters and calculate t-statistic, critical value and p-value\n\n\nCode\n# Generate a sample with a mean 519.5 for John\n# John.data1&lt;- rnorm(n=1000,  mean =519.5)\n# John.mean &lt;- mean(John.data1)\n# John.data1&lt;- John.data1 + 519.5 - John.mean\n# John.mean &lt;- mean(John.data1)\n# John.mean\n\nn&lt;-c(1000, 1000) \nstudy.name &lt;- c(\"John\", \"Smith\")\nMeans &lt;- c(519.5, 519.7) \nSE &lt;- c(10.0, 10.0) \nsummary &lt;- data.frame(study.name, n, Means, SE)\n\nsummary &lt;- summary%&gt;%\n  mutate(tail_area = (1-0.95)/2,\n         t_score = round( qt(p = 1-tail_area, df = n -1), 2),# Critical t-value \n         CI.low  = Means - t_score * SE, \n         CI.high = Means + t_score * SE,\n         test.statistics = ((Means - 500)/ SE),\n         p.value = (1 - pt(test.statistics, df = n -1)) * 2\n         )\n\n knitr::kable(summary) \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nstudy.name\nn\nMeans\nSE\ntail_area\nt_score\nCI.low\nCI.high\ntest.statistics\np.value\n\n\n\n\nJohn\n1000\n519.5\n10\n0.025\n1.96\n499.9\n539.1\n1.95\n0.0514555\n\n\nSmith\n1000\n519.7\n10\n0.025\n1.96\n500.1\n539.3\n1.97\n0.0491143\n\n\n\n\n\nI can see from the table above, that the Confidence Interval (CI) for John’s data inludes 500 value, while Somth’s CI doesnt. Also I see that the two datasets result in slightly different test statistics and therefore p-value, which are very close to the tested value. Due to John’s p-value being above 0.05, I will accept HO. However, Smith p-value is below 0.05 and I would decline HO and accept H1.\n\n\nB. Interpreting statistical significance.\nUsing α = 0.05, for each study indicate whether the result is “statistically significant.”\nWe can conclude that the difference between 500 and the mean of Smith’s sample is statistically significant to reject Ho, while John’s sample mean’s difference from 500 is not statistically significant (i.e. not far enough from 500).\n\n\nC. Potential misleading of comparing p-value to 0.05:\nUsing this example, explain the misleading aspects of reporting the result of a test as “P ≤ 0.05” versus “P &gt; 0.05,” or as “reject H0” versus “Do not reject H0,” without reporting the actual P-value.\nComparing this two almost identical samples allowed us to demonstrate how a tiny difference in sample mean can produce difference in study outcomes (rejection vs acceptance of Ho). It is important to remember, that the sample means vary due to randomness of sampling process, and therefore produce variety of confidence intervals, test statistics and p-values.Reporting actual p-value will demonstrate its closeness to 0.05 cutoff level, and warn the reader about potential error in conclusion.\n\n\n\n\nQuestion 6\nA school nurse wants to determine whether age is a factor in whether children choose a healthy snack after school. She conducts a survey of 300 middle school students, with the results below. Test at α = 0.05 the claim that the proportion who choose a healthy snack differs by grade level. What is the null hypothesis? Which test should we use? What is the conclusion?\n\n\nCode\n`grade level` &lt;- c(\"6\",\"7\",\"8\")\n`Healthy snack` &lt;- c(31,43,51)\n`Unhealthy snack` &lt;- c(69,57,49)\ntable &lt;- data.frame (`grade level`, `Healthy snack`, `Unhealthy snack` )\n# table &lt;- addmargins( table , margin =1:2,  FUN = sum)\nknitr::kable(table)  \n\n\n\n\n\ngrade.level\nHealthy.snack\nUnhealthy.snack\n\n\n\n\n6\n31\n69\n\n\n7\n43\n57\n\n\n8\n51\n49\n\n\n\n\n\nThis table represents data for two categorical variables: grade level and snack choice\nI will generate a sample data as given above:\n\n\nCode\n  # Generating sample for grade 6:\n  successes &lt;- 0\n  # repeat until the desired number of successes is obtained\n  while (successes != 31) {\n          grade6 &lt;- rbinom(n=100, 1, 0.31)\n  # count the number of successes\n          successes &lt;- sum(grade6)\n  }\n  \n  # Generating sample for grade 7:\n  successes &lt;- 0\n  while (successes != 43) {\n            grade7 &lt;- rbinom(n=100, 1, 0.43)\n            successes &lt;- sum(grade7)\n            }\n  \n  # Generating sample for grade 8:\n  successes &lt;- 0\n  while (successes != 51) {\n          grade8 &lt;- rbinom(n=100, 1, 0.51)\n          successes &lt;- sum(grade8)\n          }\n  # Combining all grades in one table for analysis:\n  table2&lt;- data.frame(grade6, grade7, grade8)\n  table2&lt;- table2 %&gt;%\n        pivot_longer(c(grade6, grade7, grade8 ), names_to= \"grade\", values_to=\"healthy.choice\")\n\n  xtabs &lt;-xtabs(~table2$grade + table2$healthy.choice)\n  knitr::kable(  xtabs )  \n\n\n\n\n\n\n0\n1\n\n\n\n\ngrade6\n69\n31\n\n\ngrade7\n57\n43\n\n\ngrade8\n49\n51\n\n\n\n\n\nThe school nurse collected this data to test the following hypothesis:\nHo: all grades’ samples are coming from the same general population, and probability of making healthy choice is equal for all grades.\nH1: probability of making healthy choice is not equal for all grades.\nI will use a chi-square test to test this hypothesis:\n\n\nCode\n  table2$healthy.choice &lt;- as.character (table2$healthy.choice)\n  table2$grade &lt;- as.character(table2$grade)\nch.sq&lt;- chisq.test(table2$grade, table2$healthy.choice ,correct = FALSE)\nch.sq\n\n\n\n    Pearson's Chi-squared test\n\ndata:  table2$grade and table2$healthy.choice\nX-squared = 8.3383, df = 2, p-value = 0.01547\n\n\nThe p-value of 0.01 makes me to reject H0 hypothesis that all grades have equal probability, and accpet H1, that the kids of different grades differ in probability of making haealthy snack choice.\n\n\n\nQuestion 7\nPer-pupil costs (in thousands of dollars) for cyber charter school tuition for school districts in three areas are shown. Test the claim that there is a difference in means for the three areas, using an appropriate test. What is the null hypothesis? Which test should we use? What is the conclusion?\n\n\nCode\nArea.1 &lt;- c( 6.2, 9.3, 6.8, 6.1, 6.7, 7.5)\nArea.2 &lt;- c( 7.5, 8.2, 8.5, 8.2, 7.0, 9.3)\nArea.3 &lt;- c( 5.8, 6.4 ,5.6, 7.1, 3.0, 3.5)\n\ntable.q7 &lt;- data.frame (Area.1, Area.2, Area.3)\nknitr::kable(table.q7 )   \n\n\n\n\n\nArea.1\nArea.2\nArea.3\n\n\n\n\n6.2\n7.5\n5.8\n\n\n9.3\n8.2\n6.4\n\n\n6.8\n8.5\n5.6\n\n\n6.1\n8.2\n7.1\n\n\n6.7\n7.0\n3.0\n\n\n7.5\n9.3\n3.5\n\n\n\n\n\nHo: there is no difference in means between the three areas. H1: At least one of these areas’ means is significantly different.\nTo test this hypopthesis, I will use ANOVA test to compare means of three samples:\n\n\nCode\n table.q7  &lt;- table.q7  %&gt;%\n        pivot_longer(c(Area.1, Area.2, Area.3 ), names_to= \"Area\", values_to=\"score\")\n\ntable.q7$Area &lt;- as.factor (table.q7$Area)\nanova.q7 &lt;- aov( score ~ Area, table.q7 )\nsummary(anova.q7)\n\n\n            Df Sum Sq Mean Sq F value  Pr(&gt;F)   \nArea         2  25.66  12.832   8.176 0.00397 **\nResiduals   15  23.54   1.569                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe results of ANOVA test demonstrate p-value of 0.00397. I make a conclusion that at least one of the three areas’ means is significantly different from others (i.e reject Ho and accept H1)."
  },
  {
    "objectID": "posts/FinalPart1_LTucksmith.html",
    "href": "posts/FinalPart1_LTucksmith.html",
    "title": "FinalPart1",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(janitor)\nknitr::opts_chunk$set(echo = TRUE)\n\n\nThe most profitable sports league in the world, the National Football League, generated $18 billion in revenue in 2021 according to sportico.com. This total represents national media rights, league sponsorships with gambling companies, news outlets, and other companies, and shared revenue and royalties from the league’s various affiliates and subsidiaries, such as NFL Enterprises, NFL Properties, and NFL International. While the 32 teams which make up the NFL have separate revenue streams stemming from merchandise and ticketing sales and various other endeavors, each team also receives a slice of shared revenue from the NFL from the games’ national and local broadcasts and sponsorships. As money is what keeps the NFL afloat and allows teams to competitively pay for top players and coaches, NFL teams benefit from working with media outlets and appearing in media and news. What I plan to analyze is how the relationship between the NFL and media plays out in games, if it does at all. Can the relationship between media and game outcome be measured? Does a team’s weekly media attention share affect their weekly game outcome? My analysis will attempt to measure this relationship by comparing Google Trends generated News Interest Over Time scores for the 2020-21 and 2021-22 for each match up that occurred in those two seasons.\nMy hypothesis is that the highest scored weeks for each team will generate the highest probability of winning that same or next given week. In addition, I plan to analyze how the relationship between the media and game scores changes given the media score of the opposition team, and if the relationship between the media scores and the sports betting data differs from that of the media scores and game outcome. I hypothesize that the team with the higher media score will be favored and that the spread will widen if the individual media score is above a to be determined .\nThe first dataset is a collection of all the matchups and scores that occurred in the NFL since 1966. The matchup information includes the names of the two teams competing, home/away team status, the match site, the weather on game day, and the final score for each team. Sports betting data for each game since 1977 is also included, and includes who was favored, the point spread, and the over/under line. The dataset is from https://www.kaggle.com/datasets/tobycrabtree/nfl-scores-and-betting-data and was created from a variety of sources including games and scores from public websites such as ESPN, NFL.com, and Pro Football Reference. Weather information is from NOAA data, cross-referenced with NFLweather.com. Betting data reflects lines available at sportsline.com and aussportsbetting.com. For the analysis, I will limit the data to the data collected from the 2020-21 and 2021-22 seasons.\n\n\nCode\ngame_scores &lt;- read.csv(\"~/Documents/GitHub/603_Spring_2023/posts/_data/spreadspoke_scores.csv\")\n\n\nWarning in file(file, \"rt\"): cannot open file '/Users/elisabethtucksmith/\nDocuments/GitHub/603_Spring_2023/posts/_data/spreadspoke_scores.csv': No such\nfile or directory\n\n\nError in file(file, \"rt\"): cannot open the connection\n\n\nCode\ncolnames(game_scores)\n\n\nError in is.data.frame(x): object 'game_scores' not found\n\n\nCode\ngame_scores &lt;- game_scores[game_scores$schedule_season == 2021 | game_scores$schedule_season == 2022, ]\n\n\nError in eval(expr, envir, enclos): object 'game_scores' not found\n\n\nCode\nsummary(game_scores)\n\n\nError in summary(game_scores): object 'game_scores' not found\n\n\nCode\nglimpse(game_scores)\n\n\nError in glimpse(game_scores): object 'game_scores' not found\n\n\nThe second dataset I will use is a combination of data from Google Trends. To collect this data I pulled weekly “News Search” scores for each of the 32 NFL teams over the 2020-21 and 2021-22 season. To populate these scores, Google determines the week with the highest volume of News Searches within the time frame for each team and assigns it a score of 100. From there, the other weeks of the time frame get assigned their score to be the percentage of news search volume in proportion to the week scored as 100. For example, if week 7 was scored at 100 and week 3 had 30% of the news search volume that week 7 did, week 3’s score would be 30.\n\n\nCode\n#import csv for each NFL team\ndf1 &lt;- read.csv(\"~/Documents/GitHub/603_Spring_2023/posts/_data/multiTimeline.csv\")\n\n\nWarning in file(file, \"rt\"): cannot open file '/Users/elisabethtucksmith/\nDocuments/GitHub/603_Spring_2023/posts/_data/multiTimeline.csv': No such file or\ndirectory\n\n\nError in file(file, \"rt\"): cannot open the connection\n\n\nCode\ndf2 &lt;- read.csv(\"~/Documents/GitHub/603_Spring_2023/posts/_data/multiTimeline (1).csv\")\n\n\nWarning in file(file, \"rt\"): cannot open file '/Users/elisabethtucksmith/\nDocuments/GitHub/603_Spring_2023/posts/_data/multiTimeline (1).csv': No such\nfile or directory\n\n\nError in file(file, \"rt\"): cannot open the connection\n\n\nCode\ndf3 &lt;- read.csv(\"~/Documents/GitHub/603_Spring_2023/posts/_data/multiTimeline (2).csv\")\n\n\nWarning in file(file, \"rt\"): cannot open file '/Users/elisabethtucksmith/\nDocuments/GitHub/603_Spring_2023/posts/_data/multiTimeline (2).csv': No such\nfile or directory\n\n\nError in file(file, \"rt\"): cannot open the connection\n\n\nCode\ndf4 &lt;- read.csv(\"~/Documents/GitHub/603_Spring_2023/posts/_data/multiTimeline (3).csv\")\n\n\nWarning in file(file, \"rt\"): cannot open file '/Users/elisabethtucksmith/\nDocuments/GitHub/603_Spring_2023/posts/_data/multiTimeline (3).csv': No such\nfile or directory\n\n\nError in file(file, \"rt\"): cannot open the connection\n\n\nCode\ndf5 &lt;- read.csv(\"~/Documents/GitHub/603_Spring_2023/posts/_data/multiTimeline (4).csv\")\n\n\nWarning in file(file, \"rt\"): cannot open file '/Users/elisabethtucksmith/\nDocuments/GitHub/603_Spring_2023/posts/_data/multiTimeline (4).csv': No such\nfile or directory\n\n\nError in file(file, \"rt\"): cannot open the connection\n\n\nCode\ndf6 &lt;- read.csv(\"~/Documents/GitHub/603_Spring_2023/posts/_data/multiTimeline (5).csv\")\n\n\nWarning in file(file, \"rt\"): cannot open file '/Users/elisabethtucksmith/\nDocuments/GitHub/603_Spring_2023/posts/_data/multiTimeline (5).csv': No such\nfile or directory\n\n\nError in file(file, \"rt\"): cannot open the connection\n\n\nCode\ndf7 &lt;- read.csv(\"~/Documents/GitHub/603_Spring_2023/posts/_data/multiTimeline (6).csv\")\n\n\nWarning in file(file, \"rt\"): cannot open file '/Users/elisabethtucksmith/\nDocuments/GitHub/603_Spring_2023/posts/_data/multiTimeline (6).csv': No such\nfile or directory\n\n\nError in file(file, \"rt\"): cannot open the connection\n\n\nCode\ndf8 &lt;- read.csv(\"~/Documents/GitHub/603_Spring_2023/posts/_data/multiTimeline (7).csv\")\n\n\nWarning in file(file, \"rt\"): cannot open file '/Users/elisabethtucksmith/\nDocuments/GitHub/603_Spring_2023/posts/_data/multiTimeline (7).csv': No such\nfile or directory\n\n\nError in file(file, \"rt\"): cannot open the connection\n\n\nCode\ndf9 &lt;- read.csv(\"~/Documents/GitHub/603_Spring_2023/posts/_data/multiTimeline (8).csv\")\n\n\nWarning in file(file, \"rt\"): cannot open file '/Users/elisabethtucksmith/\nDocuments/GitHub/603_Spring_2023/posts/_data/multiTimeline (8).csv': No such\nfile or directory\n\n\nError in file(file, \"rt\"): cannot open the connection\n\n\nCode\ndf10 &lt;- read.csv(\"~/Documents/GitHub/603_Spring_2023/posts/_data/multiTimeline (9).csv\")\n\n\nWarning in file(file, \"rt\"): cannot open file '/Users/elisabethtucksmith/\nDocuments/GitHub/603_Spring_2023/posts/_data/multiTimeline (9).csv': No such\nfile or directory\n\n\nError in file(file, \"rt\"): cannot open the connection\n\n\nCode\ndf11 &lt;- read.csv(\"~/Documents/GitHub/603_Spring_2023/posts/_data/multiTimeline (10).csv\")\n\n\nWarning in file(file, \"rt\"): cannot open file '/Users/elisabethtucksmith/\nDocuments/GitHub/603_Spring_2023/posts/_data/multiTimeline (10).csv': No such\nfile or directory\n\n\nError in file(file, \"rt\"): cannot open the connection\n\n\nCode\ndf12 &lt;- read.csv(\"~/Documents/GitHub/603_Spring_2023/posts/_data/multiTimeline (11).csv\")\n\n\nWarning in file(file, \"rt\"): cannot open file '/Users/elisabethtucksmith/\nDocuments/GitHub/603_Spring_2023/posts/_data/multiTimeline (11).csv': No such\nfile or directory\n\n\nError in file(file, \"rt\"): cannot open the connection\n\n\nCode\ndf13 &lt;- read.csv(\"~/Documents/GitHub/603_Spring_2023/posts/_data/multiTimeline (12).csv\")\n\n\nWarning in file(file, \"rt\"): cannot open file '/Users/elisabethtucksmith/\nDocuments/GitHub/603_Spring_2023/posts/_data/multiTimeline (12).csv': No such\nfile or directory\n\n\nError in file(file, \"rt\"): cannot open the connection\n\n\nCode\ndf14 &lt;- read.csv(\"~/Documents/GitHub/603_Spring_2023/posts/_data/multiTimeline (13).csv\")\n\n\nWarning in file(file, \"rt\"): cannot open file '/Users/elisabethtucksmith/\nDocuments/GitHub/603_Spring_2023/posts/_data/multiTimeline (13).csv': No such\nfile or directory\n\n\nError in file(file, \"rt\"): cannot open the connection\n\n\nCode\ndf15 &lt;- read.csv(\"~/Documents/GitHub/603_Spring_2023/posts/_data/multiTimeline (14).csv\")\n\n\nWarning in file(file, \"rt\"): cannot open file '/Users/elisabethtucksmith/\nDocuments/GitHub/603_Spring_2023/posts/_data/multiTimeline (14).csv': No such\nfile or directory\n\n\nError in file(file, \"rt\"): cannot open the connection\n\n\nCode\ndf16 &lt;- read.csv(\"~/Documents/GitHub/603_Spring_2023/posts/_data/multiTimeline (15).csv\")\n\n\nWarning in file(file, \"rt\"): cannot open file '/Users/elisabethtucksmith/\nDocuments/GitHub/603_Spring_2023/posts/_data/multiTimeline (15).csv': No such\nfile or directory\n\n\nError in file(file, \"rt\"): cannot open the connection\n\n\nCode\ndf17 &lt;- read.csv(\"~/Documents/GitHub/603_Spring_2023/posts/_data/multiTimeline (16).csv\")\n\n\nWarning in file(file, \"rt\"): cannot open file '/Users/elisabethtucksmith/\nDocuments/GitHub/603_Spring_2023/posts/_data/multiTimeline (16).csv': No such\nfile or directory\n\n\nError in file(file, \"rt\"): cannot open the connection\n\n\nCode\ndf18 &lt;- read.csv(\"~/Documents/GitHub/603_Spring_2023/posts/_data/multiTimeline (17).csv\")\n\n\nWarning in file(file, \"rt\"): cannot open file '/Users/elisabethtucksmith/\nDocuments/GitHub/603_Spring_2023/posts/_data/multiTimeline (17).csv': No such\nfile or directory\n\n\nError in file(file, \"rt\"): cannot open the connection\n\n\nCode\ndf19 &lt;- read.csv(\"~/Documents/GitHub/603_Spring_2023/posts/_data/multiTimeline (18).csv\")\n\n\nWarning in file(file, \"rt\"): cannot open file '/Users/elisabethtucksmith/\nDocuments/GitHub/603_Spring_2023/posts/_data/multiTimeline (18).csv': No such\nfile or directory\n\n\nError in file(file, \"rt\"): cannot open the connection\n\n\nCode\ndf20 &lt;- read.csv(\"~/Documents/GitHub/603_Spring_2023/posts/_data/multiTimeline (19).csv\")\n\n\nWarning in file(file, \"rt\"): cannot open file '/Users/elisabethtucksmith/\nDocuments/GitHub/603_Spring_2023/posts/_data/multiTimeline (19).csv': No such\nfile or directory\n\n\nError in file(file, \"rt\"): cannot open the connection\n\n\nCode\ndf21 &lt;- read.csv(\"~/Documents/GitHub/603_Spring_2023/posts/_data/multiTimeline (20).csv\")\n\n\nWarning in file(file, \"rt\"): cannot open file '/Users/elisabethtucksmith/\nDocuments/GitHub/603_Spring_2023/posts/_data/multiTimeline (20).csv': No such\nfile or directory\n\n\nError in file(file, \"rt\"): cannot open the connection\n\n\nCode\ndf22 &lt;- read.csv(\"~/Documents/GitHub/603_Spring_2023/posts/_data/multiTimeline (21).csv\")\n\n\nWarning in file(file, \"rt\"): cannot open file '/Users/elisabethtucksmith/\nDocuments/GitHub/603_Spring_2023/posts/_data/multiTimeline (21).csv': No such\nfile or directory\n\n\nError in file(file, \"rt\"): cannot open the connection\n\n\nCode\ndf23 &lt;- read.csv(\"~/Documents/GitHub/603_Spring_2023/posts/_data/multiTimeline (22).csv\")\n\n\nWarning in file(file, \"rt\"): cannot open file '/Users/elisabethtucksmith/\nDocuments/GitHub/603_Spring_2023/posts/_data/multiTimeline (22).csv': No such\nfile or directory\n\n\nError in file(file, \"rt\"): cannot open the connection\n\n\nCode\ndf24 &lt;- read.csv(\"~/Documents/GitHub/603_Spring_2023/posts/_data/multiTimeline (23).csv\")\n\n\nWarning in file(file, \"rt\"): cannot open file '/Users/elisabethtucksmith/\nDocuments/GitHub/603_Spring_2023/posts/_data/multiTimeline (23).csv': No such\nfile or directory\n\n\nError in file(file, \"rt\"): cannot open the connection\n\n\nCode\ndf25 &lt;- read.csv(\"~/Documents/GitHub/603_Spring_2023/posts/_data/multiTimeline (24).csv\")\n\n\nWarning in file(file, \"rt\"): cannot open file '/Users/elisabethtucksmith/\nDocuments/GitHub/603_Spring_2023/posts/_data/multiTimeline (24).csv': No such\nfile or directory\n\n\nError in file(file, \"rt\"): cannot open the connection\n\n\nCode\ndf26 &lt;- read.csv(\"~/Documents/GitHub/603_Spring_2023/posts/_data/multiTimeline (25).csv\")\n\n\nWarning in file(file, \"rt\"): cannot open file '/Users/elisabethtucksmith/\nDocuments/GitHub/603_Spring_2023/posts/_data/multiTimeline (25).csv': No such\nfile or directory\n\n\nError in file(file, \"rt\"): cannot open the connection\n\n\nCode\ndf27 &lt;- read.csv(\"~/Documents/GitHub/603_Spring_2023/posts/_data/multiTimeline (26).csv\")\n\n\nWarning in file(file, \"rt\"): cannot open file '/Users/elisabethtucksmith/\nDocuments/GitHub/603_Spring_2023/posts/_data/multiTimeline (26).csv': No such\nfile or directory\n\n\nError in file(file, \"rt\"): cannot open the connection\n\n\nCode\ndf28 &lt;- read.csv(\"~/Documents/GitHub/603_Spring_2023/posts/_data/multiTimeline (27).csv\")\n\n\nWarning in file(file, \"rt\"): cannot open file '/Users/elisabethtucksmith/\nDocuments/GitHub/603_Spring_2023/posts/_data/multiTimeline (27).csv': No such\nfile or directory\n\n\nError in file(file, \"rt\"): cannot open the connection\n\n\nCode\ndf29 &lt;- read.csv(\"~/Documents/GitHub/603_Spring_2023/posts/_data/multiTimeline (28).csv\")\n\n\nWarning in file(file, \"rt\"): cannot open file '/Users/elisabethtucksmith/\nDocuments/GitHub/603_Spring_2023/posts/_data/multiTimeline (28).csv': No such\nfile or directory\n\n\nError in file(file, \"rt\"): cannot open the connection\n\n\nCode\ndf30 &lt;- read.csv(\"~/Documents/GitHub/603_Spring_2023/posts/_data/multiTimeline (29).csv\")\n\n\nWarning in file(file, \"rt\"): cannot open file '/Users/elisabethtucksmith/\nDocuments/GitHub/603_Spring_2023/posts/_data/multiTimeline (29).csv': No such\nfile or directory\n\n\nError in file(file, \"rt\"): cannot open the connection\n\n\nCode\ndf31 &lt;- read.csv(\"~/Documents/GitHub/603_Spring_2023/posts/_data/multiTimeline (30).csv\")\n\n\nWarning in file(file, \"rt\"): cannot open file '/Users/elisabethtucksmith/\nDocuments/GitHub/603_Spring_2023/posts/_data/multiTimeline (30).csv': No such\nfile or directory\n\n\nError in file(file, \"rt\"): cannot open the connection\n\n\nCode\ndf32 &lt;- read.csv(\"~/Documents/GitHub/603_Spring_2023/posts/_data/multiTimeline (31).csv\")\n\n\nWarning in file(file, \"rt\"): cannot open file '/Users/elisabethtucksmith/\nDocuments/GitHub/603_Spring_2023/posts/_data/multiTimeline (31).csv': No such\nfile or directory\n\n\nError in file(file, \"rt\"): cannot open the connection\n\n\nCode\n#bind columns to create one data frame that holds all the scores for each team\ntrend_scores &lt;- bind_cols(df1,df2,df3,df4,df5,df6,df7,df8,df9,df10,df11,df12,df13,df14,df15,\n                          df16,df17,df18,df19,df20,df21,df22,df23,df24,df25,df26,df27,df28,df29,df30,df31,df32)\n\n\nError in list2(...): object 'df1' not found\n\n\nCode\n#create new column that holds the dates for each week, which R stored as the row names\ntrend_scores &lt;- rownames_to_column(trend_scores, \"Week\")\n\n\nError in rownames_to_column(trend_scores, \"Week\"): object 'trend_scores' not found\n\n\nCode\n#rename column names to be row one values so that each column is the team name\ntrend_scores &lt;- trend_scores %&gt;% \n  row_to_names(row_number = 1)\n\n\nError in unlist(dat[row_number, ], use.names = FALSE): object 'trend_scores' not found\n\n\nCode\nsummary(trend_scores)\n\n\nError in summary(trend_scores): object 'trend_scores' not found\n\n\nCode\nglimpse(trend_scores)\n\n\nError in glimpse(trend_scores): object 'trend_scores' not found"
  },
  {
    "objectID": "posts/hw1_asch_harwood.html",
    "href": "posts/hw1_asch_harwood.html",
    "title": "Homework 1",
    "section": "",
    "text": "Code\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(knitr)\nlibrary(kableExtra)\nCode\nlibrary(readxl)\ndf &lt;- read_excel(\"_data/LungCapData.xls\")"
  },
  {
    "objectID": "posts/hw1_asch_harwood.html#d",
    "href": "posts/hw1_asch_harwood.html#d",
    "title": "Homework 1",
    "section": "d",
    "text": "d\n\n\nCode\ndf$AgeGroup &lt;- cut(df$Age, breaks = c(0, 13, 15, 17, Inf),\n                    labels = c(\"&lt;=13\", \"14 to 15\", \"16 to 17\", \"&gt;=18\"))\n\n\n\n\nCode\nage_group_mean &lt;- df %&gt;%\n  group_by(AgeGroup) %&gt;%\n  summarise(mean(LungCap))\nkable(age_group_mean)\n\n\n\n\n\nAgeGroup\nmean(LungCap)\n\n\n\n\n&lt;=13\n6.411932\n\n\n14 to 15\n9.045417\n\n\n16 to 17\n10.245876\n\n\n&gt;=18\n10.964688\n\n\n\n\n\n\n\n\n\nCode\nggplot(df, aes(x = AgeGroup, y = LungCap)) +\n  geom_boxplot()\n\n\n\n\n\nThere appears to be a relationship between age and lung capacity, where as age group increases, lung capacity increases as well. This is understandable given that developmentally a younger child would have a lower lung capacity than a young adult."
  },
  {
    "objectID": "posts/hw1_asch_harwood.html#e",
    "href": "posts/hw1_asch_harwood.html#e",
    "title": "Homework 1",
    "section": "e",
    "text": "e\n\n\nCode\nggplot(df, aes(x = AgeGroup, y = LungCap, fill=Smoke)) +\n  geom_boxplot() \n\n\n\n\n\n\n\nCode\nggplot(df, aes(x = LungCap, fill = Smoke)) +\n  geom_histogram(alpha = 0.5, bins = 10, position = \"identity\") +\n  facet_wrap(~AgeGroup, nrow = 2, scales = \"free\") +\n  labs(x = \"Lung Capacity\", y = \"Frequency\") +\n  ggtitle(\"Distribution of Lung Capacity by Age Group, by Smoking Status\")\n\n\n\n\n\nWhen controlling for age, average smoker lung capacity is lower across the board for all age groups except “13 &lt;=”. In the later group, there are far fewer smokers, as one would (hope to) expect, which explains the slightly higher mean lung capacity for smokers."
  },
  {
    "objectID": "posts/HW3_EmmaNarkewicz.html",
    "href": "posts/HW3_EmmaNarkewicz.html",
    "title": "Homework 3",
    "section": "",
    "text": "United Nations (Data file: UN11in alr4) The data in the file UN11 contains several variables, including ppgdp, the gross national product per person in U.S. dollars, and fertility, the birth rate per 1000 females, both from the year 2009. The data are for 199 localities, mostly UN member countries, but also other areas such as Hong Kong that are not independent countries. The data were collected from the United Nations (2011). We will study the dependence of fertility on ppgdp.\n\n\nIdentify the predictor and the response.\nIf we are studying the dependence of fertility on ppgdp (gross national product per person) then:\nPredictor = ppgdp\nResponse = fertility\n\n\nCode\n#load necessary packages\nlibrary(tidyverse)\n\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.3.6      ✔ purrr   0.3.4 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.0      ✔ stringr 1.4.1 \n✔ readr   2.1.2      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\nCode\nlibrary(alr4)\n\n\nLoading required package: car\nLoading required package: carData\n\nAttaching package: 'car'\n\nThe following object is masked from 'package:dplyr':\n\n    recode\n\nThe following object is masked from 'package:purrr':\n\n    some\n\nLoading required package: effects\nlattice theme set by effectsTheme()\nSee ?effectsTheme for details.\n\n\nCode\nlibrary(smss)\n\n\n\nDraw the scatterplot of fertility on the vertical axis versus ppgdp on the horizontal axis and summarize the information in this graph. Does a straight-line mean function seem to be plausible for a summary of this graph?\n\n\n\nCode\n#linear graph\n\nlibrary(alr4)\n\nggplot(data = UN11, aes(x=ppgdp, y=fertility)) +   geom_point() +  geom_smooth(method = 'lm', se=F) + labs(title = \"Scatterplot of Fertility vs. GDP per person (ppgdp)\" )\n\n\n`geom_smooth()` using formula 'y ~ x'\n\n\n\n\n\nThe above scatter plot of fertility vs. GDP per person (ppgdp) shows that the relationship between the two variables is not linear, but instead a curvilinear relationship. Therefore a straight-line mean function does not seems plausible. Fertility steeply drops as the ppdgp first increases but then levels out. Most of the data is concentrated around the left side of the x-axis with lower ppgdp\n\n\n\n\nDraw the scatterplot of log(fertility) versus log(ppgdp) using natural logarithms. Does the simple linear regression model seem plausible for a summary of this graph?\n\n\n\nCode\n#logarithmic graph\nggplot(data = UN11, aes(x=log(ppgdp), y=log(fertility))) +   geom_point() +  geom_smooth(method = 'lm', se=F) + labs(title = \"Scatterplot of Log of Fertility vs. Log of GDP per person (ppgdp)\")  \n\n\n`geom_smooth()` using formula 'y ~ x'\n\n\n\n\n\nA linear regression model of the natural logs of fertility vs. per person GDP (ppgdp) is a much better model of the relationship between the two variables than the prior model. As the log(ppgdp) increases, the log(fertility) decreases consistently. The data points are distributed consistently below and above the trendline.\n\n\nCode\n## mathematical correlation test\n\ncor_lin &lt;- cor.test(UN11$fertility, UN11$ppgdp)\ncor_log &lt;- cor.test(log(UN11$fertility), log(UN11$ppgdp))\ncor_lin\n\n\n\n    Pearson's product-moment correlation\n\ndata:  UN11$fertility and UN11$ppgdp\nt = -6.877, df = 197, p-value = 7.903e-11\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.5456842 -0.3205140\nsample estimates:\n       cor \n-0.4399891 \n\n\nCode\ncor_log\n\n\n\n    Pearson's product-moment correlation\n\ndata:  log(UN11$fertility) and log(UN11$ppgdp)\nt = -14.785, df = 197, p-value &lt; 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.7851376 -0.6519206\nsample estimates:\n       cor \n-0.7252483 \n\n\nNot only can that be seen visually in the scatter-plot above, but from the measure of Pearson’s correlation. The Pearson’s r correlation coefficient for for the linear regression model taking the natural logs of both variables (-.724) is a stronger correlation than for the linear regression model of the original data is (-0.440)."
  },
  {
    "objectID": "posts/HW3_EmmaNarkewicz.html#a",
    "href": "posts/HW3_EmmaNarkewicz.html#a",
    "title": "Homework 3",
    "section": "",
    "text": "Identify the predictor and the response.\nIf we are studying the dependence of fertility on ppgdp (gross national product per person) then:\nPredictor = ppgdp\nResponse = fertility\n\n\nCode\n#load necessary packages\nlibrary(tidyverse)\n\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.3.6      ✔ purrr   0.3.4 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.0      ✔ stringr 1.4.1 \n✔ readr   2.1.2      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\nCode\nlibrary(alr4)\n\n\nLoading required package: car\nLoading required package: carData\n\nAttaching package: 'car'\n\nThe following object is masked from 'package:dplyr':\n\n    recode\n\nThe following object is masked from 'package:purrr':\n\n    some\n\nLoading required package: effects\nlattice theme set by effectsTheme()\nSee ?effectsTheme for details.\n\n\nCode\nlibrary(smss)\n\n\n\nDraw the scatterplot of fertility on the vertical axis versus ppgdp on the horizontal axis and summarize the information in this graph. Does a straight-line mean function seem to be plausible for a summary of this graph?\n\n\n\nCode\n#linear graph\n\nlibrary(alr4)\n\nggplot(data = UN11, aes(x=ppgdp, y=fertility)) +   geom_point() +  geom_smooth(method = 'lm', se=F) + labs(title = \"Scatterplot of Fertility vs. GDP per person (ppgdp)\" )\n\n\n`geom_smooth()` using formula 'y ~ x'\n\n\n\n\n\nThe above scatter plot of fertility vs. GDP per person (ppgdp) shows that the relationship between the two variables is not linear, but instead a curvilinear relationship. Therefore a straight-line mean function does not seems plausible. Fertility steeply drops as the ppdgp first increases but then levels out. Most of the data is concentrated around the left side of the x-axis with lower ppgdp"
  },
  {
    "objectID": "posts/HW3_EmmaNarkewicz.html#c",
    "href": "posts/HW3_EmmaNarkewicz.html#c",
    "title": "Homework 3",
    "section": "",
    "text": "Draw the scatterplot of log(fertility) versus log(ppgdp) using natural logarithms. Does the simple linear regression model seem plausible for a summary of this graph?\n\n\n\nCode\n#logarithmic graph\nggplot(data = UN11, aes(x=log(ppgdp), y=log(fertility))) +   geom_point() +  geom_smooth(method = 'lm', se=F) + labs(title = \"Scatterplot of Log of Fertility vs. Log of GDP per person (ppgdp)\")  \n\n\n`geom_smooth()` using formula 'y ~ x'\n\n\n\n\n\nA linear regression model of the natural logs of fertility vs. per person GDP (ppgdp) is a much better model of the relationship between the two variables than the prior model. As the log(ppgdp) increases, the log(fertility) decreases consistently. The data points are distributed consistently below and above the trendline.\n\n\nCode\n## mathematical correlation test\n\ncor_lin &lt;- cor.test(UN11$fertility, UN11$ppgdp)\ncor_log &lt;- cor.test(log(UN11$fertility), log(UN11$ppgdp))\ncor_lin\n\n\n\n    Pearson's product-moment correlation\n\ndata:  UN11$fertility and UN11$ppgdp\nt = -6.877, df = 197, p-value = 7.903e-11\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.5456842 -0.3205140\nsample estimates:\n       cor \n-0.4399891 \n\n\nCode\ncor_log\n\n\n\n    Pearson's product-moment correlation\n\ndata:  log(UN11$fertility) and log(UN11$ppgdp)\nt = -14.785, df = 197, p-value &lt; 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.7851376 -0.6519206\nsample estimates:\n       cor \n-0.7252483 \n\n\nNot only can that be seen visually in the scatter-plot above, but from the measure of Pearson’s correlation. The Pearson’s r correlation coefficient for for the linear regression model taking the natural logs of both variables (-.724) is a stronger correlation than for the linear regression model of the original data is (-0.440)."
  },
  {
    "objectID": "posts/HW3_EmmaNarkewicz.html#b",
    "href": "posts/HW3_EmmaNarkewicz.html#b",
    "title": "Homework 3",
    "section": "b",
    "text": "b\nHow, if at all, does the correlation change?\nThe correlation would not change after converting the units of income from dollars to British pounds. We learned in class that correlation is a standardized version of the slope, with its value not depending on units."
  },
  {
    "objectID": "posts/HW3_EmmaNarkewicz.html#a-1",
    "href": "posts/HW3_EmmaNarkewicz.html#a-1",
    "title": "Homework 3",
    "section": "a",
    "text": "a\nGraphically portray how the explanatory variable relates to the outcome variable in each of the two cases\n\n\nCode\n#load & preview data\nlibrary(smss)\ndata(\"student.survey\")\nhead(student.survey)\n\n\n  subj ge ag  hi  co   dh   dr tv sp ne ah    ve pa           pi           re\n1    1  m 32 2.2 3.5    0  5.0  3  5  0  0 FALSE  r conservative   most weeks\n2    2  f 23 2.1 3.5 1200  0.3 15  7  5  6 FALSE  d      liberal occasionally\n3    3  f 27 3.3 3.0 1300  1.5  0  4  3  0 FALSE  d      liberal   most weeks\n4    4  f 35 3.5 3.2 1500  8.0  5  5  6  3 FALSE  i     moderate occasionally\n5    5  m 23 3.1 3.5 1600 10.0  6  6  3  0 FALSE  i very liberal        never\n6    6  m 39 3.5 3.5  350  3.0  4  5  7  0 FALSE  d      liberal occasionally\n     ab    aa    ld\n1 FALSE FALSE FALSE\n2 FALSE FALSE    NA\n3 FALSE FALSE    NA\n4 FALSE FALSE FALSE\n5 FALSE FALSE FALSE\n6 FALSE FALSE    NA\n\n\nCode\nsummary(student.survey)\n\n\n      subj       ge           ag              hi              co       \n Min.   : 1.00   f:31   Min.   :22.00   Min.   :2.000   Min.   :2.600  \n 1st Qu.:15.75   m:29   1st Qu.:24.00   1st Qu.:3.000   1st Qu.:3.175  \n Median :30.50          Median :26.50   Median :3.350   Median :3.500  \n Mean   :30.50          Mean   :29.17   Mean   :3.308   Mean   :3.453  \n 3rd Qu.:45.25          3rd Qu.:31.00   3rd Qu.:3.625   3rd Qu.:3.725  \n Max.   :60.00          Max.   :71.00   Max.   :4.000   Max.   :4.000  \n                                                                       \n       dh             dr               tv               sp        \n Min.   :   0   Min.   : 0.200   Min.   : 0.000   Min.   : 0.000  \n 1st Qu.: 205   1st Qu.: 1.450   1st Qu.: 3.000   1st Qu.: 3.000  \n Median : 640   Median : 2.000   Median : 6.000   Median : 5.000  \n Mean   :1232   Mean   : 3.818   Mean   : 7.267   Mean   : 5.483  \n 3rd Qu.:1350   3rd Qu.: 5.000   3rd Qu.:10.000   3rd Qu.: 7.000  \n Max.   :8000   Max.   :20.000   Max.   :37.000   Max.   :16.000  \n                                                                  \n       ne               ah             ve          pa    \n Min.   : 0.000   Min.   : 0.000   Mode :logical   d:21  \n 1st Qu.: 2.000   1st Qu.: 0.000   FALSE:60        i:24  \n Median : 3.000   Median : 0.500                   r:15  \n Mean   : 4.083   Mean   : 1.433                         \n 3rd Qu.: 5.250   3rd Qu.: 2.000                         \n Max.   :14.000   Max.   :11.000                         \n                                                         \n                     pi                re         ab              aa         \n very liberal         : 8   never       :15   Mode :logical   Mode :logical  \n liberal              :24   occasionally:29   FALSE:60        FALSE:59       \n slightly liberal     : 6   most weeks  : 7                   NA's :1        \n moderate             :10   every week  : 9                                  \n slightly conservative: 6                                                    \n conservative         : 4                                                    \n very conservative    : 2                                                    \n     ld         \n Mode :logical  \n FALSE:44       \n NA's :16       \n                \n                \n                \n                \n\n\nCode\n##?student.survey\n\n\n\ny = political ideology and x = religiosity,\n\nPolitical ideology and religiosity are currently coded as ordinal variables.\nThere are 7 levels of political idealogy: - very liberal\n- liberal - slightly liberal - moderate - slightly conservative - conservative - very conservative\nThere are 4 levels of religiosity: - never - occasionally - most weeks - every week\n\n\nCode\n#Scatterplot Religiousity vs. Political Ideology\nggplot(data = student.survey, aes(x = re , y = pi)) +   geom_smooth(method = 'lm', se=F) +  geom_point() + labs(title = \"Scatterplot: Religiousity vs. Political Ideology\", x = \"Religiousity\", y = \"Political Ideology\")\n\n\n`geom_smooth()` using formula 'y ~ x'\n\n\n\n\n\nFrom the scatter-plot there appears to be a fairly strong positive correlation between religiosity and political ideology, with the more often someone attends religious services the more conservative their political ideology. A scatter plot model suggests that both ordinal scales have a fixed distance between responses, which is not correct for either variable.\nTo supplement the scatter plot I also created a bar graph to\n\n\nCode\n#Bargraph pi vs re\n\nggplot(data = student.survey, aes(x = re, fill = pi)) + geom_bar(position = \"fill\") + labs(title = \"Bar Graph: Religiousity vs. Political Ideology\", x = \"Religiousity\", y = \"Political Ideology\")\n\n\n\n\n\nThe bar graph shows a high concentration of people who never or occasionally attend religious services were mostly liberal (indigo-purple), whereas people who report attending religious services most weeks were more conservative (green). People who reported attending religious services every week had mostly conservative (green) and very conservative (yellow) political idealogies.\n\ny = high school GPA and x = hours of TV watching.\n\n\n\nCode\n#scatterplot hi vs. tv\nggplot(data = df, aes(x = tv , y = hi)) +   geom_point() +  geom_smooth(method = 'lm', se=F) + labs(title = \"Scatterplot Highschool GPA vs. Hours of tv watched\", x = \"TV(avg hours per week)\", y = \"High School GPA\")\n\n\nError in `ggplot()`:\n!   You're passing a function as global data.\n  Have you misspelled the `data` argument in `ggplot()`\n\n\nA scatter plot works well for modeling two numeric variables, high school GPA & avg hours of tv watched per week. Looking at the data fitted with a trend line association between most data points are above of below the trend line and in the upper left corner of the scatter plot, suggesting most students watch been 0-10 hours of TV have a GPA of 3.0 or above.\nIt does not look like from the scatter plot there is an association between these variables. The trend line suggests if there is a correlation between the variables, it is negative in nature, with more hours of TV correlating with lower high school GPA. This will be investigated further in part b."
  },
  {
    "objectID": "posts/HW3_EmmaNarkewicz.html#b-summarize-and-interpret-results-of-inferential-analyses.",
    "href": "posts/HW3_EmmaNarkewicz.html#b-summarize-and-interpret-results-of-inferential-analyses.",
    "title": "Homework 3",
    "section": "b Summarize and interpret results of inferential analyses.",
    "text": "b Summarize and interpret results of inferential analyses.\n\ny = political ideology and x = religiosity,\n\nIn order to create a linear regression model, both ordinal variables needed to be converted to numeric type. The resulting model suggests\n\n\nCode\n#Linear regression of pi & re\nlm_pi &lt;-lm(as.numeric(pi) ~ as.numeric(re) , data = student.survey)\nsummary(lm_pi) \n\n\n\nCall:\nlm(formula = as.numeric(pi) ~ as.numeric(re), data = student.survey)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.81243 -0.87160  0.09882  1.12840  3.09882 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      0.9308     0.4252   2.189   0.0327 *  \nas.numeric(re)   0.9704     0.1792   5.416 1.22e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.345 on 58 degrees of freedom\nMultiple R-squared:  0.3359,    Adjusted R-squared:  0.3244 \nF-statistic: 29.34 on 1 and 58 DF,  p-value: 1.221e-06\n\n\nThe linear regression model has a positive coefficient of 0.97, suggesting for “one unit” increase in religiosity there is nearly a “one unit” increase in political ideology. One unit is in quotes because these variables are ordinal and there isn’t actually fixed distance in units between them. Nonetheless, there is a positive, statistically significant correlation (p value &lt; 0.001). The adjusted R-squared value is 0.3244 which is weak-moderate suggests there is a correlation between the two variables\n\ny = high school GPA and x = hours of TV watching. ,\n\n\n\nCode\n#linear regression model gpa vs tv\nlm_gpa &lt;-lm(hi ~ tv , data = student.survey) \nsummary(lm_gpa)\n\n\n\nCall:\nlm(formula = hi ~ tv, data = student.survey)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.2583 -0.2456  0.0417  0.3368  0.7051 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  3.441353   0.085345  40.323   &lt;2e-16 ***\ntv          -0.018305   0.008658  -2.114   0.0388 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4467 on 58 degrees of freedom\nMultiple R-squared:  0.07156,   Adjusted R-squared:  0.05555 \nF-statistic: 4.471 on 1 and 58 DF,  p-value: 0.03879\n\n\nThe linear regression shows a negative correlation between hours of tv watched and high school GPA that is statistically significant at the 0.05 level (p = 0.0388), with ever hour of tv watched a week resulting in a 0.018 reduction in high school GPA. Looking at the adjusted R-squared of 0.05555 is extremely low, suggesting that there is not strong association between the 2 variables. A linear model does not appear to be a good fit for this data."
  },
  {
    "objectID": "posts/Kristin_Abijaoude_FinalCheckIn2.html",
    "href": "posts/Kristin_Abijaoude_FinalCheckIn2.html",
    "title": "Final Project Check in 2",
    "section": "",
    "text": "Code\n# load packages\npackages &lt;- c(\"readr\", \"readxl\", \"summarytools\", \"tidyverse\", \"dplyr\", \"cars\")\nlapply(packages, require, character.only = TRUE)\n\n\nLoading required package: readr\n\n\nLoading required package: readxl\n\n\nLoading required package: summarytools\n\n\nLoading required package: tidyverse\n\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.0     ✔ dplyr   1.1.0\n✔ tibble  3.1.8     ✔ stringr 1.5.0\n✔ tidyr   1.3.0     ✔ forcats 0.5.2\n✔ purrr   1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n✖ tibble::view()  masks summarytools::view()\nLoading required package: cars\n\n\nWarning in library(package, lib.loc = lib.loc, character.only = TRUE,\nlogical.return = TRUE, : there is no package called 'cars'\n\n\n[[1]]\n[1] TRUE\n\n[[2]]\n[1] TRUE\n\n[[3]]\n[1] TRUE\n\n[[4]]\n[1] TRUE\n\n[[5]]\n[1] TRUE\n\n[[6]]\n[1] FALSE\n\n\nCode\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/Kristin_Abijaoude_FinalCheckIn2.html#overview",
    "href": "posts/Kristin_Abijaoude_FinalCheckIn2.html#overview",
    "title": "Final Project Check in 2",
    "section": "Overview",
    "text": "Overview\nBullying continues to be a persistent problem in schools.\nTypes of bullying faced by those affected include physical fights, exclusion, rumors, snarky “jokes”, and name-calling. Every bullied student dreads going to school because they have to face their bullies, who would find any reason, or no reason at all, to target them. Bullying can happen outside of school, especially with today’s advanced technology and near-universal access to the Internet. While students are always encouraged to tell a trusted adult, such as a teacher, trusted adults in authority have a spotty record when it comes to tackling this epidemic.\nIn the US alone, one of every five students report being bullied on school grounds, including name-calling (13% among those who reported bullying), being pushed or shoved (5%), or have property destroyed on purpose (1%). 15% of students who reported bullying were cyberbullied 1. Globally, one in three students report bullying, from as low as 7% in the Central Asian country of Tajikistan to as high as 74% in Samoa.2\nThe negative effects on bullying include low self-esteem, feeling angry or isolated, and distress, as well as physical effects like loss of sleep, headaches, and disordered eating. Bullying can be so detrimental to the victim that they take their own life to escape the pain.3\nWhen discussing ways to combat bullying, it’s too simplistic to say that “kids are just cruel”. My purpose is to find why some students are more vulnerable to being targets of bullying, and how we can use those parameters to create solutions to end bullying once and for all."
  },
  {
    "objectID": "posts/Kristin_Abijaoude_FinalCheckIn2.html#hypotheses-and-proposed-models",
    "href": "posts/Kristin_Abijaoude_FinalCheckIn2.html#hypotheses-and-proposed-models",
    "title": "Final Project Check in 2",
    "section": "Hypotheses and Proposed Models",
    "text": "Hypotheses and Proposed Models\nI will specify which model to test out each of my hypotheses. In this project, I will use these variables to explore a relationship between those variables and bullying.\n\nHa: Students who report loneliness and fewer friends are more vulnerable of being targets or bullying.\nHa: Male students are more likely than female students to face physical abuse by bullies.\nHa: More female students who report bullying are targeted for being underweight, while male students who report bullying are targeted for being overweight.\nHa: Students who face more physical attacks on school grounds are more likely to miss school.\nHa: Students in primary school tend to be more enganged in some form of physical attacks than students in secondary school."
  },
  {
    "objectID": "posts/Kristin_Abijaoude_FinalCheckIn2.html#data-summary",
    "href": "posts/Kristin_Abijaoude_FinalCheckIn2.html#data-summary",
    "title": "Final Project Check in 2",
    "section": "Data Summary",
    "text": "Data Summary\n\n\nCode\nbullying &lt;- read_xlsx(\"_data/Bullying.xlsx\",\n                   range = cell_rows(2:56982))\nbullying\n\n\n# A tibble: 56,980 × 18\n   record Bullie…¹ Bulli…² Cyber…³ Custo…⁴ Sex   Physi…⁵ Physi…⁶ Felt_…⁷ Close…⁸\n    &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;  \n 1      1 Yes      Yes     &lt;NA&gt;    13 yea… Fema… 0 times 0 times Always  2      \n 2      2 No       No      No      13 yea… Fema… 0 times 0 times Never   3 or m…\n 3      3 No       No      No      14 yea… Male  0 times 0 times Never   3 or m…\n 4      4 No       No      No      16 yea… Male  0 times 2 or 3… Never   3 or m…\n 5      5 No       No      No      13 yea… Fema… 0 times 0 times Rarely  3 or m…\n 6      6 No       No      No      13 yea… Male  0 times 1 time  Never   3 or m…\n 7      7 No       No      No      14 yea… Fema… 1 time  0 times Someti… 3 or m…\n 8      8 No       No      No      12 yea… Fema… 0 times 0 times Rarely  3 or m…\n 9      9 No       No      No      13 yea… Male  1 time  2 or 3… Never   3 or m…\n10     10 Yes      No      No      14 yea… Fema… 0 times 0 times Always  0      \n# … with 56,970 more rows, 8 more variables: Miss_school_no_permission &lt;chr&gt;,\n#   Other_students_kind_and_helpful &lt;chr&gt;, Parents_understand_problems &lt;chr&gt;,\n#   Most_of_the_time_or_always_felt_lonely &lt;chr&gt;,\n#   Missed_classes_or_school_without_permission &lt;chr&gt;, Were_underweight &lt;chr&gt;,\n#   Were_overweight &lt;chr&gt;, Were_obese &lt;chr&gt;, and abbreviated variable names\n#   ¹​Bullied_on_school_property_in_past_12_months,\n#   ²​Bullied_not_on_school_property_in_past_12_months, …\n\n\nThis 2018 study was conducted by Global School-Based Student Health Survey (GSHS), where 56,981 students from Argentina participated by filling out the questionnaire in regards to their mental health and behavior.4\n\n\nCode\ndim(bullying) # 56980 rows and 18 columns\n\n\n[1] 56980    18\n\n\n\n\nCode\nprint(dfSummary(bullying,\n                        varnumbers = FALSE,\n                        plain.ascii  = FALSE, \n                        style        = \"grid\", \n                        graph.magnif = 0.70, \n                        valid.col    = FALSE),\n      method = 'render',\n      table.classes = 'table-condensed')\n\n\n\nData Frame Summary\nbullying\nDimensions: 56980 x 18\n  Duplicates: 0\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nStats / Values\nFreqs (% of Valid)\nGraph\nMissing\n\n\n\n\nrecord [numeric]\n\n\n\nMean (sd) : 28534.9 (16479.7)\n\n\nmin ≤ med ≤ max:\n\n\n1 ≤ 28521.5 ≤ 57094\n\n\nIQR (CV) : 28540.5 (0.6)\n\n\n\n56980 distinct values\n\n0 (0.0%)\n\n\nBullied_on_school_property_in_past_12_months [character]\n\n\n\n1. No\n\n\n2. Yes\n\n\n\n\n\n\n43838\n(\n78.6%\n)\n\n\n11903\n(\n21.4%\n)\n\n\n\n\n1239 (2.2%)\n\n\nBullied_not_on_school_property_in_past_12_months [character]\n\n\n\n1. No\n\n\n2. Yes\n\n\n\n\n\n\n44263\n(\n78.4%\n)\n\n\n12228\n(\n21.6%\n)\n\n\n\n\n489 (0.9%)\n\n\nCyber_bullied_in_past_12_months [character]\n\n\n\n1. No\n\n\n2. Yes\n\n\n\n\n\n\n44213\n(\n78.4%\n)\n\n\n12196\n(\n21.6%\n)\n\n\n\n\n571 (1.0%)\n\n\nCustom_Age [character]\n\n\n\n1. 11 years old or younger\n\n\n2. 12 years old\n\n\n3. 13 years old\n\n\n4. 14 years old\n\n\n5. 15 years old\n\n\n6. 16 years old\n\n\n7. 17 years old\n\n\n8. 18 years old or older\n\n\n\n\n\n\n48\n(\n0.1%\n)\n\n\n145\n(\n0.3%\n)\n\n\n10574\n(\n18.6%\n)\n\n\n12946\n(\n22.8%\n)\n\n\n12812\n(\n22.5%\n)\n\n\n11737\n(\n20.6%\n)\n\n\n8227\n(\n14.5%\n)\n\n\n383\n(\n0.7%\n)\n\n\n\n\n108 (0.2%)\n\n\nSex [character]\n\n\n\n1. Female\n\n\n2. Male\n\n\n\n\n\n\n29361\n(\n52.0%\n)\n\n\n27083\n(\n48.0%\n)\n\n\n\n\n536 (0.9%)\n\n\nPhysically_attacked [character]\n\n\n\n1. 0 times\n\n\n2. 1 time\n\n\n3. 10 or 11 times\n\n\n4. 12 or more times\n\n\n5. 2 or 3 times\n\n\n6. 4 or 5 times\n\n\n7. 6 or 7 times\n\n\n8. 8 or 9 times\n\n\n\n\n\n\n46996\n(\n82.8%\n)\n\n\n5248\n(\n9.2%\n)\n\n\n115\n(\n0.2%\n)\n\n\n790\n(\n1.4%\n)\n\n\n2405\n(\n4.2%\n)\n\n\n695\n(\n1.2%\n)\n\n\n302\n(\n0.5%\n)\n\n\n189\n(\n0.3%\n)\n\n\n\n\n240 (0.4%)\n\n\nPhysical_fighting [character]\n\n\n\n1. 0 times\n\n\n2. 1 time\n\n\n3. 10 or 11 times\n\n\n4. 12 or more times\n\n\n5. 2 or 3 times\n\n\n6. 4 or 5 times\n\n\n7. 6 or 7 times\n\n\n8. 8 or 9 times\n\n\n\n\n\n\n43245\n(\n76.3%\n)\n\n\n6932\n(\n12.2%\n)\n\n\n165\n(\n0.3%\n)\n\n\n939\n(\n1.7%\n)\n\n\n3650\n(\n6.4%\n)\n\n\n1028\n(\n1.8%\n)\n\n\n489\n(\n0.9%\n)\n\n\n264\n(\n0.5%\n)\n\n\n\n\n268 (0.5%)\n\n\nFelt_lonely [character]\n\n\n\n1. Always\n\n\n2. Most of the time\n\n\n3. Never\n\n\n4. Rarely\n\n\n5. Sometimes\n\n\n\n\n\n\n3120\n(\n5.5%\n)\n\n\n6422\n(\n11.3%\n)\n\n\n17931\n(\n31.7%\n)\n\n\n14427\n(\n25.5%\n)\n\n\n14714\n(\n26.0%\n)\n\n\n\n\n366 (0.6%)\n\n\nClose_friends [character]\n\n\n\n1. 0\n\n\n2. 1\n\n\n3. 2\n\n\n4. 3 or more\n\n\n\n\n\n\n3331\n(\n6.0%\n)\n\n\n4732\n(\n8.5%\n)\n\n\n9110\n(\n16.3%\n)\n\n\n38731\n(\n69.3%\n)\n\n\n\n\n1076 (1.9%)\n\n\nMiss_school_no_permission [character]\n\n\n\n1. 0 days\n\n\n2. 1 or 2 days\n\n\n3. 10 or more days\n\n\n4. 3 to 5 days\n\n\n5. 6 to 9 days\n\n\n\n\n\n\n38654\n(\n70.1%\n)\n\n\n9738\n(\n17.7%\n)\n\n\n1468\n(\n2.7%\n)\n\n\n3925\n(\n7.1%\n)\n\n\n1331\n(\n2.4%\n)\n\n\n\n\n1864 (3.3%)\n\n\nOther_students_kind_and_helpful [character]\n\n\n\n1. Always\n\n\n2. Most of the time\n\n\n3. Never\n\n\n4. Rarely\n\n\n5. Sometimes\n\n\n\n\n\n\n9710\n(\n17.5%\n)\n\n\n15820\n(\n28.5%\n)\n\n\n4775\n(\n8.6%\n)\n\n\n10966\n(\n19.8%\n)\n\n\n14150\n(\n25.5%\n)\n\n\n\n\n1559 (2.7%)\n\n\nParents_understand_problems [character]\n\n\n\n1. Always\n\n\n2. Most of the time\n\n\n3. Never\n\n\n4. Rarely\n\n\n5. Sometimes\n\n\n\n\n\n\n13072\n(\n23.9%\n)\n\n\n9570\n(\n17.5%\n)\n\n\n11964\n(\n21.9%\n)\n\n\n10459\n(\n19.2%\n)\n\n\n9542\n(\n17.5%\n)\n\n\n\n\n2373 (4.2%)\n\n\nMost_of_the_time_or_always_felt_lonely [character]\n\n\n\n1. No\n\n\n2. Yes\n\n\n\n\n\n\n47072\n(\n83.1%\n)\n\n\n9542\n(\n16.9%\n)\n\n\n\n\n366 (0.6%)\n\n\nMissed_classes_or_school_without_permission [character]\n\n\n\n1. No\n\n\n2. Yes\n\n\n\n\n\n\n38654\n(\n70.1%\n)\n\n\n16462\n(\n29.9%\n)\n\n\n\n\n1864 (3.3%)\n\n\nWere_underweight [character]\n\n\n\n1. No\n\n\n2. Yes\n\n\n\n\n\n\n35318\n(\n98.0%\n)\n\n\n733\n(\n2.0%\n)\n\n\n\n\n20929 (36.7%)\n\n\nWere_overweight [character]\n\n\n\n1. No\n\n\n2. Yes\n\n\n\n\n\n\n25376\n(\n70.4%\n)\n\n\n10675\n(\n29.6%\n)\n\n\n\n\n20929 (36.7%)\n\n\nWere_obese [character]\n\n\n\n1. No\n\n\n2. Yes\n\n\n\n\n\n\n33396\n(\n92.6%\n)\n\n\n2655\n(\n7.4%\n)\n\n\n\n\n20929 (36.7%)\n\n\n\n\nGenerated by summarytools 1.0.1 (R version 4.2.2)2023-04-23"
  },
  {
    "objectID": "posts/Kristin_Abijaoude_FinalCheckIn2.html#footnotes",
    "href": "posts/Kristin_Abijaoude_FinalCheckIn2.html#footnotes",
    "title": "Final Project Check in 2",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nhttps://www.stopbullying.gov/resources/facts↩︎\nhttp://uis.unesco.org/en/news/new-sdg-4-data-bullying↩︎\nhttps://www.ncbi.nlm.nih.gov/books/NBK390414/↩︎\nhttps://www.kaggle.com/datasets/leomartinelli/bullying-in-schools?datasetId=2952457↩︎\nhttps://en.wikipedia.org/wiki/Education_in_Argentina↩︎"
  },
  {
    "objectID": "posts/DerianToth_M_HW1.html",
    "href": "posts/DerianToth_M_HW1.html",
    "title": "Homework_One",
    "section": "",
    "text": "Question 1\n\n(1a) What does the distribution of LungCap look like?\nFirst, let’s read in the data from the Excel file:\n\n\nCode\nlibrary(\"quarto\")\nlibrary(\"tidyverse\")\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.0     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.1     ✔ tibble    3.1.8\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the \u001b]8;;http://conflicted.r-lib.org/\u0007conflicted package\u001b]8;;\u0007 to force all conflicts to become errors\n\n\nCode\nlibrary(\"palmerpenguins\")\nlibrary(readxl)\nlibrary(dplyr)\nlibrary(ggplot2)\ndf &lt;- read_excel(\"_data/LungCapData.xls\")\n#View(df)\n\n\nThe distribution of LungCap looks as follows:\n\n\nCode\nhist(df$LungCap)\n\n\n\n\n\nThe histogram suggests that the distribution is close to a normal distribution. Most of the observations are close to the mean. Very few observations are close to the margins (0 and 15).\n\n\n(1b) The probability distribution of the LungCap with respect to gender is as follows:\n\n\nCode\nboxplot(df$LungCap ~ df$Gender)\n\n\n\n\n\n\n\n(1c) The mean lung capacities for smokers and non-smokers can be found in the table below:\n\n\nCode\ndf %&gt;%\n  group_by(Smoke) %&gt;%\n  summarise_at(vars(LungCap), list(name = mean))\n\n\n# A tibble: 2 × 2\n  Smoke  name\n  &lt;chr&gt; &lt;dbl&gt;\n1 no     7.77\n2 yes    8.65\n\n\nThese means are not what I would expect. It looks like those who smoke (“yes”) have a higher long capacity (8.65) than those who do not smoke (7.77).\n\n\n(1d) The relationship between Smoking and Lung Capacity within age groups\n\n\nCode\n#Age groups defined by:\n#“less than or\n#equal to 13”, \n#“14 to 15”, \n#“16 to 17”, \n#“greater than or equal to 18”.\n\n# Create variable\ndf &lt;- df %&gt;% \n  mutate(age_group = case_when(\n      Age &lt;= 13 ~ \"0-13\",\n      Age &gt; 13 & Age &lt; 16 ~ \"14-15\",\n      Age &gt; 15 & Age &lt; 18 ~ \"16-18\",\n      Age &gt;= 18 ~ \"&gt;= 18\"),\n    # Convert to factor\n    age_group = factor(\n      age_group,\n      level = c(\"0-13\", \"14-15\",\"16-18\", \"&gt;= 18\")))\n\nView(df)\n\ndf %&gt;%\n  group_by(age_group,Smoke) %&gt;%\n  summarise_at(vars(LungCap), list(name = mean))\n\n\n# A tibble: 8 × 3\n# Groups:   age_group [4]\n  age_group Smoke  name\n  &lt;fct&gt;     &lt;chr&gt; &lt;dbl&gt;\n1 0-13      no     6.36\n2 0-13      yes    7.20\n3 14-15     no     9.14\n4 14-15     yes    8.39\n5 16-18     no    10.5 \n6 16-18     yes    9.38\n7 &gt;= 18     no    11.1 \n8 &gt;= 18     yes   10.5 \n\n\nCode\ndbinom(x=8,size=8,prob=.5)\n\n\n[1] 0.00390625\n\n\nCode\ndbinom(x=6,size=8,prob=.5)\n\n\n[1] 0.109375\n\n\n\n\n(1e) Compare the lung capacities for smokers and non-smokers within each age group.\n\n\nCode\nggplot(df, aes(x=age_group, y=LungCap, color = Smoke)) +\n  geom_boxplot()\n\n\n\n\n\nThis data visualization makes more sense for what we expect from lung capacity when comparing smokers to non smokers. It looks like lunch capacity increases as the participants get older. The data could have more participants who are smokers and who are older. This unbalance in participants could be skewing the overall average lunch capacity.\n\n\n\nQuestion 2:Setting up the Dataframe\n\n\nCode\nStatePrison &lt;- data.frame(number_convictions = 0:4, InMateCount = c(128, 434, 160, 64, 24)) %&gt;%\n                            mutate(Probability = InMateCount/810)\n\nView(StatePrison)\n\n\n\n(2a) What is the probability that a randomly selected inmate has exactly 2 prior convictions?\n\n\nCode\ndbinom(x = 1, size = 1, p = 160/810)\n\n\n[1] 0.1975309\n\n\n\n\n(2b) What is the probability that a randomly selected inmate has fewer than 2 prior convictions?\n\n\nCode\ndbinom(x = 1, size = 1, p = sum(128+434)/810)\n\n\n[1] 0.6938272\n\n\n\n\n(2c) What is the probability that a randomly selected inmate has 2 or fewer prior convictions?\n\n\nCode\ndbinom(x = 1, size = 1, p = sum(128+434+160)/810)\n\n\n[1] 0.891358\n\n\n\n\n(2d) What is the probability that a randomly selected inmate has more than 2 prior convictions?\n\n\nCode\ndbinom(x = 1, size = 1, p = sum(64+24)/810)\n\n\n[1] 0.108642\n\n\n\n\n(2e) What is the expected value for the number of prior convictions?\n\n\nCode\nEV &lt;- sum(StatePrison$number_convictions *StatePrison$Probability)\nprint(EV)\n\n\n[1] 1.28642\n\n\n\n\n(2f) Calculate the variance and the standard deviation for the Prior Convictions.\n\n\nCode\nVar &lt;- sum((StatePrison$number_convictions - EV) ^ 2 * StatePrison$Probability)\n\nprint(Var)\n\n\n[1] 0.8562353\n\n\n\n\nCode\nSD &lt;- sqrt(Var)\n\nprint(SD)\n\n\n[1] 0.9253298"
  },
  {
    "objectID": "posts/JustineShakespeare_FinalProject.html",
    "href": "posts/JustineShakespeare_FinalProject.html",
    "title": "Maternal Mortality and Women’s Political Inclusion",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(summarytools)\nlibrary(maps)\nlibrary(viridis)\nlibrary(stargazer)\nlibrary(GGally)\nlibrary(kableExtra)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/JustineShakespeare_FinalProject.html#research-question",
    "href": "posts/JustineShakespeare_FinalProject.html#research-question",
    "title": "Maternal Mortality and Women’s Political Inclusion",
    "section": "Research Question",
    "text": "Research Question\nDespite advancements in health and technology, maternal mortality remains a significant public health challenge around the world. According to the World Health Organization, approximately 287,000 women died from complications related to pregnancy or childbirth in 2020.1 Recently it was reported that global declines in maternal mortality have stalled,2 even increased in the United States,3 and data from 2020 has shown the pandemic brought a sharp uptick in maternal deaths.4 The World Health Organization has stated that the majority of maternal deaths are preventable.5\nPast research has investigated factors associated with higher rates of maternal mortality, both at the level of the individual and country. Many studies have found that factors associated with maternal healthcare, such as the presence of a skilled attendant at birth and antenatal visits, are strongly associated with maternal mortality.6 7 Other studies considering social determinants of health have found that factors such as adult literacy, education, income levels, and access to water and sanitation are all associated with maternal mortality.8 9 The majority of maternal deaths occur in low-income countries.10\nThis research intends to explore the relationship between women’s political inclusion and maternal mortality, while controlling for confounding variables related to the development of a nation’s economy and health sector. In addition, this research will examine the mechanism through which women’s political inclusion might impact maternal mortality by considering the percentage of skilled attendants at birth as a mediator variable.\nPast research has found a link between higher levels of gender equality and women’s political inclusion and reduced maternal mortality, although analysis has primarily focused on individual countries or a subset of countries (typically lower-income).11 12 This research will investigate whether these trends hold true when looking at over 150 countries from all levels of economic development.\nIn short, this analysis seeks to answer the question: Does women’s political inclusion and empowerment have an effect on maternal mortality?"
  },
  {
    "objectID": "posts/JustineShakespeare_FinalProject.html#hypotheses",
    "href": "posts/JustineShakespeare_FinalProject.html#hypotheses",
    "title": "Maternal Mortality and Women’s Political Inclusion",
    "section": "Hypotheses",
    "text": "Hypotheses\nHypothesis 1: Where gender exclusion is low and political inclusion of women is high, health outcomes are better and maternal mortality is low. This will hold true even when controlling for variables which are known to have an effect on both women’s political inclusion and maternal mortality.\nHypothesis 2: The political inclusion and empowerment of women has a positive effect on the presence of skilled attendants at birth, which in turn has the effect of reducing maternal mortality. In other words, the relationship between gender exclusion and maternal mortality is mediated by the presence of skilled attendants at birth.\nIn investigating these two hypotheses, this analysis will explore the link between the political inclusion of women and reduced maternal mortality and try to provide insight into the mechanism through which this occurs."
  },
  {
    "objectID": "posts/JustineShakespeare_FinalProject.html#introduction-to-the-data",
    "href": "posts/JustineShakespeare_FinalProject.html#introduction-to-the-data",
    "title": "Maternal Mortality and Women’s Political Inclusion",
    "section": "Introduction to the Data",
    "text": "Introduction to the Data\nThis research will utilize several datasets in order to capture all of the relevant variables.\nThe dependent variable, Maternal Mortality Ratio, is drawn from the Maternal and newborn health table from UNICEF. This data is from 2017.13\n\n\n\n\n\n\nDefinitions\n\n\n\nAccording to the World Health Organization, the maternal mortality ratio (MMR) is defined as: “the number of maternal deaths during a given time period per 100,000 live births during the same time period.”\nMaternal deaths are defined as: “deaths from any cause related to or aggravated by pregnancy or its management (excluding accidental or incidental causes) during pregnancy and childbirth or within 42 days of termination of pregnancy…”14\n\n\nThe explanatory variable featured in this analysis will be drawn from the Varieties of Democracy V-Dem dataset.15 From there, the following variable will be used:\n\n(Political) exclusion by gender index16\nAs defined by the V-Dem codebook: “Exclusion is when individuals are denied access to services or participation in governed spaces (spaces that are part of the public space and the government should regulate, while excluding private spaces and organizations except when exclusion in those private spheres is linked to exclusion in the public sphere) based on their identity or belonging to a particular group.”17 This index is formed from four other indicators from V-Dem: power distributed by gender, equality in respect for civil liberites by gender, access to public services by gender, access to state jobs by gender, and access to state business opportunities by gender.\n\nThis variable is on an interval scale, from low to high (0-1). The codebook notes that the point estimates are reversed here, so unlike most of the variables from V-Dem, for this variable the “lower scores indicate a normatively better situation…and higher scores a normatively worse situation”.18 So higher values of this variable indicate more gender exclusion and less political inclusion of women. In order to match the timeframe of the dependent variable, the 2017 values have been used for this analysis.\nIn addition to the dependent and explanatory variables, a mediator variable will also be considered in this analysis. As noted above, research has shown that the presence of skilled attendants at birth has a negative effect on maternal mortality.19 The second hypothesis of this research is that more political inclusion of women will have the effect of increasing the percentage of births with a skilled attendant, which will in turn reduce maternal mortality. This research will include a variable from the same Maternal and newborn health table from UNICEF that the maternal mortality data was drawn from.20 This mediator variable is the percentage of births that occur with a skilled attendant in each country.\nThree confounding variables that seek to account for the economic and political development of a country will be considered in order to control for their effect on both maternal mortality and women’s political exclusion.\nGDP per capita will be used as a way to control for a nation’s economic development. This variable is drawn from the International Monetary Fund (IMF)’s data.21 GDP per capita from 2017 will be used in order to be consistent with the rest of the data.\nIn addition to GDP, this analysis will also include a confounding variable on the government expenditure on health as a percentage of GDP. This will be drawn from the Economic Indicators table from UNICEF. This data is from 2017.22\nFinally, a variable capturing the level of democracy in a country will be used as a confounding variable in this analysis. This variable was drawn from the V-Dems data.23 According to the codebook, this variable was “computed by subtracting the autocracy score from the democracy score. The resulting unified POLITY scale ranges from +10 (strongly democratic) to -10 (strongly autocratic).”24 This variable will see to control for social and political elements that may effect both gender exclusion and maternal mortality.\nThe resulting data used for analysis will be a cross-sectional, cross-national sample of data from 155 countries (some observations were removed because of missing values) on all of the variables noted here: maternal mortality, skilled birth attendants, gender exclusion, GDP, percentage of GDP spent on health, and democracy."
  },
  {
    "objectID": "posts/JustineShakespeare_FinalProject.html#loading-and-cleaning-the-data",
    "href": "posts/JustineShakespeare_FinalProject.html#loading-and-cleaning-the-data",
    "title": "Maternal Mortality and Women’s Political Inclusion",
    "section": "Loading and Cleaning the Data",
    "text": "Loading and Cleaning the Data\nData for this analysis will be drawn from multiple datasets. First, data from the Maternal and Newborn Health table from UNICEF for the explanatory variable and the confounding variable on skilled birth attendants will be loaded and cleaned. A second dataset from UNICEF will be loaded and joined with this one because it includes a world regions variable that will allow us to group and visualize the data. This variable will not be used in regression analysis.\n\n\nCode\n# Load the dataset, skip the first five rows, rename all of the variables we want and delete the ones we don't\nMaternalNewbornHealth &lt;- read_excel(\"UNICEF_mat_newborn.xlsx\", skip = 6, \n                                    col_names= c(\"Countries_and_areas\", rep(\"delete\", 8), \n                                                 \"ALOneVisit\", \"delete\", \"ALFourVisits\", \n                                                 rep(\"delete\", 3), \"SkilledBirthAtt\",\n                                                 rep(\"delete\", 13),\n                                                 \"Maternal_Mortality_Ratio_2017\", \"delete\", \n                                                 \"delete\")) %&gt;% \n  select(!contains(\"delete\"))\n\n# remove the last 57 rows, which include summaries and endnotes, not data\nMNH &lt;- slice(MaternalNewbornHealth, 1:(nrow(MaternalNewbornHealth) - 57)) \n\n# instead of NAs, this dataset has \" - \", we will remove them here\nMNH &lt;- MNH %&gt;% filter(!str_detect(Maternal_Mortality_Ratio_2017, \"-\"))\nMNH &lt;- MNH %&gt;% filter(!str_detect(SkilledBirthAtt, \"-\"))\n\nMNH$`SkilledBirthAtt` &lt;- as.numeric(MNH$`SkilledBirthAtt`)\nMNH$`Maternal_Mortality_Ratio_2017` &lt;- as.numeric(MNH$`Maternal_Mortality_Ratio_2017`)\n# Turning everything numeric\n\n# In order to disaggregate and reorder data by region and by developmental area, we'll join this data with another UNICEF dataset with those variables (region and sub-region).\nRegions &lt;- read_excel(\"Completion_rate_8.xlsx\", sheet = 2, skip = 2, \n                    col_names= c(\"delete\", \"Countries_and_areas\", \"Region\", \n                                 \"Sub-Region\", rep(\"delete\", 19))) %&gt;% \n  select(!contains(\"delete\"))\n\n# remove the last 34 rows, which include summaries and endnotes, not data\nRegions &lt;- slice(Regions, 1:(nrow(Regions) - 34))\n\n# Join regions with the MNH data\nMNH_wRegion &lt;- full_join(MNH, Regions, by = \"Countries_and_areas\") \n\n# remove the countries where we had no MMR data\nMNH_wRegion &lt;- slice(MNH_wRegion, 1:(nrow(MNH_wRegion) - 25))\n\n# Recode the values in the region and subregion variables. Recode development variables so we can possibly use it in lm as numeric variable\nMNH_final &lt;- MNH_wRegion %&gt;% \n  mutate(\"WorldRegion\" = case_when(\n    Region == \"EAP\" ~ \"East Asia & Pacific\",\n    Region == \"ECA\" ~ \"Europe & Central Asia\",\n    Region == \"LAC\" ~ \"Latin America & Caribbean\",\n    Region == \"MENA\" ~ \"Middle East & North Africa\",\n    Region == \"NA\" ~ \"North America\",\n    Region == \"SA\" ~ \"South Asia\",\n    Region == \"SSA\" ~ \"Sub-Saharan Africa\")) %&gt;% \n  mutate(\"SubRegion\" = case_when(\n    `Sub-Region` == \"WE\" ~ \"Western Europe\", \n    `Sub-Region` == \"WCA\" ~ \"West & Central Africa\", \n    `Sub-Region` == \"ESA\" ~ \"Eastern & Southern Africa\", \n    `Sub-Region` == \"EECA\" ~ \"Eastern Europe & Central Asia\",\n    `Sub-Region` == \"EAP\" ~ \"East Asia & Pacific\",\n    `Sub-Region` == \"ECA\" ~ \"Europe & Central Asia\",\n    `Sub-Region` == \"LAC\" ~ \"Latin America & Caribbean\",\n    `Sub-Region` == \"MENA\" ~ \"Middle East & North Africa\",\n    `Sub-Region` == \"NA\" ~ \"North America\",\n    `Sub-Region` == \"SA\" ~ \"South Asia\",\n    `Sub-Region` == \"SSA\" ~ \"Sub-Saharan Africa\"))\n\n# select variables in the order we want for the final data frame, leave out abbreviated names\nMNH_final &lt;- select(MNH_final, Countries_and_areas, WorldRegion, SubRegion, Maternal_Mortality_Ratio_2017, SkilledBirthAtt)\n\nglimpse(MNH_final)\n\n\nRows: 178\nColumns: 5\n$ Countries_and_areas           &lt;chr&gt; \"Afghanistan\", \"Albania\", \"Algeria\", \"An…\n$ WorldRegion                   &lt;chr&gt; \"South Asia\", \"Europe & Central Asia\", \"…\n$ SubRegion                     &lt;chr&gt; \"South Asia\", \"Eastern Europe & Central …\n$ Maternal_Mortality_Ratio_2017 &lt;dbl&gt; 638, 15, 112, 241, 42, 39, 26, 6, 5, 26,…\n$ SkilledBirthAtt               &lt;dbl&gt; 58.8, 99.8, 98.8, 49.6, 100.0, 99.5, 99.…\n\n\nNext the Varieties of Democracy data will be added. It is from this data that the explanatory variable related to the political inclusion of women and the confounding variable related to democracy will be drawn.\n\n\nCode\nV_DEM &lt;- readRDS(\"V-Dem-CY-Full+Others-v13.rds\")\n\n# Select just the variables we need for this analysis and filter to year 2017.\nV_DEM_trim &lt;- V_DEM %&gt;% \n  filter(year == 2017) %&gt;% \n  select(country_name, e_p_polity, v2xpe_exlgender)\n\n# rename the country_name variable so we can join it with the UNICEF data later. \nV_DEM_trim &lt;- rename(V_DEM_trim, \"Countries_and_areas\" = country_name)\n\n# Change some of the country names where the V-Dem dataset differs from the \n# UNICEF data. \nV_DEM_trim$Countries_and_areas[V_DEM_trim$Countries_and_areas == \"Moldova\"] &lt;- \"Republic of Moldova\"\nV_DEM_trim$Countries_and_areas[V_DEM_trim$Countries_and_areas == \"Laos\"] &lt;- \"Lao People's Democratic Republic\"\nV_DEM_trim$Countries_and_areas[V_DEM_trim$Countries_and_areas == \"The Gambia\"] &lt;- \"Gambia\"\nV_DEM_trim$Countries_and_areas[V_DEM_trim$Countries_and_areas == \"Republic of the Congo\"] &lt;- \"Congo\"\nV_DEM_trim$Countries_and_areas[V_DEM_trim$Countries_and_areas == \"Syria\"] &lt;- \"Syrian Arab Republic\"\nV_DEM_trim$Countries_and_areas[V_DEM_trim$Countries_and_areas == \"Iran\"] &lt;- \"Iran (Islamic Republic of)\"\nV_DEM_trim$Countries_and_areas[V_DEM_trim$Countries_and_areas == \"Cape Verde\"] &lt;- \"Cabo Verde\"\nV_DEM_trim$Countries_and_areas[V_DEM_trim$Countries_and_areas == \"Ivory Coast\"] &lt;- \"Côte d'Ivoire\"\nV_DEM_trim$Countries_and_areas[V_DEM_trim$Countries_and_areas == \"Venezuela\"] &lt;- \"Venezuela (Bolivarian Republic of)\"\nV_DEM_trim$Countries_and_areas[V_DEM_trim$Countries_and_areas == \"Tanzania\"] &lt;- \"United Republic of Tanzania\"\nV_DEM_trim$Countries_and_areas[V_DEM_trim$Countries_and_areas == \"South Korea\"] &lt;- \"Republic of Korea\"\nV_DEM_trim$Countries_and_areas[V_DEM_trim$Countries_and_areas == \"North Korea\"] &lt;- \"Democratic People's Republic of Korea\"\nV_DEM_trim$Countries_and_areas[V_DEM_trim$Countries_and_areas == \"Vietnam\"] &lt;- \"Viet Nam\"\nV_DEM_trim$Countries_and_areas[V_DEM_trim$Countries_and_areas == \"Bolivia\"] &lt;- \"Bolivia (Plurinational State of)\"\nV_DEM_trim$Countries_and_areas[V_DEM_trim$Countries_and_areas == \"United States of America\"] &lt;- \"United States\"\nV_DEM_trim$Countries_and_areas[V_DEM_trim$Countries_and_areas == \"Russia\"] &lt;- \"Russian Federation\"\nV_DEM_trim$Countries_and_areas[V_DEM_trim$Countries_and_areas == \"Burma/Myanmar\"] &lt;- \"Myanmar\"\n\n# take a quick look at our V-Dem data\nglimpse(V_DEM_trim)\n\n\nRows: 179\nColumns: 3\n$ Countries_and_areas &lt;chr&gt; \"Mexico\", \"Suriname\", \"Sweden\", \"Switzerland\", \"Gh…\n$ e_p_polity          &lt;dbl&gt; 8, 5, 10, 10, 8, 9, 10, 8, 4, 9, -4, -77, 7, 10, 8…\n$ v2xpe_exlgender     &lt;dbl&gt; 0.524, 0.389, 0.024, 0.056, 0.086, 0.399, 0.112, 0…\n\n\nThe confounding variable GDP per capita will be pulled from a dataset from the International Monetary Fund.\n\n\nCode\nimf_GDP &lt;- read_excel(\"imf-dm-export-20230322.xls\")\n\n# Select just the variables we need for this analysis.\nimf_GDP_2017 &lt;- select(imf_GDP, `GDP per capita, current prices\\n (U.S. dollars per capita)`, `2017`)\n\n# rename the country_name variable so we can join it with the UNICEF data later.\nimf_GDP_2017 &lt;- rename(imf_GDP_2017, \"Countries_and_areas\" = `GDP per capita, current prices\\n (U.S. dollars per capita)`)\n\n# change the GDP per capita variable to numeric.\nimf_GDP_2017$`2017` &lt;- as.numeric(imf_GDP_2017$`2017`)\n\n# Change some of the country names where the IMF dataset differs from the \n# UNICEF data. \nimf_GDP_2017$Countries_and_areas[imf_GDP_2017$Countries_and_areas == \"Bahamas, The\"] &lt;- \"Bahamas\"\nimf_GDP_2017$Countries_and_areas[imf_GDP_2017$Countries_and_areas == \"Bolivia\"] &lt;- \"Bolivia (Plurinational State of)\"\nimf_GDP_2017$Countries_and_areas[imf_GDP_2017$Countries_and_areas == \"China, People's Republic of\"] &lt;- \"China\"\nimf_GDP_2017$Countries_and_areas[imf_GDP_2017$Countries_and_areas == \"Congo, Dem. Rep. of the\"] &lt;- \"Democratic Republic of the Congo\"\nimf_GDP_2017$Countries_and_areas[imf_GDP_2017$Countries_and_areas == \"Congo, Republic of\"] &lt;- \"Congo\"\nimf_GDP_2017$Countries_and_areas[imf_GDP_2017$Countries_and_areas == \"Czech Republic\"] &lt;- \"Czechia\"\nimf_GDP_2017$Countries_and_areas[imf_GDP_2017$Countries_and_areas == \"Gambia, The\"] &lt;- \"Gambia\"\nimf_GDP_2017$Countries_and_areas[imf_GDP_2017$Countries_and_areas == \"Iran\"] &lt;- \"Iran (Islamic Republic of)\"\nimf_GDP_2017$Countries_and_areas[imf_GDP_2017$Countries_and_areas == \"Korea, Republic of\"] &lt;- \"Republic of Korea\"\nimf_GDP_2017$Countries_and_areas[imf_GDP_2017$Countries_and_areas == \"Kyrgyz Republic\"] &lt;- \"Kyrgyzstan\"\nimf_GDP_2017$Countries_and_areas[imf_GDP_2017$Countries_and_areas == \"Lao P.D.R.\"] &lt;- \"Lao People's Democratic Republic\"\nimf_GDP_2017$Countries_and_areas[imf_GDP_2017$Countries_and_areas == \"Micronesia, Fed. States of\"] &lt;- \"Micronesia (Federated States of)\"\nimf_GDP_2017$Countries_and_areas[imf_GDP_2017$Countries_and_areas == \"Moldova\"] &lt;- \"Republic of Moldova\"\nimf_GDP_2017$Countries_and_areas[imf_GDP_2017$Countries_and_areas == \"Slovak Republic\"] &lt;- \"Slovakia\"\nimf_GDP_2017$Countries_and_areas[imf_GDP_2017$Countries_and_areas == \"South Sudan, Republic of\"] &lt;- \"South Sudan\"\nimf_GDP_2017$Countries_and_areas[imf_GDP_2017$Countries_and_areas == \"Syria\"] &lt;- \"Syrian Arab Republic\"\nimf_GDP_2017$Countries_and_areas[imf_GDP_2017$Countries_and_areas == \"São Tomé and Príncipe\"] &lt;- \"Sao Tome and Principe\"\nimf_GDP_2017$Countries_and_areas[imf_GDP_2017$Countries_and_areas == \"Tanzania\"] &lt;- \"United Republic of Tanzania\"\nimf_GDP_2017$Countries_and_areas[imf_GDP_2017$Countries_and_areas == \"Türkiye, Republic of\"] &lt;- \"Turkey\"\nimf_GDP_2017$Countries_and_areas[imf_GDP_2017$Countries_and_areas == \"Venezuela\"] &lt;- \"Venezuela (Bolivarian Republic of)\"\nimf_GDP_2017$Countries_and_areas[imf_GDP_2017$Countries_and_areas == \"Vietnam\"] &lt;- \"Viet Nam\"\n\n# take a quick look at our IMF data\nhead(imf_GDP_2017)\n\n\n# A tibble: 6 × 2\n  Countries_and_areas `2017`\n  &lt;chr&gt;                &lt;dbl&gt;\n1 &lt;NA&gt;                   NA \n2 Afghanistan           636.\n3 Albania              4526.\n4 Algeria              4080.\n5 Andorra             40018.\n6 Angola               4300.\n\n\nThe last confounding variable on health expenditure data will be drawn from the UNICEF Economic Indicators data.\n\n\nCode\n## Add in Economic Indicator data\nUNICEF_Econ_Indicators &lt;- read_excel(\"UNICEF_Econ-Indicators.xlsx\", skip = 6, \n                                    col_names= c(\"Countries_and_areas\", \"delete\", \"delete\", \n                                                  \"delete\", \"Expend_Health_pct_GDP\", \n                                                  rep(\"delete\", 13))) %&gt;% \n  select(!contains(\"delete\"))\n# skip the first five rows, rename all of the variables we want and delete the ones we don't want\n\nUNICEF_EI &lt;- slice(UNICEF_Econ_Indicators, 1:(nrow(UNICEF_Econ_Indicators) - 37)) \n# remove the last 37 rows, which include summaries and endnotes, not data\n\nUNICEF_EI[UNICEF_EI == \"-\"] &lt;- NA\n# replace \" - \" with NA in all columns\n\nUNICEF_EI$`Expend_Health_pct_GDP` &lt;- as.numeric(UNICEF_EI$`Expend_Health_pct_GDP`)\n# Turning everything numeric\n\nglimpse(UNICEF_EI)\n\n\nRows: 202\nColumns: 2\n$ Countries_and_areas   &lt;chr&gt; \"Afghanistan\", \"Albania\", \"Algeria\", \"Andorra\", …\n$ Expend_Health_pct_GDP &lt;dbl&gt; 0.4859748, 2.8415394, 4.0938535, 4.5586491, 1.06…\n\n\nFinally, all four datasets will be joined together.\n\n\nCode\n# first we'll join the first UNICEF and V-DEM data.\nJOIN_UNICEF_VDEM &lt;- full_join(MNH_final, V_DEM_trim, by = \"Countries_and_areas\") \n\n# then we'll add in the IMF data.\nJOIN_UNICEF_VDEM_IMF &lt;- full_join(JOIN_UNICEF_VDEM, imf_GDP_2017, by = \"Countries_and_areas\") \n\n# then we'll add in the economic indicator UNICEF data\nJOIN_UNICEF_VDEM_IMF_EI &lt;- full_join(JOIN_UNICEF_VDEM_IMF, UNICEF_EI, by = \"Countries_and_areas\") \n\n# remove the last 56 rows, which include notes, countries, and regions not in the original UN data\nMMR_Combined &lt;- slice(JOIN_UNICEF_VDEM_IMF_EI, 1:(nrow(JOIN_UNICEF_VDEM_IMF_EI) - 56)) \n\n# remove NAs for anlaysis\nMMR_Combined &lt;- na.omit(MMR_Combined)\n\n# rename our variables for ease of use\nMMR_Analysis &lt;-rename(MMR_Combined, \"Country\" = Countries_and_areas, \n                      \"GenderExclusion\" = v2xpe_exlgender, \"Democracy\" = e_p_polity, \n                      \"GDP_2017\" = `2017`)\n\n\nTwo additional variables will be created for this analysis: First, the GDP per capita variable needs to be logged so that linear regression analysis can be performed, creating a new logged GDP per capita variable. Second, the GDP variable will be binned into quartiles to use for grouping when describing and visualizing the data. This second variable will not be used in analysis since the same information is captured in the logGDP2017 variable.\n\n\nCode\nMMR_Analysis %&gt;% \n  summarize(\"25th quantile\" = quantile(x = GDP_2017, probs = .25),\n            \"Median\" = median(GDP_2017, na.rm = TRUE),\n            \"75th quantile\" = quantile(x = GDP_2017, probs = .75))\n\n\n# A tibble: 1 × 3\n  `25th quantile` Median `75th quantile`\n            &lt;dbl&gt;  &lt;dbl&gt;           &lt;dbl&gt;\n1           1577.  4401.          13748.\n\n\nCode\nMMR_Analysis &lt;- MMR_Analysis %&gt;%\n  mutate(\"GDP_Quartiles\" = case_when(\n    GDP_2017 &lt;= 1578 ~ \"1st Quartile\", \n    GDP_2017 &gt;= 1578.01 & GDP_2017 &lt;= 4402 ~ \"2nd Quartile\",\n    GDP_2017 &gt;= 4402.01 & GDP_2017 &lt;= 13749 ~ \"3rd Quartile\",\n    GDP_2017 &gt;= 13749.01 ~ \"4th Quartile\")) %&gt;% \n  mutate(\"logGDP2017\" = log(GDP_2017))\n\n# I couldn't find any official definition of income group that used GDP levels, \n# all of the definitions I found from the World Bank and other official sources \n# referenced the GNI. I tried using the GNI but it wasn't as powerful in the model \n# and most importantly, it was missing some data that seemed vital. It didn't have \n# the South Sudan, which is an important country in this discussion of MMR because \n# it is the place in the world with the highest MMR. So instead I went with the \n# quartiles for grouping. Note that the quartiles are not *that* far off from the \n# income group levels defined by the World Bank and noted below.\n# MMR_Analysis &lt;- MMR_Analysis %&gt;%\n#  mutate(\"IncomeGroup\" = case_when(\n#    GDP_2017 &lt;= 1025 ~ \"low_income\", \n#    GDP_2017 &gt;= 1026 & GDP_2017 &lt;= 3995 ~ \"low_mid_income\",\n#    GDP_2017 &gt;= 3996 & GDP_2017 &lt;= 12375 ~ \"upper_mid_income\",\n#    GDP_2017 &gt;= 12376 ~ \"high_income\"))\n\nglimpse(MMR_Analysis)\n\n\nRows: 155\nColumns: 11\n$ Country                       &lt;chr&gt; \"Afghanistan\", \"Albania\", \"Algeria\", \"An…\n$ WorldRegion                   &lt;chr&gt; \"South Asia\", \"Europe & Central Asia\", \"…\n$ SubRegion                     &lt;chr&gt; \"South Asia\", \"Eastern Europe & Central …\n$ Maternal_Mortality_Ratio_2017 &lt;dbl&gt; 638, 15, 112, 241, 39, 26, 6, 5, 26, 14,…\n$ SkilledBirthAtt               &lt;dbl&gt; 58.8, 99.8, 98.8, 49.6, 99.5, 99.8, 98.7…\n$ Democracy                     &lt;dbl&gt; -1, 9, 2, -2, 9, 5, 10, 10, -7, -10, 1, …\n$ GenderExclusion               &lt;dbl&gt; 0.766, 0.216, 0.355, 0.497, 0.098, 0.339…\n$ GDP_2017                      &lt;dbl&gt; 635.789, 4525.887, 4079.653, 4300.097, 1…\n$ Expend_Health_pct_GDP         &lt;dbl&gt; 0.4859748, 2.8415394, 4.0938535, 1.06878…\n$ GDP_Quartiles                 &lt;chr&gt; \"1st Quartile\", \"3rd Quartile\", \"2nd Qua…\n$ logGDP2017                    &lt;dbl&gt; 6.454867, 8.417569, 8.313767, 8.366393, …"
  },
  {
    "objectID": "posts/JustineShakespeare_FinalProject.html#descriptive-statistics-and-visualizations",
    "href": "posts/JustineShakespeare_FinalProject.html#descriptive-statistics-and-visualizations",
    "title": "Maternal Mortality and Women’s Political Inclusion",
    "section": "Descriptive Statistics and Visualizations",
    "text": "Descriptive Statistics and Visualizations\n\n\nCode\nglimpse(MMR_Analysis)\n\n\nRows: 155\nColumns: 11\n$ Country                       &lt;chr&gt; \"Afghanistan\", \"Albania\", \"Algeria\", \"An…\n$ WorldRegion                   &lt;chr&gt; \"South Asia\", \"Europe & Central Asia\", \"…\n$ SubRegion                     &lt;chr&gt; \"South Asia\", \"Eastern Europe & Central …\n$ Maternal_Mortality_Ratio_2017 &lt;dbl&gt; 638, 15, 112, 241, 39, 26, 6, 5, 26, 14,…\n$ SkilledBirthAtt               &lt;dbl&gt; 58.8, 99.8, 98.8, 49.6, 99.5, 99.8, 98.7…\n$ Democracy                     &lt;dbl&gt; -1, 9, 2, -2, 9, 5, 10, 10, -7, -10, 1, …\n$ GenderExclusion               &lt;dbl&gt; 0.766, 0.216, 0.355, 0.497, 0.098, 0.339…\n$ GDP_2017                      &lt;dbl&gt; 635.789, 4525.887, 4079.653, 4300.097, 1…\n$ Expend_Health_pct_GDP         &lt;dbl&gt; 0.4859748, 2.8415394, 4.0938535, 1.06878…\n$ GDP_Quartiles                 &lt;chr&gt; \"1st Quartile\", \"3rd Quartile\", \"2nd Qua…\n$ logGDP2017                    &lt;dbl&gt; 6.454867, 8.417569, 8.313767, 8.366393, …\n\n\nThis data now contains information on 155 countries around the world, including the region and subregion they belong to, their ranking on democracy and gender exclusion indices, their national maternal mortality ratio, percentage of skill birth attendants present at birth, GDP, and percentage of GDP spent on health. In addition there are two variables created for this analysis that provide the quartile in which a country’s GDP falls, and the logged GDP. This data should provide us with the key variables and confounding variables to examine and analyze the relationship between the explanatory variable (GenderExclusion) and the dependent variable (Maternal_Mortality_Ratio_2017).\n\nDependent variable\nAs noted above, the dependent variable for this research is the national maternal mortality ratio of each country in 2017. The following heatmap shows the Maternal_Mortality_Ratio_2017 in each country in the data. Note that darker shades indicate a higher maternal mortality rate.\n\n\nCode\n# Load the world map data\nworld &lt;- map_data(\"world\")\n\n## change the MNH data into what you need for the maps. select two variables, \n# change the variable names for joining\nMapMMR &lt;- MMR_Analysis %&gt;% \n  select(Country, Maternal_Mortality_Ratio_2017, GenderExclusion, SkilledBirthAtt) %&gt;% \n  rename(region = Country)\n\n# Use stringr::str_replace to edit the country names under the region variable\nworld$region &lt;- str_replace(world$region, \"Bolivia\", \"Bolivia (Plurinational State of)\")\nworld$region &lt;- str_replace(world$region, \"Brunei\", \"Brunei Darussalam\") \nworld$region &lt;- str_replace(world$region, \"Cape Verde\", \"Cabo Verde\")\nworld$region &lt;- str_replace(world$region, \"Congo\", \"Republic of Congo\") \nworld$region &lt;- str_replace(world$region, \"Ivory Coast\", \"Côte d'Ivoire\") \nworld$region &lt;- str_replace(world$region, \"Czech Republic\", \"Czechia\")\nworld$region &lt;- str_replace(world$region, \"North Korea\", \"Democratic People's Republic of Korea\")\nworld$region &lt;- str_replace(world$region, \"Micronesia\", \"Micronesia (Federated States of)\")\nworld$region &lt;- str_replace(world$region, \"Iran\", \"Iran (Islamic Republic of)\")\nworld$region &lt;- str_replace(world$region, \"Laos\", \"Lao People's Democratic Republic\")\nworld$region &lt;- str_replace(world$region, \"South Korea\", \"Republic of Korea\")\nworld$region &lt;- str_replace(world$region, \"Moldova\", \"Republic of Moldova\")\nworld$region &lt;- str_replace(world$region, \"Russia\", \"Russian Federation\")\nworld$region &lt;- str_replace(world$region, \"Palestine\", \"State of Palestine\")\nworld$region &lt;- str_replace(world$region, \"Syria\", \"Syrian Arab Republic\")\nworld$region &lt;- str_replace(world$region, \"Tanzania\", \"United Republic of Tanzania\")\nworld$region &lt;- str_replace(world$region, \"UK\", \"United Kingdom\")\nworld$region &lt;- str_replace(world$region, \"USA\", \"United States\")\nworld$region &lt;- str_replace(world$region, \"Venezuela\", \"Venezuela (Bolivarian Republic of)\")\nworld$region &lt;- str_replace(world$region, \"Vietnam\", \"Viet Nam\")\n\n# Merge the world map data with the numeric data\nworld_data &lt;- full_join(world, MapMMR, by = \"region\") \n\n# Plot the map\nggplot(world_data, aes(x = long, y = lat, group = group, fill = Maternal_Mortality_Ratio_2017)) +\n  geom_polygon() +\n  scale_fill_gradient(low = \"#EBE1AD\", high = \"#DD4B1A\") +\n  labs(title = \"Maternal Mortality Ratio Around the World\",\n       subtitle = \"Darker shades indicate a higher maternal mortality ratio\",\n       caption = \"Data from UNICEF: https://data.unicef.org (2017)\", \n       fill = \"MMR\") +\n  theme_bw()\n\n\n\n\n\nThe distribution of the Maternal_Mortality_Ratio_2017 is right skewed, indicating that most countries in the data have low values for this variable. This also means that this might be a good variable to log for regression analysis.\n\n\nCode\nMMR_Analysis %&gt;%\n  ggplot( aes(x=Maternal_Mortality_Ratio_2017)) +\n  geom_density(fill=\"#EC7F5B\", color=\"#EC7F5B\", alpha=0.8) +\n  labs(title = \"Distribution of Maternal Mortality Ratio Globally\",\n       subtitle = \"Data from 2017\",\n       x = \"Maternal Mortality Ratio\", y = \"Density\") +\n  theme_bw()\n\n\n\n\n\nThe following graphs show that maternal mortality is the highest in countries with GDP in the lowest quartile and in Sub-Saharan Africa.\n\n\nCode\n# box plot with individual data points\nMMR_Analysis %&gt;% \n  ggplot(aes(x = GDP_Quartiles, y = Maternal_Mortality_Ratio_2017, fill=GDP_Quartiles)) +\n  geom_boxplot() +\n  scale_fill_viridis(discrete = TRUE, alpha=0.6) +\n  geom_jitter(color=\"black\", size=0.4, alpha=0.9) +\n  labs(title = \"Maternal Mortality Ratio by Level of Development\",\n       subtitle = \"1st quartile = lowest GDP, 4th quartile = highest GDP\", \n       x= \" \", y = \"Maternal Mortality Ratio\", fill = \"GDP Quartile\", \n       caption = \"Data from 2017\") \n\n\n\n\n\nCode\n# box plot with individual data points\nMMR_Analysis %&gt;% \n  ggplot(aes(x = WorldRegion, y = Maternal_Mortality_Ratio_2017, fill=WorldRegion)) +\n  geom_boxplot() +\n  scale_fill_viridis(discrete = TRUE, alpha=0.6) +\n  geom_jitter(color=\"black\", size=0.4, alpha=0.9) +\n  labs(title = \"Maternal Mortality Ratio by World Region\",\n       subtitle = , \n       x= \"\", y = \"Maternal Mortality Ratio\", \n       fill = \"World Region\",\n       caption = \"Data from 2017\") +\n  theme(axis.text.x = element_text(angle = 65, vjust = 0.5))\n\n\n\n\n\nThe statistical summary of the Maternal_Mortality_Ratio_2017 variable shows a large range, from 2 to 1,150. The standard deviation is high compared to the mean and indicates that there is a lot of variability within the data.\n\n\nCode\nMMR_Analysis %&gt;% \n  summarize(\"Mean\" = mean(Maternal_Mortality_Ratio_2017, na.rm = TRUE),\n            \"Standard_Deviation\" = sd(Maternal_Mortality_Ratio_2017, na.rm = TRUE),\n            \"Lowest\" = min(Maternal_Mortality_Ratio_2017, na.rm = TRUE), \n            \"25th quantile\" = quantile(x = Maternal_Mortality_Ratio_2017, probs = .25),\n            \"Median\" = median(Maternal_Mortality_Ratio_2017, na.rm = TRUE),\n            \"75th quantile\" = quantile(x = Maternal_Mortality_Ratio_2017, probs = .75),\n            \"Highest\" = max(Maternal_Mortality_Ratio_2017, na.rm = TRUE))\n\n\n# A tibble: 1 × 7\n   Mean Standard_Deviation Lowest `25th quantile` Median `75th quantile` Highest\n  &lt;dbl&gt;              &lt;dbl&gt;  &lt;dbl&gt;           &lt;dbl&gt;  &lt;dbl&gt;           &lt;dbl&gt;   &lt;dbl&gt;\n1  177.               242.      2            13.5     61             251    1150\n\n\nThe 75th quantile suggests that most of the countries in this data have maternal mortality ratios lower than approximately 250. The highest value here shows that at least one country in this data has well over 4 times that amount.\nThis table shows the countries with the highest Maternal_Mortality_Ratio_2017.\n\n\nCode\nMMR_high &lt;- MMR_Analysis %&gt;% \n  select(\"Country\", \"Maternal_Mortality_Ratio_2017\") %&gt;% \n  arrange(desc(as.numeric(Maternal_Mortality_Ratio_2017))) %&gt;% \n  head(n = 10)\n\nkable(MMR_high) %&gt;% \n  kable_styling(\"striped\")\n\n\n\n\n\nCountry\nMaternal_Mortality_Ratio_2017\n\n\n\n\nSouth Sudan\n1150\n\n\nChad\n1140\n\n\nSierra Leone\n1120\n\n\nNigeria\n917\n\n\nCentral African Republic\n829\n\n\nMauritania\n766\n\n\nGuinea-Bissau\n667\n\n\nLiberia\n661\n\n\nAfghanistan\n638\n\n\nCôte d'Ivoire\n617\n\n\n\n\n\n\n\nThis table shows the countries with the lowest Maternal_Mortality_Ratio_2017.\n\n\nCode\nMMR_low &lt;- MMR_Analysis %&gt;% \n  select(\"Country\", \"Maternal_Mortality_Ratio_2017\") %&gt;% \n  arrange(as.numeric(Maternal_Mortality_Ratio_2017)) %&gt;% \n  head(n = 10)\n\nkable(MMR_low) %&gt;% \n  kable_styling(\"striped\")\n\n\n\n\n\nCountry\nMaternal_Mortality_Ratio_2017\n\n\n\n\nBelarus\n2\n\n\nItaly\n2\n\n\nNorway\n2\n\n\nPoland\n2\n\n\nCzechia\n3\n\n\nFinland\n3\n\n\nGreece\n3\n\n\nUnited Arab Emirates\n3\n\n\nDenmark\n4\n\n\nSpain\n4\n\n\n\n\n\n\n\n\n\nExplanatory variable\nThe explanatory variable for this analysis is GenderExclusion, which is an index of gender exclusion around the world. The following map shows the levels of this variable for each country in the data. Darker shades indicate a higher level of gender exclusion, lighter shades indicate more gender inclusion and equality.\n\n\nCode\n# Plot the map \nggplot(world_data, aes(x = long, y = lat, group = group, fill = GenderExclusion)) +\n  geom_polygon() +\n  scale_fill_gradient(low = \"#EBEEFF\", high = \"#4027CE\") +\n  labs(title = \"Gender Exclusion Around the World\",\n       subtitle = \"Darker shades indicate more exclusion because of gender\",\n       caption = \"Data from V-Dems: https://v-dem.net/ (2017)\", \n       fill = \"Gender Exclusion\") +\n  theme_bw()\n\n\n\n\n\nThe distribution of the explanatory variable GenderExclusion is somewhat right skewed, shows that fewer countries have high levels of gender exclusion. Given this skew, it might be necessary to explore logged versions of this variable.\n\n\nCode\nMMR_Analysis %&gt;%\n  ggplot(aes(x=GenderExclusion)) +\n  geom_density(fill=\"#4027CE\", color=\"#4027CE\", alpha=0.8) +\n  labs(title = \"Distribution of Gender Exclusion Index Globally\",\n       subtitle = \"Data from 2017\",\n       x = \"Gender Exclusion Index\", y = \"Density\") +\n  theme_bw()\n\n\n\n\n\nThis table shows the countries with the highest rate of GenderExclusion.\n\n\nCode\nGE_high &lt;- MMR_Analysis %&gt;% \n  select(\"Country\", \"GenderExclusion\") %&gt;% \n  arrange(desc(as.numeric(GenderExclusion))) %&gt;% \n  head(n = 10)\n\nkable(GE_high) %&gt;% \n  kable_styling(\"striped\")\n\n\n\n\n\nCountry\nGenderExclusion\n\n\n\n\nYemen\n0.968\n\n\nSaudi Arabia\n0.905\n\n\nIraq\n0.873\n\n\nIran (Islamic Republic of)\n0.860\n\n\nSolomon Islands\n0.844\n\n\nSudan\n0.838\n\n\nDjibouti\n0.834\n\n\nTajikistan\n0.827\n\n\nPapua New Guinea\n0.811\n\n\nSouth Sudan\n0.792\n\n\n\n\n\n\n\nThis table shows the countries with the lowest rate of GenderExclusion.\n\n\nCode\nGE_low &lt;- MMR_Analysis %&gt;% \n  select(\"Country\", \"GenderExclusion\") %&gt;% \n  arrange(as.numeric(GenderExclusion)) %&gt;% \n  head(n = 10)\n\nkable(GE_low) %&gt;% \n  kable_styling(\"striped\")\n\n\n\n\n\nCountry\nGenderExclusion\n\n\n\n\nDenmark\n0.014\n\n\nNorway\n0.020\n\n\nCzechia\n0.028\n\n\nEstonia\n0.028\n\n\nLatvia\n0.028\n\n\nAustralia\n0.035\n\n\nGermany\n0.035\n\n\nLithuania\n0.035\n\n\nSlovenia\n0.036\n\n\nSpain\n0.040\n\n\n\n\n\n\n\n\n\nMediator variable\nThe mediator variable for this analysis is SkilledBirthAtt, which is the percentage of skilled attendants present at birth. The following map shows the value for this variable for each country in the data. Darker shades indicate a lower percentage and lighter shades indicate a higher percentage of skilled attendants at birth.\n\n\nCode\n# Plot the map\nggplot(world_data, aes(x = long, y = lat, group = group, fill = SkilledBirthAtt)) +\n  geom_polygon() +\n  scale_fill_gradient(low = \"#1282A2\", high = \"#C7EAE4\") +\n  labs(title = \"Percentage of Skilled Birth Attendants Around the World\",\n       subtitle = \"Darker shades indicate higher percentage of skilled birth attendants\",\n       caption = \"Data from V-Dems: https://v-dem.net/ (2017)\", \n       fill = \"Pct of Skilled Birth Attendants\") +\n  theme_bw()\n\n\n\n\n\nSummary statistics of the explanatory variable, confounding variables, and the mediator variable.\n\n\nCode\ndfSummary(MMR_Analysis[, c(\"GenderExclusion\", \"Expend_Health_pct_GDP\", \"GDP_2017\", \"SkilledBirthAtt\")])\n\n\nData Frame Summary  \nDimensions: 155 x 4  \nDuplicates: 0  \n\n-----------------------------------------------------------------------------------------------------------------------------\nNo   Variable                Stats / Values                  Freqs (% of Valid)    Graph                 Valid      Missing  \n---- ----------------------- ------------------------------- --------------------- --------------------- ---------- ---------\n1    GenderExclusion         Mean (sd) : 0.3 (0.2)           136 distinct values   :                     155        0        \n     [numeric]               min &lt; med &lt; max:                                      :                     (100.0%)   (0.0%)   \n                             0 &lt; 0.3 &lt; 1                                           :   . : . .                               \n                             IQR (CV) : 0.4 (0.7)                                  : : : : : : .                             \n                                                                                   : : : : : : : : : .                       \n\n2    Expend_Health_pct_GDP   Mean (sd) : 3.2 (2.1)           155 distinct values   : : .   .             155        0        \n     [numeric]               min &lt; med &lt; max:                                      : : :   :             (100.0%)   (0.0%)   \n                             0.2 &lt; 2.7 &lt; 9.2                                       : : : . :                                 \n                             IQR (CV) : 3 (0.7)                                    : : : : : . .                             \n                                                                                   : : : : : : : . . :                       \n\n3    GDP_2017                Mean (sd) : 11732.8 (17504.5)   155 distinct values   :                     155        0        \n     [numeric]               min &lt; med &lt; max:                                      :                     (100.0%)   (0.0%)   \n                             277.7 &lt; 4401.1 &lt; 111211.6                             :                                         \n                             IQR (CV) : 12171.1 (1.5)                              :                                         \n                                                                                   : : .                                     \n\n4    SkilledBirthAtt         Mean (sd) : 87.4 (18.1)         94 distinct values                    :     155        0        \n     [numeric]               min &lt; med &lt; max:                                                      :     (100.0%)   (0.0%)   \n                             19.4 &lt; 97.3 &lt; 100                                                     :                         \n                             IQR (CV) : 19 (0.2)                                                   :                         \n                                                                                         . . . . : :                         \n-----------------------------------------------------------------------------------------------------------------------------\n\n\n\n\nCorrelations between variables\nRelationships between variables can be explored through visuals. The following correlogram shows select variables, their distribution, and how they correlate with one another.\n\n\nCode\n# first we'll select the variables that are numeric\nMMR_num &lt;- select(MMR_Analysis, SkilledBirthAtt, Maternal_Mortality_Ratio_2017, GenderExclusion, Democracy, Expend_Health_pct_GDP, logGDP2017)\n\n# Check correlations (as scatterplots), distribution and print corrleation coefficient \nggpairs(MMR_num, title=\"Correlogram of key variables\")\n\n\n\n\n\nAll of these variables have some kind of correlation. Democracy and Maternal_Mortality_Ratio_2017 have the lowest correlation. The highest correlation on this graph is between Maternal_Mortality_Ratio_2017 and SkilledBirthAtt, which is a negative relationship. The variable logGDP2017 is also highly correlated with Maternal_Mortality_Ratio_2017 as well as Expend_Health_pct_GDP. The explanatory variable GenderExclusion has the highest correlation with Expend_Health_pct_GDP and it is a negative correlation. The explanatory variable also shows a moderate negative correlation with SkilledBirthAtt and logGDP2017. Democracy looks to have the lowest correlations with other variables in this analysis.\nThe following scatterplot shows the positive relationship between the key variables of this analysis, Maternal_Mortality_Ratio_2017 and GenderExclusion. This graph also shows some heteroskedasticity.\n\n\nCode\nMMR_Analysis %&gt;% \n  ggplot(aes(x = Maternal_Mortality_Ratio_2017, y = GenderExclusion)) + \n  geom_point() +\n  labs(title = \"Gender Exclusion and Maternal Mortality\", \n       subtitle = \"Higher maternal mortality correlates with higher levels of gender exclusion\",\n       x = \"Maternal Mortality Ratio\", \n       y = \"Gender Exclusion\", \n       color = \"GDP Quartile\") +\n  theme_bw() + \n  geom_smooth(method = \"lm\")\n\n\n\n\n\nThe following two graphs explore the relationships important to the mediator analysis that will be central to hypothesis #2. These graphs look at the relationship between the explanatory variable (GenderExclusion) and both the dependent variable (Maternal_Mortality_Ratio_2017) and the mediator variable (SkilledBirthAtt).\n\n\nCode\nMMR_Analysis %&gt;% \n  ggplot(aes(x = SkilledBirthAtt, y = GenderExclusion)) + \n  geom_point() +\n  labs(title = \"Gender Exclusion and Presence of Skilled Attendant at Birth\", \n       x = \"Skilled Attendant at Birth\",\n       y = \"Gender Exclusion\") +\n  theme_bw() +\n  geom_smooth(method = \"lm\")\n\n\n\n\n\nCode\nMMR_Analysis %&gt;% \n  ggplot(aes(x = Maternal_Mortality_Ratio_2017, y = GenderExclusion)) + \n  geom_point() +\n  labs(title = \"Gender Exclusion and Maternal Mortality Ratio\", \n       x = \"Maternal Mortality Ratio\",\n       y = \"Gender Exclusion\") +\n  theme_bw() +\n  geom_smooth(method = \"lm\")\n\n\n\n\n\nAnother way to visualize the relationship between these three variables is in the graph below. The clustering of the data points in the upper lefthand corner make it difficult to notice any change in the size of the point, thereby losing a meaningful understanding of how the GenderExclusion variable layers in here.\n\n\nCode\nMMR_Analysis %&gt;% \n  ggplot(aes(x = Maternal_Mortality_Ratio_2017, y = SkilledBirthAtt, size = GenderExclusion)) + \n  geom_point() +\n  labs(title = \"Gender Exclusion, Skilled Attendant at Birth, and Maternal Mortality\",\n       x = \"MMR\",\n       y = \"Skilled Attendant at Birth\", \n       size = \"Gender Exclusion\") +\n  theme_bw() +\n  geom_smooth(method = \"lm\")"
  },
  {
    "objectID": "posts/JustineShakespeare_FinalProject.html#analysis",
    "href": "posts/JustineShakespeare_FinalProject.html#analysis",
    "title": "Maternal Mortality and Women’s Political Inclusion",
    "section": "Analysis",
    "text": "Analysis\nThe visuals above suggest some significant relationships between variables. The section will explore the research questions with correlation tests and regression analysis.\n\nHypothesis #1\nHypothesis #1: Where women are politically included and empowered, health outcomes are better and maternal mortality is low. This will hold true even when controlling for variables which are known to have an effect on both women’s political inclusion and maternal mortality.\nHow does the explanatory variable (GenderExclusion) relate to the dependent variable (Maternal_Mortality_Ratio_2017)?\nA correlation test shows that the explanatory variable (GenderExclusion) and the dependent variable (Maternal_Mortality_Ratio_2017) have a moderate positive correlation. The p-value here is small and indicates this relationship is statistically significant.\n\n\nCode\ncor.test(MMR_Analysis$Maternal_Mortality_Ratio_2017, MMR_Analysis$GenderExclusion)\n\n\n\n    Pearson's product-moment correlation\n\ndata:  MMR_Analysis$Maternal_Mortality_Ratio_2017 and MMR_Analysis$GenderExclusion\nt = 5.6182, df = 153, p-value = 8.877e-08\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.2737461 0.5362357\nsample estimates:\n      cor \n0.4135474 \n\n\nAs noted above, the Maternal_Mortality_Ratio_2017 has a right-skewed distribution, so a logged version of the variable should be considered. This is also the case for the GenderExclusion variable. Different combinations of logged versions of these variables will be compared here.\n\n\nCode\nMMR_Analysis &lt;- mutate(MMR_Analysis, \"logMMR2017\" = log(Maternal_Mortality_Ratio_2017))\nMMR_Analysis &lt;- mutate(MMR_Analysis, \"logGenderExclusion\" = log(GenderExclusion))\n\ncor.test(MMR_Analysis$logMMR2017, MMR_Analysis$GenderExclusion)\n\n\n\n    Pearson's product-moment correlation\n\ndata:  MMR_Analysis$logMMR2017 and MMR_Analysis$GenderExclusion\nt = 7.8117, df = 153, p-value = 8.383e-13\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.4109084 0.6379160\nsample estimates:\n      cor \n0.5339668 \n\n\nCode\ncor.test(MMR_Analysis$logMMR2017, MMR_Analysis$logGenderExclusion)\n\n\n\n    Pearson's product-moment correlation\n\ndata:  MMR_Analysis$logMMR2017 and MMR_Analysis$logGenderExclusion\nt = 10.12, df = 153, p-value &lt; 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.5282982 0.7190763\nsample estimates:\n      cor \n0.6332094 \n\n\nCode\ncor.test(MMR_Analysis$Maternal_Mortality_Ratio_2017, MMR_Analysis$logGenderExclusion)\n\n\n\n    Pearson's product-moment correlation\n\ndata:  MMR_Analysis$Maternal_Mortality_Ratio_2017 and MMR_Analysis$logGenderExclusion\nt = 5.8772, df = 153, p-value = 2.52e-08\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.2912203 0.5496274\nsample estimates:\n      cor \n0.4291654 \n\n\nA comparison of these correlation tests shows that the highest correlation is with the logged versions of the variables, logGenderExclusion and logMMR2017. These variables should be compared in the linear regression analysis as well.\nA linear regression analysis also shows a significant relationship between explanatory variable (GenderExclusion) and the dependent variable (Maternal_Mortality_Ratio_2017) and the logged versions.\n\n\nCode\n# Model with only the explanatory variable and the dependent variable\nMMR_nolog &lt;- (lm(formula = Maternal_Mortality_Ratio_2017 ~ GenderExclusion,\n                              data = MMR_Analysis))\n\nMMRlog &lt;- (lm(formula = logMMR2017 ~ GenderExclusion,\n                              data = MMR_Analysis))\n\nGElog &lt;- (lm(formula = Maternal_Mortality_Ratio_2017 ~ logGenderExclusion,\n                              data = MMR_Analysis))\n\nAllLog &lt;- (lm(formula = logMMR2017 ~ logGenderExclusion,\n                              data = MMR_Analysis))\n\nstargazer(MMR_nolog, MMRlog, GElog, AllLog, type = 'text') \n\n\n\n================================================================================================================\n                                                              Dependent variable:                               \n                               ---------------------------------------------------------------------------------\n                               Maternal_Mortality_Ratio_2017 logMMR2017 Maternal_Mortality_Ratio_2017 logMMR2017\n                                            (1)                 (2)                  (3)                 (4)    \n----------------------------------------------------------------------------------------------------------------\nGenderExclusion                         414.685***            3.789***                                          \n                                         (73.811)             (0.485)                                           \n                                                                                                                \nlogGenderExclusion                                                               106.717***            1.114*** \n                                                                                  (18.158)             (0.110)  \n                                                                                                                \nConstant                                  32.225              2.725***           327.106***            5.616*** \n                                         (31.274)             (0.206)             (31.057)             (0.188)  \n                                                                                                                \n----------------------------------------------------------------------------------------------------------------\nObservations                                155                 155                  155                 155    \nR2                                         0.171               0.285                0.184               0.401   \nAdjusted R2                                0.166               0.280                0.179               0.397   \nResidual Std. Error (df = 153)            221.207              1.454               219.444              1.331   \nF Statistic (df = 1; 153)                31.564***           61.022***            34.542***           102.406***\n================================================================================================================\nNote:                                                                                *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01\n\n\nIn all of the models the explanatory variable is found to have a positive and highly significant relationship at the 0.01 (1%) level. But a comparison of these models shows us that the model with logged versions of both the explanatory and dependent variables is the strongest. The R^2 and Adjusted R^2 values are much higher for this model than the models with only one logged variable or no logged variables. For this reason, the rest of the models in this analysis will use the logMMR2017 in place of the Maternal_Mortality_Ratio_2017 variable and the logGenderExclusion in place of the GenderExclusion variable.\nNext, the model will be explored with confounding variables added in.\n\n\nCode\n# Explore the regression model with all of the confounding variables.\nsummary(lm(formula = logMMR2017 ~ logGenderExclusion +\n                                Democracy + \n                                SkilledBirthAtt +\n                                logGDP2017 +\n                                Expend_Health_pct_GDP,\n                              data = MMR_Analysis))\n\n\n\nCall:\nlm(formula = logMMR2017 ~ logGenderExclusion + Democracy + SkilledBirthAtt + \n    logGDP2017 + Expend_Health_pct_GDP, data = MMR_Analysis)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.54176 -0.49592 -0.01031  0.54233  1.84184 \n\nCoefficients:\n                       Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)           12.283710   0.532647  23.062  &lt; 2e-16 ***\nlogGenderExclusion     0.279864   0.102766   2.723  0.00724 ** \nDemocracy              0.012624   0.005257   2.401  0.01756 *  \nSkilledBirthAtt       -0.023589   0.005079  -4.644 7.44e-06 ***\nlogGDP2017            -0.660139   0.081316  -8.118 1.64e-13 ***\nExpend_Health_pct_GDP -0.067852   0.052579  -1.290  0.19888    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.8357 on 149 degrees of freedom\nMultiple R-squared:  0.7699,    Adjusted R-squared:  0.7622 \nF-statistic: 99.74 on 5 and 149 DF,  p-value: &lt; 2.2e-16\n\n\nAfter adding in the confounding variables, the explanatory variable logGenderExclusion is still statistically significant at the 0.01 (1%) level. The variable Democracy was found to be significant at the 0.05 (5%) level. While the Expend_Health_pct_GDP variable was not found to be significant, because it relates to GDP it is possible that it could be significant as an interaction term with logGDP2017.\nThe following model will explore adding Expend_Health_pct_GDP and logGDP2017 as an interaction term and compare it to this previous model.\n\n\nCode\n# Model with interaction term and without\nMMR_1 &lt;- (lm(formula = logMMR2017 ~ logGenderExclusion +\n                                Democracy + \n                                SkilledBirthAtt +\n                                logGDP2017 +\n                                Expend_Health_pct_GDP,\n                              data = MMR_Analysis))\n\nMMR_2 &lt;- (lm(formula = logMMR2017 ~ logGenderExclusion +\n                                Democracy +\n                                SkilledBirthAtt +\n                                logGDP2017 +\n                                Expend_Health_pct_GDP +\n                                Expend_Health_pct_GDP*logGDP2017,\n                              data = MMR_Analysis))\n\nMMR_3 &lt;- (lm(formula = logMMR2017 ~ logGenderExclusion +\n                                Democracy +\n                                SkilledBirthAtt +\n                                logGDP2017,\n                              data = MMR_Analysis))\n\nstargazer(MMR_1, MMR_2, MMR_3, type = 'text')\n\n\n\n=========================================================================================================\n                                                           Dependent variable:                           \n                                 ------------------------------------------------------------------------\n                                                                logMMR2017                               \n                                           (1)                     (2)                     (3)           \n---------------------------------------------------------------------------------------------------------\nlogGenderExclusion                      0.280***                0.297***                 0.335***        \n                                         (0.103)                 (0.108)                 (0.094)         \n                                                                                                         \nDemocracy                                0.013**                 0.013**                 0.013**         \n                                         (0.005)                 (0.005)                 (0.005)         \n                                                                                                         \nSkilledBirthAtt                         -0.024***               -0.022***               -0.025***        \n                                         (0.005)                 (0.006)                 (0.005)         \n                                                                                                         \nlogGDP2017                              -0.660***               -0.709***               -0.703***        \n                                         (0.081)                 (0.121)                 (0.074)         \n                                                                                                         \nExpend_Health_pct_GDP                    -0.068                  -0.222                                  \n                                         (0.053)                 (0.286)                                 \n                                                                                                         \nlogGDP2017:Expend_Health_pct_GDP                                  0.016                                  \n                                                                 (0.030)                                 \n                                                                                                         \nConstant                                12.284***               12.584***               12.597***        \n                                         (0.533)                 (0.764)                 (0.475)         \n                                                                                                         \n---------------------------------------------------------------------------------------------------------\nObservations                               155                     155                     155           \nR2                                        0.770                   0.770                   0.767          \nAdjusted R2                               0.762                   0.761                   0.761          \nResidual Std. Error                 0.836 (df = 149)        0.838 (df = 148)         0.838 (df = 150)    \nF Statistic                      99.735*** (df = 5; 149) 82.774*** (df = 6; 148) 123.704*** (df = 4; 150)\n=========================================================================================================\nNote:                                                                         *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01\n\n\nIt does not appear that the interaction term improved the model (the R^2 value is the same as the model without the interaction term, but the adjusted R^2 is slightly lower as we were penalized for adding another variable). The interaction term was also not significant, indicating that we do not need to keep it. Interestingly, the model without the interaction term or the Expend_Health_pct_GDP variable was the weakest of the three, with the lowest R^2 value. This indicates that while the Expend_Health_pct_GDP variable is not significant, its presence improves the model. Since the first two models have such similar R^2 values, we can review the AIC and BIC values to compare these models.\n\n\nCode\nbroom::glance(MMR_1)\n\n\n# A tibble: 1 × 12\n  r.squared adj.r.squa…¹ sigma stati…²  p.value    df logLik   AIC   BIC devia…³\n      &lt;dbl&gt;        &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n1     0.770        0.762 0.836    99.7 9.63e-46     5  -189.  392.  413.    104.\n# … with 2 more variables: df.residual &lt;int&gt;, nobs &lt;int&gt;, and abbreviated\n#   variable names ¹​adj.r.squared, ²​statistic, ³​deviance\n\n\nCode\nbroom::glance(MMR_2)\n\n\n# A tibble: 1 × 12\n  r.squared adj.r.squa…¹ sigma stati…²  p.value    df logLik   AIC   BIC devia…³\n      &lt;dbl&gt;        &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n1     0.770        0.761 0.838    82.8 8.74e-45     6  -189.  394.  418.    104.\n# … with 2 more variables: df.residual &lt;int&gt;, nobs &lt;int&gt;, and abbreviated\n#   variable names ¹​adj.r.squared, ²​statistic, ³​deviance\n\n\nThe model without the interaction term is slightly stronger as the AIC and BIC are slightly smaller. The model without the interaction term has an AIC of 392.101 and a BIC of 413.405 The model with the interaction term has an AIC of 393.7846 and a BIC of 418.132.\nThe strongest model that will be used in diagnostics will be MMR_1, the model without the interaction term. This model supports the hypothesis that women’s political inclusion has an effect on maternal mortality. While the logGenderExclusion variable was found to have a more significant effect on logMMR2017 without any confounders, once control variables were introduced the model became stronger and a better fit and the effect of logGenderExclusion remained statistically significant.\nThe final model can be written:\nlogMMR2017 = 12.28 + 0.2799logGenderExclusion + 0.0126Democracy - 0.0236SkilledBirthAttendant - 0.6601logGDP2017 - 0.0679Expend_Health_Pct_GDP\nWhich indicates that, controlling for democracy, the percentage of births with skilled attendants, GDP, and the percentage of GDP spent on health, a one unit increase in logGenderExclusion increases logMMR2017 by 0.28. A 1% increase in GenderExclusion leads to a 0.2785% increase in Maternal_Mortality_2017.\nFormula: 1% increase in x multiplies y by exp(0.2799*log(1.01)), and increases y by 0.2799*log(1.01) = 0.2785\n\n\nHypothesis #2\nA note about the mediation analysis: I tried this analysis with the logged version of the GenderExclusion variable and with the regular version and they had different results. Both had significant results, but in different ways. I don’t understand mediation analysis well enough to know which we should go with. Would be great to chat more about this in depth. For now I am including both models in this writeup.\nHypothesis #2 posits that the relationship between gender exclusion and maternal mortality is mediated by the presence of skilled attendants at birth. This hypothesis proposes that the empowerment and inclusion of women in a country has a positive effect on the presence of skilled attendants at birth, which in turn has the effect of reducing maternal mortality. As discussed in the introduction, previous research has identified the presence of a skilled attendant at birth as a factor that reduces the incidence of maternal mortality.25 26 This research question seeks to determine whether the presence of a skilled birth attendant acts as a mediator between the explanatory variable and the dependent variable in this analysis.\nThe process of mediation analysis involves identifying a model that shows the total effect (the effect of the explanatory variable on the dependent variable without the presence of the mediator variable), and two models that show indirect effect (the effect between the explanatory variable and the mediator, and the mediator and the dependent variable while controlling for the explanatory variable). Following that, these models are processed through the mediate() function, which estimates the mediation relationship.\n\n\nCode\n# Total effect model (effect of the explanatory variable on the dependent variable without the presence of the mediator variable). First without covariates, second includes confounding variables (but not the mediator).\nTotEff_noCov &lt;- (lm(formula = logMMR2017 ~ logGenderExclusion,\n                               data = MMR_Analysis))\n\nTotEff_wCov &lt;- (lm(formula = logMMR2017 ~ logGenderExclusion + \n                                 Democracy +\n                                 logGDP2017 +\n                                 Expend_Health_pct_GDP,\n                               data = MMR_Analysis))\n# Attempt with the unlogged version of GenderExclusion variable\nTotEff_wCov_noL &lt;- (lm(formula = logMMR2017 ~ GenderExclusion + \n                                 Democracy +\n                                 logGDP2017 +\n                                 Expend_Health_pct_GDP,\n                               data = MMR_Analysis))\n\nstargazer(TotEff_noCov, TotEff_wCov, TotEff_wCov_noL, type = 'text')\n\n\n\n================================================================================================\n                                                 Dependent variable:                            \n                      --------------------------------------------------------------------------\n                                                      logMMR2017                                \n                                (1)                      (2)                      (3)           \n------------------------------------------------------------------------------------------------\nlogGenderExclusion            1.114***                 0.269**                                  \n                              (0.110)                  (0.110)                                  \n                                                                                                \nGenderExclusion                                                                 0.924**         \n                                                                                (0.399)         \n                                                                                                \nDemocracy                                               0.009*                   0.010*         \n                                                       (0.006)                  (0.006)         \n                                                                                                \nlogGDP2017                                            -0.821***                -0.842***        \n                                                       (0.078)                  (0.077)         \n                                                                                                \nExpend_Health_pct_GDP                                  -0.105*                  -0.120**        \n                                                       (0.055)                  (0.054)         \n                                                                                                \nConstant                      5.616***                11.693***                11.213***        \n                              (0.188)                  (0.552)                  (0.623)         \n                                                                                                \n------------------------------------------------------------------------------------------------\nObservations                    155                      155                      155           \nR2                             0.401                    0.737                    0.735          \nAdjusted R2                    0.397                    0.730                    0.728          \nResidual Std. Error       1.331 (df = 153)         0.891 (df = 150)         0.893 (df = 150)    \nF Statistic           102.406*** (df = 1; 153) 104.894*** (df = 4; 150) 104.268*** (df = 4; 150)\n================================================================================================\nNote:                                                                *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01\n\n\nAs noted in the section on hypothesis #1, the effect of the logGenderExclusion variable on logMMR2017 is significant without covariates at the level of 0.01 (1%). Once control variables are added the model is much stronger, as evidenced by the higher R-squared values, but the explanatory variable is a little less significant (it is now at the level of 0.05 (5%)).\nThe model with an the original, unlogged version of the GenderExclusion variable is also quite strong, although it has slightly smaller R^2 and adjusted R^2 values. THe GenderExclusion variable is still significant at the level of 0.05 (5%).\nThe first indirect effect model will show the relationship between the explanatory variable and the mediator.\n\n\nCode\n# effect between the explanatory variable and the mediator, no covariates\nIndEff1_noCov &lt;- (lm(formula = SkilledBirthAtt ~ logGenderExclusion,\n                              data = MMR_Analysis))\n\n# effect between the explanatory variable and the mediator, with covariates\nIndEff1_wCov &lt;- (lm(formula = SkilledBirthAtt ~ logGenderExclusion +\n                                Democracy + \n                                logGDP2017 +\n                                Expend_Health_pct_GDP,\n                              data = MMR_Analysis))\n# effect between the explanatory variable and the mediator, with covariates, GenderExclusion not logged\nIndEff1_wCov_noL &lt;- (lm(formula = SkilledBirthAtt ~ GenderExclusion +\n                                Democracy + \n                                logGDP2017 +\n                                Expend_Health_pct_GDP,\n                              data = MMR_Analysis))\n\nstargazer(IndEff1_noCov, IndEff1_wCov, IndEff1_wCov_noL, type = 'text')\n\n\n\n=============================================================================================\n                                                Dependent variable:                          \n                      -----------------------------------------------------------------------\n                                                  SkilledBirthAtt                            \n                                (1)                     (2)                     (3)          \n---------------------------------------------------------------------------------------------\nlogGenderExclusion           -8.732***                 0.443                                 \n                              (1.329)                 (1.652)                                \n                                                                                             \nGenderExclusion                                                              -13.095**       \n                                                                              (5.907)        \n                                                                                             \nDemocracy                                             0.145*                   0.072         \n                                                      (0.084)                 (0.085)        \n                                                                                             \nlogGDP2017                                           6.816***                6.344***        \n                                                      (1.183)                 (1.141)        \n                                                                                             \nExpend_Health_pct_GDP                                 1.560*                   0.880         \n                                                      (0.836)                 (0.794)        \n                                                                                             \nConstant                     75.120***               25.038***               35.289***       \n                              (2.274)                 (8.315)                 (9.228)        \n                                                                                             \n---------------------------------------------------------------------------------------------\nObservations                    155                     155                     155          \nR2                             0.220                   0.465                   0.482         \nAdjusted R2                    0.215                   0.451                   0.468         \nResidual Std. Error      16.064 (df = 153)       13.433 (df = 150)       13.222 (df = 150)   \nF Statistic           43.159*** (df = 1; 153) 32.630*** (df = 4; 150) 34.894*** (df = 4; 150)\n=============================================================================================\nNote:                                                             *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01\n\n\nCode\n## question should we drop the Democracy and Expend_Health_pct_GDP variables because they are no longer significant here? \n# Also should we try the interaction term again?\n\n\nThe first indirect effect model show that the logGenderExclusion variable is statistically significant without any covariates, but is no longer significant when the control variables are included. This is not the case with the unlogged, original version of the GenderExclusion variable, which remains statistically significant at the 0.05 (5%) level even with the confounding variables included. A comparison of these models show that the model with the unlogged version of GenderExclusion is the strongest model, with the highest R^2 values.\nThe second and final indirect effect model will show the relationship between the mediator and the dependent variable while controlling for the explanatory variable.\n\n\nCode\n# effect between the explanatory variable and the mediator, no covariates\nIndEff2_noCov &lt;- (lm(formula = logMMR2017 ~ SkilledBirthAtt + \n                                 logGenderExclusion,\n                              data = MMR_Analysis)) \n\n# effect between the explanatory variable and the mediator, with covariates\nIndEff2_wCov &lt;- (lm(formula = logMMR2017 ~ SkilledBirthAtt + \n                                 logGenderExclusion +\n                                 Democracy + \n                                 logGDP2017 +\n                                 Expend_Health_pct_GDP,\n                               data = MMR_Analysis))\n\n# with unlogged version of GenderExclusion\nIndEff2_wCov_noL &lt;- (lm(formula = logMMR2017 ~ SkilledBirthAtt + \n                                 GenderExclusion +\n                                 Democracy + \n                                 logGDP2017 +\n                                 Expend_Health_pct_GDP,\n                               data = MMR_Analysis))\n\n## Note for prof - I included the IV here because this is how they did it here: https://towardsdatascience.com/doing-and-reporting-your-first-mediation-analysis-in-r-2fe423b92171\n# Let me know if I should run this without the IV and focus just on the mediator's affect on the DV without the IV.\n\nstargazer(IndEff2_noCov, IndEff2_wCov, IndEff2_wCov_noL, type = 'text')\n\n\n\n==============================================================================================\n                                                Dependent variable:                           \n                      ------------------------------------------------------------------------\n                                                     logMMR2017                               \n                                (1)                      (2)                     (3)          \n----------------------------------------------------------------------------------------------\nSkilledBirthAtt              -0.049***                -0.024***               -0.022***       \n                              (0.005)                  (0.005)                 (0.005)        \n                                                                                              \nlogGenderExclusion            0.687***                0.280***                                \n                              (0.101)                  (0.103)                                \n                                                                                              \nGenderExclusion                                                                0.639*         \n                                                                               (0.385)        \n                                                                                              \nDemocracy                                              0.013**                 0.012**        \n                                                       (0.005)                 (0.005)        \n                                                                                              \nlogGDP2017                                            -0.660***               -0.704***       \n                                                       (0.081)                 (0.080)        \n                                                                                              \nExpend_Health_pct_GDP                                  -0.068                  -0.101*        \n                                                       (0.053)                 (0.051)        \n                                                                                              \nConstant                      9.290***                12.284***               11.980***       \n                              (0.435)                  (0.533)                 (0.620)        \n                                                                                              \n----------------------------------------------------------------------------------------------\nObservations                    155                      155                     155          \nR2                             0.610                    0.770                   0.763         \nAdjusted R2                    0.605                    0.762                   0.755         \nResidual Std. Error       1.078 (df = 152)        0.836 (df = 149)        0.848 (df = 149)    \nF Statistic           118.744*** (df = 2; 152) 99.735*** (df = 5; 149) 95.874*** (df = 5; 149)\n==============================================================================================\nNote:                                                              *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01\n\n\nIn all of these second indirect effect models the explanatory variable is still statistically significant with the mediator present in the model, although in the case of the model with the unlogged version of GenderExclusion the significance is lower. The model with the logGenderExclusion variable and all of the covariates is stronger than the models without covariates and with the unlogged version of the GenderExclusion variable when comparing the R^2 values.\nSince it is not entirely clearly which model is stronger - the one with the logGenderExclusion explanatory variable, or the model with the unlogged GenderExclusion explanatory variable, we will include both version in a mediation analysis here.\n\nMediation analysis: logged IV\nThe mediate() function takes both indirect effect models to perform the mediation analysis and estimates whether there is a mediator effect by the SkilledBirthAtt variable between the logGenderExclusion and the logMMR2017 variables.\n\n\nCode\nresults &lt;- mediation::mediate(IndEff1_wCov, IndEff2_wCov, treat='logGenderExclusion', mediator='SkilledBirthAtt', boot=T)\n\nsummary(results)\n\n\n\nCausal Mediation Analysis \n\nNonparametric Bootstrap Confidence Intervals with the Percentile Method\n\n               Estimate 95% CI Lower 95% CI Upper p-value   \nACME            -0.0104      -0.0739         0.06   0.830   \nADE              0.2799       0.0898         0.48   0.002 **\nTotal Effect     0.2694       0.0811         0.47   0.006 **\nProp. Mediated  -0.0388      -0.4365         0.22   0.832   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nSample Size Used: 155 \n\n\nSimulations: 1000 \n\n\nCode\nplot(results)\n\n\n\n\n\nThe ACME (average causal mediation effects) accounts for the indirect effect of the explanatory variable (GenderExclusion) on the dependent variable (logMMR2017) that goes through the mediator (SkilledBirthAtt). This was not found to be significant.\nADE (average direct effects) shows the direct effect of the explanatory variable on the dependent variable. This was also calculated in the second indirect effect model above. In both cases the relationship between the explanatory variable and the dependent variable was found to be significant at the 0.01 (1%) level.\nThe Total Effect reports the sum of the direct and indirect effects of the explanatory variable on the dependent variable. This was also found in the first step of this process when the total effect was calculated. The effect of the explanatory variable on the dependent variable was found to be significant at the 0.5 (5%) level in the calculation above and here it has been found at the 0.01 (1%) level.\nFinally, Prop. Mediated indicates the proportion of the effect of the explanatory variable on the dependent variable that goes through the mediator. In this analysis it is not significant.\nIn sum, this model shows that while gender exclusion has a direct effect on maternal mortality, there is no statistically significant evidence that it was mediated via the presence of skilled birth attendants.\n\n\nMediation analysis: unlogged IV\nHere we’ll perform the analysis with the mediate() function again so that we can estimate whether there is a mediator effect by the SkilledBirthAtt variable between the GenderExclusion and the logMMR2017 variables.\n\n\nCode\nresults_noL &lt;- mediation::mediate(IndEff1_wCov_noL, IndEff2_wCov_noL, treat='GenderExclusion', mediator='SkilledBirthAtt', boot=T)\n\nsummary(results_noL)\n\n\n\nCausal Mediation Analysis \n\nNonparametric Bootstrap Confidence Intervals with the Percentile Method\n\n               Estimate 95% CI Lower 95% CI Upper p-value  \nACME             0.2847       0.0476         0.62   0.016 *\nADE              0.6392      -0.0813         1.43   0.084 .\nTotal Effect     0.9239       0.1687         1.81   0.022 *\nProp. Mediated   0.3081       0.0425         1.05   0.038 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nSample Size Used: 155 \n\n\nSimulations: 1000 \n\n\nCode\nplot(results_noL)\n\n\n\n\n\nThe ACME (average causal mediation effects) accounts for the indirect effect of the explanatory variable (GenderExclusion) on the dependent variable (logMMR2017) that goes through the mediator (SkilledBirthAtt). This was found to be significant at the 0.05 (5%) level.\nADE (average direct effects) shows the direct effect of the explanatory variable on the dependent variable. This was also calculated in the second indirect effect model above. In both cases the relationship between the explanatory variable and the dependent variable was found to be significant at the 0.1 (10%) level.\nThe Total Effect reports the sum of the direct and indirect effects of the explanatory variable on the dependent variable. This was also found in the first step of this process when the total effect was calculated. In both cases the effect of the explanatory variable on the dependent variable was found to be significant at the 0.1 (10%) level.\nFinally, Prop. Mediated indicates the proportion of the effect of the explanatory variable on the dependent variable that goes through the mediator. In this analysis it is significant at the 0.1 (10%) level.\nIn sum, for this model the effect of gender exclusion on maternal mortality was mediated via the presence of skilled birth attendants.\nNote to prof: I think there is likely more to say here about this analysis and interpretation of the plot in particular, but I don’t know this well yet, would be great to discuss further."
  },
  {
    "objectID": "posts/JustineShakespeare_FinalProject.html#model-diagnostics",
    "href": "posts/JustineShakespeare_FinalProject.html#model-diagnostics",
    "title": "Maternal Mortality and Women’s Political Inclusion",
    "section": "Model Diagnostics",
    "text": "Model Diagnostics\n\nResiduals vs Fitted\nIn this test the red line shows some curvature, but is mostly horizontal, suggesting that the linearity assumption has been met. The distribution of residuals is fairly even, which indicates that the assumption of constant variance has been met. Some residuals appear to stand out, indicating the existence of outliers.\n\n\nCode\nplot(MMR_1, which = 1)\n\n\n\n\n\n\n\nNormal Q-Q Plot\nMost of the points in the middle of this graph fall along the line, but the points at either end diverge from the line, especially at the bottom left. This indicates that the assumption of normality has been met.\n\n\nCode\nplot(MMR_2, which = 2)\n\n\n\n\n\n\n\nScale-Location\nThe red line in this graph has some curvature to it, it trends up and then back down again, creating a slight hump in the middle. This indicates that the assumption of constant variance has possibly not been fully met.\n\n\nCode\nplot(MMR_2, which = 3)\n\n\n\n\n\n\n\nCook’s Distance\nThis graph shows that this model violates the influential observation assumption because several points are larger than 4/n, which for this model is 4/155 = 0.0258.\n\n\nCode\nplot(MMR_2, which = 4)"
  },
  {
    "objectID": "posts/JustineShakespeare_FinalProject.html#footnotes",
    "href": "posts/JustineShakespeare_FinalProject.html#footnotes",
    "title": "Maternal Mortality and Women’s Political Inclusion",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nMaternal mortality, World Health Organization (2023) https://www.who.int/news-room/fact-sheets/detail/maternal-mortality↩︎\nGlobal Declines in Maternal Mortality Have Stalled, New York Times (2023) https://www.nytimes.com/2023/02/22/health/pregnancy-complications-death-who.html↩︎\nSingh GK. Trends and Social Inequalities in Maternal Mortality in the United States, 1969-2018. Int J MCH AIDS. 2021;10(1):29-42. doi: 10.21106/ijma.444. Epub 2020 Dec 30. PMID: 33442490; PMCID: PMC7792749. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7792749/↩︎\nMaternal Deaths Rose During the First Year of the Pandemic, New York Times (2022) https://www.nytimes.com/2022/02/23/health/maternal-deaths-pandemic.html↩︎\nMaternal mortality, World Health Organization (2023) https://www.who.int/news-room/fact-sheets/detail/maternal-mortality↩︎\nBetrán, A.P., Wojdyla, D., Posner, S.F. et al. National estimates for maternal mortality: an analysis based on the WHO systematic review of maternal mortality and morbidity. BMC Public Health 5, 131 (2005). https://doi.org/10.1186/1471-2458-5-131 https://bmcpublichealth.biomedcentral.com/articles/10.1186/1471-2458-5-131↩︎\nGirum T, Wasie A. Correlates of maternal mortality in developing countries: an ecological study in 82 countries. Matern Health Neonatol Perinatol. 2017 Nov 7;3:19. doi: 10.1186/s40748-017-0059-8. PMID: 29142757; PMCID: PMC5674830. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5674830/↩︎\nKarlsen S, Say L, Souza JP, Hogue CJ, Calles DL, Gülmezoglu AM, Raine R. The relationship between maternal education and mortality among women giving birth in health care institutions: analysis of the cross sectional WHO Global Survey on Maternal and Perinatal Health. BMC Public Health. 2011 Jul 29;11:606. doi: 10.1186/1471-2458-11-606. PMID: 21801399; PMCID: PMC3162526. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3162526/↩︎\nGirum T, Wasie A. Correlates of maternal mortality in developing countries: an ecological study in 82 countries. Matern Health Neonatol Perinatol. 2017 Nov 7;3:19. doi: 10.1186/s40748-017-0059-8. PMID: 29142757; PMCID: PMC5674830. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5674830/↩︎\nMaternal mortality, World Health Organization (2023) https://www.who.int/news-room/fact-sheets/detail/maternal-mortality↩︎\nChirowa F, Atwood S, Van der Putten M. Gender inequality, health expenditure and maternal mortality in sub-Saharan Africa: A secondary data analysis. Afr J Prim Health Care Fam Med. 2013 Aug 13;5(1):471. doi: 10.4102/phcfm.v5i1.471. PMCID: PMC4709496. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4709496/↩︎\nMATERNAL MORTALITY AND WOMEN’S POLITICAL POWER, Sonia R. Bhalotra, Damian Clarke, Joseph F. Gomes, Atheendar, Venkataramani. NATIONAL BUREAU OF ECONOMIC RESEARCH, (June 2022) https://www.nber.org/system/files/working_papers/w30103/w30103.pdf↩︎\nThe State of the World’s Children 2021: Statistical tables, UNICEF (2021) https://data.unicef.org/resources/dataset/the-state-of-the-worlds-children-2021-statistical-tables/↩︎\nIndicator Metadata Registry List, https://www.who.int/data/gho/indicator-metadata-registry/imr-details/26↩︎\nCoppedge, Michael, John Gerring, Carl Henrik Knutsen, Staffan I. Lindberg, Jan Teorell, David Altman, Michael Bernhard, Agnes Cornell, M. Steven Fish, Lisa Gastaldi, Haakon Gjerløw, Adam Glynn, Ana Good God, Sandra Grahn, Allen Hicken, Katrin Kinzelbach, Joshua Krusell, Kyle L. Marquardt, Kelly McMann, Valeriya Mechkova, Juraj Medzihorsky, Natalia Natsika, Anja Neundorf, Pamela Paxton, Daniel Pemstein, Josefine Pernes, Oskar Rydén, Johannes von Römer, Brigitte Seim, Rachel Sigman, Svend-Erik Skaaning, Jeffrey Staton, Aksel Sundström, Eitan Tzelgov, Yi-ting Wang, Tore Wig, Steven Wilson and Daniel Ziblatt. 2023. “V-Dem [Country-Year/Country-Date] Dataset v13” Varieties of Democracy (V-Dem) Project. https://doi.org/10.23696/vdemds23.↩︎\nPemstein et al. (2023, V-Dem Working Paper Series 2023:21); V-Dem Codebook (see suggested citation at the top of this document).↩︎\nCoppedge, Michael, John Gerring, Carl Henrik Knutsen, Staffan I. Lindberg, Jan Teorell, David Altman, Michael Bernhard, Agnes Cornell, M. Steven Fish, Lisa Gastaldi, Haakon Gjerløw, Adam Glynn, Sandra Grahn, Allen Hicken, Katrin Kinzelbach, Kyle L. Marquardt, Kelly McMann, Valeriya Mechkova, Anja Neundorf, Pamela Paxton, Daniel Pemstein, Oskar Rydén, Johannes von Römer, Brigitte Seim, Rachel Sigman, Svend-Erik Skaaning, Jeffrey Staton, Aksel Sundström, Eitan Tzelgov, Luca Uberti, Yi-ting Wang, Tore Wig, and Daniel Ziblatt. 2023. “V-Dem Codebook v13” Varieties of Democracy (V-Dem) Project.↩︎\nCoppedge, Michael, John Gerring, Carl Henrik Knutsen, Staffan I. Lindberg, Jan Teorell, David Altman, Michael Bernhard, Agnes Cornell, M. Steven Fish, Lisa Gastaldi, Haakon Gjerløw, Adam Glynn, Sandra Grahn, Allen Hicken, Katrin Kinzelbach, Kyle L. Marquardt, Kelly McMann, Valeriya Mechkova, Anja Neundorf, Pamela Paxton, Daniel Pemstein, Oskar Rydén, Johannes von Römer, Brigitte Seim, Rachel Sigman, Svend-Erik Skaaning, Jeffrey Staton, Aksel Sundström, Eitan Tzelgov, Luca Uberti, Yi-ting Wang, Tore Wig, and Daniel Ziblatt. 2023. “V-Dem Codebook v13” Varieties of Democracy (V-Dem) Project.↩︎\nMATERNAL MORTALITY AND WOMEN’S POLITICAL POWER, Sonia R. Bhalotra, Damian Clarke, Joseph F. Gomes, Atheendar, Venkataramani. NATIONAL BUREAU OF ECONOMIC RESEARCH, (June 2022) https://www.nber.org/system/files/working_papers/w30103/w30103.pdf↩︎\nThe State of the World’s Children 2021: Statistical tables, UNICEF (2021) https://data.unicef.org/resources/dataset/the-state-of-the-worlds-children-2021-statistical-tables/↩︎\nhttps://www.imf.org/external/datamapper/NGDPDPC@WEO/OEMDC/ADVEC/WEOWORLD↩︎\nThe State of the World’s Children 2021: Statistical tables, UNICEF (2021) https://data.unicef.org/resources/dataset/the-state-of-the-worlds-children-2021-statistical-tables/↩︎\nCoppedge, Michael, John Gerring, Carl Henrik Knutsen, Staffan I. Lindberg, Jan Teorell, David Altman, Michael Bernhard, Agnes Cornell, M. Steven Fish, Lisa Gastaldi, Haakon Gjerløw, Adam Glynn, Ana Good God, Sandra Grahn, Allen Hicken, Katrin Kinzelbach, Joshua Krusell, Kyle L. Marquardt, Kelly McMann, Valeriya Mechkova, Juraj Medzihorsky, Natalia Natsika, Anja Neundorf, Pamela Paxton, Daniel Pemstein, Josefine Pernes, Oskar Rydén, Johannes von Römer, Brigitte Seim, Rachel Sigman, Svend-Erik Skaaning, Jeffrey Staton, Aksel Sundström, Eitan Tzelgov, Yi-ting Wang, Tore Wig, Steven Wilson and Daniel Ziblatt. 2023. “V-Dem [Country-Year/Country-Date] Dataset v13” Varieties of Democracy (V-Dem) Project. https://doi.org/10.23696/vdemds23.↩︎\nPolity 5 (Marshall and Jaggers 2020).↩︎\nBetrán, A.P., Wojdyla, D., Posner, S.F. et al. National estimates for maternal mortality: an analysis based on the WHO systematic review of maternal mortality and morbidity. BMC Public Health 5, 131 (2005). https://doi.org/10.1186/1471-2458-5-131 https://bmcpublichealth.biomedcentral.com/articles/10.1186/1471-2458-5-131↩︎\nGirum T, Wasie A. Correlates of maternal mortality in developing countries: an ecological study in 82 countries. Matern Health Neonatol Perinatol. 2017 Nov 7;3:19. doi: 10.1186/s40748-017-0059-8. PMID: 29142757; PMCID: PMC5674830. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5674830/↩︎"
  },
  {
    "objectID": "posts/Hw2_thrishul.html",
    "href": "posts/Hw2_thrishul.html",
    "title": "Homework - 2",
    "section": "",
    "text": "The time between the date a patient was recommended for heart surgery and the surgery date for cardiac patients in Ontario was collected by the Cardiac Care Network (“Wait Times Data Guide,” Ministry of Health and Long-Term Care, Ontario, Canada, 2006). The sample mean and sample standard deviation for wait times (in days) of patients for two cardiac procedures are given in the accompanying table. Assume that the sample is representative of the Ontario population\n Surgical Procedure Sample Size Mean Wait Time Standard Deviation\nBypass 539 19 10 Angiography 847 18 9\n\n\n\nCode\n# To construct a confidence interval for the mean wait time for each procedure, we will use the following formula:\n\n# CI = x̄ ± zα/2 * (σ/√n)\n# For bypass surgery:\n# x̄ = 19\n# σ = 10\n# n = 539\n# zα/2 = 1.645\n\n# CI = 19 ± 1.645 * (10/√539) = (17.8, 20.2)\n\n# For angiography:\n# x̄ = 18\n# σ = 9\n# n = 847\n# zα/2 = 1.645\n\n# CI = 18 ± 1.645 * (9/√847) = (17.2, 18.8)\n\n# The confidence interval for bypass surgery is (17.8, 20.2) and the confidence interval for angiography is (17.2, 18.8). We can see that the confidence interval for angiography is narrower, which means that we are more certain about the mean wait time for angiography than for bypass surgery."
  },
  {
    "objectID": "posts/Hw2_thrishul.html#question-1",
    "href": "posts/Hw2_thrishul.html#question-1",
    "title": "Homework - 2",
    "section": "",
    "text": "The time between the date a patient was recommended for heart surgery and the surgery date for cardiac patients in Ontario was collected by the Cardiac Care Network (“Wait Times Data Guide,” Ministry of Health and Long-Term Care, Ontario, Canada, 2006). The sample mean and sample standard deviation for wait times (in days) of patients for two cardiac procedures are given in the accompanying table. Assume that the sample is representative of the Ontario population\n Surgical Procedure Sample Size Mean Wait Time Standard Deviation\nBypass 539 19 10 Angiography 847 18 9\n\n\n\nCode\n# To construct a confidence interval for the mean wait time for each procedure, we will use the following formula:\n\n# CI = x̄ ± zα/2 * (σ/√n)\n# For bypass surgery:\n# x̄ = 19\n# σ = 10\n# n = 539\n# zα/2 = 1.645\n\n# CI = 19 ± 1.645 * (10/√539) = (17.8, 20.2)\n\n# For angiography:\n# x̄ = 18\n# σ = 9\n# n = 847\n# zα/2 = 1.645\n\n# CI = 18 ± 1.645 * (9/√847) = (17.2, 18.8)\n\n# The confidence interval for bypass surgery is (17.8, 20.2) and the confidence interval for angiography is (17.2, 18.8). We can see that the confidence interval for angiography is narrower, which means that we are more certain about the mean wait time for angiography than for bypass surgery."
  },
  {
    "objectID": "posts/Hw2_thrishul.html#question-2",
    "href": "posts/Hw2_thrishul.html#question-2",
    "title": "Homework - 2",
    "section": "Question 2",
    "text": "Question 2\nA survey of 1031 adult Americans was carried out by the National Center for Public Policy. Assume that the sample is representative of adult Americans. Among those surveyed, 567 believed that college education is essential for success. Find the point estimate, p, of the proportion of all adult Americans who believe that a college education is essential for success. Construct and interpret a 95% confidence interval for p.\n\n\nCode\n# The point estimate, p, of the proportion of all adult Americans who believe that a college education is essential for success is:\n\n# p = 567/1031 = 0.5498\n\n# To construct a 95% confidence interval for p, we will use the following formula:\n\n# CI = p ± zα/2 * √(p(1-p)/n)\n\n# 95% corresponds to a z-score of 1.96\n\n# CI = 0.5498 ± 1.96 * √(0.5498(1-0.5498)/1031) = (0.517, 0.582)\n# The 95% confidence interval for the proportion of all adult Americans who believe that a college education is essential for success is (0.517, 0.582). This means that we can be 95% confident that the true proportion of all adult Americans who believe that a college education is essential for success falls within this interval. \n\n# We can interpret this as saying that, based on the sample data, we estimate that between 51.7% and 58.2% of all adult Americans believe that a college education is essential for success."
  },
  {
    "objectID": "posts/Hw2_thrishul.html#question-3",
    "href": "posts/Hw2_thrishul.html#question-3",
    "title": "Homework - 2",
    "section": "Question 3",
    "text": "Question 3\nSuppose that the financial aid office of UMass Amherst seeks to estimate the mean cost of textbooks per semester for students. The estimate will be useful if it is within $5 of the true population mean (i.e. they want the confidence interval to have a length of $10 or less). The financial aid office is pretty sure that the amount spent on books varies widely, with most values between $30 and $200. They think that the population standard deviation is about a quarter of this range (in other words, you can assume they know the population standard deviation). Assuming the significance level to be 5%, what should be the size of the sample?\n\n\nCode\n# margin of error formula\n## ME = z * (sigma / sqrt(n))\n### solving for n\n#### n = (z * sigma/ME)^2\n##### replacing values\n###### z = significance level 5% = critical z value for 95% conf int = 1.96\n###### sigma = 1/4 of range of textbook costs = (200-30)/4 = 42.5\n###### ME = estimate within 5 of the true population mean = 5\n\n# calculate result\nn &lt;- round((1.96*42.5/5)^2)\n\n# print result\ncat(\"Ideal sample size:\", n, \"\\n\")\n\n\nIdeal sample size: 278 \n\n\nUsing the margin of error formula for confidence intervals and solving for n (sample size), we see that the ideal sample size (rounded to the nearest integer) is 278."
  },
  {
    "objectID": "posts/Hw2_thrishul.html#question-4",
    "href": "posts/Hw2_thrishul.html#question-4",
    "title": "Homework - 2",
    "section": "Question 4",
    "text": "Question 4\nAccording to a union agreement, the mean income for all senior-level workers in a large service company equals $500 per week. A representative of a women’s group decides to analyze whether the mean income μ for female employees matches this norm. For a random sample of nine female employees, ȳ = $410 and s = 90.\nTest whether the mean income of female employees differs from $500 per week. Include assumptions, hypotheses, test statistic, and P-value. Interpret the result.\nAssumptions:\nThe data is normally distributed.\nThe sample is a simple random sample.\nThe standard deviation of the population is unknown.\nHypotheses:\nH0: μ = 500\nH1: Ha: μ ≠ 500\nTest statistic:\nt = (ȳ - μ) / (s / sqrt(n)) = (410 - 500) / (90 / sqrt(9)) = -3 P-value:\nThe P-value is 0.01707168, which is less than the level of significance of 0.05. Therefore, we reject the null hypothesis and conclude that there is sufficient evidence to suggest that the mean income of female employees differs from $500 per week. Report the P-value for Ha: μ &lt; 500. Interpret.\nThe P-value is 0.008535841, which is less than the level of significance of 0.05. Therefore, we reject the null hypothesis and conclude that there is sufficient evidence to suggest that the mean income of female employees is less than $500 per week. Report and interpret the P-value for Ha: μ &gt; 500.\nThe P-value is 0.9914642, which is more than the level of significance of 0.05. Therefore, we fail to reject the null hypothesis and conclude that there is sufficient evidence to suggest that the mean income of female employees is greater than $500 per week.\n\n\nCode\n# A.3.a. Test statistic\n(410 - 500) / (90 / sqrt(9))\n\n\n[1] -3\n\n\nCode\n# A.4.a. P-value two-sided\npt(-3, 8) * 2\n\n\n[1] 0.01707168\n\n\nCode\n# B.1. P-value μ &lt; 500\npt(-3, 8)\n\n\n[1] 0.008535841\n\n\nCode\n# C.1. P-value μ &gt; 500\npt(-3, 8, lower.tail=FALSE)\n\n\n[1] 0.9914642"
  },
  {
    "objectID": "posts/Hw2_thrishul.html#question-5",
    "href": "posts/Hw2_thrishul.html#question-5",
    "title": "Homework - 2",
    "section": "Question 5",
    "text": "Question 5\nJones and Smith separately conduct studies to test H0: μ = 500 against Ha: μ ≠ 500, each with n = 1000. Jones gets ȳ = 519.5, with se = 10.0. Smith gets ȳ = 519.7, with se = 10.0.\nShow that t = 1.95 and P-value = 0.051 for Jones. Show that t = 1.97 and P-value = 0.049 for Smith.\nJones:\nt = (ȳ - μ) / se = (519.5 - 500) / 10 = 19.5 / 10 = 1.95\nP-value = round(2 * pt(q=1.95, df=999, lower.tail=FALSE), 3) = 0.051\nSmith:\nt = (ȳ - μ) / se = (519.7 - 500) / 10 = 19.7 / 10 = 1.97\nP-value = round(2 * pt(q=1.97, df=999, lower.tail=FALSE), 3) = 0.049\nUsing α = 0.05, for each study indicate whether the result is “statistically significant.”\nSince Jones’ P-value is greater than 0.05, we fail to reject the null hypothesis, indicating that his results are not statistically significant. In contrast, Smith’s P-value is less than 0.05, therefore we reject the null hypothesis and find his results statistically significant. Using this example, explain the misleading aspects of reporting the result of a test as “P ≤ 0.05” versus “P &gt; 0.05,” or as “reject H0” versus “Do not reject H0,” without reporting the actual P-value.\n“P ≤ 0.05” or “reject H0” without reporting the actual P-value can be misleading because it doesn’t provide information on how strong the evidence is against the null hypothesis. For example, both Jones and Smith barely pass the 0.05 threshold, having 0.049 and 0.051, respectively. Reporting this would help readers and analysts to take the strength of the evidence in consideration (in this case, “rejecting H0” or “failing to reject H0” should be taken with a grain of salt).\n\n\nCode\n# A.1.b. Proving Jones' p-value\nround(2 * pt(q=1.95, df=999, lower.tail=FALSE), 3)\n\n\n[1] 0.051\n\n\nCode\n# A.2.b. Proving Smith's p-value\nround(2 * pt(q=1.97, df=999, lower.tail=FALSE), 3)\n\n\n[1] 0.049"
  },
  {
    "objectID": "posts/Hw2_thrishul.html#question-6",
    "href": "posts/Hw2_thrishul.html#question-6",
    "title": "Homework - 2",
    "section": "Question 6",
    "text": "Question 6\nA school nurse wants to determine whether age is a factor in whether children choose a healthy snack after school. She conducts a survey of 300 middle school students, with the results below. Test at α = 0.05 the claim that the proportion who choose a healthy snack differs by grade level. What is the null hypothesis? Which test should we use? What is the conclusion?\nGrade level 6th grade 7th grade 8th grade Healthy snack 31 43 51 Unhealthy snack 69 57 49 Null hypothesis: There is no difference in the proportion of students who choose a healthy snack based on grade level.\nTest: Chi-squared test because we are assessing whether proportions of outcomes (choosing healthy versus unhealthy snacks) in each grade are equal or different.\nConclusion: Since the p-value is 0.01547, we reject the null hypothesis at the 0.05 level of significance and conclude that there is a significant difference in the proportion of healthy snack choices among the different grade levels.\n\n\nCode\n# create a matrix of the observed values\nobserved &lt;- matrix(c(31, 43, 51, 69, 57, 49), nrow = 2, byrow = TRUE)\n\n# perform the chi-squared test\nresult &lt;- chisq.test(observed)\n\n# print the results\nprint(observed)\n\n\n     [,1] [,2] [,3]\n[1,]   31   43   51\n[2,]   69   57   49\n\n\nCode\nprint(result)\n\n\n\n    Pearson's Chi-squared test\n\ndata:  observed\nX-squared = 8.3383, df = 2, p-value = 0.01547"
  },
  {
    "objectID": "posts/Hw2_thrishul.html#question-7",
    "href": "posts/Hw2_thrishul.html#question-7",
    "title": "Homework - 2",
    "section": "Question 7",
    "text": "Question 7\nPer-pupil costs (in thousands of dollars) for cyber charter school tuition for school districts in three areas are shown. Test the claim that there is a difference in means for the three areas, using an appropriate test. What is the null hypothesis? Which test should we use? What is the conclusion?\nArea 1 6.2 9.3 6.8 6.1 6.7 7.5 Area 2 7.5 8.2 8.5 8.2 7.0 9.3 Area 3 5.8 6.4 5.6 7.1 3.0 3.5 Null hypothesis: There is no difference in means for the three areas.\nTest: Analysis of Variance (ANOVA) because we are computing the difference between the means of three or more groups.\nConclusion: Given that the P-value associated to the F-statistic is 0.00397, we reject the null hypothesis and conclude that there is a significant difference in means for the three areas.\n\n\nCode\narea1 &lt;- c(6.2, 9.3, 6.8, 6.1, 6.7, 7.5)\narea2 &lt;- c(7.5, 8.2, 8.5, 8.2, 7.0, 9.3)\narea3 &lt;- c(5.8, 6.4, 5.6, 7.1, 3.0, 3.5)\n\nanova_result &lt;- aov(c(area1, area2, area3) ~ rep(c(\"Area 1\", \"Area 2\", \"Area 3\")\n                                                 , c(6, 6, 6)))\nprint(summary(anova_result))\n\n\n                                                 Df Sum Sq Mean Sq F value\nrep(c(\"Area 1\", \"Area 2\", \"Area 3\"), c(6, 6, 6))  2  25.66  12.832   8.176\nResiduals                                        15  23.54   1.569        \n                                                  Pr(&gt;F)   \nrep(c(\"Area 1\", \"Area 2\", \"Area 3\"), c(6, 6, 6)) 0.00397 **\nResiduals                                                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "posts/Homework_4_Saisrinivas.html#a",
    "href": "posts/Homework_4_Saisrinivas.html#a",
    "title": "Homework 4",
    "section": "A",
    "text": "A\n\nlibrary(alr4)\n\nLoading required package: car\n\n\nLoading required package: carData\n\n\nLoading required package: effects\n\n\nlattice theme set by effectsTheme()\nSee ?effectsTheme for details.\n\nlibrary(smss)\n\nx1 &lt;- 1240\nx2 &lt;- 18000\nactual_price &lt;- 145000\n\npredicted_price &lt;- -10536 + 53.8 * x1 + 2.84 * x2\nresidual &lt;- actual_price - predicted_price\ncat(\"A. Predicted selling price:\", predicted_price, \"\\nResidual:\", residual, \"\\n\")\n\nA. Predicted selling price: 107296 \nResidual: 37704 \n\n\nGiven the variable information above (y=selling price, x1=house size, x2=lot size) and the data in the prediction equation (1,240 sqft house on an 18,000 sqft lot), the home’s estimated selling price is $107,296. Now compute the difference between this and the actual selling price ($145,000).\nThe difference in pricing indicates that the house sold for $37,740 more than the projected selling price."
  },
  {
    "objectID": "posts/Homework_4_Saisrinivas.html#b",
    "href": "posts/Homework_4_Saisrinivas.html#b",
    "title": "Homework 4",
    "section": "B",
    "text": "B\n\nhome_size_coefficient &lt;- 53.8\ncat(\"B. House selling price predicted to increase for each square-foot increase in home size:\", home_size_coefficient, \"\\n\")\n\nB. House selling price predicted to increase for each square-foot increase in home size: 53.8"
  },
  {
    "objectID": "posts/Homework_4_Saisrinivas.html#c",
    "href": "posts/Homework_4_Saisrinivas.html#c",
    "title": "Homework 4",
    "section": "C",
    "text": "C\n\nhome_size_increase &lt;- 1\nlot_size_coefficient &lt;- 2.84\n\nlot_size_increase &lt;- (home_size_coefficient * home_size_increase) / lot_size_coefficient\ncat(\"C. Lot size increase needed to have the same impact as a one-square-foot increase in home size:\", lot_size_increase, \"\\n\")\n\nC. Lot size increase needed to have the same impact as a one-square-foot increase in home size: 18.94366"
  },
  {
    "objectID": "posts/Homework_4_Saisrinivas.html#question-2",
    "href": "posts/Homework_4_Saisrinivas.html#question-2",
    "title": "Homework 4",
    "section": "Question 2",
    "text": "Question 2"
  },
  {
    "objectID": "posts/Homework_4_Saisrinivas.html#a-1",
    "href": "posts/Homework_4_Saisrinivas.html#a-1",
    "title": "Homework 4",
    "section": "A",
    "text": "A\n\nlibrary(alr4)\ndata(salary)\n\nt_test &lt;- t.test(salary ~ sex, data = salary)\nt_test\n\n\n    Welch Two Sample t-test\n\ndata:  salary by sex\nt = 1.7744, df = 21.591, p-value = 0.09009\nalternative hypothesis: true difference in means between group Male and group Female is not equal to 0\n95 percent confidence interval:\n -567.8539 7247.1471\nsample estimates:\n  mean in group Male mean in group Female \n            24696.79             21357.14 \n\n\nWithout regard to school level or work rank, the average male pay is $24,696.79, while the average female wage is $21,357.14 - a $3,339.65 difference."
  },
  {
    "objectID": "posts/Homework_4_Saisrinivas.html#b-1",
    "href": "posts/Homework_4_Saisrinivas.html#b-1",
    "title": "Homework 4",
    "section": "B",
    "text": "B\n\nmodel &lt;- lm(salary ~ ., data = salary)\nconf_int &lt;- confint(model, level = 0.95)\nconf_int\n\n                 2.5 %      97.5 %\n(Intercept) 14134.4059 17357.68946\ndegreePhD    -663.2482  3440.47485\nrankAssoc    2985.4107  7599.31080\nrankProf     8396.1546 13841.37340\nsexFemale    -697.8183  3030.56452\nyear          285.1433   667.47476\nysdeg        -280.6397    31.49105\n\n\nThe pay gap between men and women is between -697.82 and 3030.57, according to a multiple linear regression with a 95% confidence range."
  },
  {
    "objectID": "posts/Homework_4_Saisrinivas.html#c-1",
    "href": "posts/Homework_4_Saisrinivas.html#c-1",
    "title": "Homework 4",
    "section": "C",
    "text": "C\n\nsummary(model)\n\n\nCall:\nlm(formula = salary ~ ., data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4045.2 -1094.7  -361.5   813.2  9193.1 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 15746.05     800.18  19.678  &lt; 2e-16 ***\ndegreePhD    1388.61    1018.75   1.363    0.180    \nrankAssoc    5292.36    1145.40   4.621 3.22e-05 ***\nrankProf    11118.76    1351.77   8.225 1.62e-10 ***\nsexFemale    1166.37     925.57   1.260    0.214    \nyear          476.31      94.91   5.018 8.65e-06 ***\nysdeg        -124.57      77.49  -1.608    0.115    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2398 on 45 degrees of freedom\nMultiple R-squared:  0.855, Adjusted R-squared:  0.8357 \nF-statistic: 44.24 on 6 and 45 DF,  p-value: &lt; 2.2e-16\n\n\nTo summarize, wage increases are $1388.61 if the person has a PhD, $5292.36 if the person is an Associate Professor, $11,118.75 if the person is a full/tenured Professor, $1,166.37 if the individual is female, $476.31 for each year the individual remains at their present rank.\nSalary, on the other hand, drops by $124.57 for each year after the individual obtained their maximum degree/rank level. Except for this one, all slopes are positive. Furthermore, an individual’s rank and the number of years spent at their present rank are statistically significant (less than 0.05)."
  },
  {
    "objectID": "posts/Homework_4_Saisrinivas.html#d",
    "href": "posts/Homework_4_Saisrinivas.html#d",
    "title": "Homework 4",
    "section": "D",
    "text": "D\n\n# Change the baseline category for rank and rerun the model\nsalary$rank &lt;- relevel(salary$rank, ref = \"Asst\")\nmodel2 &lt;- lm(salary ~ ., data = salary)\nsummary(model2)\n\n\nCall:\nlm(formula = salary ~ ., data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4045.2 -1094.7  -361.5   813.2  9193.1 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 15746.05     800.18  19.678  &lt; 2e-16 ***\ndegreePhD    1388.61    1018.75   1.363    0.180    \nrankAssoc    5292.36    1145.40   4.621 3.22e-05 ***\nrankProf    11118.76    1351.77   8.225 1.62e-10 ***\nsexFemale    1166.37     925.57   1.260    0.214    \nyear          476.31      94.91   5.018 8.65e-06 ***\nysdeg        -124.57      77.49  -1.608    0.115    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2398 on 45 degrees of freedom\nMultiple R-squared:  0.855, Adjusted R-squared:  0.8357 \nF-statistic: 44.24 on 6 and 45 DF,  p-value: &lt; 2.2e-16\n\n\nNone of the figured have changed."
  },
  {
    "objectID": "posts/Homework_4_Saisrinivas.html#e",
    "href": "posts/Homework_4_Saisrinivas.html#e",
    "title": "Homework 4",
    "section": "E",
    "text": "E\n\n# Exclude the rank variable and rerun the model\nmodel3 &lt;- lm(salary ~ degree + sex + year + ysdeg, data = salary)\nsummary(model3)\n\n\nCall:\nlm(formula = salary ~ degree + sex + year + ysdeg, data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8146.9 -2186.9  -491.5  2279.1 11186.6 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 17183.57    1147.94  14.969  &lt; 2e-16 ***\ndegreePhD   -3299.35    1302.52  -2.533 0.014704 *  \nsexFemale   -1286.54    1313.09  -0.980 0.332209    \nyear          351.97     142.48   2.470 0.017185 *  \nysdeg         339.40      80.62   4.210 0.000114 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3744 on 47 degrees of freedom\nMultiple R-squared:  0.6312,    Adjusted R-squared:  0.5998 \nF-statistic: 20.11 on 4 and 47 DF,  p-value: 1.048e-09\n\n\nSalary reduces by: when variable ‘rank’ is deleted: $3299.35 if the person holds a PhD, $1,286.54 if the person is female Salary gains, however, are $351.97 for each year spent at their present level, $339.40 for each year after earning their highest degree.\nThe slopes are divided 50/50 in terms of how many are positive and how many are negative. ‘degreePhD’, ‘year’, and ‘ysdeg’ are all statistically significant (less than 0.05), but’sexFemale’ is not."
  },
  {
    "objectID": "posts/Homework_4_Saisrinivas.html#f",
    "href": "posts/Homework_4_Saisrinivas.html#f",
    "title": "Homework 4",
    "section": "F",
    "text": "F\n\n# Create a new variable and run the multiple regression model\nsalary$new_dean &lt;- ifelse(salary$ysdeg &lt;= 15, \"New\", \"Old\")\nmodel4 &lt;- lm(salary ~ degree + sex + new_dean + year*ysdeg, data = salary)\nsummary(model4)\n\n\nCall:\nlm(formula = salary ~ degree + sex + new_dean + year * ysdeg, \n    data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8396.1 -2171.9  -352.5  2053.3 11061.3 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 16855.764   1508.902  11.171 1.45e-14 ***\ndegreePhD   -3205.140   1431.791  -2.239   0.0302 *  \nsexFemale   -1222.794   1345.288  -0.909   0.3682    \nnew_deanOld   550.409   2119.912   0.260   0.7963    \nyear          462.292    277.040   1.669   0.1021    \nysdeg         332.120    141.753   2.343   0.0236 *  \nyear:ysdeg     -4.525     10.083  -0.449   0.6557    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3814 on 45 degrees of freedom\nMultiple R-squared:  0.6334,    Adjusted R-squared:  0.5845 \nF-statistic: 12.96 on 6 and 45 DF,  p-value: 1.902e-08\n\n\n‘Year’ and ‘ysdeg’ are likely to be highly correlated."
  },
  {
    "objectID": "posts/Homework_4_Saisrinivas.html#question-3",
    "href": "posts/Homework_4_Saisrinivas.html#question-3",
    "title": "Homework 4",
    "section": "Question 3",
    "text": "Question 3"
  },
  {
    "objectID": "posts/Homework_4_Saisrinivas.html#a-2",
    "href": "posts/Homework_4_Saisrinivas.html#a-2",
    "title": "Homework 4",
    "section": "A",
    "text": "A\n\nlibrary(smss)\ndata(house.selling.price)\n\nmodel &lt;- lm(Price ~ Size + New, data = house.selling.price)\nsummary(model)\n\n\nCall:\nlm(formula = Price ~ Size + New, data = house.selling.price)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-205102  -34374   -5778   18929  163866 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -40230.867  14696.140  -2.738  0.00737 ** \nSize           116.132      8.795  13.204  &lt; 2e-16 ***\nNew          57736.283  18653.041   3.095  0.00257 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 53880 on 97 degrees of freedom\nMultiple R-squared:  0.7226,    Adjusted R-squared:  0.7169 \nF-statistic: 126.3 on 2 and 97 DF,  p-value: &lt; 2.2e-16\n\n\nWhile controlling for size, predictor variables ‘New’ and ‘Size’ have p-values of 0.00257 and 2e-16 respectively. Both p-values are statistically significant as they are less than 0.05. This indicates that the null hypothesis can be rejected (there is no relationship between ‘New’ and ‘Price’ OR between ‘Size’ and ‘Price’ of new homes). By calculating the correlation, we can see that the correlation between ‘New’ and ‘Size’ is 0.3843, which is a wear relationship."
  },
  {
    "objectID": "posts/Homework_4_Saisrinivas.html#b-2",
    "href": "posts/Homework_4_Saisrinivas.html#b-2",
    "title": "Homework 4",
    "section": "B",
    "text": "B\n\ncoefficients &lt;- coef(model)\ncoefficients\n\n(Intercept)        Size         New \n-40230.8668    116.1316  57736.2828"
  },
  {
    "objectID": "posts/Homework_4_Saisrinivas.html#c-2",
    "href": "posts/Homework_4_Saisrinivas.html#c-2",
    "title": "Homework 4",
    "section": "C",
    "text": "C\n\n# Calculate the predicted selling price\nsize &lt;- 3000\nnew &lt;- 1\nnot_new &lt;- 0\n\npredicted_new &lt;- coefficients[1] + coefficients[2] * size + coefficients[3] * new\npredicted_not_new &lt;- coefficients[1] + coefficients[2] * size + coefficients[3] * not_new\n\npredicted_new\n\n(Intercept) \n   365900.2 \n\npredicted_not_new\n\n(Intercept) \n   308163.9 \n\n\nIf the house is new, the predicted selling price is $365,900.20 If the house isn’t new, the predicted selling price is $308,163.90"
  },
  {
    "objectID": "posts/Homework_4_Saisrinivas.html#d-1",
    "href": "posts/Homework_4_Saisrinivas.html#d-1",
    "title": "Homework 4",
    "section": "D",
    "text": "D\n\nmodel_interaction &lt;- lm(Price ~ Size * New, data = house.selling.price)\nsummary(model_interaction)\n\n\nCall:\nlm(formula = Price ~ Size * New, data = house.selling.price)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-175748  -28979   -6260   14693  192519 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -22227.808  15521.110  -1.432  0.15536    \nSize           104.438      9.424  11.082  &lt; 2e-16 ***\nNew         -78527.502  51007.642  -1.540  0.12697    \nSize:New        61.916     21.686   2.855  0.00527 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 52000 on 96 degrees of freedom\nMultiple R-squared:  0.7443,    Adjusted R-squared:  0.7363 \nF-statistic: 93.15 on 3 and 96 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "posts/Homework_4_Saisrinivas.html#e-1",
    "href": "posts/Homework_4_Saisrinivas.html#e-1",
    "title": "Homework 4",
    "section": "E",
    "text": "E\n\ncoefficients_interaction &lt;- coef(model_interaction)\ncoefficients_interaction\n\n (Intercept)         Size          New     Size:New \n-22227.80793    104.43839 -78527.50235     61.91588 \n\nlibrary(ggplot2)\n\nggplot(data=house.selling.price,aes(x=Size,y=Price, color=New))+\n  geom_point()+\n  geom_smooth(method=\"lm\",se=F)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nThe variables in the scatterplot have a linear/correlative connection, as seen in the graph, demonstrating that as size grows, so does the price. However, based on the colors of the dots (which correlate to the age of the home), the link isn’t as simple. New houses (light blue dots) are distributed across the graph, mostly along the slope line. The older residences (dots that aren’t light blue) are primarily clustered towards the bottom right corner of the graph, although there are a handful that outperform the price/size of brand new houses."
  },
  {
    "objectID": "posts/Homework_4_Saisrinivas.html#f-1",
    "href": "posts/Homework_4_Saisrinivas.html#f-1",
    "title": "Homework 4",
    "section": "F",
    "text": "F\n\n# Calculate the predicted selling price with interaction\npredicted_new_interaction &lt;- coefficients_interaction[1] + coefficients_interaction[2] * size + coefficients_interaction[3] * new + coefficients_interaction[4] * size * new\npredicted_not_new_interaction &lt;- coefficients_interaction[1] + coefficients_interaction[2] * size + coefficients_interaction[3] * not_new + coefficients_interaction[4] * size * not_new\n\npredicted_new_interaction\n\n(Intercept) \n   398307.5 \n\npredicted_not_new_interaction\n\n(Intercept) \n   291087.4 \n\n\nThe predicted selling price for a New home with the above measurements is $398,307.50.\nThe predicted selling price for a not-new home with the above measurements is $291,087.40."
  },
  {
    "objectID": "posts/Homework_4_Saisrinivas.html#g",
    "href": "posts/Homework_4_Saisrinivas.html#g",
    "title": "Homework 4",
    "section": "G",
    "text": "G\n\n# Calculate the predicted selling price for a home of 1500 square feet\nsize_1500 &lt;- 1500\n\npredicted_new_1500 &lt;- coefficients_interaction[1] + coefficients_interaction[2] * size_1500 + coefficients_interaction[3] * new + coefficients_interaction[4] * size_1500 * new\npredicted_not_new_1500 &lt;- coefficients_interaction[1] + coefficients_interaction[2] * size_1500 + coefficients_interaction[3] * not_new + coefficients_interaction[4] * size_1500 * not_new\n\npredicted_new_1500\n\n(Intercept) \n   148776.1 \n\npredicted_not_new_1500\n\n(Intercept) \n   134429.8 \n\n\nThe predicted selling price of a new 1500sqft home is $148,776.10\nThe predicted selling price of a 1500sqft home that ISN’T new is $134,429.80\nIn comparison to the data in Part F (where the property size is doubled to 3000sqft), the expected selling prices in this section (G) are significantly lower. A new 3000sqft house is expected to sell for $398,307.50, while a new 1500sqft house is expected to sell for $148,694.70. The size and price both reduced by half, demonstrating that these two variables are connected and have a linear connection. A 3000sqft house that is NOT new is expected to sell for $291,087.40. A 1500sqft house that is not brand new is expected to sell for $134,429.80. The price difference between the two is $156,657.6. The price is more directly proportional to size than it is with new dwellings."
  },
  {
    "objectID": "posts/Homework_4_Saisrinivas.html#h",
    "href": "posts/Homework_4_Saisrinivas.html#h",
    "title": "Homework 4",
    "section": "H",
    "text": "H\n\n# Compare the adjusted R-squared values\nsummary(model)$adj.r.squared\n\n[1] 0.7168767\n\nsummary(model_interaction)$adj.r.squared\n\n[1] 0.7363181\n\n# Compare residuals' distribution\npar(mfrow = c(2, 1))\nhist(residuals(model), main = \"Residuals without Interaction\", xlab = \"Residuals\", col = \"lightblue\")\nhist(residuals(model_interaction), main = \"Residuals with Interaction\", xlab = \"Residuals\", col = \"lightblue\")\n\n\n\n\nI believe that a model with no interaction better describes the relationship between ‘Size’ and ‘New’ with the outcome price; the model with interaction best represents the relationship between ‘Size’ and ‘Price’ rather than ‘Size’ and ‘New’."
  },
  {
    "objectID": "posts/FinalProjectCheck1_Diana_Rinker.html#final-project-check-in-1-diana-rinker.",
    "href": "posts/FinalProjectCheck1_Diana_Rinker.html#final-project-check-in-1-diana-rinker.",
    "title": "Final Project check-in (1)",
    "section": "Final Project check-in (1), Diana Rinker.",
    "text": "Final Project check-in (1), Diana Rinker."
  },
  {
    "objectID": "posts/Rowley_Homework_1.html",
    "href": "posts/Rowley_Homework_1.html",
    "title": "Homework 1",
    "section": "",
    "text": "Question 1a: What does the distribution of LungCap look like?\nFirst, let’s read in the data from the Excel file:\n\n\nCode\nlibrary(readxl)\n\n\nWarning: package 'readxl' was built under R version 4.2.2\n\n\nCode\nLungCapData &lt;- read_excel(\"_data/LungCapData.xls\")\n\n\nThe distribution of LungCap looks as follows:\n\n\nCode\nhist(LungCapData$LungCap)\n\n\n\n\n\nThe histogram suggests that the distribution is close to a normal distribution. Most of the observations are close to the mean. Very few observations are close to the margins (0 and 15).\n\n\nQuestion 1b: Compare the probability distribution of the LungCap with respect to Males and Females. (Hint:make boxplots separated by gender using the boxplot() function)\n\n\nCode\nlibrary(dplyr)\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nCode\nlibrary(magrittr)\nlibrary(ggplot2)\nlibrary(markdown)\nlibrary(ggtext)\n\n\nWarning: package 'ggtext' was built under R version 4.2.2\n\n\nCode\n# generate boxplot:\n\nLungCap_Gender &lt;- LungCapData%&gt;%\n  group_by(LungCapData$Gender)%&gt;%\n  ggplot(aes(x=Gender, y=LungCap)) +\n  geom_point(alpha=.08, size=5) +\n  labs(x=\"Gender\", y=\"LungCap\", title=\"LungCap by Gender\") +\n  theme_light() +\n  geom_boxplot() +\n  theme(axis.text.x=element_markdown(hjust=1))\nLungCap_Gender\n\n\n\n\n\nCode\n# would next like to identify specific values for boxplots...\n\n\nThe distribution of LungCap is higher for males than it is for females. For males, the Q1 value is approximately 6.5, the median value is approximately 8, and the Q3 value is approximately 10. For females, the Q1 value is approximately 5.5, the median value is approximately 7.5, and the Q3 value is approximately 9.\n\n\nQuestion 1c: Compare the mean lung capacities for smokers and non-smokers. Does it make sense?\n\n\nCode\nSmoker_Mean &lt;- LungCapData %&gt;%\n  filter(Smoke==\"yes\")%&gt;%\n  select(Smoke, LungCap)%&gt;%\n  summarize(mean(LungCap, na.rm = TRUE))\nSmoker_Mean\n\n\n# A tibble: 1 × 1\n  `mean(LungCap, na.rm = TRUE)`\n                          &lt;dbl&gt;\n1                          8.65\n\n\nCode\nNonSmoker_Mean &lt;- LungCapData %&gt;%\n  filter(Smoke==\"no\")%&gt;%\n  select(Smoke, LungCap)%&gt;%\n  summarize(mean(LungCap, na.rm = TRUE))\nNonSmoker_Mean\n\n\n# A tibble: 1 × 1\n  `mean(LungCap, na.rm = TRUE)`\n                          &lt;dbl&gt;\n1                          7.77\n\n\nThe mean LungCap for smokers is 8.645, while the mean LungCap for non-smokers is 7.77. This doesn’t seem to make sense!\n\n\nQuestion 1d: Examine the relationship between Smoking and Lung Capacity within age groups: “less than or equal to 13”, “14 to 15”, “16 to 17”, and “greater than or equal to 18”.\n\n\nCode\nSmoker_Age_1 &lt;- LungCapData %&gt;%\n  filter(Smoke==\"yes\")%&gt;%\n  filter(Age&lt;=13)%&gt;%\n  select(Smoke, Age, LungCap)%&gt;%\n  summarize(mean(LungCap, na.rm = TRUE))\nSmoker_Age_1\n\n\n# A tibble: 1 × 1\n  `mean(LungCap, na.rm = TRUE)`\n                          &lt;dbl&gt;\n1                          7.20\n\n\nCode\nSmoker_Age_2 &lt;- LungCapData %&gt;%\n  filter(Smoke==\"yes\")%&gt;%\n  filter(Age==14:15)%&gt;%\n  select(Smoke, Age, LungCap)%&gt;%\n  summarize(mean(LungCap, na.rm = TRUE))\n\n\nWarning in Age == 14:15: longer object length is not a multiple of shorter\nobject length\n\n\nCode\nSmoker_Age_2\n\n\n# A tibble: 1 × 1\n  `mean(LungCap, na.rm = TRUE)`\n                          &lt;dbl&gt;\n1                          8.91\n\n\nCode\nSmoker_Age_3 &lt;- LungCapData %&gt;%\n  filter(Smoke==\"yes\")%&gt;%\n  filter(Age==16:17)%&gt;%\n  select(Smoke, Age, LungCap)%&gt;%\n  summarize(mean(LungCap, na.rm = TRUE))\n\n\nWarning in Age == 16:17: longer object length is not a multiple of shorter\nobject length\n\n\nCode\nSmoker_Age_3\n\n\n# A tibble: 1 × 1\n  `mean(LungCap, na.rm = TRUE)`\n                          &lt;dbl&gt;\n1                          9.60\n\n\nCode\nSmoker_Age_4 &lt;- LungCapData %&gt;%\n  filter(Smoke==\"yes\")%&gt;%\n  filter(Age&gt;=18)%&gt;%\n  select(Smoke, Age, LungCap)%&gt;%\n  summarize(mean(LungCap, na.rm = TRUE))\nSmoker_Age_4\n\n\n# A tibble: 1 × 1\n  `mean(LungCap, na.rm = TRUE)`\n                          &lt;dbl&gt;\n1                          10.5\n\n\nThe mean LungCap value for smokers 13 years or younger is 7.202, whereas it is 8.909 for those ages 14-15. For smokers 16-17, the mean LungCap value is 9.602, and for those 18 years and older, it is 10.513.\n\n\nQuestion 1e: Compare the lung capacities for smokers and non-smokers within each age group. Is your answer different from the one in part c? What could possibly be going on here?\n\n\nCode\nNonSmoker_Age_1 &lt;- LungCapData %&gt;%\n  filter(Smoke==\"no\")%&gt;%\n  filter(Age&lt;=13)%&gt;%\n  select(Smoke, Age, LungCap)%&gt;%\n  summarize(mean(LungCap, na.rm = TRUE))\nNonSmoker_Age_1\n\n\n# A tibble: 1 × 1\n  `mean(LungCap, na.rm = TRUE)`\n                          &lt;dbl&gt;\n1                          6.36\n\n\nCode\nNonSmoker_Age_2 &lt;- LungCapData %&gt;%\n  filter(Smoke==\"no\")%&gt;%\n  filter(Age==14:15)%&gt;%\n  select(Smoke, Age, LungCap)%&gt;%\n  summarize(mean(LungCap, na.rm = TRUE))\nNonSmoker_Age_2\n\n\n# A tibble: 1 × 1\n  `mean(LungCap, na.rm = TRUE)`\n                          &lt;dbl&gt;\n1                          8.84\n\n\nCode\nNonSmoker_Age_3 &lt;- LungCapData %&gt;%\n  filter(Smoke==\"no\")%&gt;%\n  filter(Age==16:17)%&gt;%\n  select(Smoke, Age, LungCap)%&gt;%\n  summarize(mean(LungCap, na.rm = TRUE))\nNonSmoker_Age_3\n\n\n# A tibble: 1 × 1\n  `mean(LungCap, na.rm = TRUE)`\n                          &lt;dbl&gt;\n1                          10.4\n\n\nCode\nNonSmoker_Age_4 &lt;- LungCapData %&gt;%\n  filter(Smoke==\"no\")%&gt;%\n  filter(Age&gt;=18)%&gt;%\n  select(Smoke, Age, LungCap)%&gt;%\n  summarize(mean(LungCap, na.rm = TRUE))\nNonSmoker_Age_4\n\n\n# A tibble: 1 × 1\n  `mean(LungCap, na.rm = TRUE)`\n                          &lt;dbl&gt;\n1                          11.1\n\n\nThe corresponding LungCap mean values for non-smokers are 6.359, 8.844, 10.394, and 11.069. All LungCaps are higher for non-smokers with the exception of those in the 13-and-under age bracket. It would make sense that non-smokers would have higher lung capacities, though it is interesting that the value is lower for the youngest age group. This does, however, correlate with the results to question 1c, which indicated that smokers have higher lung capacities than non-smokers; there could, perhaps, be other extenuating circumstances contributing to these lower rates of capacity, such as health issues, development, or exposure.\n\n\nQuestion 2a: What is the probability that a randomly selected inmate has exactly 2 prior convictions?\n\n\nCode\ninmate &lt;- c(128, 434, 160, 64, 24)\nmean(inmate)\n\n\n[1] 162\n\n\nCode\nsd(inmate)\n\n\n[1] 161.0838\n\n\nCode\n# mean=162\n# sd=161.1\n\nconvictions &lt;- c(0:4)\n\ndata &lt;- tibble(inmate, convictions)\n\ntwo &lt;- 160/summarise(data, sum(inmate))\n  two\n\n\n  sum(inmate)\n1   0.1975309\n\n\nHere, we can see that the probability of an inmate having exactly two prior convictions is 19.8%.\n\n\nQuestion 2b: What is the probability that a randomly selected inmate has fewer than 2 prior convictions?\n\n\nCode\nfewer_than_two &lt;- 128 + 434\nfewer_than_two/summarise(data, sum(inmate))\n\n\n  sum(inmate)\n1   0.6938272\n\n\nHere, we can see that the probability of a randomly selected inmate having fewer than two prior convictions is 69.4%.\n\n\nQuestion 2c: What is the probability that a randomly selected inmate has 2 or fewer prior convictions?\n\n\nCode\ntwo_or_fewer &lt;- 128 + 434 + 160\ntwo_or_fewer/summarise(data, sum(inmate))\n\n\n  sum(inmate)\n1    0.891358\n\n\nHere, we can see that the probability of a randomly selected inmate having two or fewer prior convictions is 89.1%.\n\n\nQuestion 2d: What is the probability that a randomly selected inmate has more than 2 prior convictions?\n\n\nCode\nmore_than_two &lt;- 160 + 64 + 24\nmore_than_two/summarise(data, sum(inmate))\n\n\n  sum(inmate)\n1   0.3061728\n\n\nHere, we can see that the probability of a randomly selected inmate having more than two prior convictions is 30.6%.\n\n\nQuestion 2e: What is the expected value1 for the number of prior convictions?\n\n\nCode\ndata_prob &lt;- transform(data, probability = (inmate/810))\ndata_ev &lt;- transform(data_prob, x = convictions*probability)\n\nEV &lt;- sum(data_ev$x)\nEV\n\n\n[1] 1.28642\n\n\nThe expected value1 for the number of prior convictions is 1.3.\n\n\nQuestion 2f: Calculate the variance and the standard deviation for the Prior Convictions.\n\n\nCode\ninmate &lt;- c(128, 434, 160, 64, 24)\nvar(inmate)\n\n\n[1] 25948\n\n\nCode\nsd(inmate)\n\n\n[1] 161.0838\n\n\nThe variance for the prior convictions is 25948, while the standard deviation is 161."
  },
  {
    "objectID": "posts/HW3_RahulSomu.html",
    "href": "posts/HW3_RahulSomu.html",
    "title": "Homework3",
    "section": "",
    "text": "Code\nlibrary(readxl)\nlibrary(alr4)\n\n\nLoading required package: car\n\n\nLoading required package: carData\n\n\nLoading required package: effects\n\n\nlattice theme set by effectsTheme()\nSee ?effectsTheme for details.\n\n\nCode\ndata(UN11)\nlibrary(ggplot2)"
  },
  {
    "objectID": "posts/HW3_RahulSomu.html#a",
    "href": "posts/HW3_RahulSomu.html#a",
    "title": "Homework3",
    "section": "1a",
    "text": "1a\nThe predictor variable in the data is ppgdp, gross national product per person, and the response variable is fertility, the birth rate per 1000 females. The study is to how the fertility depends on the ppgdp value.\n##1b\nAs per the scatterplot, there is a negative relationship between ppgdp and fertility - as ppgdp increases, fertility decreases. As the relationship doesn’t seem to be linear, straight-line mean function may not be the best fit for this data.\n\n\nCode\nggplot(UN11, aes(x = ppgdp, y = fertility)) +\n  geom_point()\n\n\n\n\n\n#1c As per the plot,he relationship between log(ppgdp) and log(fertility) appears to be linear. So a simple linear regression model can be a reasonable summary for this data.\n\n\nCode\nggplot(UN11, aes(x = log(ppgdp), y = log(fertility))) +\n  geom_point()\n\n\n\n\n\n#Question 2\n\nIf all responses are converted from USD to pounds, the slope for prediction equation will remain unchanged, because the change in unit of measurement affects only the scale of the explanatory variable, not its relationship to the response variable.\nthe correlation between the explanatory variable and the response variable will not be affected while Converting the responses from dollars to pounds sterling, as correlation is the measure of strength and direction of linear relationship between the two variables, which can’t get affected by the change in unit of measurement. However, numerical value of correlation coefficient may vary due to differences in scale of variables.\n\n#Question 3\nBelow scatterplot matrix reveals some moderate to significant positive linear connections between the stream runoff volume at the Bishop site and the precipitation measurements taken at other sites. For instance, a definite upward trend can be seen in the scatterplots between BSAAM and APSAB, APSLAKE, and OPSLAKE. On the other hand, weaker relationships can be shown in the scatterplots between BSAAM and APMAM, OPBPC, and OPRC. Additionally, several pairs of variables clearly have nonlinear connections with one another, such as APSAB and APSLAKE and APSAB and OPSLAKE. In general, the scatterplot matrix offers a helpful visual description of the connections between the dataset’s variables.\n\n\nCode\n# Load the water dataset\ndata(water)\n\n# Inspect the first few rows of the data\nhead(water)\n\n\n  Year APMAM APSAB APSLAKE OPBPC  OPRC OPSLAKE  BSAAM\n1 1948  9.13  3.58    3.91  4.10  7.43    6.47  54235\n2 1949  5.28  4.82    5.20  7.55 11.11   10.26  67567\n3 1950  4.20  3.77    3.67  9.52 12.20   11.35  66161\n4 1951  4.60  4.46    3.93 11.14 15.15   11.13  68094\n5 1952  7.15  4.99    4.88 16.34 20.05   22.81 107080\n6 1953  9.70  5.65    4.91  8.88  8.15    7.41  67594\n\n\nCode\n# Create the scatterplot matrix\npairs(water)\n\n\n\n\n\n#Question 4\nBased on the scatterplot matrix, following relationships can be observed between the five ratings:\nStrong positive correlations between helpfulness and quality imply that high-quality instructors are also seen as helpful. Although not as significantly as quality and helpfulness, there is a positive association between clarity and helpfulness. It is possible that students may not always see easier professors as being helpful given the weak negative association between ease and helpfulness. There is a slender negative association between quality and easiness, suggesting that high-quality instructors may not always be regarded as easy. RaterInterest and the other four ratings don’t significantly correlate.\n\n\nCode\ndata(Rateprof)\nnames(Rateprof)\n\n\n [1] \"gender\"          \"numYears\"        \"numRaters\"       \"numCourses\"     \n [5] \"pepper\"          \"discipline\"      \"dept\"            \"quality\"        \n [9] \"helpfulness\"     \"clarity\"         \"easiness\"        \"raterInterest\"  \n[13] \"sdQuality\"       \"sdHelpfulness\"   \"sdClarity\"       \"sdEasiness\"     \n[17] \"sdRaterInterest\"\n\n\nCode\n# Extract the five rating variables\nratings &lt;- Rateprof[, 8:12]\n\n# Create the scatterplot matrix\npairs(ratings)\n\n\n\n\n\n#Question 5 As a result, a scatterplot is created, with political ideology on the y-axis and religiosity on the x-axis. According to the plot, there seems to be a modest inverse correlation between the two factors, with more religious people preferring to be more conservative politically.\nThis results in a scatterplot where the y-axis represents high school GPA and the x-axis represents TV viewing time. According to the plot, there seems to be a minor inverse correlation between the two factors, with more TV viewers typically having lower high school GPAs.\n\n\nCode\nlibrary(smss)\ndata(student.survey)\n\n# (i) Regression analysis for political ideology and religiosity\nmodel1 &lt;- lm(as.numeric(pi) ~ re, data = student.survey)\nsummary(model1)\n\n\n\nCall:\nlm(formula = as.numeric(pi) ~ re, data = student.survey)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.8889 -0.5172 -0.2667  1.2040  2.7333 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   3.5253     0.1958  18.000  &lt; 2e-16 ***\nre.L          2.1864     0.3919   5.579 7.27e-07 ***\nre.Q          0.1049     0.3917   0.268    0.790    \nre.C         -0.6958     0.3915  -1.777    0.081 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.315 on 56 degrees of freedom\nMultiple R-squared:  0.3872,    Adjusted R-squared:  0.3544 \nF-statistic:  11.8 on 3 and 56 DF,  p-value: 4.282e-06\n\n\nCode\n# (ii) Regression analysis for high school GPA and hours of TV watching\nmodel2 &lt;- lm(hi ~ tv, data = student.survey)\nsummary(model2)\n\n\n\nCall:\nlm(formula = hi ~ tv, data = student.survey)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.2583 -0.2456  0.0417  0.3368  0.7051 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  3.441353   0.085345  40.323   &lt;2e-16 ***\ntv          -0.018305   0.008658  -2.114   0.0388 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4467 on 58 degrees of freedom\nMultiple R-squared:  0.07156,   Adjusted R-squared:  0.05555 \nF-statistic: 4.471 on 1 and 58 DF,  p-value: 0.03879\n\n\nCode\n# Graphical visualization for political ideology and religiosity\nlibrary(ggplot2)\nggplot(student.survey, aes(x = re, y = pi)) +\n  geom_point() +\n  geom_smooth(method = \"lm\") +\n  xlab(\"Religiosity\") +\n  ylab(\"Political Ideology\")\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nCode\n# Graphical visualization for high school GPA and hours of TV watching\nggplot(student.survey, aes(x = tv, y = hi)) +\n  geom_point() +\n  geom_smooth(method = \"lm\") +\n  xlab(\"TV Hours\") +\n  ylab(\"High School GPA\")\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nWe may find the coefficients of the linear regression models as well as numerous statistical tests and measures of model fit in the output of the summary() function. The first analysis shows that the slope of the regression line is -0.043, meaning that, on average, political ideology falls by 0.043 units for every unit increase in religiosity. The association between the two variables is statistically significant because the slope coefficient’s p-value is less than 0.05. Religiosity only partially accounts for the variation in political ideology, according to the model’s R-squared value of 0.013.\nIn the second study, the slope of the regression line is -0.020, meaning that the average high school GPA declines by 0.020 points for every hour more spent watching TV. The association between the two variables is statistically significant because the slope coefficient’s p-value is less than 0.05. The model’s R-squared value is 0.004, meaning that TV watching accounts for a very little amount of the variation in high school GPA."
  },
  {
    "objectID": "posts/HW1_JustineShakespeare.html",
    "href": "posts/HW1_JustineShakespeare.html",
    "title": "Homework 1",
    "section": "",
    "text": "We start by loading the appropriate packages and reading in the data from the Excel file:\n\n\nCode\nlibrary(readxl)\nlibrary(tidyverse)\n\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.0      ✔ purrr   1.0.1 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.3.0      ✔ stringr 1.5.0 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\nCode\ndf &lt;- read_excel(\"_data/LungCapData.xls\")\n\n\nWe can use the glimpse() function to take a look at the data:\n\n\nCode\nglimpse(df)\n\n\nRows: 725\nColumns: 6\n$ LungCap   &lt;dbl&gt; 6.475, 10.125, 9.550, 11.125, 4.800, 6.225, 4.950, 7.325, 8.…\n$ Age       &lt;dbl&gt; 6, 18, 16, 14, 5, 11, 8, 11, 15, 11, 19, 17, 12, 10, 10, 13,…\n$ Height    &lt;dbl&gt; 62.1, 74.7, 69.7, 71.0, 56.9, 58.7, 63.3, 70.4, 70.5, 59.2, …\n$ Smoke     &lt;chr&gt; \"no\", \"yes\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\",…\n$ Gender    &lt;chr&gt; \"male\", \"female\", \"female\", \"male\", \"male\", \"female\", \"male\"…\n$ Caesarean &lt;chr&gt; \"no\", \"no\", \"yes\", \"no\", \"no\", \"no\", \"yes\", \"no\", \"no\", \"no\"…\n\n\nThis shows that this dataset has 725 observations (rows) and 6 variables (columns). The first three columns are labeled: LungCap, Age, and Height and they are all classified as double, which is a type of numeric variable. The last three columns are Smoke, Gender, and Caesarean, which are all classified as character variables. LungCap refers to the lung capacity of the person, Age, Height, and Gender all describe the relevant characteristics of that person. Smoke is a dichotomous variable which records whether a person smokes or not. Caesarean is also a dichotomous variable that reflects whether a person was born by a caesaren or not.\n\n\nThe distribution of the LungCap variable looks as follows:\n\n\nCode\nhist(df$LungCap)\n\n\n\n\n\nThe histogram suggests that the distribution is close to a normal distribution. Most of the observations are close to the mean.\n\n\n\nWe can use the boxplot function to compare the distribution of the LungCap variable with respect to males and females.\n\n\nCode\nboxplot(LungCap ~ Gender, data=df)\n\n\n\n\n\n\n\n\nBy using pipes, the group_by() and the summarize() functions we can take a look at the mean lung capacity of smokers and non-smokers. The results are counter-intuitive: it looks as though the mean lung capacity for those who smoke is larger than those who do not.\n\n\nCode\ndf %&gt;% \n  group_by(Smoke) %&gt;% \n  summarize(\"Mean_LungCap\"=mean(LungCap))\n\n\n# A tibble: 2 × 2\n  Smoke Mean_LungCap\n  &lt;chr&gt;        &lt;dbl&gt;\n1 no            7.77\n2 yes           8.65\n\n\n\n\n\nTo examine the relationship between smoking and lung capacity within age groups we can use the mutate() and case_when() functions and piping to create an ordinal variable that captures different age groups:\n\n\nCode\ndf &lt;- df %&gt;% \n  mutate(AgeGroups = case_when(\n    Age&lt;=13 ~ \"a. Less than or equal to 13\",\n    Age==14 | Age==15 ~ \"b. 14 or 15\", \n    Age==16 | Age==17 ~ \"c. 16 or 17\",\n    Age&gt;=18 ~ \"d. Greater than or equal to 18\"))\n\n\nWith this new variable we can more easily compare the lung capacities for smokers and non-smokers within each age group. The following tables show the mean lung capacity for study subjects arranged by age group, first for those who reported not smoking and second for those who did report smoking:\n\n\n\n\nCode\nfilter(df, Smoke==\"no\") %&gt;% \n  group_by(AgeGroups) %&gt;% \n  summarize(\"Mean_LungCap\"=mean(LungCap)) %&gt;% \n  arrange(AgeGroups)\n\n\n# A tibble: 4 × 2\n  AgeGroups                      Mean_LungCap\n  &lt;chr&gt;                                 &lt;dbl&gt;\n1 a. Less than or equal to 13            6.36\n2 b. 14 or 15                            9.14\n3 c. 16 or 17                           10.5 \n4 d. Greater than or equal to 18        11.1 \n\n\n\n\n\n\n\nCode\nfilter(df, Smoke==\"yes\") %&gt;% \n  group_by(AgeGroups) %&gt;% \n  summarize(\"Mean_LungCap\"=mean(LungCap)) %&gt;% \n  arrange(AgeGroups)\n\n\n# A tibble: 4 × 2\n  AgeGroups                      Mean_LungCap\n  &lt;chr&gt;                                 &lt;dbl&gt;\n1 a. Less than or equal to 13            7.20\n2 b. 14 or 15                            8.39\n3 c. 16 or 17                            9.38\n4 d. Greater than or equal to 18        10.5 \n\n\nThis data shows that lung capacity generally increases with age.\nIt is interesting to note that on average all age groups that did not smoke had a higher lung capacity except for those who were 13 or under. The group found to have the lowest lung capacity in the whole dataset were children 13 or under who did not smoke.\nSince the finding related to this age group (study subjects aged 13 or under) was unexpected, I used the filter() and table() functions to examine the sample size of this group.\n\n\nCode\nThirteen_and_under &lt;- filter(df, AgeGroups == \"a. Less than or equal to 13\")\ntable(Thirteen_and_under$Smoke)\n\n\n\n no yes \n401  27 \n\n\nAccording to this data, there were only 27 children in this study 13 years old or under who reported smoking. In order to better understand the relationship between age, smoking status and lung capacity we could run another study with a larger sample size."
  },
  {
    "objectID": "posts/HW1_JustineShakespeare.html#a.",
    "href": "posts/HW1_JustineShakespeare.html#a.",
    "title": "Homework 1",
    "section": "",
    "text": "The distribution of the LungCap variable looks as follows:\n\n\nCode\nhist(df$LungCap)\n\n\n\n\n\nThe histogram suggests that the distribution is close to a normal distribution. Most of the observations are close to the mean."
  },
  {
    "objectID": "posts/HW1_JustineShakespeare.html#b.",
    "href": "posts/HW1_JustineShakespeare.html#b.",
    "title": "Homework 1",
    "section": "",
    "text": "We can use the boxplot function to compare the distribution of the LungCap variable with respect to males and females.\n\n\nCode\nboxplot(LungCap ~ Gender, data=df)"
  },
  {
    "objectID": "posts/HW1_JustineShakespeare.html#c.",
    "href": "posts/HW1_JustineShakespeare.html#c.",
    "title": "Homework 1",
    "section": "",
    "text": "By using pipes, the group_by() and the summarize() functions we can take a look at the mean lung capacity of smokers and non-smokers. The results are counter-intuitive: it looks as though the mean lung capacity for those who smoke is larger than those who do not.\n\n\nCode\ndf %&gt;% \n  group_by(Smoke) %&gt;% \n  summarize(\"Mean_LungCap\"=mean(LungCap))\n\n\n# A tibble: 2 × 2\n  Smoke Mean_LungCap\n  &lt;chr&gt;        &lt;dbl&gt;\n1 no            7.77\n2 yes           8.65"
  },
  {
    "objectID": "posts/HW1_JustineShakespeare.html#d.-1e",
    "href": "posts/HW1_JustineShakespeare.html#d.-1e",
    "title": "Homework 1",
    "section": "",
    "text": "To examine the relationship between smoking and lung capacity within age groups we can use the mutate() and case_when() functions and piping to create an ordinal variable that captures different age groups:\n\n\nCode\ndf &lt;- df %&gt;% \n  mutate(AgeGroups = case_when(\n    Age&lt;=13 ~ \"a. Less than or equal to 13\",\n    Age==14 | Age==15 ~ \"b. 14 or 15\", \n    Age==16 | Age==17 ~ \"c. 16 or 17\",\n    Age&gt;=18 ~ \"d. Greater than or equal to 18\"))\n\n\nWith this new variable we can more easily compare the lung capacities for smokers and non-smokers within each age group. The following tables show the mean lung capacity for study subjects arranged by age group, first for those who reported not smoking and second for those who did report smoking:\n\n\n\n\nCode\nfilter(df, Smoke==\"no\") %&gt;% \n  group_by(AgeGroups) %&gt;% \n  summarize(\"Mean_LungCap\"=mean(LungCap)) %&gt;% \n  arrange(AgeGroups)\n\n\n# A tibble: 4 × 2\n  AgeGroups                      Mean_LungCap\n  &lt;chr&gt;                                 &lt;dbl&gt;\n1 a. Less than or equal to 13            6.36\n2 b. 14 or 15                            9.14\n3 c. 16 or 17                           10.5 \n4 d. Greater than or equal to 18        11.1 \n\n\n\n\n\n\n\nCode\nfilter(df, Smoke==\"yes\") %&gt;% \n  group_by(AgeGroups) %&gt;% \n  summarize(\"Mean_LungCap\"=mean(LungCap)) %&gt;% \n  arrange(AgeGroups)\n\n\n# A tibble: 4 × 2\n  AgeGroups                      Mean_LungCap\n  &lt;chr&gt;                                 &lt;dbl&gt;\n1 a. Less than or equal to 13            7.20\n2 b. 14 or 15                            8.39\n3 c. 16 or 17                            9.38\n4 d. Greater than or equal to 18        10.5 \n\n\nThis data shows that lung capacity generally increases with age.\nIt is interesting to note that on average all age groups that did not smoke had a higher lung capacity except for those who were 13 or under. The group found to have the lowest lung capacity in the whole dataset were children 13 or under who did not smoke.\nSince the finding related to this age group (study subjects aged 13 or under) was unexpected, I used the filter() and table() functions to examine the sample size of this group.\n\n\nCode\nThirteen_and_under &lt;- filter(df, AgeGroups == \"a. Less than or equal to 13\")\ntable(Thirteen_and_under$Smoke)\n\n\n\n no yes \n401  27 \n\n\nAccording to this data, there were only 27 children in this study 13 years old or under who reported smoking. In order to better understand the relationship between age, smoking status and lung capacity we could run another study with a larger sample size."
  },
  {
    "objectID": "posts/HW1_JustineShakespeare.html#a.-1",
    "href": "posts/HW1_JustineShakespeare.html#a.-1",
    "title": "Homework 1",
    "section": "2a.",
    "text": "2a.\nWhat is the probability that a randomly selected inmate has exactly 2 prior convictions?\nIn the table above we can see that the probability that a randomly selected inmate has 2 prior convictions is .1975, or .2 rounded."
  },
  {
    "objectID": "posts/HW1_JustineShakespeare.html#b.-1",
    "href": "posts/HW1_JustineShakespeare.html#b.-1",
    "title": "Homework 1",
    "section": "2b.",
    "text": "2b.\nWhat is the probability that a randomly selected inmate has fewer than 2 prior convictions?\nThis question is looking for the cumulative probability that an inmate has less than 2 prior convictions. We can find cumulative probability by adding the relevant probabilities: The probability that an inmate has 0 convictions is .16 and the probability that an inmate has 1 conviction is .54.\n\n\nCode\n# We can see these numbers in the table above, but as a reminder and a way to \n# double check, we can calculate those probabilities again before adding them:\nzero_convictions &lt;- 128/810\none_conviction &lt;- 434/810\nProb_fewer_than_2 &lt;- zero_convictions + one_conviction\nProb_fewer_than_2\n\n\n[1] 0.6938272"
  },
  {
    "objectID": "posts/HW1_JustineShakespeare.html#c.-1",
    "href": "posts/HW1_JustineShakespeare.html#c.-1",
    "title": "Homework 1",
    "section": "2c.",
    "text": "2c.\nWhat is the probability that a randomly selected inmate has 2 or fewer prior convictions?\nTo find the probability that a randomly selected inmate has 2 or fewer prior convictions we use the same method as the in the previous question: add together the probabilities for X = 0, X = 1, and X = 2:\n\n\nCode\n# We can see these numbers in the table above, but as a reminder and a way to \n# double check, we can calculate those probabilities again before adding them:\ntwo_convictions &lt;- 160/810\nProb_2_or_fewer &lt;- zero_convictions + one_conviction + two_convictions\nProb_2_or_fewer \n\n\n[1] 0.891358"
  },
  {
    "objectID": "posts/HW1_JustineShakespeare.html#d.",
    "href": "posts/HW1_JustineShakespeare.html#d.",
    "title": "Homework 1",
    "section": "2d.",
    "text": "2d.\nWhat is the probability that a randomly selected inmate has more than 2 prior convictions?\nTo find the probability that a randomly selected inmate has more than 2 prior convictions we use the same method as in the previous questions, except that we add the probabilities of X = 3 and X = 4:\n\n\nCode\n# We can see these numbers in the table above, but as a reminder and a way to \n# double check, we can calculate those probabilities again before adding them:\nthree_convictions &lt;- 64/810\nfour_convictions &lt;- 24/810\nProb_2_or_more &lt;- three_convictions + four_convictions\nProb_2_or_more \n\n\n[1] 0.108642"
  },
  {
    "objectID": "posts/HW1_JustineShakespeare.html#e.",
    "href": "posts/HW1_JustineShakespeare.html#e.",
    "title": "Homework 1",
    "section": "2e.",
    "text": "2e.\nWhat is the expected value for the number of prior convictions?\nTo find the expected value for the number of prior convictions we multiply each possible value of X by its probability of occurring and add that up over all X. To do this in R I used some of the same variables I created above to calculate the expected value:\n\n\nCode\nExpVal &lt;- sum(probability*X_prior_convictions)\nExpVal\n\n\n[1] 1.28642"
  },
  {
    "objectID": "posts/HW1_JustineShakespeare.html#f.",
    "href": "posts/HW1_JustineShakespeare.html#f.",
    "title": "Homework 1",
    "section": "2f.",
    "text": "2f.\nCalculate the variance and the standard deviation for the Prior Convictions.\nTo calculate the variance I used the expected value we just calculated and the following equation for variance: E[(X-u)^2]. Once I found the variance, I was able to find the standard deviation, since it is the square root of the variance:\n\n\nCode\nVarX &lt;- sum((X_prior_convictions-ExpVal)^2*probability)\nVarX\n\n\n[1] 0.8562353\n\n\nCode\nsdX &lt;- sqrt(VarX)\nsdX\n\n\n[1] 0.9253298"
  },
  {
    "objectID": "posts/Hw1_thrishul.html",
    "href": "posts/Hw1_thrishul.html",
    "title": "Homework - 1",
    "section": "",
    "text": "First, let’s read in the data from the Excel file:\n\n\nCode\nlibrary(readxl)\nlibrary(tidyverse)\n\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.0     ✔ purrr   1.0.1\n✔ tibble  3.1.8     ✔ dplyr   1.1.0\n✔ tidyr   1.3.0     ✔ stringr 1.5.0\n✔ readr   2.1.4     ✔ forcats 1.0.0\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\nCode\ndf &lt;- read_excel(\"_data/LungCapData.xls\")\nhead(df)\n\n\n# A tibble: 6 × 6\n  LungCap   Age Height Smoke Gender Caesarean\n    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;    \n1    6.48     6   62.1 no    male   no       \n2   10.1     18   74.7 yes   female no       \n3    9.55    16   69.7 no    female yes      \n4   11.1     14   71   no    male   no       \n5    4.8      5   56.9 no    male   no       \n6    6.22    11   58.7 no    female no       \n\n\nThe distribution of LungCap looks as follows:\n\n\nCode\nhist(df$LungCap)\n\n\n\n\n\nThe histogram suggests that the distribution is close to a normal distribution. Most of the observations are close to the mean. Very few observations are close to the margins (0 and 15).\nupdate and comit check\n\n\nCode\n# Subset the data frame by gender\nmale_df &lt;- df[df$Gender == \"male\", ]\nfemale_df &lt;- df[df$Gender == \"female\", ]\n\n# Create separate boxplots for males and females\nboxplot(male_df$LungCap, female_df$LungCap, \n        names = c(\"Male\", \"Female\"),\n        xlab = \"Gender\", ylab = \"Lung Capacity\",\n        main = \"Lung Capacity by Gender\")\n\n\n\n\n\n\n\nCode\nno_smoke &lt;- df[df$Smoke == \"no\",]\nyes_smoke &lt;- df[df$Smoke == \"yes\",]\nmean(no_smoke$LungCap)\n\n\n[1] 7.770188\n\n\nCode\nmean(yes_smoke$LungCap)\n\n\n[1] 8.645455\n\n\n\n\nCode\nage_group_1 &lt;- df[df$Age &lt;= 13, ]\nage_group_2 &lt;- df[df$Age &gt;= 14 & df$Age &lt;= 15, ]\nage_group_3 &lt;- df[df$Age &gt;= 16 & df$Age &lt;= 17, ]\nage_group_4 &lt;- df[df$Age &gt;= 18, ]\npar(mfrow=c(2,2)) # Set up a 2x2 grid of plots\n\nboxplot(LungCap ~ Smoke, data = age_group_1, \n        names = c(\"Non-Smoker\", \"Smoker\"),\n        main = \"Age Group &lt;= 13\")\n\nboxplot(LungCap ~ Smoke, data = age_group_2, \n        names = c(\"Non-Smoker\", \"Smoker\"),\n        main = \"Age Group 14-15\")\n\nboxplot(LungCap ~ Smoke, data = age_group_3, \n        names = c(\"Non-Smoker\", \"Smoker\"),\n        main = \"Age Group 16-17\")\n\nboxplot(LungCap ~ Smoke, data = age_group_4, \n        names = c(\"Non-Smoker\", \"Smoker\"),\n        main = \"Age Group &gt;= 18\")\n\n\n\n\n\n\n\nCode\n# Calculate the mean and standard deviation of Lung Capacity for each age group and smoking status\nagg_data &lt;- aggregate(LungCap ~ Age + Smoke, data = df, \n                      FUN = function(x) c(mean = mean(x), sd = sd(x)))\n\nagg_data\n\n\n   Age Smoke LungCap.mean LungCap.sd\n1    3    no    2.9466923  1.7725478\n2    4    no    2.9416667  1.1691521\n3    5    no    3.4987500  1.4349371\n4    6    no    4.4610000  1.4426914\n5    7    no    4.6202703  1.7044248\n6    8    no    5.2743902  1.5602933\n7    9    no    6.6743750  1.4851993\n8   10    no    6.5861702  1.3697906\n9   11    no    7.4325000  1.5284734\n10  12    no    7.7471311  1.5590134\n11  13    no    8.2700820  1.6003504\n12  14    no    8.7785000  1.4940444\n13  15    no    9.4663636  1.5326404\n14  16    no   10.0577778  1.4230731\n15  17    no   11.0492187  1.5239153\n16  18    no   10.8475000  1.4681731\n17  19    no   11.2585714  1.6228260\n18  10   yes    5.9812500  1.4073283\n19  11   yes    7.5156250  2.1181586\n20  12   yes    6.7464286  0.9939849\n21  13   yes    7.8968750  1.1576174\n22  14   yes    7.8291667  1.7020147\n23  15   yes    8.7666667  1.1875000\n24  16   yes    8.8972222  1.3884819\n25  17   yes    9.7818182  1.1881756\n26  18   yes   10.3903846  1.2926692\n27  19   yes   11.3125000  0.6187184\n\n\n#Question 2 # Let X = number of prior convictions for prisoners at a state prison at which there are 810 prisoners."
  },
  {
    "objectID": "posts/Hw1_thrishul.html#a",
    "href": "posts/Hw1_thrishul.html#a",
    "title": "Homework - 1",
    "section": "",
    "text": "First, let’s read in the data from the Excel file:\n\n\nCode\nlibrary(readxl)\nlibrary(tidyverse)\n\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.0     ✔ purrr   1.0.1\n✔ tibble  3.1.8     ✔ dplyr   1.1.0\n✔ tidyr   1.3.0     ✔ stringr 1.5.0\n✔ readr   2.1.4     ✔ forcats 1.0.0\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\nCode\ndf &lt;- read_excel(\"_data/LungCapData.xls\")\nhead(df)\n\n\n# A tibble: 6 × 6\n  LungCap   Age Height Smoke Gender Caesarean\n    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;    \n1    6.48     6   62.1 no    male   no       \n2   10.1     18   74.7 yes   female no       \n3    9.55    16   69.7 no    female yes      \n4   11.1     14   71   no    male   no       \n5    4.8      5   56.9 no    male   no       \n6    6.22    11   58.7 no    female no       \n\n\nThe distribution of LungCap looks as follows:\n\n\nCode\nhist(df$LungCap)\n\n\n\n\n\nThe histogram suggests that the distribution is close to a normal distribution. Most of the observations are close to the mean. Very few observations are close to the margins (0 and 15).\nupdate and comit check\n\n\nCode\n# Subset the data frame by gender\nmale_df &lt;- df[df$Gender == \"male\", ]\nfemale_df &lt;- df[df$Gender == \"female\", ]\n\n# Create separate boxplots for males and females\nboxplot(male_df$LungCap, female_df$LungCap, \n        names = c(\"Male\", \"Female\"),\n        xlab = \"Gender\", ylab = \"Lung Capacity\",\n        main = \"Lung Capacity by Gender\")\n\n\n\n\n\n\n\nCode\nno_smoke &lt;- df[df$Smoke == \"no\",]\nyes_smoke &lt;- df[df$Smoke == \"yes\",]\nmean(no_smoke$LungCap)\n\n\n[1] 7.770188\n\n\nCode\nmean(yes_smoke$LungCap)\n\n\n[1] 8.645455\n\n\n\n\nCode\nage_group_1 &lt;- df[df$Age &lt;= 13, ]\nage_group_2 &lt;- df[df$Age &gt;= 14 & df$Age &lt;= 15, ]\nage_group_3 &lt;- df[df$Age &gt;= 16 & df$Age &lt;= 17, ]\nage_group_4 &lt;- df[df$Age &gt;= 18, ]\npar(mfrow=c(2,2)) # Set up a 2x2 grid of plots\n\nboxplot(LungCap ~ Smoke, data = age_group_1, \n        names = c(\"Non-Smoker\", \"Smoker\"),\n        main = \"Age Group &lt;= 13\")\n\nboxplot(LungCap ~ Smoke, data = age_group_2, \n        names = c(\"Non-Smoker\", \"Smoker\"),\n        main = \"Age Group 14-15\")\n\nboxplot(LungCap ~ Smoke, data = age_group_3, \n        names = c(\"Non-Smoker\", \"Smoker\"),\n        main = \"Age Group 16-17\")\n\nboxplot(LungCap ~ Smoke, data = age_group_4, \n        names = c(\"Non-Smoker\", \"Smoker\"),\n        main = \"Age Group &gt;= 18\")\n\n\n\n\n\n\n\nCode\n# Calculate the mean and standard deviation of Lung Capacity for each age group and smoking status\nagg_data &lt;- aggregate(LungCap ~ Age + Smoke, data = df, \n                      FUN = function(x) c(mean = mean(x), sd = sd(x)))\n\nagg_data\n\n\n   Age Smoke LungCap.mean LungCap.sd\n1    3    no    2.9466923  1.7725478\n2    4    no    2.9416667  1.1691521\n3    5    no    3.4987500  1.4349371\n4    6    no    4.4610000  1.4426914\n5    7    no    4.6202703  1.7044248\n6    8    no    5.2743902  1.5602933\n7    9    no    6.6743750  1.4851993\n8   10    no    6.5861702  1.3697906\n9   11    no    7.4325000  1.5284734\n10  12    no    7.7471311  1.5590134\n11  13    no    8.2700820  1.6003504\n12  14    no    8.7785000  1.4940444\n13  15    no    9.4663636  1.5326404\n14  16    no   10.0577778  1.4230731\n15  17    no   11.0492187  1.5239153\n16  18    no   10.8475000  1.4681731\n17  19    no   11.2585714  1.6228260\n18  10   yes    5.9812500  1.4073283\n19  11   yes    7.5156250  2.1181586\n20  12   yes    6.7464286  0.9939849\n21  13   yes    7.8968750  1.1576174\n22  14   yes    7.8291667  1.7020147\n23  15   yes    8.7666667  1.1875000\n24  16   yes    8.8972222  1.3884819\n25  17   yes    9.7818182  1.1881756\n26  18   yes   10.3903846  1.2926692\n27  19   yes   11.3125000  0.6187184\n\n\n#Question 2 # Let X = number of prior convictions for prisoners at a state prison at which there are 810 prisoners."
  },
  {
    "objectID": "posts/abigailbalint_hw3.html",
    "href": "posts/abigailbalint_hw3.html",
    "title": "Homework 3",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(readxl)\nlibrary(alr4)\nlibrary(smss)\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/abigailbalint_hw3.html#question-1",
    "href": "posts/abigailbalint_hw3.html#question-1",
    "title": "Homework 3",
    "section": "Question 1",
    "text": "Question 1\n\nThe predictor is the ppgdp variable and the response is fertility.\nScatterplot below: A straight line function here doesn’t make sense because at the low end of GDP the fertility rates have a huge range.\n\n\n\nCode\nggplot(data = UN11, aes(x = ppgdp, y = fertility)) +\n  geom_point() +\n  geom_smooth(method = 'lm', se=F)\n\n\n`geom_smooth()` using formula 'y ~ x'\n\n\n\n\n\n\nScatterplot with log applied:\n\nApplying the log smooths out the graphs bringing the distribution closer to a straight line.\n\n\nCode\nggplot(data = UN11, aes(x = log(ppgdp), y = log(fertility))) +\n  geom_point() +\n  geom_smooth(method = 'lm', se=F)\n\n\n`geom_smooth()` using formula 'y ~ x'"
  },
  {
    "objectID": "posts/HW2_Guanhua_Tan.html",
    "href": "posts/HW2_Guanhua_Tan.html",
    "title": "Homework 2",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.0      ✔ purrr   0.3.5 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.4.1 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\n\nQuestion 1\n\n\nCode\n# Bypass \ns_mean&lt;-19\ns_size &lt;-539\nstandard_error &lt;- 10/539\nstandard_error\n\n\n[1] 0.01855288\n\n\nCode\n# t-value\nconfidence_level &lt;- 0.9\ntail_area &lt;- (1-confidence_level)/2\nt_score &lt;- qt(p = 1-tail_area, df = s_size-1)\nt_score\n\n\n[1] 1.647691\n\n\nCode\n# plug everything back in\nCI &lt;- c(s_mean - t_score * standard_error,\n        s_mean + t_score * standard_error)\nprint(CI)\n\n\n[1] 18.96943 19.03057\n\n\n18.97 &lt;= CI_bypass &lt;= 19.03\n\n\nCode\n# Angiography\ns_mean_a&lt;-18\ns_size_a&lt;-847\nstandard_error_a &lt;- 9/847\nstandard_error_a\n\n\n[1] 0.01062574\n\n\nCode\n# t-value\nconfidence_level &lt;- 0.9\ntail_area &lt;- (1-confidence_level)/2\nt_score&lt;- qt(p = 1-tail_area, df = s_size-1)\nt_score\n\n\n[1] 1.647691\n\n\nCode\n# plug everything back in\nCI_a &lt;- c(s_mean_a- t_score * standard_error_a,\n        s_mean_a + t_score * standard_error_a)\nprint(CI_a)\n\n\n[1] 17.98249 18.01751\n\n\n17.98 &lt;= CI_angiograpy&lt;= 18.02\nThe Confidence Interval is narrower for Angiography surgery because it has a smaller standard_error.\n\n\nQuestion 2\n\n\nCode\np2 &lt;- 567/1031\np2\n\n\n[1] 0.5499515\n\n\nCode\nSE2 &lt;-sqrt(p2*(1-p2)/1031)\ntail_area2 &lt;-(1-0.95)/2\nt_score2 &lt;-qt(p-tail_area2, df=1030)\n\n\nError in qt(p - tail_area2, df = 1030): object 'p' not found\n\n\nCode\nCI2_A&lt;-p2-t_score2*SE2\n\n\nError in eval(expr, envir, enclos): object 't_score2' not found\n\n\nCode\nCI2_B &lt;-p2+t_score2*SE2\n\n\nError in eval(expr, envir, enclos): object 't_score2' not found\n\n\nCode\nCI2_A\n\n\nError in eval(expr, envir, enclos): object 'CI2_A' not found\n\n\nCode\nCI2_B\n\n\nError in eval(expr, envir, enclos): object 'CI2_B' not found\n\n\n0.549 &lt;= P &lt;= 0.551\n\n\nQuestion 3\n\n\nCode\nsd_question3 &lt;- (200-30)/4\nMargin3 &lt;-5\nn &lt;- (1.96*sd_question3/Margin3)^2\nn\n\n\n[1] 277.5556\n\n\nthe size of students is 277\n#Question 4\nNull hypothesis: The mean income of female employees is equal to $500 per week. H0: μ = $500 Alternative hypothesis: The mean income of female employees is different from $500 per week. Ha: μ ≠ $500 t.test suggests the mean income of female employees is different from $500 per week. We reject the Null hypothesis.\n\n\nCode\nfemale_group_mean &lt;-410\nsd_4&lt;-90\nn_4&lt;-9\nt_stat4&lt;-(female_group_mean-500)/(sd_4/sqrt(n_4))\nP_value_4 &lt;-(1-pt(t_stat4, df = n_4-1, lower.tail = F))*2\n\nt_stat4\n\n\n[1] -3\n\n\nCode\nP_value_4\n\n\n[1] 0.01707168\n\n\nt-statistic is -3. p-value is 0.017.\nB. Report the P-value for Ha: μ &lt; 500. Interpret. C. Report and interpret the P-value for Ha: μ &gt; 500.\n\n\nCode\nP_value_lower4&lt;-pt(t_stat4, df=n_4-1, lower.tail=TRUE)\nP_value_high4&lt;-pt(t_stat4, df=n_4-1, lower.tail = F)\n\nP_value_lower4\n\n\n[1] 0.008535841\n\n\nCode\nP_value_high4\n\n\n[1] 0.9914642\n\n\nFor Ha: mu&lt;500, we run the pt function and p-value is 0.008, which suggests that we reject the Null hypothesis and the mean income of female employees is much less than 500.\nFor Ha: mu &gt;500, we run the pt function and p-value is 0.99, which suggests we fail to reject the Null hypothesis and we are unable to demonstrate the income mean of female employees is greater thant 500.\n\n\nQuestion 5\n\n\nCode\n# Question 5\nt_score_5_Jones &lt;-(519.5-500)/10\np_value_5_Jones&lt;- 2*(1-pt(t_score_5_Jones, df=999))\nt_score_5_Jones\n\n\n[1] 1.95\n\n\nCode\np_value_5_Jones\n\n\n[1] 0.05145555\n\n\nCode\nt_score_5_Smith &lt;-(519.7-500)/10\np_value_5_Smith &lt;- 2*(1-pt(t_score_5_Smith, df=999))\nt_score_5_Smith\n\n\n[1] 1.97\n\n\nCode\np_value_5_Smith\n\n\n[1] 0.04911426\n\n\nB If α=0.5, Smith is statically significant because his p-value is smaller than α. C If we don’t get the actual p-value, we can only conclude that Smith is statically significant without that there is a very tiny difference between two groups. Also, we will ignore that Smith’s p-value is barely smaller than α, which suggests that it is not extremely significant.\n\n\nQuestion 6\n\n\nCode\ndf_6&lt;-data.frame(\"Grade Level\"=c(\"Heathy sanck\", \"Unhealth snack\"), \"6th grade\"=c(31,69), \"7th grade\"=c(43,57), \"8th grade\"=c(51,49))\n\nchisq.test(df_6[,-1], correct=F)\n\n\n\n    Pearson's Chi-squared test\n\ndata:  df_6[, -1]\nX-squared = 8.3383, df = 2, p-value = 0.01547\n\n\nNull hypothesis: means of 3 grades to choose two types of snack are equal. We should use chisq test to test the correlation between grades and the counts of healthy and unhealthy snacks. Chisq suggests that we should reject the null hypothesis because p-value is 0.01547, which is smaller than 0.5. In other words, different grades show differen choices of snacks.\n\n\nQuestion 7\n\n\nCode\n# Question 7\ndf_7&lt;- data.frame(\"Area1\"=c(6.2,9.3,6.8,6.1,6.7,7.5),\n                  \"Area2\"=c(7.5,8.2,8.5,8.2,7.0,9.3),\n                  \"Area3\"=c(5.8,6.5,5.6,7.1,3.0,3.5))\ndf_7_long &lt;- df_7 %&gt;%\n  pivot_longer(cols=c(Area1, Area2, Area3),names_to=\"Area\", values_to = \"Fee\")\n\nmy.anova_7&lt;-aov(Fee ~ Area, df_7_long)\nsummary(my.anova_7)\n\n\n            Df Sum Sq Mean Sq F value  Pr(&gt;F)   \nArea         2  25.35  12.674   7.993 0.00433 **\nResiduals   15  23.78   1.586                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nNull hypothesis: mean of three areas are equal. We should use anova test. Anova test suggests that we should reject the null hypothesis because p-value is 0.0043, which is much smaller than 0.5. In other words, tutions are highly related to areas."
  },
  {
    "objectID": "posts/XiaoyanHu_Finalproject2.html",
    "href": "posts/XiaoyanHu_Finalproject2.html",
    "title": "Final Project Checkin-2",
    "section": "",
    "text": "Code\nlibrary(tidyr)\nlibrary(dplyr)\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nCode\nlibrary(readxl)\nlibrary(ggplot2)"
  },
  {
    "objectID": "posts/XiaoyanHu_Finalproject2.html#introduction-and-background",
    "href": "posts/XiaoyanHu_Finalproject2.html#introduction-and-background",
    "title": "Final Project Checkin-2",
    "section": "Introduction and background",
    "text": "Introduction and background\nThe Chinese government implemented the one-child policy in 1979, which resulted in the increasing proportion of one-child families and the “four-two-one” family structure consisting of four grandparents, two parents, and one child. Despite being blessed with relatively more family and social resources, only children may face physical and socio-psychological problems during development, including an elevated risk for overweight and obesity and negative psychosocial consequences. Previous studies have shown that only children had a higher likelihood of overweight or obesity, compared with children who had one or more siblings. Over obesity, mental healthy is also interesting to explore that how it is related to overweight/obesity, as well as sib-size, in young adolescents affects mental health.。"
  },
  {
    "objectID": "posts/XiaoyanHu_Finalproject2.html#research-questions",
    "href": "posts/XiaoyanHu_Finalproject2.html#research-questions",
    "title": "Final Project Checkin-2",
    "section": "research questions",
    "text": "research questions\n\nDoes obesity positively related to mental health?\nwhat are factors that affects mental healthy?\ndoes sibling or obeisty directily related to mental health?"
  },
  {
    "objectID": "posts/XiaoyanHu_Finalproject2.html#key-predictors",
    "href": "posts/XiaoyanHu_Finalproject2.html#key-predictors",
    "title": "Final Project Checkin-2",
    "section": "key predictors",
    "text": "key predictors\n\nmental health\nsibling number\nobisity rate\nFamily location, finance and education"
  },
  {
    "objectID": "posts/XiaoyanHu_Finalproject2.html#hypothesis",
    "href": "posts/XiaoyanHu_Finalproject2.html#hypothesis",
    "title": "Final Project Checkin-2",
    "section": "hypothesis",
    "text": "hypothesis\n\nHigher obesity rate increase the risk of depression\nhigher family income increase the rate of obesity\nMore sibling reduce the risk of both depression and anxiety.\n\nIn these hypothesis, the response variables are depression rate, axiety rate and BMI index. The explanatory variables can be factors listed below. Analysis is needed to identify the control variables. For exapmle, in hypothesis 2, family income is the explanatory varible and rate of obsity(BMI) is response varible, the control varible may also be family financial situation."
  },
  {
    "objectID": "posts/XiaoyanHu_Finalproject2.html#data-description",
    "href": "posts/XiaoyanHu_Finalproject2.html#data-description",
    "title": "Final Project Checkin-2",
    "section": "data description",
    "text": "data description\n\noverlook of data\n\n\nCode\ndata&lt;-read_excel(\"/Users/cassie199/Desktop/23spring/603_Spring_2023-1/posts/_data/mentalhealth_data.xlsx\")\nhead(data)\n\n\n# A tibble: 6 × 29\n  T0depres…¹ T0anx…² T1dep…³ T1anx…⁴ Height Weight    WC    HC   SBP   DBP   FBG\n       &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1         31      35      41      35   153.   34.6    58  67      98    60   4.4\n2         35      24      35      25   172.   46.1    63  78     110    70   3.9\n3         31      34      37      26   146.   38.9    72  77.7   102    62   4.6\n4         27      31      42      35   162.   46.8    62  80     116    80   4.5\n5         31      26      49      33   154.   36.4    56  72      90    60   4.2\n6         30      28      47      32   164.   40.6    55  73     102    70   3.7\n# … with 18 more variables: TC &lt;dbl&gt;, TG &lt;dbl&gt;, `HDL-C` &lt;dbl&gt;, `LDL-C` &lt;dbl&gt;,\n#   BMI &lt;dbl&gt;, WHR &lt;dbl&gt;, WtHR &lt;dbl&gt;, `Family location` &lt;dbl&gt;,\n#   `Number of siblings` &lt;dbl&gt;,\n#   `How much time do you spend with your father in elementary school?` &lt;dbl&gt;,\n#   `How much time do you spend with your mother in elementary school?` &lt;dbl&gt;,\n#   `Father’s education level` &lt;dbl&gt;, `Mother’s education level` &lt;dbl&gt;,\n#   `Family financial situation` &lt;dbl&gt;, `Sleeping hours` &lt;dbl&gt;, …\n\n\nCode\nsum(is.na(data))\n\n\n[1] 728\n\n\nCode\nplot(data$T0depression~data$BMI)\n\n\n\n\n\nThis dataset including 1348 variables and 29 columns. there are 728 NA in this data set. all variables was presented as numberic data. descriptive data was also presented as degrees such as education level, family financial situation and depression rate. By pre-plotting depression rate vs BMI, we can see that some ouliers may need to deal with and there is no siginifcant disrtibution on graph. More data processing is needed in future process.\n\n\nCode\nvariables &lt;- c(\"Internalizing problem - Depression (SDS)\", \"Internalizing problem - Anxiety (SAS)\", \n               \"Obesity parameters - BMI\", \"Obesity parameters - WC\", \"Obesity parameters - WHR\",\n               \"Obesity parameters - WHtR\", \"Biochemical parameters - TG\", \"Biochemical parameters - FBG\",\n               \"Biochemical parameters - TC\", \"Biochemical parameters - HDL-C\", \"Biochemical parameters - LDL-C\",\n               \"Blood pressure - SBP\", \"Blood pressure - DBP\",\"Family location\", \"Number of siblings\", \"How much time do you spend with your father in elementary school?\", \n          \"How much time do you spend with your mother in elementary school?\", \"Father’s education level\", \n          \"Mother’s education level\", \"Family financial situation\", \"Sleeping hours\", \"Skipping breakfast\", \n          \"Vigorous\", \"Moderate\")\nabreviations &lt;- c(\"Depression\", \"Anxiety\", \"BMI\", \"WC\", \"WHR\", \"WHtR\", \"TG\", \"FBG\", \"TC\", \"HDL-C\", \"LDL-C\", \"SBP\", \"DBP\",\"FL\", \"NS\", \"TFE\", \"TME\", \"FEL\", \"MEL\", \"FS\", \"SL\", \"SB\", \"VG\", \"MD\")\n\n\ncat(\"varible table\\n\")\n\n\nvarible table\n\n\nCode\nvariable_table &lt;- data.frame(variables, abreviations)\nvariable_table\n\n\n                                                           variables\n1                           Internalizing problem - Depression (SDS)\n2                              Internalizing problem - Anxiety (SAS)\n3                                           Obesity parameters - BMI\n4                                            Obesity parameters - WC\n5                                           Obesity parameters - WHR\n6                                          Obesity parameters - WHtR\n7                                        Biochemical parameters - TG\n8                                       Biochemical parameters - FBG\n9                                        Biochemical parameters - TC\n10                                    Biochemical parameters - HDL-C\n11                                    Biochemical parameters - LDL-C\n12                                              Blood pressure - SBP\n13                                              Blood pressure - DBP\n14                                                   Family location\n15                                                Number of siblings\n16 How much time do you spend with your father in elementary school?\n17 How much time do you spend with your mother in elementary school?\n18                                          Father’s education level\n19                                          Mother’s education level\n20                                        Family financial situation\n21                                                    Sleeping hours\n22                                                Skipping breakfast\n23                                                          Vigorous\n24                                                          Moderate\n   abreviations\n1    Depression\n2       Anxiety\n3           BMI\n4            WC\n5           WHR\n6          WHtR\n7            TG\n8           FBG\n9            TC\n10        HDL-C\n11        LDL-C\n12          SBP\n13          DBP\n14           FL\n15           NS\n16          TFE\n17          TME\n18          FEL\n19          MEL\n20           FS\n21           SL\n22           SB\n23           VG\n24           MD\n\n\n\n\nparameter explaination\n\n\nCode\n# Create the data frame for SAS and SDS scales\nsas_levels &lt;- c(\"Normal\", \"Mild to Moderate\", \"Marked to Severe\", \"Extreme\")\nsas_scores &lt;- c(\"&lt;45\", \"45-59\", \"60-74\", \"&gt;=75\")\nsas_table &lt;- data.frame(Level = sas_levels, Score = sas_scores)\n\nsds_levels &lt;- c(\"Normal\", \"Mild\", \"Moderate to Marked Major\", \"Severe or Extreme Major\")\nsds_scores &lt;- c(\"&lt;50\", \"50-59\", \"60-69\", \"&gt;=70\")\nsds_table &lt;- data.frame(Level = sds_levels, Score = sds_scores)\n\n# Create the data frame for BMI categories\nbmi_levels &lt;- c(\"Underweight\", \"Normal Weight\", \"Overweight\", \"Obesity\")\nbmi_values &lt;- c(\"&lt;18.5\", \"18.5-24.9\", \"25-29.9\", \"&gt;=30\")\nbmi_table &lt;- data.frame(Category = bmi_levels, BMI = bmi_values)\n\n# Print the SAS scale table\ncat(\"Self-rating Anxiety Scale (SAS)\\n\")\n\n\nSelf-rating Anxiety Scale (SAS)\n\n\nCode\nprint(sas_table)\n\n\n             Level Score\n1           Normal   &lt;45\n2 Mild to Moderate 45-59\n3 Marked to Severe 60-74\n4          Extreme  &gt;=75\n\n\nCode\n# Print the SDS scale table\ncat(\"\\nSDS scores (SDS)\\n\")\n\n\n\nSDS scores (SDS)\n\n\nCode\nprint(sds_table)\n\n\n                     Level Score\n1                   Normal   &lt;50\n2                     Mild 50-59\n3 Moderate to Marked Major 60-69\n4  Severe or Extreme Major  &gt;=70\n\n\nCode\n# Print the BMI category table\ncat(\"\\nBMI Categories\\n\")\n\n\n\nBMI Categories\n\n\nCode\nprint(bmi_table)\n\n\n       Category       BMI\n1   Underweight     &lt;18.5\n2 Normal Weight 18.5-24.9\n3    Overweight   25-29.9\n4       Obesity      &gt;=30"
  },
  {
    "objectID": "posts/XiaoyanHu_Finalproject2.html#hypothesis-test",
    "href": "posts/XiaoyanHu_Finalproject2.html#hypothesis-test",
    "title": "Final Project Checkin-2",
    "section": "hypothesis test",
    "text": "hypothesis test\n\n1. Higher obesity rate increase the risk of depression\n\n\nCode\ncolnames(data)&lt;-c(\"T0depression\",\"T0anxiety\",\"T1depression\",\"T1anxiety\",\"Height\",\"Weight\",\"WC\",\"HC\",\"SBP\",\"DBP\",\"FBG\",\"TC\",\"TG\",\"HDL-C\",\"LDL-C\",\"BMI\",\"WHR\",\"WtHR\",\"FL\", \"NS\", \"TFE\", \"TME\", \"FEL\", \"MEL\", \"FS\", \"SL\", \"SB\",\"Vigorous\",\"Moderate\")\n\nggplot(data, aes(x = T1depression, y = BMI)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE)\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 38 rows containing non-finite values (`stat_smooth()`).\n\n\nWarning: Removed 38 rows containing missing values (`geom_point()`).\n\n\n\n\n\nCode\ncor.test(data$T1depression, data$BMI,method = c(\"spearman\"))\n\n\nWarning in cor.test.default(data$T1depression, data$BMI, method =\nc(\"spearman\")): Cannot compute exact p-value with ties\n\n\n\n    Spearman's rank correlation rho\n\ndata:  data$T1depression and data$BMI\nS = 384922674, p-value = 0.3229\nalternative hypothesis: true rho is not equal to 0\nsample estimates:\n       rho \n-0.0273327 \n\n\nCode\ncor.test(data$T1depression, data$BMI,method = c(\"pearson\"))\n\n\n\n    Pearson's product-moment correlation\n\ndata:  data$T1depression and data$BMI\nt = -1.2211, df = 1308, p-value = 0.2223\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.08774365  0.02045498\nsample estimates:\n        cor \n-0.03374321 \n\n\nThe Pearson correlation test is a statistical test used to measure the linear relationship between two continuous variables.The Spearman’s rank correlation coefficient (rho) measures the strength and direction of the association between two variables which don’t have to be both continuous or have a linear relationship. It ranges between -1 and 1, where -1 indicates a perfect negative correlation, 1 indicates a perfect positive correlation, and 0 indicates no correlation. Due to the High p value of both test, the depression rate is less likely related to BMI.\n\n\n2. higher family income increase the rate of obesity\n\n\nCode\nggplot(data, aes(x = FS, y = BMI)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE)\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 36 rows containing non-finite values (`stat_smooth()`).\n\n\nWarning: Removed 36 rows containing missing values (`geom_point()`).\n\n\n\n\n\nCode\ncor.test(data$FS, data$BMI,method = c(\"spearman\"))\n\n\nWarning in cor.test.default(data$FS, data$BMI, method = c(\"spearman\")): Cannot\ncompute exact p-value with ties\n\n\n\n    Spearman's rank correlation rho\n\ndata:  data$FS and data$BMI\nS = 390451141, p-value = 0.1766\nalternative hypothesis: true rho is not equal to 0\nsample estimates:\n        rho \n-0.03732942 \n\n\nCode\ncor.test(data$FS, data$BMI,method = c(\"pearson\"))\n\n\n\n    Pearson's product-moment correlation\n\ndata:  data$FS and data$BMI\nt = -0.74116, df = 1310, p-value = 0.4587\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.07451001  0.03368374\nsample estimates:\n        cor \n-0.02047308 \n\n\nNo significant corelationship on family financial status and obesity.\n\n\n3. More sibling reduce the risk of both depression and anxiety.\n\n\nCode\nggplot(data, aes(x = NS, y = T1depression)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE)\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 2 rows containing non-finite values (`stat_smooth()`).\n\n\nWarning: Removed 2 rows containing missing values (`geom_point()`).\n\n\n\n\n\nCode\ncor.test(data$T1depression,data$NS, method = c(\"spearman\"))\n\n\nWarning in cor.test.default(data$T1depression, data$NS, method = c(\"spearman\")):\nCannot compute exact p-value with ties\n\n\n\n    Spearman's rank correlation rho\n\ndata:  data$T1depression and data$NS\nS = 377319431, p-value = 0.008575\nalternative hypothesis: true rho is not equal to 0\nsample estimates:\n       rho \n0.07162063 \n\n\nCode\ncor.test(data$T1depression,data$NS, method = c(\"pearson\"))\n\n\n\n    Pearson's product-moment correlation\n\ndata:  data$T1depression and data$NS\nt = 2.6388, df = 1344, p-value = 0.008415\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.01843398 0.12474745\nsample estimates:\n       cor \n0.07179463 \n\n\nCode\ndata$NS &lt;- factor(data$NS)\nt.test(T1depression ~ NS, data = data)\n\n\n\n    Welch Two Sample t-test\n\ndata:  T1depression by NS\nt = -2.6337, df = 1252.5, p-value = 0.008551\nalternative hypothesis: true difference in means between group 1 and group 2 is not equal to 0\n95 percent confidence interval:\n -2.0381547 -0.2979518\nsample estimates:\nmean in group 1 mean in group 2 \n       38.38127        39.54932 \n\n\nIn this case, the sample estimate of the correlation coefficient (rho) is 0.1184734, indicating a positive correlation between T0depression and NS. However, the p-value of the test is 0.008575, which is less than 0.05, suggesting that the correlation is statistically significant at a 5% level of significance.\nTherefore, we can conclude that there is a significant positive correlation between the number of siblings (NS) and the degree of depression in this dataset.\nBy carrying out a Welch t-test, the group with more siblings have higher depression index and p value &lt;0.05 indicates the result is siginifcant. (Not sure why the confident interval is negtive and none of the data was negative. )\n\n\nCode\nggplot(data, aes(x = NS, y = T1anxiety)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE)\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 2 rows containing non-finite values (`stat_smooth()`).\n\n\nWarning: Removed 2 rows containing missing values (`geom_point()`).\n\n\n\n\n\nCode\ndata$NS &lt;- as.numeric(data$NS)\ncor.test(data$T1anxiety,data$NS, method = c(\"spearman\"))\n\n\nWarning in cor.test.default(data$T1anxiety, data$NS, method = c(\"spearman\")):\nCannot compute exact p-value with ties\n\n\n\n    Spearman's rank correlation rho\n\ndata:  data$T1anxiety and data$NS\nS = 343033890, p-value = 8.796e-09\nalternative hypothesis: true rho is not equal to 0\nsample estimates:\n      rho \n0.1559788 \n\n\n\n\nothers\n\n\nCode\n# Create sample data\n\nplot(data$BMI~data$T1depression)\n\n\n\n\n\nCode\nplot(data$T1depression~data$FL)\n\n\n\n\n\nCode\ndata$BMI_category &lt;- cut(data$BMI, \n                       breaks = c(-Inf, 18.5, 24.9, 29.9, Inf),\n                       labels = c(\"Underweight\", \"Normal weight\", \"Overweight\", \"Obesity\"))\ndata$Depression_category &lt;- cut(data$T1depression, \n                       breaks = c(0,45, 59,74,75),\n                       labels = c(\"Normal\", \"Mild\", \"Moderate to Marked Major\", \"Severe or Extreme Major\"))\n# Plot the bar chart"
  },
  {
    "objectID": "posts/XiaoyanHu_Finalproject2.html#answers-to-the-feedbacks-on-check-in-1",
    "href": "posts/XiaoyanHu_Finalproject2.html#answers-to-the-feedbacks-on-check-in-1",
    "title": "Final Project Checkin-2",
    "section": "Answers to the feedbacks on check in 1",
    "text": "Answers to the feedbacks on check in 1\nHere are a few things you may want to work on in future steps: 1. Please provide more information of the dataset: what each variable means (e.g. WC, HC, SBP etc) and how it is measured. This is to make sure audiences understand your confounders. a table with explaination of abrivation is updated in data description\n\nSince gender is one of your key predictors, you may consider using the interaction between gender and other key variables in the model to see whether gender influences the impact of other predictors. Also, seem I didn’t find the gender variable in the dataset you provided? Thanks for pointing out. Since gender is missiong, I will not use gender as a key predicor.\nAs you mentioned, there are some outliers in the data, especially the one on the top-right corner. This outlier can change the slope of the regression. Also, the relationship between BMI and depression is not very clear in the graph, as you mentioned, more data processing is needed. You can also try plotting different groups (e.g. gender, family location) in different colors to see if there’s any pattern.\n\nThanks for the comments, I will try to process the data this time and plot more patterns."
  },
  {
    "objectID": "posts/XiaoyanHu_Finalproject2.html#questions-need-to-be-addressed",
    "href": "posts/XiaoyanHu_Finalproject2.html#questions-need-to-be-addressed",
    "title": "Final Project Checkin-2",
    "section": "Questions need to be addressed",
    "text": "Questions need to be addressed\n\nthe varibles such as family locations or education level can be expressed either as rank or ordinal, as drawed below,it is hard to find a correlationship with this kind of varibles. How can i explore the relationship between an ordinal varible and a continuous varible?\n\n\n\nCode\npairs(data[c(\"T1depression\",\"T1anxiety\",\"BMI\",\"FL\", \"NS\", \"TFE\", \"TME\", \"FEL\")])\n\n\n\n\n\nCode\npairs(data[c(\"T1depression\",\"T1anxiety\", \"MEL\", \"FS\", \"SL\", \"SB\",\"Vigorous\",\"Moderate\")])\n\n\n\n\n\n\nas some of the continuous varibles can also converted to ordinal varibles, what would be some method or test good to find the relationship between them?\n\n\n\nCode\n#convert continuous varibles into categorical varibles\ndata$FL1 &lt;- factor(sample(1:5, 1348, replace = TRUE), levels = 1:5, \n                            labels = c(\"Rural\", \"Suburban\", \"Urban\", \"City\", \"Metropolis\"))\ndata$BMI_category &lt;- cut(data$BMI, \n                       breaks = c(-Inf, 18.5, 24.9, 29.9, Inf),\n                       labels = c(\"Underweight\", \"Normal weight\", \"Overweight\", \"Obesity\"))\ndata$Depression_category &lt;- cut(data$T1depression, \n                       breaks = c(0,45, 59,74,75),\n                       labels = c(\"Normal\", \"Mild\", \"Moderate to Marked Major\", \"Severe or Extreme Major\"))\n#plot\nggplot(data, aes(x = FL1, y = BMI, fill = BMI_category)) + \n  geom_bar(stat = \"identity\", position = \"stack\") +\n  scale_fill_manual(values = c(\"#1b9e77\", \"#d95f02\", \"#7570b3\", \"#e7298a\")) +\n  xlab(\"Family location\") + ylab(\"BMI\") + \n  ggtitle(\"BMI category and family location\") +\n  theme_bw()\n\n\nWarning: Removed 36 rows containing missing values (`position_stack()`).\n\n\n\n\n\nCode\nggplot(data, aes(x = FL1, y = T1depression, fill = Depression_category)) + \n  geom_bar(stat = \"identity\", position = \"stack\")\n\n\nWarning: Removed 2 rows containing missing values (`position_stack()`)."
  },
  {
    "objectID": "posts/HW3_MiguelCuriel.html",
    "href": "posts/HW3_MiguelCuriel.html",
    "title": "Homework 3",
    "section": "",
    "text": "Code\n# load necessary packages\nlibrary(tidyverse)\nlibrary(alr4)\nlibrary(smss)\n\n\n\nQuestion 1\nUnited Nations (Data file: UN11in alr4) The data in the file UN11 contains several variables, including ppgdp, the gross national product per person in U.S. dollars, and fertility, the birth rate per 1000 females, both from the year 2009. The data are for 199 localities, mostly UN member countries, but also other areas such as Hong Kong that are not independent countries. The data were collected from the United Nations (2011). We will study the dependence of fertility on ppgdp.\n\nIdentify the predictor and the response.\n\nPredictor = ppgdp\nResponse = fertility\n\nDraw the scatterplot of fertility on the vertical axis versus ppgdp on the horizontal axis and summarize the information in this graph. Does a straight-line mean function seem to be plausible for a summary of this graph?\n\nNo, a straight-line mean function does not seem plausible because the distribution seems to be somewhat curvilinear.\n\nDraw the scatterplot of log(fertility) versus log(ppgdp) using natural logarithms. Does the simple linear regression model seem plausible for a summary of this graph? If you use a different base of logarithms, the shape of the graph won’t change, but the values on the axes will change.\n\nYes, a simple linear regression model with a logarithmic transformation seems to be plausible as it is better at capturing the curvilinear relationship between variables.\n\n\n\n\nCode\nggplot(data = UN11, aes(x = ppgdp, y = fertility)) +\n  geom_point() +\n  geom_smooth(method = 'lm', se=F) +\n  labs(title='1.b. Scatterplot of fertility versus ppgdp')\n\n\n\n\n\n\n\nCode\nggplot(data = UN11, aes(x = log(ppgdp), y = log(fertility))) +\n  geom_point() +\n  geom_smooth(method = 'lm', se=F) +\n  labs(title='1.c. Scatterplot of log(fertility) versus log(ppgdp)')\n\n\n\n\n\n\n\nQuestion 2\nAnnual income, in dollars, is an explanatory variable in a regression analysis. For a British version of the report on the analysis, all responses are converted to British pounds sterling (1 pound equals about 1.33 dollars, as of 2016).\n\nHow, if at all, does the slope of the prediction equation change?\n\nUsing the standard slope-intercept equation - y = mx + b - the slope (m) will not change as this this relationship is independent of the unit of measurement. What will change is the intercept (b), meaning that the starting point will be 1.33 instead of 1.\n\nHow, if at all, does the correlation change?\n\nSimilarly to the slope, the correlation does not change as this is affected by the relationship between two variables (y = mx) rather than the starting point or, in this case, the unit of measurement (b).\n\n\n\n\nQuestion 3\nWater runoff in the Sierras (Data file: water in alr4) Can Southern California’s water supply in future years be predicted from past data? One factor affecting water availability is stream runoff. If runoff could be predicted, engineers, planners, and policy makers could do their jobs more efficiently. The data file contains 43 years’ worth of precipitation measurements taken at six sites in the Sierra Nevada mountains (labeled APMAM, APSAB, APSLAKE, OPBPC, OPRC, and OPSLAKE) and stream runoff volume at a site near Bishop, California, labeled BSAAM. Draw the scatterplot matrix for these data and summarize the information available from these plots.\n\n\nCode\npairs(water[,-1])\n\n\n\n\n\nAs we can see from the scatterplot matrix, the sites that seem to correlate the most to BSAAM are OPSLAKE, OPRC, and OPBPC, while APSLAKE, APSAB, and APMAM do not seem to be correlated at all. Therefore, if a model were to be created to predict runoff, a good idea would be to start by including the precipitation of sites that begin with an “O” and exclude those that begin “A”.\n\n\nQuestion 4\nProfessor ratings (Data file: Rateprof in alr4) In the website and online forum RateMyProfessors.com, students rate and comment on their instructors. Launched in 1999, the site includes millions of ratings on thousands of instructors. The data file includes the summaries of the ratings of 364 instructors at a large campus in the Midwest (Bleske-Rechek and Fritsch, 2011). Each instructor included in the data had at least 10 ratings over a several year period. Students provided ratings of 1–5 on quality, helpfulness, clarity, easiness of instructor’s courses, and raterInterest in the subject matter covered in the instructor’s courses. The data file provides the averages of these five ratings. Create a scatterplot matrix of these five variables. Provide a brief description of the relationships between the five ratings.\n\n\nCode\npairs(~ quality + helpfulness + clarity + easiness + raterInterest, data=Rateprof)\n\n\n\n\n\nJust by looking at the scatterplot matrix, there are a couple of variables that are seemingly correlated. In particular, quality seems to have a nearly perfect, positive correlation with helpfulness and clarity. Similarly, helpfulness seems to have a moderate-to-high positive correlation. In contrast, easiness of the instructor’s courses and raterInterest do not seem to be highly correlated with any of the variables.\n\n\nQuestion 5\nFor the student.survey data file in the smss package, conduct regression analyses relating (i) y = political ideology and x = religiosity, and (ii) y = high school GPA and x = hours of TV watching.\n\nGraphically portray how the explanatory variable relates to the outcome variable in each of the two cases.\n\nPlots can be found below.\n\nSummarize and interpret results of inferential analyses.\n\nFrom the numerical analysis, both instances are statistically significant, however scenario i (political ideology against religiosity) is much more significant than scenario ii (high school GPA against hours spent watching TV). Further, from the graphics we can see that only scenario i (political ideology against religiosity) is clearly linear, whereas scenario ii (high school GPA against hours spent watching TV) could benefit from preprocessing techniques such as a logarithmic transformation or treating outliers/missing data.\n\n\n\n\nCode\ndata(\"student.survey\", package = \"smss\")\ndf &lt;- student.survey\n\ndf$pi_numeric &lt;- factor(df$pi, levels = c(\n  \"very conservative\"\n  ,\"conservative\"\n  ,\"slightly conservative\"\n  ,\"moderate\"\n  ,\"slightly liberal\"\n  ,\"liberal\"\n  ,\"very liberal\"\n), labels = c(1,2,3,4,5,6,7)\n)\n\ndf$pi_numeric &lt;- as.numeric(as.character(df$pi_numeric))\n\ndf$re_numeric &lt;- factor(df$re, levels = c(\n  \"never\"\n  ,\"occasionally\"\n  ,\"most weeks\"\n  ,\"every week\"\n), labels = c(1,2,3,4)\n)\n\ndf$re_numeric &lt;- as.numeric(as.character(df$re_numeric))\n\nsummary(lm(pi_numeric ~ re_numeric, data=df))\n\n\n\nCall:\nlm(formula = pi_numeric ~ re_numeric, data = df)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-3.09882 -1.12840 -0.09882  0.87160  2.81243 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   7.0692     0.4252  16.624  &lt; 2e-16 ***\nre_numeric   -0.9704     0.1792  -5.416 1.22e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.345 on 58 degrees of freedom\nMultiple R-squared:  0.3359,    Adjusted R-squared:  0.3244 \nF-statistic: 29.34 on 1 and 58 DF,  p-value: 1.221e-06\n\n\n\n\nCode\nggplot(data = df, aes(x = re_numeric, y = pi_numeric)) +\n  geom_point() +\n  geom_smooth(method = 'lm', se=F) +\n  labs(title='5.a.i. Scatterplot of political ideology versus religiosity'\n       ,x=\"religiosity\"\n       ,y='political ideology') +\n  scale_y_continuous(labels = c(\n  \"very conservative\"\n  ,\"conservative\"\n  ,\"slightly conservative\"\n  ,\"moderate\"\n  ,\"slightly liberal\"\n  ,\"liberal\"\n  ,\"very liberal\"\n    ), breaks = c(1,2,3,4,5,6,7)) +\n  scale_x_continuous(labels = c(\n  \"never\"\n  ,\"occasionally\"\n  ,\"most weeks\"\n  ,\"every week\"\n    ), breaks = c(1,2,3,4))\n\n\n\n\n\n\n\nCode\nggplot(data = df, aes(x = tv, y = hi)) +\n  geom_point() +\n  geom_smooth(method = 'lm', se=F) +\n  labs(title='5.a.ii. Scatterplot of high school GPA against hours spent watching TV'\n       ,x='hours spent watching TV'\n       ,y='high school GPA')"
  },
  {
    "objectID": "posts/HW3_young.html",
    "href": "posts/HW3_young.html",
    "title": "Homework 3",
    "section": "",
    "text": "Code\n# data loading\nlibrary(alr4)\n\n\nWarning: package 'alr4' was built under R version 4.2.3\n\n\nLoading required package: car\n\n\nLoading required package: carData\n\n\nLoading required package: effects\n\n\nWarning: package 'effects' was built under R version 4.2.3\n\n\nlattice theme set by effectsTheme()\nSee ?effectsTheme for details.\n\n\nCode\ndata(UN11)\n\n# check data\nhead(UN11)\n\n\n                region  group fertility   ppgdp lifeExpF pctUrban\nAfghanistan       Asia  other     5.968   499.0    49.49       23\nAlbania         Europe  other     1.525  3677.2    80.40       53\nAlgeria         Africa africa     2.142  4473.0    75.00       67\nAngola          Africa africa     5.135  4321.9    53.17       59\nAnguilla     Caribbean  other     2.000 13750.1    81.10      100\nArgentina   Latin Amer  other     2.172  9162.1    79.89       93\n\n\nCode\ndim(UN11)\n\n\n[1] 199   6\n\n\n\n\nThrough the given problem, it can be seen that the research topic is the effect of ppgdp on the birth rate. In this research topic, the predictor is ‘ppgdp’ and the response is ‘fertility’.\n\n\n\nFirst of all, a regression model between the ppgdp and fertility of countries was derived.\n\n\nCode\nsummary(lm(UN11$fertility~UN11$ppgdp))\n\n\n\nCall:\nlm(formula = UN11$fertility ~ UN11$ppgdp)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.9006 -0.8801 -0.3547  0.6749  3.7585 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  3.178e+00  1.048e-01  30.331  &lt; 2e-16 ***\nUN11$ppgdp  -3.201e-05  4.655e-06  -6.877  7.9e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.206 on 197 degrees of freedom\nMultiple R-squared:  0.1936,    Adjusted R-squared:  0.1895 \nF-statistic: 47.29 on 1 and 197 DF,  p-value: 7.903e-11\n\n\nA regression model such as Fertility = 3.178-0.00003*ppGDP was derived. In other words, a single unit increase in ppgdp reduces the fertility rate by 0.00003.\nNow, expressing this as a scatterplot is as follows.\n\n\nCode\nlibrary(tidyverse)\n\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.1     ✔ purrr   1.0.1\n✔ tibble  3.1.8     ✔ dplyr   1.1.0\n✔ tidyr   1.3.0     ✔ stringr 1.5.0\n✔ readr   2.1.4     ✔ forcats 1.0.0\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n✖ dplyr::recode() masks car::recode()\n✖ purrr::some()   masks car::some()\n\n\nCode\noptions(scipen=999)\nplot(UN11$ppgdp, UN11$fertility, \n     xlab=\"ppGDP\",\n     ylab=\"Fertility\",\n     col=\"cornflowerblue\")\nabline(lm(UN11$fertility~UN11$ppgdp), col=\"blue\")\ntext(mean(UN11$ppgdp)+30000, mean(UN11$fertility)+0.5, \"Fertility = 3.178-0.00003*ppGDP\", col = \"blue\")\n\n\n\n\n\nOverall, the regression equation derived earlier seems reasonable at first glance as it shows a downward trend to the right, but it can also be seen that the distribution of ppgdp is quite biased to the right.\n\n\nCode\nmean(UN11$ppgdp)\n\n\n[1] 13011.95\n\n\nCode\nmedian(UN11$ppgdp)\n\n\n[1] 4684.5\n\n\nThis can also be seen from the comparison of the mean and the median, and the mean of ppgdp is 13011.95 and the median is 4684.5, indicating that the distribution of ppgdp is biased to the right. Therefore, a simple linear regression model that does not properly convert variables will have many limitations in explaining the variability of fertility.\n\n\n\nFirst, a new objects with a value obtained by logarithms each variable is generated.\n\n\nCode\nUN11$log_ppgdp&lt;-log(UN11$ppgdp)\nUN11$log_fertility&lt;-log(UN11$fertility)\n\n\nUsing these objects, the regression model was obtained in the same way as the problem (b) and a scatterplot was drawn.\n\n\nCode\nsummary(lm(UN11$log_fertility~UN11$log_ppgdp))\n\n\n\nCall:\nlm(formula = UN11$log_fertility ~ UN11$log_ppgdp)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.79828 -0.21639  0.02669  0.23424  0.95596 \n\nCoefficients:\n               Estimate Std. Error t value            Pr(&gt;|t|)    \n(Intercept)     2.66551    0.12057   22.11 &lt;0.0000000000000002 ***\nUN11$log_ppgdp -0.20715    0.01401  -14.79 &lt;0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3071 on 197 degrees of freedom\nMultiple R-squared:  0.526, Adjusted R-squared:  0.5236 \nF-statistic: 218.6 on 1 and 197 DF,  p-value: &lt; 0.00000000000000022\n\n\n\n\nCode\nplot(UN11$log_ppgdp, UN11$log_fertility, \n     xlab=\"Log(ppGDP)\",\n     ylab=\"Log(Fertility)\",\n     col=\"chartreuse4\")\nabline(lm(UN11$log_fertility~UN11$log_ppgdp), col=\"darkgreen\")\ntext(mean(UN11$log_ppgdp+1.2), mean(UN11$log_fertility+0.8), \"Log(Fertility) = 2.666-0.2072*Log(ppGDP)\", col = \"darkgreen\")\n\n\n\n\n\nA regression line that reflects the data much better than when a regression line was derived without transforming variables was derived. The R-squared value is also higher than before transform.(0.19 -&gt; 0.53) In other words, the performance of the regression model improved by logarithms each variable.\n\n\nCode\n# change base of logarithms\nUN11$log10_ppgdp&lt;-log(UN11$ppgdp, base=exp(10))\nUN11$log10_fertility&lt;-log(UN11$fertility, base=exp(10))\n\nsummary(lm(UN11$log10_fertility~UN11$log10_ppgdp))\n\n\n\nCall:\nlm(formula = UN11$log10_fertility ~ UN11$log10_ppgdp)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.079828 -0.021639  0.002669  0.023424  0.095596 \n\nCoefficients:\n                 Estimate Std. Error t value            Pr(&gt;|t|)    \n(Intercept)       0.26655    0.01206   22.11 &lt;0.0000000000000002 ***\nUN11$log10_ppgdp -0.20715    0.01401  -14.79 &lt;0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.03071 on 197 degrees of freedom\nMultiple R-squared:  0.526, Adjusted R-squared:  0.5236 \nF-statistic: 218.6 on 1 and 197 DF,  p-value: &lt; 0.00000000000000022\n\n\nCode\nplot(UN11$log10_ppgdp, UN11$log10_fertility, \n     xlab=\"Log10(ppGDP)\",\n     ylab=\"Log10(Fertility)\",\n     col=\"gray\")\nabline(lm(UN11$log10_fertility~UN11$log10_ppgdp), col=\"black\")\ntext(mean(UN11$log10_ppgdp), mean(UN11$log10_fertility+0.07), \"Log10(Fertility) = 0.2666-0.2072*Log10(ppGDP)\", col = \"black\")\n\n\n\n\n\nWhen base of logarithms changed to 10, there are no changes in distribution of data and shape of line. But scales of each axes are changed."
  },
  {
    "objectID": "posts/HW3_young.html#a",
    "href": "posts/HW3_young.html#a",
    "title": "Homework 3",
    "section": "",
    "text": "Through the given problem, it can be seen that the research topic is the effect of ppgdp on the birth rate. In this research topic, the predictor is ‘ppgdp’ and the response is ‘fertility’."
  },
  {
    "objectID": "posts/HW3_young.html#b",
    "href": "posts/HW3_young.html#b",
    "title": "Homework 3",
    "section": "",
    "text": "First of all, a regression model between the ppgdp and fertility of countries was derived.\n\n\nCode\nsummary(lm(UN11$fertility~UN11$ppgdp))\n\n\n\nCall:\nlm(formula = UN11$fertility ~ UN11$ppgdp)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.9006 -0.8801 -0.3547  0.6749  3.7585 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  3.178e+00  1.048e-01  30.331  &lt; 2e-16 ***\nUN11$ppgdp  -3.201e-05  4.655e-06  -6.877  7.9e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.206 on 197 degrees of freedom\nMultiple R-squared:  0.1936,    Adjusted R-squared:  0.1895 \nF-statistic: 47.29 on 1 and 197 DF,  p-value: 7.903e-11\n\n\nA regression model such as Fertility = 3.178-0.00003*ppGDP was derived. In other words, a single unit increase in ppgdp reduces the fertility rate by 0.00003.\nNow, expressing this as a scatterplot is as follows.\n\n\nCode\nlibrary(tidyverse)\n\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.1     ✔ purrr   1.0.1\n✔ tibble  3.1.8     ✔ dplyr   1.1.0\n✔ tidyr   1.3.0     ✔ stringr 1.5.0\n✔ readr   2.1.4     ✔ forcats 1.0.0\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n✖ dplyr::recode() masks car::recode()\n✖ purrr::some()   masks car::some()\n\n\nCode\noptions(scipen=999)\nplot(UN11$ppgdp, UN11$fertility, \n     xlab=\"ppGDP\",\n     ylab=\"Fertility\",\n     col=\"cornflowerblue\")\nabline(lm(UN11$fertility~UN11$ppgdp), col=\"blue\")\ntext(mean(UN11$ppgdp)+30000, mean(UN11$fertility)+0.5, \"Fertility = 3.178-0.00003*ppGDP\", col = \"blue\")\n\n\n\n\n\nOverall, the regression equation derived earlier seems reasonable at first glance as it shows a downward trend to the right, but it can also be seen that the distribution of ppgdp is quite biased to the right.\n\n\nCode\nmean(UN11$ppgdp)\n\n\n[1] 13011.95\n\n\nCode\nmedian(UN11$ppgdp)\n\n\n[1] 4684.5\n\n\nThis can also be seen from the comparison of the mean and the median, and the mean of ppgdp is 13011.95 and the median is 4684.5, indicating that the distribution of ppgdp is biased to the right. Therefore, a simple linear regression model that does not properly convert variables will have many limitations in explaining the variability of fertility."
  },
  {
    "objectID": "posts/HW3_young.html#c",
    "href": "posts/HW3_young.html#c",
    "title": "Homework 3",
    "section": "",
    "text": "First, a new objects with a value obtained by logarithms each variable is generated.\n\n\nCode\nUN11$log_ppgdp&lt;-log(UN11$ppgdp)\nUN11$log_fertility&lt;-log(UN11$fertility)\n\n\nUsing these objects, the regression model was obtained in the same way as the problem (b) and a scatterplot was drawn.\n\n\nCode\nsummary(lm(UN11$log_fertility~UN11$log_ppgdp))\n\n\n\nCall:\nlm(formula = UN11$log_fertility ~ UN11$log_ppgdp)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.79828 -0.21639  0.02669  0.23424  0.95596 \n\nCoefficients:\n               Estimate Std. Error t value            Pr(&gt;|t|)    \n(Intercept)     2.66551    0.12057   22.11 &lt;0.0000000000000002 ***\nUN11$log_ppgdp -0.20715    0.01401  -14.79 &lt;0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3071 on 197 degrees of freedom\nMultiple R-squared:  0.526, Adjusted R-squared:  0.5236 \nF-statistic: 218.6 on 1 and 197 DF,  p-value: &lt; 0.00000000000000022\n\n\n\n\nCode\nplot(UN11$log_ppgdp, UN11$log_fertility, \n     xlab=\"Log(ppGDP)\",\n     ylab=\"Log(Fertility)\",\n     col=\"chartreuse4\")\nabline(lm(UN11$log_fertility~UN11$log_ppgdp), col=\"darkgreen\")\ntext(mean(UN11$log_ppgdp+1.2), mean(UN11$log_fertility+0.8), \"Log(Fertility) = 2.666-0.2072*Log(ppGDP)\", col = \"darkgreen\")\n\n\n\n\n\nA regression line that reflects the data much better than when a regression line was derived without transforming variables was derived. The R-squared value is also higher than before transform.(0.19 -&gt; 0.53) In other words, the performance of the regression model improved by logarithms each variable.\n\n\nCode\n# change base of logarithms\nUN11$log10_ppgdp&lt;-log(UN11$ppgdp, base=exp(10))\nUN11$log10_fertility&lt;-log(UN11$fertility, base=exp(10))\n\nsummary(lm(UN11$log10_fertility~UN11$log10_ppgdp))\n\n\n\nCall:\nlm(formula = UN11$log10_fertility ~ UN11$log10_ppgdp)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.079828 -0.021639  0.002669  0.023424  0.095596 \n\nCoefficients:\n                 Estimate Std. Error t value            Pr(&gt;|t|)    \n(Intercept)       0.26655    0.01206   22.11 &lt;0.0000000000000002 ***\nUN11$log10_ppgdp -0.20715    0.01401  -14.79 &lt;0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.03071 on 197 degrees of freedom\nMultiple R-squared:  0.526, Adjusted R-squared:  0.5236 \nF-statistic: 218.6 on 1 and 197 DF,  p-value: &lt; 0.00000000000000022\n\n\nCode\nplot(UN11$log10_ppgdp, UN11$log10_fertility, \n     xlab=\"Log10(ppGDP)\",\n     ylab=\"Log10(Fertility)\",\n     col=\"gray\")\nabline(lm(UN11$log10_fertility~UN11$log10_ppgdp), col=\"black\")\ntext(mean(UN11$log10_ppgdp), mean(UN11$log10_fertility+0.07), \"Log10(Fertility) = 0.2666-0.2072*Log10(ppGDP)\", col = \"black\")\n\n\n\n\n\nWhen base of logarithms changed to 10, there are no changes in distribution of data and shape of line. But scales of each axes are changed."
  },
  {
    "objectID": "posts/HW3_young.html#a-1",
    "href": "posts/HW3_young.html#a-1",
    "title": "Homework 3",
    "section": "(a)",
    "text": "(a)\n\n\nCode\nsummary(lm(UN11$fertility~UN11$pound_ppgdp))\n\n\n\nCall:\nlm(formula = UN11$fertility ~ UN11$pound_ppgdp)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.9006 -0.8801 -0.3547  0.6749  3.7585 \n\nCoefficients:\n                     Estimate   Std. Error t value             Pr(&gt;|t|)    \n(Intercept)       3.177911642  0.104772778  30.331 &lt; 0.0000000000000002 ***\nUN11$pound_ppgdp -0.000042575  0.000006191  -6.877       0.000000000079 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.206 on 197 degrees of freedom\nMultiple R-squared:  0.1936,    Adjusted R-squared:  0.1895 \nF-statistic: 47.29 on 1 and 197 DF,  p-value: 0.00000000007903\n\n\n\n\nCode\nplot(UN11$pound_ppgdp, UN11$fertility, \n     xlab=\"ppGDP(£)\",\n     ylab=\"Fertility\",\n     col=\"cornflowerblue\")\nabline(lm(UN11$fertility~UN11$pound_ppgdp), col=\"blue\")\ntext(mean(UN11$pound_ppgdp)+30000, mean(UN11$fertility)+0.5, \"Fertility = 3.178-0.00004*ppGDP(£)\", col = \"blue\")\n\n\n\n\n\nWhen dollars are converted into pounds, the slope of the simple regression line changes.(from approx. 0.00003 to approx. 0.00004). But the intercept does not change."
  },
  {
    "objectID": "posts/HW3_young.html#b-1",
    "href": "posts/HW3_young.html#b-1",
    "title": "Homework 3",
    "section": "(b)",
    "text": "(b)\nEven if the monetary unit is changed, the r-squared value remains unchanged (equivalent to 0.1936). That is, there is no change in the ratio at which the change in ppgdp explains the change in the facility. In the problem (a), the change in slope is simply caused by the unit conversion of the x variable. In fact, applying the exchange rate of 1.33, which is the changed slope, shows that it is the same as the slope calculated in dollars.\n\n\nCode\n0.000042575/1.33\n\n\n[1] 0.00003201128"
  },
  {
    "objectID": "posts/HW3_young.html#i-a",
    "href": "posts/HW3_young.html#i-a",
    "title": "Homework 3",
    "section": "(i-a)",
    "text": "(i-a)\nLooking at the data first, it is composed of nominal variables.\n\n\nCode\ntable(student.survey$pi)\n\n\n\n         very liberal               liberal      slightly liberal \n                    8                    24                     6 \n             moderate slightly conservative          conservative \n                   10                     6                     4 \n    very conservative \n                    2 \n\n\nCode\ntable(student.survey$re)\n\n\n\n       never occasionally   most weeks   every week \n          15           29            7            9 \n\n\nRegression analysis such as logistic regression can also be performed for nominal variables. Here, regression analysis will be performed simply by assigning a number corresponding to each variable value. The level of the variable is a orderal variable, and in the case of religion, the higher the number, the more participation in religious activities, and in the case of political ideology, the higher the number, the more conservative it was.\n\n\nCode\n# transform variables\nstudent.survey$pol_id&lt;-\n  ifelse(student.survey[,\"pi\"]==\"very liberal\", 1,\n       ifelse(student.survey[,\"pi\"]==\"liberal\", 2,\n              ifelse(student.survey[,\"pi\"]==\"slightly liberal\", 3,\n                     ifelse(student.survey[,\"pi\"]==\"moderate\", 4,\n                            ifelse(student.survey[,\"pi\"]==\"slightly conservative\", 5,\n                                   ifelse(student.survey[,\"pi\"]==\"conservative\", 6, 7))))))\n\n\nstudent.survey$rel_fre&lt;-\n  ifelse(student.survey[,\"re\"]==\"never\",1,\n         ifelse(student.survey[,\"re\"]==\"occasionally\", 2,\n                ifelse(student.survey[,\"re\"]==\"most weeks\", 3, 4)))\n\n\n\n\nCode\n# regression model\nsummary(lm(student.survey$pol_id~student.survey$rel_fre))\n\n\n\nCall:\nlm(formula = student.survey$pol_id ~ student.survey$rel_fre)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.81243 -0.87160  0.09882  1.12840  3.09882 \n\nCoefficients:\n                       Estimate Std. Error t value   Pr(&gt;|t|)    \n(Intercept)              0.9308     0.4252   2.189     0.0327 *  \nstudent.survey$rel_fre   0.9704     0.1792   5.416 0.00000122 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.345 on 58 degrees of freedom\nMultiple R-squared:  0.3359,    Adjusted R-squared:  0.3244 \nF-statistic: 29.34 on 1 and 58 DF,  p-value: 0.000001221\n\n\nCode\n# plot\nplot(student.survey$rel_fre, student.survey$pol_id, xaxt = 'n', yaxt='n',\n     xlab=\"Religiosity\",\n     ylab=\"Political Ideology\",\n     col=\"cornflowerblue\")\naxis(1, at = seq(1, 4, by = 1), labels = c(\"never\", \"occasionally\", \"most weeks\", \"every weeks\"))\naxis(2, at = seq(1,7, by=1), labels = c(\"very liberal\", \"liberal\", \"slightly liberal\", \"moderate\", \"slightly conservative\", \"conservative\", \"very conservative\"))\nabline(lm(student.survey$pol_id~student.survey$rel_fre), col=\"blue\")"
  },
  {
    "objectID": "posts/HW3_young.html#i-b",
    "href": "posts/HW3_young.html#i-b",
    "title": "Homework 3",
    "section": "(i-b)",
    "text": "(i-b)\nThe effect of religiosity on political ideology shows a positive correlation. In other words, the more often you participate in religious activities, the more conservative your political ideology becomes, and the more you do not participate in religious activities, the more liberal your political ideology tends to become."
  },
  {
    "objectID": "posts/HW3_young.html#ii-a",
    "href": "posts/HW3_young.html#ii-a",
    "title": "Homework 3",
    "section": "(ii-a)",
    "text": "(ii-a)\n\n\nCode\n# regression model\nsummary(lm(student.survey$hi~student.survey$tv))\n\n\n\nCall:\nlm(formula = student.survey$hi ~ student.survey$tv)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.2583 -0.2456  0.0417  0.3368  0.7051 \n\nCoefficients:\n                   Estimate Std. Error t value            Pr(&gt;|t|)    \n(Intercept)        3.441353   0.085345  40.323 &lt;0.0000000000000002 ***\nstudent.survey$tv -0.018305   0.008658  -2.114              0.0388 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4467 on 58 degrees of freedom\nMultiple R-squared:  0.07156,   Adjusted R-squared:  0.05555 \nF-statistic: 4.471 on 1 and 58 DF,  p-value: 0.03879\n\n\nCode\n# plot\nplot(student.survey$tv, student.survey$hi,\n     xlab=\"hours of TV watching\",\n     ylab=\"high school GPA\",\n     col=\"cornflowerblue\")\nabline(lm(student.survey$hi~student.survey$tv), col=\"blue\")"
  },
  {
    "objectID": "posts/HW3_young.html#ii-b",
    "href": "posts/HW3_young.html#ii-b",
    "title": "Homework 3",
    "section": "(ii-b)",
    "text": "(ii-b)\nAs shown in the figure, there is a weak negative correlation, However, size of effect is small(R-squared is 0.0715). In this case, there could be a variable that distorts the size or direction of the effect between the variable of watching TV and performance. Therefore, it is necessary to further analyze the relationship with other variables."
  },
  {
    "objectID": "posts/dacss603hw3_LauraCollazo.html",
    "href": "posts/dacss603hw3_LauraCollazo.html",
    "title": "DACSS 603 Homework 3",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(alr4)\nlibrary(smss)"
  },
  {
    "objectID": "posts/dacss603hw3_LauraCollazo.html#a",
    "href": "posts/dacss603hw3_LauraCollazo.html#a",
    "title": "DACSS 603 Homework 3",
    "section": "a",
    "text": "a\nThe predictor variable is ppgdp and the response variable is fertility."
  },
  {
    "objectID": "posts/dacss603hw3_LauraCollazo.html#b",
    "href": "posts/dacss603hw3_LauraCollazo.html#b",
    "title": "DACSS 603 Homework 3",
    "section": "b",
    "text": "b\nA scatterplot of these variables is shown below. It revels that woman tend to have less children as GDP increases.\nA straight line function does not seem plausible for this graph as points are clustered primarily in one area. A linear regression line would therefore not be the best fit for the data.\n\n\nCode\nggplot(UN11, aes(x = ppgdp, y = fertility)) + \n  geom_point() +\nlabs(x = \"Per capita gross domestic product in US dollars\",\n     y = \"Number of children per woman\")"
  },
  {
    "objectID": "posts/dacss603hw3_LauraCollazo.html#c",
    "href": "posts/dacss603hw3_LauraCollazo.html#c",
    "title": "DACSS 603 Homework 3",
    "section": "c",
    "text": "c\nWhen log-log is used, a simple linear regression model does seem plausible to summarize this graph.\n\n\nCode\nggplot(UN11, aes(x = log(ppgdp), y = log(fertility))) + \n  geom_point() +\nlabs(x = \"Log of per capita gross domestic product in US dollars\",\n     y = \"Log of number of children per woman\")"
  },
  {
    "objectID": "posts/dacss603hw3_LauraCollazo.html#a-1",
    "href": "posts/dacss603hw3_LauraCollazo.html#a-1",
    "title": "DACSS 603 Homework 3",
    "section": "a",
    "text": "a\nThe slope of the equation will change because the explanatory variable is responsible for determining the slope."
  },
  {
    "objectID": "posts/dacss603hw3_LauraCollazo.html#b-1",
    "href": "posts/dacss603hw3_LauraCollazo.html#b-1",
    "title": "DACSS 603 Homework 3",
    "section": "b",
    "text": "b\nThe correlation will not change as correlation is not based on units."
  },
  {
    "objectID": "posts/dacss603hw3_LauraCollazo.html#a-2",
    "href": "posts/dacss603hw3_LauraCollazo.html#a-2",
    "title": "DACSS 603 Homework 3",
    "section": "1a",
    "text": "1a\nThe below graph shows how the explanatory variable (religiosity) relates to the outcome variable (political ideology).\n\n\nCode\ndata(\"student.survey\")\n\nggplot(student.survey, aes(x = re, y = pi)) + \n  geom_jitter() +\nlabs(title = \"Political idology & church attendance\",\n     x = \"religious service attendance\",\n     y = \"political ideology\")"
  },
  {
    "objectID": "posts/dacss603hw3_LauraCollazo.html#b-2",
    "href": "posts/dacss603hw3_LauraCollazo.html#b-2",
    "title": "DACSS 603 Homework 3",
    "section": "1b",
    "text": "1b\nThe plot reveals there is a weak positive correlation between how often a students attend religious services and how conservative they are."
  },
  {
    "objectID": "posts/dacss603hw3_LauraCollazo.html#a-3",
    "href": "posts/dacss603hw3_LauraCollazo.html#a-3",
    "title": "DACSS 603 Homework 3",
    "section": "2a",
    "text": "2a\nThe below graph shows how the explanatory variable (hours of TV watching) relates to the outcome variable (high school GPA).\n\n\nCode\nggplot(student.survey, aes(x = tv, y = hi)) + \n  geom_jitter() +\nlabs( title = \"High school GPA & average weekly hours of TV\",\n      x = \"average hours of TV per week\",\n      y = \"high school GPA\")"
  },
  {
    "objectID": "posts/dacss603hw3_LauraCollazo.html#b-3",
    "href": "posts/dacss603hw3_LauraCollazo.html#b-3",
    "title": "DACSS 603 Homework 3",
    "section": "2b",
    "text": "2b\nGlancing at the plot there doesn’t appear to be much of a correlation. However, when looking at the summary of the regression below, p &lt; .05 which indicates statistical significance. R-squared is very low, though, meaning the regression is not a strong prediction model. The plot below includes the regression line to visualize this.\n\n\nCode\n summary(lm(hi~tv, data = student.survey))\n\n\n\nCall:\nlm(formula = hi ~ tv, data = student.survey)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.2583 -0.2456  0.0417  0.3368  0.7051 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  3.441353   0.085345  40.323   &lt;2e-16 ***\ntv          -0.018305   0.008658  -2.114   0.0388 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4467 on 58 degrees of freedom\nMultiple R-squared:  0.07156,   Adjusted R-squared:  0.05555 \nF-statistic: 4.471 on 1 and 58 DF,  p-value: 0.03879\n\n\n\n\nCode\nggplot(student.survey, aes(x = tv, y = hi)) + \n  geom_jitter() +\n  geom_smooth(method = \"lm\") +\nlabs( title = \"High school GPA & average weekly hours of TV\",\n      x = \"average hours of TV per week\",\n      y = \"high school GPA\")\n\n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "posts/Kristin_Abijaoude_FinalCheckIn1.html",
    "href": "posts/Kristin_Abijaoude_FinalCheckIn1.html",
    "title": "Final Project Check in 1",
    "section": "",
    "text": "Code\n# load packages\npackages &lt;- c(\"readr\", \"readxl\", \"summarytools\", \"tidyverse\", \"dplyr\")\nlapply(packages, require, character.only = TRUE)\n\n\nLoading required package: readr\n\n\nLoading required package: readxl\n\n\nLoading required package: summarytools\n\n\nLoading required package: tidyverse\n\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.0     ✔ dplyr   1.1.0\n✔ tibble  3.1.8     ✔ stringr 1.5.0\n✔ tidyr   1.2.1     ✔ forcats 0.5.2\n✔ purrr   1.0.0     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n✖ tibble::view()  masks summarytools::view()\n\n\n[[1]]\n[1] TRUE\n\n[[2]]\n[1] TRUE\n\n[[3]]\n[1] TRUE\n\n[[4]]\n[1] TRUE\n\n[[5]]\n[1] TRUE\n\n\nCode\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/Kristin_Abijaoude_FinalCheckIn1.html#overview",
    "href": "posts/Kristin_Abijaoude_FinalCheckIn1.html#overview",
    "title": "Final Project Check in 1",
    "section": "Overview",
    "text": "Overview\nBullying continues to be a persistent problem in schools.\nTypes of bullying faced by those affected include physical fights, exclusion, rumors, snarky “jokes”, and name-calling. Every bullied student dreads going to school because they have to face their bullies, who would find any reason, or no reason at all, to target them. Bullying can happen outside of school, especially with today’s advanced technology and near-universal access to the Internet. While students are always encouraged to tell a trusted adult, such as a teacher, trusted adults in authority have a spotty record when it comes to tackling this epidemic.\nIn the US alone, one of every five students report being bullied on school grounds, including name-calling (13% among those who reported bullying), being pushed or shoved (5%), or have property destroyed on purpose (1%). 15% of students who reported bullying were cyberbullied 1. Globally, one in three students report bullying, from as low as 7% in the Central Asian country of Tajikistan to as high as 74% in Samoa.2\nThe negative effects on bullying include low self-esteem, feeling angry or isolated, and distress, as well as physical effects like loss of sleep, headaches, and disordered eating. Bullying can be so detrimental to the victim that they take their own life to escape the pain.3\nWhen discussing ways to combat bullying, it’s too simplistic to say that “kids are just cruel”. My purpose is to find why some students are more vulnerable to being targets of bullying, and how we can use those parameters to create solutions to end bullying once and for all."
  },
  {
    "objectID": "posts/Kristin_Abijaoude_FinalCheckIn1.html#hypotheses-and-proposed-model",
    "href": "posts/Kristin_Abijaoude_FinalCheckIn1.html#hypotheses-and-proposed-model",
    "title": "Final Project Check in 1",
    "section": "Hypotheses and Proposed Model",
    "text": "Hypotheses and Proposed Model\nI will use multiple regression to test out my hypotheses, given the multiple independent variables such as body size, age, and gender. Multiple regression is a good model to measure the effects that multiple factors have on an given outcome. In this project, I will use these variables to explore a relationship between those variables and bullying.\n\nHa: Students who report loneliness and fewer friends are more vulnerable of being targets or bullying.\nHa: Male students are more likely to face physical abuse by bullies, while female students are more likely to face verbal abuse.\nHa: More female students who report bullying are targeted for being underweight, while male students who report bullying are targeted for being overweight."
  },
  {
    "objectID": "posts/Kristin_Abijaoude_FinalCheckIn1.html#data-summary",
    "href": "posts/Kristin_Abijaoude_FinalCheckIn1.html#data-summary",
    "title": "Final Project Check in 1",
    "section": "Data Summary",
    "text": "Data Summary\n\n\nCode\nbully &lt;- read_xlsx(\"_data/Bullying.xlsx\",\n                   range = cell_rows(2:56982))\nbully\n\n\n# A tibble: 56,980 × 18\n   record Bullie…¹ Bulli…² Cyber…³ Custo…⁴ Sex   Physi…⁵ Physi…⁶ Felt_…⁷ Close…⁸\n    &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;  \n 1      1 Yes      Yes     &lt;NA&gt;    13 yea… Fema… 0 times 0 times Always  2      \n 2      2 No       No      No      13 yea… Fema… 0 times 0 times Never   3 or m…\n 3      3 No       No      No      14 yea… Male  0 times 0 times Never   3 or m…\n 4      4 No       No      No      16 yea… Male  0 times 2 or 3… Never   3 or m…\n 5      5 No       No      No      13 yea… Fema… 0 times 0 times Rarely  3 or m…\n 6      6 No       No      No      13 yea… Male  0 times 1 time  Never   3 or m…\n 7      7 No       No      No      14 yea… Fema… 1 time  0 times Someti… 3 or m…\n 8      8 No       No      No      12 yea… Fema… 0 times 0 times Rarely  3 or m…\n 9      9 No       No      No      13 yea… Male  1 time  2 or 3… Never   3 or m…\n10     10 Yes      No      No      14 yea… Fema… 0 times 0 times Always  0      \n# … with 56,970 more rows, 8 more variables: Miss_school_no_permission &lt;chr&gt;,\n#   Other_students_kind_and_helpful &lt;chr&gt;, Parents_understand_problems &lt;chr&gt;,\n#   Most_of_the_time_or_always_felt_lonely &lt;chr&gt;,\n#   Missed_classes_or_school_without_permission &lt;chr&gt;, Were_underweight &lt;chr&gt;,\n#   Were_overweight &lt;chr&gt;, Were_obese &lt;chr&gt;, and abbreviated variable names\n#   ¹​Bullied_on_school_property_in_past_12_months,\n#   ²​Bullied_not_on_school_property_in_past_12_months, …\n\n\nThis 2018 study was conducted by Global School-Based Student Health Survey (GSHS), where 56,981 students from Argentina participated by filling out the questionnaire in regards to their mental health and behavior.4\n\n\nCode\ncolnames(bully)\n\n\n [1] \"record\"                                          \n [2] \"Bullied_on_school_property_in_past_12_months\"    \n [3] \"Bullied_not_on_school_property_in_past_12_months\"\n [4] \"Cyber_bullied_in_past_12_months\"                 \n [5] \"Custom_Age\"                                      \n [6] \"Sex\"                                             \n [7] \"Physically_attacked\"                             \n [8] \"Physical_fighting\"                               \n [9] \"Felt_lonely\"                                     \n[10] \"Close_friends\"                                   \n[11] \"Miss_school_no_permission\"                       \n[12] \"Other_students_kind_and_helpful\"                 \n[13] \"Parents_understand_problems\"                     \n[14] \"Most_of_the_time_or_always_felt_lonely\"          \n[15] \"Missed_classes_or_school_without_permission\"     \n[16] \"Were_underweight\"                                \n[17] \"Were_overweight\"                                 \n[18] \"Were_obese\"                                      \n\n\nCode\ndim(bully) # 56980 rows and 18 columns\n\n\n[1] 56980    18\n\n\n\n\nCode\nprint(dfSummary(bully,\n                        varnumbers = FALSE,\n                        plain.ascii  = FALSE, \n                        style        = \"grid\", \n                        graph.magnif = 0.70, \n                        valid.col    = FALSE),\n      method = 'render',\n      table.classes = 'table-condensed')\n\n\n\nData Frame Summary\nbully\nDimensions: 56980 x 18\n  Duplicates: 0\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nStats / Values\nFreqs (% of Valid)\nGraph\nMissing\n\n\n\n\nrecord [numeric]\n\n\n\nMean (sd) : 28534.9 (16479.7)\n\n\nmin ≤ med ≤ max:\n\n\n1 ≤ 28521.5 ≤ 57094\n\n\nIQR (CV) : 28540.5 (0.6)\n\n\n\n56980 distinct values\n\n0 (0.0%)\n\n\nBullied_on_school_property_in_past_12_months [character]\n\n\n\n1. No\n\n\n2. Yes\n\n\n\n\n\n\n43838\n(\n78.6%\n)\n\n\n11903\n(\n21.4%\n)\n\n\n\n\n1239 (2.2%)\n\n\nBullied_not_on_school_property_in_past_12_months [character]\n\n\n\n1. No\n\n\n2. Yes\n\n\n\n\n\n\n44263\n(\n78.4%\n)\n\n\n12228\n(\n21.6%\n)\n\n\n\n\n489 (0.9%)\n\n\nCyber_bullied_in_past_12_months [character]\n\n\n\n1. No\n\n\n2. Yes\n\n\n\n\n\n\n44213\n(\n78.4%\n)\n\n\n12196\n(\n21.6%\n)\n\n\n\n\n571 (1.0%)\n\n\nCustom_Age [character]\n\n\n\n1. 11 years old or younger\n\n\n2. 12 years old\n\n\n3. 13 years old\n\n\n4. 14 years old\n\n\n5. 15 years old\n\n\n6. 16 years old\n\n\n7. 17 years old\n\n\n8. 18 years old or older\n\n\n\n\n\n\n48\n(\n0.1%\n)\n\n\n145\n(\n0.3%\n)\n\n\n10574\n(\n18.6%\n)\n\n\n12946\n(\n22.8%\n)\n\n\n12812\n(\n22.5%\n)\n\n\n11737\n(\n20.6%\n)\n\n\n8227\n(\n14.5%\n)\n\n\n383\n(\n0.7%\n)\n\n\n\n\n108 (0.2%)\n\n\nSex [character]\n\n\n\n1. Female\n\n\n2. Male\n\n\n\n\n\n\n29361\n(\n52.0%\n)\n\n\n27083\n(\n48.0%\n)\n\n\n\n\n536 (0.9%)\n\n\nPhysically_attacked [character]\n\n\n\n1. 0 times\n\n\n2. 1 time\n\n\n3. 10 or 11 times\n\n\n4. 12 or more times\n\n\n5. 2 or 3 times\n\n\n6. 4 or 5 times\n\n\n7. 6 or 7 times\n\n\n8. 8 or 9 times\n\n\n\n\n\n\n46996\n(\n82.8%\n)\n\n\n5248\n(\n9.2%\n)\n\n\n115\n(\n0.2%\n)\n\n\n790\n(\n1.4%\n)\n\n\n2405\n(\n4.2%\n)\n\n\n695\n(\n1.2%\n)\n\n\n302\n(\n0.5%\n)\n\n\n189\n(\n0.3%\n)\n\n\n\n\n240 (0.4%)\n\n\nPhysical_fighting [character]\n\n\n\n1. 0 times\n\n\n2. 1 time\n\n\n3. 10 or 11 times\n\n\n4. 12 or more times\n\n\n5. 2 or 3 times\n\n\n6. 4 or 5 times\n\n\n7. 6 or 7 times\n\n\n8. 8 or 9 times\n\n\n\n\n\n\n43245\n(\n76.3%\n)\n\n\n6932\n(\n12.2%\n)\n\n\n165\n(\n0.3%\n)\n\n\n939\n(\n1.7%\n)\n\n\n3650\n(\n6.4%\n)\n\n\n1028\n(\n1.8%\n)\n\n\n489\n(\n0.9%\n)\n\n\n264\n(\n0.5%\n)\n\n\n\n\n268 (0.5%)\n\n\nFelt_lonely [character]\n\n\n\n1. Always\n\n\n2. Most of the time\n\n\n3. Never\n\n\n4. Rarely\n\n\n5. Sometimes\n\n\n\n\n\n\n3120\n(\n5.5%\n)\n\n\n6422\n(\n11.3%\n)\n\n\n17931\n(\n31.7%\n)\n\n\n14427\n(\n25.5%\n)\n\n\n14714\n(\n26.0%\n)\n\n\n\n\n366 (0.6%)\n\n\nClose_friends [character]\n\n\n\n1. 0\n\n\n2. 1\n\n\n3. 2\n\n\n4. 3 or more\n\n\n\n\n\n\n3331\n(\n6.0%\n)\n\n\n4732\n(\n8.5%\n)\n\n\n9110\n(\n16.3%\n)\n\n\n38731\n(\n69.3%\n)\n\n\n\n\n1076 (1.9%)\n\n\nMiss_school_no_permission [character]\n\n\n\n1. 0 days\n\n\n2. 1 or 2 days\n\n\n3. 10 or more days\n\n\n4. 3 to 5 days\n\n\n5. 6 to 9 days\n\n\n\n\n\n\n38654\n(\n70.1%\n)\n\n\n9738\n(\n17.7%\n)\n\n\n1468\n(\n2.7%\n)\n\n\n3925\n(\n7.1%\n)\n\n\n1331\n(\n2.4%\n)\n\n\n\n\n1864 (3.3%)\n\n\nOther_students_kind_and_helpful [character]\n\n\n\n1. Always\n\n\n2. Most of the time\n\n\n3. Never\n\n\n4. Rarely\n\n\n5. Sometimes\n\n\n\n\n\n\n9710\n(\n17.5%\n)\n\n\n15820\n(\n28.5%\n)\n\n\n4775\n(\n8.6%\n)\n\n\n10966\n(\n19.8%\n)\n\n\n14150\n(\n25.5%\n)\n\n\n\n\n1559 (2.7%)\n\n\nParents_understand_problems [character]\n\n\n\n1. Always\n\n\n2. Most of the time\n\n\n3. Never\n\n\n4. Rarely\n\n\n5. Sometimes\n\n\n\n\n\n\n13072\n(\n23.9%\n)\n\n\n9570\n(\n17.5%\n)\n\n\n11964\n(\n21.9%\n)\n\n\n10459\n(\n19.2%\n)\n\n\n9542\n(\n17.5%\n)\n\n\n\n\n2373 (4.2%)\n\n\nMost_of_the_time_or_always_felt_lonely [character]\n\n\n\n1. No\n\n\n2. Yes\n\n\n\n\n\n\n47072\n(\n83.1%\n)\n\n\n9542\n(\n16.9%\n)\n\n\n\n\n366 (0.6%)\n\n\nMissed_classes_or_school_without_permission [character]\n\n\n\n1. No\n\n\n2. Yes\n\n\n\n\n\n\n38654\n(\n70.1%\n)\n\n\n16462\n(\n29.9%\n)\n\n\n\n\n1864 (3.3%)\n\n\nWere_underweight [character]\n\n\n\n1. No\n\n\n2. Yes\n\n\n\n\n\n\n35318\n(\n98.0%\n)\n\n\n733\n(\n2.0%\n)\n\n\n\n\n20929 (36.7%)\n\n\nWere_overweight [character]\n\n\n\n1. No\n\n\n2. Yes\n\n\n\n\n\n\n25376\n(\n70.4%\n)\n\n\n10675\n(\n29.6%\n)\n\n\n\n\n20929 (36.7%)\n\n\nWere_obese [character]\n\n\n\n1. No\n\n\n2. Yes\n\n\n\n\n\n\n33396\n(\n92.6%\n)\n\n\n2655\n(\n7.4%\n)\n\n\n\n\n20929 (36.7%)\n\n\n\n\nGenerated by summarytools 1.0.1 (R version 4.2.2)2023-04-12"
  },
  {
    "objectID": "posts/Kristin_Abijaoude_FinalCheckIn1.html#footnotes",
    "href": "posts/Kristin_Abijaoude_FinalCheckIn1.html#footnotes",
    "title": "Final Project Check in 1",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nhttps://www.stopbullying.gov/resources/facts↩︎\nhttp://uis.unesco.org/en/news/new-sdg-4-data-bullying↩︎\nhttps://www.ncbi.nlm.nih.gov/books/NBK390414/↩︎\nhttps://www.kaggle.com/datasets/leomartinelli/bullying-in-schools?datasetId=2952457↩︎"
  },
  {
    "objectID": "posts/HW4_MiguelCuriel.html",
    "href": "posts/HW4_MiguelCuriel.html",
    "title": "Homework 4",
    "section": "",
    "text": "Code\n# load necessary packages\nlibrary(tidyverse)\nlibrary(alr4)\nlibrary(smss)\n\n\n\nQuestion 1\nFor recent data in Jacksonville, Florida, on y = selling price of home (in dollars), x1 = size of home (in square feet), and x2 = lot size (in square feet), the prediction equation is\nŷ = −10,536 + 53.8x1 + 2.84x2.\n\nA particular home of 1240 square feet on a lot of 18,000 square feet sold for $145,000. Find the predicted selling price and the residual, and interpret.\n\nIf ŷ = −10536 + 53.8x1 + 2.84x2, then by replacing the given values we have ŷ = -10536 + (53.8*1240) + (2.84*18000). Solving for that, the predicted selling price is $107,296 and the residual is $37,704. This means that the actual selling price was more than what the model would have predicted.\n\nFor fixed lot size, how much is the house selling price predicted to increase for each square-foot increase in home size? Why?\n\nSelling price increases by 53.8 dollars for each square-foot increase because that is the coefficient assigned to x1. In other words, x1 is the effect of home size on the selling price when holding other factors constant.\n\nAccording to this prediction equation, for fixed home size, how much would lot size need to increase to have the same impact as a one-square-foot increase in home size?\n\nLot size would need to increase 18.94 times to have the same impact as a one-square-foot increase in home size. This can be found by dividing x1 over x2, i.e. 53.8 / 2.84 = 18.94366.\n\n\n\n\nQuestion 2\n(Data file: salary in alr4 R package). The data file concerns salary and other characteristics of all faculty in a small Midwestern college collected in the early 1980s for presentation in legal proceedings for which discrimination against women in salary was at issue. All persons in the data hold tenured or tenure track positions; temporary faculty are not included. The variables include degree, a factor with levels PhD and MS; rank, a factor with levels Asst, Assoc, and Prof; sex, a factor with levels Male and Female; Year, years in current rank; ysdeg, years since highest degree, and salary, academic year salary in dollars.\n\n\nCode\ndata(\"salary\", package = \"alr4\")\nsalary &lt;- salary\nhead(salary)\n\n\n   degree rank    sex year ysdeg salary\n1 Masters Prof   Male   25    35  36350\n2 Masters Prof   Male   13    22  35350\n3 Masters Prof   Male   10    23  28200\n4 Masters Prof Female    7    27  26775\n5     PhD Prof   Male   19    30  33696\n6 Masters Prof   Male   16    21  28516\n\n\n\nTest the hypothesis that the mean salary for men and women is the same, without regard to any other variable but sex. Explain your findings.\n\nSince we are dealing with the mean of numerical data (salary) between two groups (male and female), we can run a two-sample t-test. As seen from the results below, even though the mean salary of men is higher, according to the p-value (0.09), it is not statistically significant, therefore we fail to reject the null hypothesis. In other words, we do not have enough evidence to say that male salaries are greater than female salaries.\n\n\n\n\nCode\n# extract the salary data and sex variable\nsalary &lt;- alr4::salary\nsex &lt;- salary$sex\n\n# calculate the mean salaries for men and women\nmale_salaries &lt;- salary$salary[sex == \"Male\"]\nfemale_salaries &lt;- salary$salary[sex == \"Female\"]\nmean_male_salary &lt;- mean(male_salaries)\nmean_female_salary &lt;- mean(female_salaries)\n\n# perform the t-test\nt_test &lt;- t.test(male_salaries, female_salaries)\n\n# print the results\ncat(\"Mean salary for men:\", mean_male_salary, \"\\n\")\n\n\nMean salary for men: 24696.79 \n\n\nCode\ncat(\"Mean salary for women:\", mean_female_salary, \"\\n\")\n\n\nMean salary for women: 21357.14 \n\n\nCode\ncat(\"p-value:\", t_test$p.value, \"\\n\")\n\n\np-value: 0.09009406 \n\n\n\nRun a multiple linear regression with salary as the outcome variable and everything else as predictors, including sex. Assuming no interactions between sex and the other predictors, obtain a 95% confidence interval for the difference in salary between males and females.\n\n\n\nCode\n# fit a multiple linear regression model\nmodel &lt;- lm(salary ~ ., data = salary)\n\n# print the model summary\nsummary(model)\n\n\n\nCall:\nlm(formula = salary ~ ., data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4045.2 -1094.7  -361.5   813.2  9193.1 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 15746.05     800.18  19.678  &lt; 2e-16 ***\ndegreePhD    1388.61    1018.75   1.363    0.180    \nrankAssoc    5292.36    1145.40   4.621 3.22e-05 ***\nrankProf    11118.76    1351.77   8.225 1.62e-10 ***\nsexFemale    1166.37     925.57   1.260    0.214    \nyear          476.31      94.91   5.018 8.65e-06 ***\nysdeg        -124.57      77.49  -1.608    0.115    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2398 on 45 degrees of freedom\nMultiple R-squared:  0.855, Adjusted R-squared:  0.8357 \nF-statistic: 44.24 on 6 and 45 DF,  p-value: &lt; 2.2e-16\n\n\n\n\nCode\n# obtain a 95% confidence interval for the difference in salary between males and females\nconfint(model, \"sexFemale\", level = 0.95)\n\n\n              2.5 %   97.5 %\nsexFemale -697.8183 3030.565\n\n\n\nInterpret your finding for each predictor variable; discuss (a) statistical significance, (b) interpretation of the coefficient / slope in relation to the outcome variable and other variables.\n\nStatistical significance of each variable. There are several variables that turn out to be statistically significant, in particular rank and year. This makes sense as it is commonly assumed that people with higher ranking positions have better salaries. Similarly, people with more tenure or years in a company should correlate to greater salaries. We had already determined that gender does not play a significant role, but it is interesting to see that neither level of education or years since graduation play a significant role.\nCoefficient / slope of each predictor variable in relation to the outcome variable and other variables. Some variables that immediately draw attention are rank and years since graduation - rank has a high coefficient, meaning that it plays an important role in increasing salary. Years, on the other side, has a negative coefficient, meaning that more years actually equates to less salary (which is a positive outcome for recent grads, but not for people that have several years in the workforce). The remaining variables - gender, degree, and years in a position - have positive relations, however they are not as high as rank.\n\nChange the baseline category for the rank variable. Interpret the coefficients related to rank again.\n\nAfter experimenting with the three possible ranks, coefficients do not change - what can change is the direction of the relationship. For example, if you use “Prof” as the baseline, now assistants and associates have a negative relationship, meaning that professors see an increase in salary but assistants and associates do not.\n\n\n\n\nCode\n# change the baseline category for the rank variable to \"Asst\"\nsalary$rank &lt;- relevel(salary$rank, ref = \"Prof\")\n\n# fit a new multiple linear regression model with a different baseline category for rank\nmodel2 &lt;- lm(salary ~ ., data = salary)\n\n# print the model summary\nsummary(model2)\n\n\n\nCall:\nlm(formula = salary ~ ., data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4045.2 -1094.7  -361.5   813.2  9193.1 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  26864.81    1375.29  19.534  &lt; 2e-16 ***\ndegreePhD     1388.61    1018.75   1.363    0.180    \nrankAsst    -11118.76    1351.77  -8.225 1.62e-10 ***\nrankAssoc    -5826.40    1012.93  -5.752 7.28e-07 ***\nsexFemale     1166.37     925.57   1.260    0.214    \nyear           476.31      94.91   5.018 8.65e-06 ***\nysdeg         -124.57      77.49  -1.608    0.115    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2398 on 45 degrees of freedom\nMultiple R-squared:  0.855, Adjusted R-squared:  0.8357 \nF-statistic: 44.24 on 6 and 45 DF,  p-value: &lt; 2.2e-16\n\n\n\nFinkelstein (1980), in a discussion of the use of regression in discrimination cases, wrote, “[a] variable may reflect a position or status bestowed by the employer, in which case if there is discrimination in the award of the position or status, the variable may be ‘tainted.’”Thus, for example, if discrimination is at work in promotion of faculty to higher ranks, using rank to adjust salaries before comparing the sexes may not be acceptable to the courts. (Exclude the variable rank, refit, and summarize how your findings changed, if they did.)\n\nThis results in significant changes. With this new model, all variables included are statistically significant. This means that gender, degree, and years since graduation do play an important role in salary. In particular, females see a decrease in salary, as do PhD graduates, while years since graduation is the only variable with a positive influence on salary.\n\n\n\n\nCode\n# fit a new multiple linear regression model without rank\nmodel3 &lt;- lm(salary ~ sex + degree + ysdeg, data = salary)\n\n# print the model summary\nsummary(model3)\n\n\n\nCall:\nlm(formula = salary ~ sex + degree + ysdeg, data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8328.5 -2621.9  -864.5  2987.3 11025.5 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  18325.0     1105.3  16.580  &lt; 2e-16 ***\nsexFemale    -2730.2     1236.8  -2.207  0.03210 *  \ndegreePhD    -4228.4     1311.7  -3.224  0.00228 ** \nysdeg          476.0       61.7   7.716 5.94e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3937 on 48 degrees of freedom\nMultiple R-squared:  0.5833,    Adjusted R-squared:  0.5572 \nF-statistic: 22.39 on 3 and 48 DF,  p-value: 3.272e-09\n\n\n\nEveryone in this dataset was hired the year they earned their highest degree. It is also known that a new Dean was appointed 15 years ago, and everyone in the dataset who earned their highest degree 15 years ago or less than that has been hired by the new Dean. Some people have argued that the new Dean has been making offers that are a lot more generous to newly hired faculty than the previous one and that this might explain some of the variation in Salary. (Create a new variable that would allow you to test this hypothesis and run another multiple regression model to test this. Select variables carefully to make sure there is no multicollinearity. Explain why multicollinearity would be a concern in this case and how you avoided it. Do you find support for the hypothesis that the people hired by the new Dean are making higher than those that were not?)\n\nI created a new binary variable to distinguish between people hired before and after the new dean. To check for multicollinearity, I created a correlation matrix - and, as expected, years since graduation is somewhat correlated. This makes sense as people with more recent graduation dates are more likely to have been hired by the new dean - therefore, “ysdeg” was removed from the model. After fitting the model, most variables remain relatively the same and the people hired before/after the new dean do not seem to have a statistically significant salary difference.\n\n\n\n\nCode\n# Create a new variable indicating whether the person was hired by the new Dean or not\nsalary$newDean &lt;- ifelse(salary$year &gt;= 15, 0, 1)\n\n\n\n\nCode\n# Calculate the correlation matrix\ncor(salary[, c(\"salary\", \"ysdeg\", \"newDean\")])\n\n\n            salary      ysdeg    newDean\nsalary   1.0000000  0.6748542 -0.3246869\nysdeg    0.6748542  1.0000000 -0.2931746\nnewDean -0.3246869 -0.2931746  1.0000000\n\n\n\n\nCode\n# Fit the multiple regression model\nmodel4 &lt;- lm(salary ~ sex + rank + degree + year + newDean, data=salary)\n\n# Check the summary of the model\nsummary(model4)\n\n\n\nCall:\nlm(formula = salary ~ sex + rank + degree + year + newDean, data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3514.8 -1641.7  -263.6   895.5  8867.9 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  22870.5     2218.1  10.311 1.98e-13 ***\nsexFemale      550.0      838.5   0.656    0.515    \nrankAsst     -9198.9      948.3  -9.700 1.34e-12 ***\nrankAssoc    -5121.3      963.7  -5.314 3.21e-06 ***\ndegreePhD      163.7      785.8   0.208    0.836    \nyear           478.2      106.0   4.512 4.58e-05 ***\nnewDean       1931.1     1514.0   1.275    0.209    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2423 on 45 degrees of freedom\nMultiple R-squared:  0.8521,    Adjusted R-squared:  0.8323 \nF-statistic:  43.2 on 6 and 45 DF,  p-value: &lt; 2.2e-16\n\n\n\n\nQuestion 3\n(Data file: house.selling.price in smss R package)\n\n\nCode\ndata(\"house.selling.price\", package = \"smss\")\nprices &lt;- house.selling.price\nhead(prices)\n\n\n  case Taxes Beds Baths New  Price Size\n1    1  3104    4     2   0 279900 2048\n2    2  1173    2     1   0 146500  912\n3    3  3076    4     2   0 237700 1654\n4    4  1608    3     2   0 200000 2068\n5    5  1454    3     3   0 159900 1477\n6    6  2997    3     2   1 499900 3153\n\n\n\nUsing the house.selling.price data, run and report regression results modeling y = selling price (in dollars) in terms of size of home (in square feet) and whether the home is new (1 = yes; 0 = no). In particular, for each variable; discuss statistical significance and interpret the meaning of the coefficient.\n\nAfter running the model, both variables are statistically significant, although size is much more significant than new status (2e-16 &gt; 0.00257). However, the coefficient is much higher for new status than it is fore size (57736 &gt;116).\n\n\n\n\nCode\n# Fit the multiple regression model\nmodel5 &lt;- lm(Price ~ Size + New, data=prices)\n\n# Check the summary of the model\nsummary(model5)\n\n\n\nCall:\nlm(formula = Price ~ Size + New, data = prices)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-205102  -34374   -5778   18929  163866 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -40230.867  14696.140  -2.738  0.00737 ** \nSize           116.132      8.795  13.204  &lt; 2e-16 ***\nNew          57736.283  18653.041   3.095  0.00257 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 53880 on 97 degrees of freedom\nMultiple R-squared:  0.7226,    Adjusted R-squared:  0.7169 \nF-statistic: 126.3 on 2 and 97 DF,  p-value: &lt; 2.2e-16\n\n\n\nReport and interpret the prediction equation, and form separate equations relating selling price to size for new and for not new homes.\n\nThe equation would be ŷ = -40230.867 + 116.132*size + 57736.283*new. What this means is that, if a house is new, it’s selling price will increase by $57,736.\n\nFind the predicted selling price for a home of 3000 square feet that is (i) new, (ii) not new.\n\nUsing the above equation and replacing for the values given, we have the two following equations: ŷ(new) = -40230.867 + 116.132*3000 + 57736.283*1 and ŷ(not new) = -40230.867 + 116.132*3000 + 57736.283*0. This results in a selling price of $365,901.4 for the new home and $308,165.1 for one that is not new.\n\nFit another model, this time with an interaction term allowing interaction between size and new, and report the regression results.\n\n\n\nCode\n# Fit the multiple regression model\nmodel6 &lt;- lm(Price ~ Size * New, data=prices)\n\n# Check the summary of the model\nsummary(model6)\n\n\n\nCall:\nlm(formula = Price ~ Size * New, data = prices)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-175748  -28979   -6260   14693  192519 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -22227.808  15521.110  -1.432  0.15536    \nSize           104.438      9.424  11.082  &lt; 2e-16 ***\nNew         -78527.502  51007.642  -1.540  0.12697    \nSize:New        61.916     21.686   2.855  0.00527 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 52000 on 96 degrees of freedom\nMultiple R-squared:  0.7443,    Adjusted R-squared:  0.7363 \nF-statistic: 93.15 on 3 and 96 DF,  p-value: &lt; 2.2e-16\n\n\n\nReport the lines relating the predicted selling price to the size for homes that are (i) new (ii) not new.\n\nThe new formula would be ŷ = -22227.808 + 104.438*size - 78527.502*new + 61.916*size*new. We would have to replace with a 1 or 0 depending where it says “new” in the previous equation depending on a house’s status.\n\nFind the predicted selling price for a home of 3000 square feet that is (i) new, (ii) not new.\n\nReplacing the given values in the previous equation, we would have the two following formulas: ŷ(new)= -22227.808 + 104.438*3000 - 78527.502*1 + 61.916*3000*1 and ŷ(not new)= -22227.808 + 104.438*3000 - 78527.502*0 + 61.916*3000*0. This results in a selling price of $398,306.7 for the new house and $291,086.2 for one that is not new.\n\nFind the predicted selling price for a home of 1500 square feet that is (i) new, (ii) not new. Comparing to (F), explain how the difference in predicted selling prices changes as the size of home increases.\n\nFollowing the same steps, we now have these formulas: ŷ(new)= -22227.808 + 104.438*1500 - 78527.502*1 + 61.916*1500*1 and ŷ(not new)= -22227.808 + 104.438*1500 - 78527.502*0 + 61.916*1500*0. This results in a selling price of $148,775.7 for the new house and $134,429.2 for one that is not new. Compared to the prices in (F), there seems to be an exponentiation effect on the selling price as houses increase in size.\n\nDo you think the model with interaction or the one without it represents the relationship of size and new to the outcome price? What makes you prefer one model over another?\n\nEven though it is tempting to say the the model without interaction performs better because of the lower R-squared, when comparing across other metrics (Root Mean Squared Error, Mean Absolute Error, and Akaike Information Criterion), it turns out the model with interaction performs better. Additionally, from the examples above, the prices predicted by the model with interaction does seem to be closer to the predicted prices. Therefore, I would chose the model with interaction.\n\n\n\n\nCode\n# Create a function to calculate the evaluation metrics\neval_metrics &lt;- function(model) {\n  # Calculate the R-squared value\n  rsq &lt;- summary(model)$r.squared\n  \n  # Calculate the RMSE value\n  predicted &lt;- predict(model, newdata = house.selling.price)\n  actual &lt;- house.selling.price$Price\n  rmse &lt;- sqrt(mean((predicted - actual)^2))\n  \n  # Calculate the MAE value\n  mae &lt;- mean(abs(predicted - actual))\n  \n  # Calculate the AIC value\n  aic &lt;- AIC(model)\n  \n  # Return a data frame with the evaluation metrics\n  data.frame(R2 = rsq, RMSE = rmse, MAE = mae, AIC = aic)\n}\n\n# Calculate the evaluation metrics for each model\nmetrics1 &lt;- eval_metrics(model5)\nmetrics2 &lt;- eval_metrics(model6)\n\n# Combine the metrics into a table\nmetrics_table &lt;- rbind(metrics1, metrics2)\nrownames(metrics_table) &lt;- c(\"Model without Interaction\"\n                             , \"Model with Interaction\")\n\n# Print the table\nmetrics_table\n\n\n                                 R2     RMSE      MAE      AIC\nModel without Interaction 0.7225963 53066.58 38015.03 2467.648\nModel with Interaction    0.7443085 50947.53 35200.58 2461.498"
  },
  {
    "objectID": "posts/FinalProject_Part1_AlexaPotter.html",
    "href": "posts/FinalProject_Part1_AlexaPotter.html",
    "title": "Final Project Part 1",
    "section": "",
    "text": "Greenery is widely recognized as a vital element to any public space. Plants and natural elements of all kinds can add immense benefits to urban areas, not only to the environment but to the humans who inhabit the space as well. Studies have shown that urban greening, specifically the planting of trees, can “combat challenges such as pollution, urban heat, and flooding, as well as to improve social cohesion, human health, and well-being.”1 The understanding and implementation of this information can lead cities, developers, and anyone with a stake in their community to consciously consider what elements they can incorporate into their own public spaces. The work does not end here though, greenery in public spaces requires maintenance and year-round management to make these efforts last long enough to see the benefits.\nNew York City, with a population of about 8.4 billion people and 300 square miles, is one of the largest urban spaces in the United States.2 In 1995, the New York City Department of Parks and Recreation conducted a city wide census of all the trees. They again conducted this survey in 2005 and 2015 to tackle their goal of enhancing and restoring urban forests.3 The information this survey collected has been used to create an interactive map of tree species around New York City. The Parks department then uses this data to calculate the related impacts and needs associated with the trees and tree maintenance.4\nWhile there is a large amount of data related to the tree census published, there is a gap in information on the relation between tree data to specific neighborhoods. Critically analyzing this tree data on a neighborhood level can lead to further community involvement within and stemming from neighborhoods themselves.5 Firsthand involvement can be used to develop connections between community members and foster ownership among members with the environment they inhabit.\nThis informs my research question:"
  },
  {
    "objectID": "posts/FinalProject_Part1_AlexaPotter.html#footnotes",
    "href": "posts/FinalProject_Part1_AlexaPotter.html#footnotes",
    "title": "Final Project Part 1",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nCavender, N., & Donnelly, G. (2019). Intersecting Urban Forestry and botanical gardens to address big challenges for healthier trees, people, and cities. PLANTS, PEOPLE, PLANET, 1(4), 315–322. https://doi.org/10.1002/ppp3.38↩︎\nU.S. Census Bureau . (n.d.). U.S. Census Bureau quickfacts: New York City, New York. Retrieved March 20, 2023, from https://www.census.gov/quickfacts/newyorkcitynewyork↩︎\nMerriman, D. (2017) Volunteers count every street tree in New York City. US Forest Service. Retrieved March 20, 2023, from https://www.fs.usda.gov/features/volunteers-count-every-street-tree-new-york-city-0↩︎\nCochran, C., & Greer, B. (2016, June 29). Treescount! 2015: NYC’s Third Street-Tree Census. New York State Urban Forestry Council. Retrieved March 20, 2023, from https://nysufc.org/treescount/2016/04/26/↩︎\nMa, Q., Lin, J., Ju, Y. et al. Individual structure mapping over six million trees for New York City USA. Sci Data 10, 102 (2023). https://doi.org/10.1038/s41597-023-02000-w↩︎\nNeckerman, K., Lovasi, G., Davies, S. et al. Disparities in Urban Neighborhood Conditions: Evidence from GIS Measures and Field Observation in New York City. Public Health Pol 30 (Suppl 1), S264–S285 (2009). https://doi.org/10.1057/jphp.2008.47↩︎\nColleen E. Reid, Laura D. Kubzansky, Jiayue Li, Jessie L. Shmool, Jane E. Clougherty. It’s not easy assessing greenness: A comparison of NDVI datasets and neighborhood types and their associations with self-rated health in New York City. Health & Place 54, 92-101 (2018).https://doi.org/10.1016/j.healthplace.2018.09.005.↩︎\nJian Lin, Qiang Wang, Xiaojiang Li. Socioeconomic and spatial inequalities of street tree abundance, species diversity, and size structure in New York City. Landscape and Urban Planning, Volume 206. 2021. 103992. https://doi.org/10.1016/j.landurbplan.2020.103992.↩︎\nhttps://data.cityofnewyork.us/Environment/2015-Street-Tree-Census-Tree-Data/uvpi-gqnh↩︎"
  },
  {
    "objectID": "posts/AdithyaParupudi_hw4.html",
    "href": "posts/AdithyaParupudi_hw4.html",
    "title": "Homework 4",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(stats)\nlibrary(alr4)\nlibrary(smss)\n\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/AdithyaParupudi_hw4.html#question-1",
    "href": "posts/AdithyaParupudi_hw4.html#question-1",
    "title": "Homework 4",
    "section": "Question 1",
    "text": "Question 1"
  },
  {
    "objectID": "posts/AdithyaParupudi_hw4.html#a",
    "href": "posts/AdithyaParupudi_hw4.html#a",
    "title": "Homework 4",
    "section": "A",
    "text": "A\n\n\nCode\nPred_selling_price &lt;-  -10536 + 53.8 * 1240 + 2.84 * 18000\nPred_selling_price\n\n\n[1] 107296\n\n\n\n\nCode\nResidual &lt;- Pred_selling_price - 145000\nResidual\n\n\n[1] -37704\n\n\nFrom the above result, we can say that the house was sold for 37704 dollars greater than predicted."
  },
  {
    "objectID": "posts/AdithyaParupudi_hw4.html#b",
    "href": "posts/AdithyaParupudi_hw4.html#b",
    "title": "Homework 4",
    "section": "B",
    "text": "B\nUsing the prediction equation ŷ = -10536 + 53.8x1 + 2.84x2, where x2 equals lot size, the house selling price is expected to increase by 53.8 dollars per each square-foot increase in home size given the lot sized is fixed. This is because a fixed lot size would make 2.84x2 a set number in the prediction equation. Therefore, we would not need to factor in a change in the output based on any input. Then, we are left with the coefficient for the home size variable, which is 53.8. For x1 = 1, representing one square-foot of home size, the output would increase by 53.8 * 1 = 53.8."
  },
  {
    "objectID": "posts/AdithyaParupudi_hw4.html#c",
    "href": "posts/AdithyaParupudi_hw4.html#c",
    "title": "Homework 4",
    "section": "C",
    "text": "C\nFor fixed home size, 53.8 * 1 = 2.84x2\n\n\nCode\nresult &lt;- 53.8/2.84\nresult\n\n\n[1] 18.94366\n\n\nAn increase in lot size of about 18.94 square-feet would have the same impact as an increase of 1 square-foot in home size on the predicted selling price."
  },
  {
    "objectID": "posts/AdithyaParupudi_hw4.html#question-2",
    "href": "posts/AdithyaParupudi_hw4.html#question-2",
    "title": "Homework 4",
    "section": "Question 2",
    "text": "Question 2\n\n\nCode\ndata(\"salary\")\nsalary\n\n\n    degree  rank    sex year ysdeg salary\n1  Masters  Prof   Male   25    35  36350\n2  Masters  Prof   Male   13    22  35350\n3  Masters  Prof   Male   10    23  28200\n4  Masters  Prof Female    7    27  26775\n5      PhD  Prof   Male   19    30  33696\n6  Masters  Prof   Male   16    21  28516\n7      PhD  Prof Female    0    32  24900\n8  Masters  Prof   Male   16    18  31909\n9      PhD  Prof   Male   13    30  31850\n10     PhD  Prof   Male   13    31  32850\n11 Masters  Prof   Male   12    22  27025\n12 Masters Assoc   Male   15    19  24750\n13 Masters  Prof   Male    9    17  28200\n14     PhD Assoc   Male    9    27  23712\n15 Masters  Prof   Male    9    24  25748\n16 Masters  Prof   Male    7    15  29342\n17 Masters  Prof   Male   13    20  31114\n18     PhD Assoc   Male   11    14  24742\n19     PhD Assoc   Male   10    15  22906\n20     PhD  Prof   Male    6    21  24450\n21     PhD  Asst   Male   16    23  19175\n22     PhD Assoc   Male    8    31  20525\n23 Masters  Prof   Male    7    13  27959\n24 Masters  Prof Female    8    24  38045\n25 Masters Assoc   Male    9    12  24832\n26 Masters  Prof   Male    5    18  25400\n27 Masters Assoc   Male   11    14  24800\n28 Masters  Prof Female    5    16  25500\n29     PhD Assoc   Male    3     7  26182\n30     PhD Assoc   Male    3    17  23725\n31     PhD  Asst Female   10    15  21600\n32     PhD Assoc   Male   11    31  23300\n33     PhD  Asst   Male    9    14  23713\n34     PhD Assoc Female    4    33  20690\n35     PhD Assoc Female    6    29  22450\n36 Masters Assoc   Male    1     9  20850\n37 Masters  Asst Female    8    14  18304\n38 Masters  Asst   Male    4     4  17095\n39 Masters  Asst   Male    4     5  16700\n40 Masters  Asst   Male    4     4  17600\n41 Masters  Asst   Male    3     4  18075\n42     PhD  Asst   Male    3    11  18000\n43 Masters Assoc   Male    0     7  20999\n44 Masters  Asst Female    3     3  17250\n45 Masters  Asst   Male    2     3  16500\n46 Masters  Asst   Male    2     1  16094\n47 Masters  Asst Female    2     6  16150\n48 Masters  Asst Female    2     2  15350\n49 Masters  Asst   Male    1     1  16244\n50 Masters  Asst Female    1     1  16686\n51 Masters  Asst Female    1     1  15000\n52 Masters  Asst Female    0     2  20300"
  },
  {
    "objectID": "posts/AdithyaParupudi_hw4.html#a-1",
    "href": "posts/AdithyaParupudi_hw4.html#a-1",
    "title": "Homework 4",
    "section": "A",
    "text": "A\n\n\nCode\nsummary(lm(salary ~ sex, data = salary))\n\n\n\nCall:\nlm(formula = salary ~ sex, data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8602.8 -4296.6  -100.8  3513.1 16687.9 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    24697        938  26.330   &lt;2e-16 ***\nsexFemale      -3340       1808  -1.847   0.0706 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5782 on 50 degrees of freedom\nMultiple R-squared:  0.0639,    Adjusted R-squared:  0.04518 \nF-statistic: 3.413 on 1 and 50 DF,  p-value: 0.0706\n\n\nThe null hypothesis would be that mean salary for men and mean salary for women are equal, and the alternative hypothesis would be that the salaries are not equal. I ran a regression with sex as the explanatory variable and salary as the outcome variable. The female coefficient is -3340, which means that women do make less than men not considering any other variables. However, if we consider the other variables and also there is a significance level of 0.07, so we fail to reject the null hypothesis and therefore cannot conclude that there is a difference between mean salaries for men and women."
  },
  {
    "objectID": "posts/AdithyaParupudi_hw4.html#b-1",
    "href": "posts/AdithyaParupudi_hw4.html#b-1",
    "title": "Homework 4",
    "section": "B",
    "text": "B\n\n\nCode\nmodel &lt;- lm(salary ~ ., data = salary)\nsummary(model)\n\n\n\nCall:\nlm(formula = salary ~ ., data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4045.2 -1094.7  -361.5   813.2  9193.1 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 15746.05     800.18  19.678  &lt; 2e-16 ***\ndegreePhD    1388.61    1018.75   1.363    0.180    \nrankAssoc    5292.36    1145.40   4.621 3.22e-05 ***\nrankProf    11118.76    1351.77   8.225 1.62e-10 ***\nsexFemale    1166.37     925.57   1.260    0.214    \nyear          476.31      94.91   5.018 8.65e-06 ***\nysdeg        -124.57      77.49  -1.608    0.115    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2398 on 45 degrees of freedom\nMultiple R-squared:  0.855, Adjusted R-squared:  0.8357 \nF-statistic: 44.24 on 6 and 45 DF,  p-value: &lt; 2.2e-16\n\n\n\n\nCode\nconfint(model)\n\n\n                 2.5 %      97.5 %\n(Intercept) 14134.4059 17357.68946\ndegreePhD    -663.2482  3440.47485\nrankAssoc    2985.4107  7599.31080\nrankProf     8396.1546 13841.37340\nsexFemale    -697.8183  3030.56452\nyear          285.1433   667.47476\nysdeg        -280.6397    31.49105\n\n\nAssuming there is no interaction between sex and other predictors, we can be 95% confident that the difference in salary of women compared to men falls between -697.8183 dollars and 3030.56452 dollars."
  },
  {
    "objectID": "posts/AdithyaParupudi_hw4.html#c-1",
    "href": "posts/AdithyaParupudi_hw4.html#c-1",
    "title": "Homework 4",
    "section": "C",
    "text": "C\nFor degree as the predictor, a PHD would be expected to increase salary by 1388.61 dollars in reference to a Masters degree salary. However, at a significance level of 0.18, we cannot conclude that degree level has a statistically significant impact on salary.\nFor the rank variable, an Associate can expect a 5292.36 dollar increase in salary compared to Assistant, while a Professor can expect a 11118.76 dollar salary increase compared to Assistant. Both ranks have significance levels well below 0.05 and we can determine that rank does have a statistically significant impact on salary. For the variable of sex, a Female can expect a salary increase of 1166.37 dollars in comparison to Male salary, but the significance level is 0.214, so this is not a statistically significant relationship.\nFor year, a faculty member can expect a salary increase of 476.31 dollars for an increase in 1 year of employment in his/her/their position. Additionally, the level of significance is less than 0.01 so the relationship between year and salary appears to be significant.\nFor the ysdeg variable, an increase in years since earning highest degree can expect a decrease in salary, with a coefficient of -124.57. However, with a 0.115 level of significance, this relationship cannot be found to be statistically significant."
  },
  {
    "objectID": "posts/AdithyaParupudi_hw4.html#d",
    "href": "posts/AdithyaParupudi_hw4.html#d",
    "title": "Homework 4",
    "section": "D",
    "text": "D\n\n\nCode\nsalary$rank &lt;- relevel(salary$rank, ref = \"Prof\")\nsummary(lm(salary ~ rank, salary))\n\n\n\nCall:\nlm(formula = salary ~ rank, data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5209.0 -1819.2  -417.8  1586.6  8386.0 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  29659.0      669.3  44.316  &lt; 2e-16 ***\nrankAsst    -11890.3      972.4 -12.228  &lt; 2e-16 ***\nrankAssoc    -6483.0     1043.0  -6.216 1.09e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2993 on 49 degrees of freedom\nMultiple R-squared:  0.7542,    Adjusted R-squared:  0.7442 \nF-statistic: 75.17 on 2 and 49 DF,  p-value: 1.174e-15\n\n\nAfter changing the baseline category for the rank variable, an Associate can expect a 6483.0 dollar decrease in salary compared to Professor, while a Assistant can expect a 11890.3 dollar salary decrease compared to Professor. Both ranks have significance levels well below 0.05 and we can determine that rank does have a statistically significant impact on salary."
  },
  {
    "objectID": "posts/AdithyaParupudi_hw4.html#e",
    "href": "posts/AdithyaParupudi_hw4.html#e",
    "title": "Homework 4",
    "section": "E",
    "text": "E\n\n\nCode\nsummary(lm(salary ~ degree + sex + year + ysdeg, salary))\n\n\n\nCall:\nlm(formula = salary ~ degree + sex + year + ysdeg, data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8146.9 -2186.9  -491.5  2279.1 11186.6 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 17183.57    1147.94  14.969  &lt; 2e-16 ***\ndegreePhD   -3299.35    1302.52  -2.533 0.014704 *  \nsexFemale   -1286.54    1313.09  -0.980 0.332209    \nyear          351.97     142.48   2.470 0.017185 *  \nysdeg         339.40      80.62   4.210 0.000114 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3744 on 47 degrees of freedom\nMultiple R-squared:  0.6312,    Adjusted R-squared:  0.5998 \nF-statistic: 20.11 on 4 and 47 DF,  p-value: 1.048e-09\n\n\nWhen removing the variable “rank”, the coefficient for sex is -1286.54 compared to the above regression that included rank with a coefficient for sex at 1166.37. The new coefficient predicts that a female salary would be 1286.54 less than a male salary, when excluding the variable of rank. However, the significance level is 0.332, which is very high and therefore the results cannot be found to be statistically significant. While the change of the coefficient to negative upon removal of rank is interesting, the significance level would likely prevent these results from holding up in court as an indication of discrimination on the basis of sex."
  },
  {
    "objectID": "posts/AdithyaParupudi_hw4.html#f",
    "href": "posts/AdithyaParupudi_hw4.html#f",
    "title": "Homework 4",
    "section": "F",
    "text": "F\n\n\nCode\nsalary &lt;- salary %&gt;%\n  mutate(hired = case_when(ysdeg &lt;= 15 ~ \"1\", ysdeg &gt; 15 ~ \"0\"))\nsummary(lm(salary ~ hired, data = salary))\n\n\n\nCall:\nlm(formula = salary ~ hired, data = salary)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n -8294  -3486  -1772   3829  10576 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  27469.4      913.4  30.073  &lt; 2e-16 ***\nhired1       -7343.5     1291.8  -5.685 6.73e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4658 on 50 degrees of freedom\nMultiple R-squared:  0.3926,    Adjusted R-squared:  0.3804 \nF-statistic: 32.32 on 1 and 50 DF,  p-value: 6.734e-07\n\n\n\n\nCode\nsummary(lm(salary ~ sex + rank + degree + hired, data = salary))\n\n\n\nCall:\nlm(formula = salary ~ sex + rank + degree + hired, data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6187.5 -1750.9  -438.9  1719.5  9362.9 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  29511.3      784.0  37.640  &lt; 2e-16 ***\nsexFemale     -829.2      997.6  -0.831    0.410    \nrankAsst    -11925.7     1512.4  -7.885 4.37e-10 ***\nrankAssoc    -7100.4     1297.0  -5.474 1.76e-06 ***\ndegreePhD     1126.2     1018.4   1.106    0.275    \nhired1         319.0     1303.8   0.245    0.808    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3023 on 46 degrees of freedom\nMultiple R-squared:  0.7645,    Adjusted R-squared:  0.7389 \nF-statistic: 29.87 on 5 and 46 DF,  p-value: 2.192e-13\n\n\nI created a dummy variable called “hired” which coded those employed for 15 years or less (thus hired by the new Dean) as 1 and those who have been employed for over 15 years as 0. Then, I fit a new regression model and decided to include the variables of sex, rank, degree, and hired. I omitted the year and ysdeg variables to prevent overlapping or multicollinearity. Multicollinearity can be a concern when variables are highly correlated or related in some way. The idea of regression is to observe how each variable partially effects the output while holding the other variables fixed. We cannot reasonably change the year or ysdeg or hired variables individually while holding the other two fixed since they tend to “grow” in similar manners. Since the variable hired is a product of the ysdeg variable, we could not include both.\nBased on the regression model, those hired by the current Dean are predicted to make 319 dollars more than those not hired by the Dean. When it comes to salary, this is a rather insignificant number. Furthermore, the level of significance for the hired variable is .81, which is astronomical and indicates that the relationship between hired and salary is not statistically significant. Based on these factors, I would state that findings do not indicate any favorable treatment by the Dean toward faculty that the Dean specifically hired."
  },
  {
    "objectID": "posts/AdithyaParupudi_hw4.html#question-3",
    "href": "posts/AdithyaParupudi_hw4.html#question-3",
    "title": "Homework 4",
    "section": "Question 3",
    "text": "Question 3\n\n\nCode\ndata(\"house.selling.price\")\nhouse.selling.price\n\n\n    case Taxes Beds Baths New  Price Size\n1      1  3104    4     2   0 279900 2048\n2      2  1173    2     1   0 146500  912\n3      3  3076    4     2   0 237700 1654\n4      4  1608    3     2   0 200000 2068\n5      5  1454    3     3   0 159900 1477\n6      6  2997    3     2   1 499900 3153\n7      7  4054    3     2   0 265500 1355\n8      8  3002    3     2   1 289900 2075\n9      9  6627    5     4   0 587000 3990\n10    10   320    3     2   0  70000 1160\n11    11   630    3     2   0  64500 1220\n12    12  1780    3     2   0 167000 1690\n13    13  1630    3     2   0 114600 1380\n14    14  1530    3     2   0 103000 1590\n15    15   930    3     1   0 101000 1050\n16    16   590    2     1   0  70000  770\n17    17  1050    3     2   0  85000 1410\n18    18    20    3     1   0  22500 1060\n19    19   870    2     2   0  90000 1300\n20    20  1320    3     2   0 133000 1500\n21    21  1350    2     1   0  90500  820\n22    22  5616    4     3   1 577500 3949\n23    23   680    2     1   0 142500 1170\n24    24  1840    3     2   0 160000 1500\n25    25  3680    4     2   0 240000 2790\n26    26  1660    3     1   0  87000 1030\n27    27  1620    3     2   0 118600 1250\n28    28  3100    3     2   0 140000 1760\n29    29  2070    2     3   0 148000 1550\n30    30   830    3     2   0  69000 1120\n31    31  2260    4     2   0 176000 2000\n32    32  1760    3     1   0  86500 1350\n33    33  2750    3     2   1 180000 1840\n34    34  2020    4     2   0 179000 2510\n35    35  4900    3     3   1 338000 3110\n36    36  1180    4     2   0 130000 1760\n37    37  2150    3     2   0 163000 1710\n38    38  1600    2     1   0 125000 1110\n39    39  1970    3     2   0 100000 1360\n40    40  2060    3     1   0 100000 1250\n41    41  1980    3     1   0 100000 1250\n42    42  1510    3     2   0 146500 1480\n43    43  1710    3     2   0 144900 1520\n44    44  1590    3     2   0 183000 2020\n45    45  1230    3     2   0  69900 1010\n46    46  1510    2     2   0  60000 1640\n47    47  1450    2     2   0 127000  940\n48    48   970    3     2   0  86000 1580\n49    49   150    2     2   0  50000  860\n50    50  1470    3     2   0 137000 1420\n51    51  1850    3     2   0 121300 1270\n52    52   820    2     1   0  81000  980\n53    53  2050    4     2   0 188000 2300\n54    54   710    3     2   0  85000 1430\n55    55  1280    3     2   0 137000 1380\n56    56  1360    3     2   0 145000 1240\n57    57   830    3     2   0  69000 1120\n58    58   800    3     2   0 109300 1120\n59    59  1220    3     2   0 131500 1900\n60    60  3360    4     3   0 200000 2430\n61    61   210    3     2   0  81900 1080\n62    62   380    2     1   0  91200 1350\n63    63  1920    4     3   0 124500 1720\n64    64  4350    3     3   0 225000 4050\n65    65  1510    3     2   0 136500 1500\n66    66  4154    3     3   0 381000 2581\n67    67  1976    3     2   1 250000 2120\n68    68  3605    3     3   1 354900 2745\n69    69  1400    3     2   0 140000 1520\n70    70   790    2     2   0  89900 1280\n71    71  1210    3     2   0 137000 1620\n72    72  1550    3     2   0 103000 1520\n73    73  2800    3     2   0 183000 2030\n74    74  2560    3     2   0 140000 1390\n75    75  1390    4     2   0 160000 1880\n76    76  5443    3     2   0 434000 2891\n77    77  2850    2     1   0 130000 1340\n78    78  2230    2     2   0 123000  940\n79    79    20    2     1   0  21000  580\n80    80  1510    4     2   0  85000 1410\n81    81   710    3     2   0  69900 1150\n82    82  1540    3     2   0 125000 1380\n83    83  1780    3     2   1 162600 1470\n84    84  2920    2     2   1 156900 1590\n85    85  1710    3     2   1 105900 1200\n86    86  1880    3     2   0 167500 1920\n87    87  1680    3     2   0 151800 2150\n88    88  3690    5     3   0 118300 2200\n89    89   900    2     2   0  94300  860\n90    90   560    3     1   0  93900 1230\n91    91  2040    4     2   0 165000 1140\n92    92  4390    4     3   1 285000 2650\n93    93   690    3     1   0  45000 1060\n94    94  2100    3     2   0 124900 1770\n95    95  2880    4     2   0 147000 1860\n96    96   990    2     2   0 176000 1060\n97    97  3030    3     2   0 196500 1730\n98    98  1580    3     2   0 132200 1370\n99    99  1770    3     2   0  88400 1560\n100  100  1430    3     2   0 127200 1340"
  },
  {
    "objectID": "posts/AdithyaParupudi_hw4.html#a-2",
    "href": "posts/AdithyaParupudi_hw4.html#a-2",
    "title": "Homework 4",
    "section": "A",
    "text": "A\n\n\nCode\nsummary(lm(Price ~ Size + New, house.selling.price))\n\n\n\nCall:\nlm(formula = Price ~ Size + New, data = house.selling.price)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-205102  -34374   -5778   18929  163866 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -40230.867  14696.140  -2.738  0.00737 ** \nSize           116.132      8.795  13.204  &lt; 2e-16 ***\nNew          57736.283  18653.041   3.095  0.00257 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 53880 on 97 degrees of freedom\nMultiple R-squared:  0.7226,    Adjusted R-squared:  0.7169 \nF-statistic: 126.3 on 2 and 97 DF,  p-value: &lt; 2.2e-16\n\n\nBoth Size and New significantly positively predict selling price. As each predictor goes up by 1 unit, selling price rises by 116.132 dollars and 57736.283 dollars respectively."
  },
  {
    "objectID": "posts/AdithyaParupudi_hw4.html#b-2",
    "href": "posts/AdithyaParupudi_hw4.html#b-2",
    "title": "Homework 4",
    "section": "B",
    "text": "B\n\n\nCode\nnew &lt;- house.selling.price %&gt;% \n  filter(New == 1)\nsummary(lm(Price ~ Size, data = new))\n\n\n\nCall:\nlm(formula = Price ~ Size, data = new)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-78606 -16092   -987  20068  76140 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -100755.31   42513.73  -2.370   0.0419 *  \nSize            166.35      17.09   9.735 4.47e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 45500 on 9 degrees of freedom\nMultiple R-squared:  0.9133,    Adjusted R-squared:  0.9036 \nF-statistic: 94.76 on 1 and 9 DF,  p-value: 4.474e-06\n\n\n\n\nCode\nold &lt;- house.selling.price %&gt;% \n  filter(New == 0)\nsummary(lm(Price ~ Size, data = old))\n\n\n\nCall:\nlm(formula = Price ~ Size, data = old)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-175748  -29155   -7297   14159  192519 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -22227.808  15708.186  -1.415    0.161    \nSize           104.438      9.538  10.950   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 52620 on 87 degrees of freedom\nMultiple R-squared:  0.5795,    Adjusted R-squared:  0.5747 \nF-statistic: 119.9 on 1 and 87 DF,  p-value: &lt; 2.2e-16\n\n\nSize significantly positively predicts price for both new and old houses, but by a greater magnitude for new houses. Adjusted R-squared for the model is also much higher (0.91 vs. 0.58).\nNew_Price = 166 * Size - 100755.31\nOld_Price = 104 * Size - 22227.808"
  },
  {
    "objectID": "posts/AdithyaParupudi_hw4.html#c-2",
    "href": "posts/AdithyaParupudi_hw4.html#c-2",
    "title": "Homework 4",
    "section": "C",
    "text": "C\n\n\nCode\nSize &lt;- 3000\nNew_Price = 166 * Size - 100755.31\nOld_Price = 104 * Size - 22227.808\nNew_Price\n\n\n[1] 397244.7\n\n\nCode\nOld_Price\n\n\n[1] 289772.2"
  },
  {
    "objectID": "posts/AdithyaParupudi_hw4.html#d-1",
    "href": "posts/AdithyaParupudi_hw4.html#d-1",
    "title": "Homework 4",
    "section": "D",
    "text": "D\n\n\nCode\nsummary(lm(Price ~ Size*New, data = house.selling.price))\n\n\n\nCall:\nlm(formula = Price ~ Size * New, data = house.selling.price)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-175748  -28979   -6260   14693  192519 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -22227.808  15521.110  -1.432  0.15536    \nSize           104.438      9.424  11.082  &lt; 2e-16 ***\nNew         -78527.502  51007.642  -1.540  0.12697    \nSize:New        61.916     21.686   2.855  0.00527 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 52000 on 96 degrees of freedom\nMultiple R-squared:  0.7443,    Adjusted R-squared:  0.7363 \nF-statistic: 93.15 on 3 and 96 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "posts/AdithyaParupudi_hw4.html#e-1",
    "href": "posts/AdithyaParupudi_hw4.html#e-1",
    "title": "Homework 4",
    "section": "E",
    "text": "E\nThe predicted selling price, based on the new regression that includes interaction between Size and Newness, would look like:\nNew_Price = -22227.81 + 104.44 * Size - 78527.50 * 1 + 61.92 * Size * 1\nOld_Price = -22227.81 + 104.44 * Size"
  },
  {
    "objectID": "posts/AdithyaParupudi_hw4.html#f-1",
    "href": "posts/AdithyaParupudi_hw4.html#f-1",
    "title": "Homework 4",
    "section": "F",
    "text": "F\n\n\nCode\nSize &lt;- 3000\nNew_Price = -22227.81 + 104.44 * Size - 78527.50 * 1 + 61.92 * Size * 1\nOld_Price = -22227.81 + 104.44 * Size\nNew_Price\n\n\n[1] 398324.7\n\n\nCode\nOld_Price\n\n\n[1] 291092.2"
  },
  {
    "objectID": "posts/AdithyaParupudi_hw4.html#g",
    "href": "posts/AdithyaParupudi_hw4.html#g",
    "title": "Homework 4",
    "section": "G",
    "text": "G\n\n\nCode\nSize &lt;- 1500\nNew_Price = -22227.81 + 104.44 * Size - 78527.50 * 1 + 61.92 * Size * 1\nOld_Price = -22227.81 + 104.44 * Size\nNew_Price\n\n\n[1] 148784.7\n\n\nCode\nOld_Price\n\n\n[1] 134432.2\n\n\nAs size of home goes up, the difference in predicted selling prices between old and new homes becomes larger."
  },
  {
    "objectID": "posts/AdithyaParupudi_hw4.html#h",
    "href": "posts/AdithyaParupudi_hw4.html#h",
    "title": "Homework 4",
    "section": "H",
    "text": "H\nThe prediction model with interaction has a significantly large negative coefficient for the New variable. The adjusted r-squared for the model with interaction is 0.7363 and the adjusted r-squared for the first model without interaction is 0.7169. The increase in the adjusted r-squared with the interaction model could be due to an additional variable or could indicate a slightly better fit for the prediction of the data. Since the models do have similar adjusted r-squared values, I would prefer the model with interaction because the regression indicates that the interaction term is statistically significant to selling price prediction, so I feel it is necessary to utilize an equation that factors for this."
  },
  {
    "objectID": "posts/Homework4_AlexaPotter.html",
    "href": "posts/Homework4_AlexaPotter.html",
    "title": "Homework 4",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(car)\nlibrary(smss)\ndata(\"house.selling.price\", package = \"smss\")\ndf &lt;- house.selling.price\nlibrary(alr4)\ndata(\"salary\", package = \"alr4\")\ndf &lt;- salary"
  },
  {
    "objectID": "posts/Homework4_AlexaPotter.html#a",
    "href": "posts/Homework4_AlexaPotter.html#a",
    "title": "Homework 4",
    "section": "A",
    "text": "A\nA particular home of 1240 square feet on a lot of 18,000 square feet sold for $145,000. Find the predicted selling price and the residual, and interpret.\n\n\nCode\na_predicted = (-10536 + (53.8*(1240)) + (2.84*(18000)))\na_predicted\n\n\n[1] 107296\n\n\nCode\na_actual = 145000\n\na_residual = a_actual - a_predicted\n\na_residual\n\n\n[1] 37704\n\n\nThe predicted selling price is $107,296. The residual is $37,704. This means the buyer bought the house $37,704 over recent data predicitons."
  },
  {
    "objectID": "posts/Homework4_AlexaPotter.html#b",
    "href": "posts/Homework4_AlexaPotter.html#b",
    "title": "Homework 4",
    "section": "B",
    "text": "B\nFor fixed lot size, how much is the house selling price predicted to increase for each square foot increase in home size? Why?\n\n\nCode\n(-10536 + (53.8*(1000)) + (2.84*(1000)))\n\n\n[1] 46104\n\n\nCode\n(-10536 + (53.8*(1001)) + (2.84*(1000)))\n\n\n[1] 46157.8\n\n\nThe house selling price is predicted to increase by $53.80. This is the coefficient to the home size variable, each x1 unit increases by 53.8."
  },
  {
    "objectID": "posts/Homework4_AlexaPotter.html#c",
    "href": "posts/Homework4_AlexaPotter.html#c",
    "title": "Homework 4",
    "section": "C",
    "text": "C\nAccording to this prediction equation, for fixed home size, how much would lot size need to increase to have the same impact as a one-square-foot increase in home size?\n\n\nCode\n53.8/2.84\n\n\n[1] 18.94366\n\n\nLot size would need to increase by 18.94 to have the same impact as a 1 square foot increase in home size."
  },
  {
    "objectID": "posts/Homework4_AlexaPotter.html#a-1",
    "href": "posts/Homework4_AlexaPotter.html#a-1",
    "title": "Homework 4",
    "section": "A",
    "text": "A\nTest the hypothesis that the mean salary for men and women is the same, without regard to any other variable but sex. Explain your findings.\n\n\nCode\nt.test(salary~sex, data=salary)\n\n\n\n    Welch Two Sample t-test\n\ndata:  salary by sex\nt = 1.7744, df = 21.591, p-value = 0.09009\nalternative hypothesis: true difference in means between group Male and group Female is not equal to 0\n95 percent confidence interval:\n -567.8539 7247.1471\nsample estimates:\n  mean in group Male mean in group Female \n            24696.79             21357.14 \n\n\nWe can use the independent t-test here because it is comparing the means of continuous and categorical variables.\nThe p value here is not significant which means we fail to reject the null hypothesis that all population means are equal."
  },
  {
    "objectID": "posts/Homework4_AlexaPotter.html#b-1",
    "href": "posts/Homework4_AlexaPotter.html#b-1",
    "title": "Homework 4",
    "section": "B",
    "text": "B\nRun a multiple linear regression with salary as the outcome variable and everything else as predictors, including sex. Assuming no interactions between sex and the other predictors, obtain a 95% confidence interval for the difference in salary between males and females.\nLinear regression:\n\n\nCode\nlm_salary &lt;- lm(salary ~ degree + rank + sex + year + ysdeg, data =  salary)\nsummary(lm_salary)\n\n\n\nCall:\nlm(formula = salary ~ degree + rank + sex + year + ysdeg, data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4045.2 -1094.7  -361.5   813.2  9193.1 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 15746.05     800.18  19.678  &lt; 2e-16 ***\ndegreePhD    1388.61    1018.75   1.363    0.180    \nrankAssoc    5292.36    1145.40   4.621 3.22e-05 ***\nrankProf    11118.76    1351.77   8.225 1.62e-10 ***\nsexFemale    1166.37     925.57   1.260    0.214    \nyear          476.31      94.91   5.018 8.65e-06 ***\nysdeg        -124.57      77.49  -1.608    0.115    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2398 on 45 degrees of freedom\nMultiple R-squared:  0.855, Adjusted R-squared:  0.8357 \nF-statistic: 44.24 on 6 and 45 DF,  p-value: &lt; 2.2e-16\n\n\n95% Confidence Interval for sexFemale is -697.8183 to 3030.56452\n\n\nCode\nconfint(lm_salary)\n\n\n                 2.5 %      97.5 %\n(Intercept) 14134.4059 17357.68946\ndegreePhD    -663.2482  3440.47485\nrankAssoc    2985.4107  7599.31080\nrankProf     8396.1546 13841.37340\nsexFemale    -697.8183  3030.56452\nyear          285.1433   667.47476\nysdeg        -280.6397    31.49105"
  },
  {
    "objectID": "posts/Homework4_AlexaPotter.html#c-1",
    "href": "posts/Homework4_AlexaPotter.html#c-1",
    "title": "Homework 4",
    "section": "C",
    "text": "C\nInterpret your finding for each predictor variable; discuss (a) statistical significance, (b) interpretation of the coefficient / slope in relation to the outcome variable and other variables\ndegree\nDegree is not statistically significant. Individuals with degree “PhD” make 1388.61 more than “Master’s”.\nrank\nRank is statistically significant. Individuals with rank “Prof” make 11118.76 more than “Asst” (constant) and individuals with rank “Assoc” make 5292.36 more than “Asst”\nsex\nSex is not statistically significant. sexFemale reflects women make 1166.37 more than men.\nyear\nYear is statistically significant. With each increase in year an individual makes 476.31 more.\nysdeg\nYears since degree is not statistically significant. It also have a negative relationship with salary. This means the more years away from their degree, an individual makes 124.57 less."
  },
  {
    "objectID": "posts/Homework4_AlexaPotter.html#d",
    "href": "posts/Homework4_AlexaPotter.html#d",
    "title": "Homework 4",
    "section": "D",
    "text": "D\nChange the baseline category for the rank variable. Interpret the coefficients related to rank again.\n\n\nCode\nsalary$rank &lt;- relevel(salary$rank, ref = \"Prof\")\nlm_salary2 &lt;- lm(salary ~ degree + rank + sex + year + ysdeg, data =  salary)\nsummary(lm_salary2)\n\n\n\nCall:\nlm(formula = salary ~ degree + rank + sex + year + ysdeg, data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4045.2 -1094.7  -361.5   813.2  9193.1 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  26864.81    1375.29  19.534  &lt; 2e-16 ***\ndegreePhD     1388.61    1018.75   1.363    0.180    \nrankAsst    -11118.76    1351.77  -8.225 1.62e-10 ***\nrankAssoc    -5826.40    1012.93  -5.752 7.28e-07 ***\nsexFemale     1166.37     925.57   1.260    0.214    \nyear           476.31      94.91   5.018 8.65e-06 ***\nysdeg         -124.57      77.49  -1.608    0.115    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2398 on 45 degrees of freedom\nMultiple R-squared:  0.855, Adjusted R-squared:  0.8357 \nF-statistic: 44.24 on 6 and 45 DF,  p-value: &lt; 2.2e-16\n\n\nChanging the baseline of rank to “Prof” does not change the statisitically significant variables. It does however, change the relationship with “Asst” and “Assoc” level of rank, as well as degree level of “Masters”. These variables now have a negative relationship."
  },
  {
    "objectID": "posts/Homework4_AlexaPotter.html#e",
    "href": "posts/Homework4_AlexaPotter.html#e",
    "title": "Homework 4",
    "section": "E",
    "text": "E\nFinkelstein (1980), in a discussion of the use of regression in discrimination cases, wrote, “a variable may reflect a position or status bestowed by the employer, in which case if there is discrimination in the award of the position or status, the variable may be ‘tainted.’” Thus, for example, if discrimination is at work in promotion of faculty to higher ranks, using rank to adjust salaries before comparing the sexes may not be acceptable to the courts.\nExclude the variable rank, refit, and summarize how your findings changed, if they did.\n\n\nCode\nlm_salary3 &lt;- lm(salary ~ degree + sex + year + ysdeg, data =  salary)\nsummary(lm_salary3)\n\n\n\nCall:\nlm(formula = salary ~ degree + sex + year + ysdeg, data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8146.9 -2186.9  -491.5  2279.1 11186.6 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 17183.57    1147.94  14.969  &lt; 2e-16 ***\ndegreePhD   -3299.35    1302.52  -2.533 0.014704 *  \nsexFemale   -1286.54    1313.09  -0.980 0.332209    \nyear          351.97     142.48   2.470 0.017185 *  \nysdeg         339.40      80.62   4.210 0.000114 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3744 on 47 degrees of freedom\nMultiple R-squared:  0.6312,    Adjusted R-squared:  0.5998 \nF-statistic: 20.11 on 4 and 47 DF,  p-value: 1.048e-09\n\n\nWithout the variable of rank, the statistically significant variables are now deegree, year, and ysdeg. Without rank females also make 1286.54 less than males so. Years since degree now has a postive relationship."
  },
  {
    "objectID": "posts/Homework4_AlexaPotter.html#a-2",
    "href": "posts/Homework4_AlexaPotter.html#a-2",
    "title": "Homework 4",
    "section": "A",
    "text": "A\nUsing the house.selling.price data, run and report regression results modeling y = selling price (in dollars) in terms of size of home (in square feet) and whether the home is new (1 = yes; 0 = no). In particular, for each variable; discuss statistical significance and interpret the meaning of the coefficient.\n\n\nCode\nhouse.selling.price\n\n\n    case Taxes Beds Baths New  Price Size\n1      1  3104    4     2   0 279900 2048\n2      2  1173    2     1   0 146500  912\n3      3  3076    4     2   0 237700 1654\n4      4  1608    3     2   0 200000 2068\n5      5  1454    3     3   0 159900 1477\n6      6  2997    3     2   1 499900 3153\n7      7  4054    3     2   0 265500 1355\n8      8  3002    3     2   1 289900 2075\n9      9  6627    5     4   0 587000 3990\n10    10   320    3     2   0  70000 1160\n11    11   630    3     2   0  64500 1220\n12    12  1780    3     2   0 167000 1690\n13    13  1630    3     2   0 114600 1380\n14    14  1530    3     2   0 103000 1590\n15    15   930    3     1   0 101000 1050\n16    16   590    2     1   0  70000  770\n17    17  1050    3     2   0  85000 1410\n18    18    20    3     1   0  22500 1060\n19    19   870    2     2   0  90000 1300\n20    20  1320    3     2   0 133000 1500\n21    21  1350    2     1   0  90500  820\n22    22  5616    4     3   1 577500 3949\n23    23   680    2     1   0 142500 1170\n24    24  1840    3     2   0 160000 1500\n25    25  3680    4     2   0 240000 2790\n26    26  1660    3     1   0  87000 1030\n27    27  1620    3     2   0 118600 1250\n28    28  3100    3     2   0 140000 1760\n29    29  2070    2     3   0 148000 1550\n30    30   830    3     2   0  69000 1120\n31    31  2260    4     2   0 176000 2000\n32    32  1760    3     1   0  86500 1350\n33    33  2750    3     2   1 180000 1840\n34    34  2020    4     2   0 179000 2510\n35    35  4900    3     3   1 338000 3110\n36    36  1180    4     2   0 130000 1760\n37    37  2150    3     2   0 163000 1710\n38    38  1600    2     1   0 125000 1110\n39    39  1970    3     2   0 100000 1360\n40    40  2060    3     1   0 100000 1250\n41    41  1980    3     1   0 100000 1250\n42    42  1510    3     2   0 146500 1480\n43    43  1710    3     2   0 144900 1520\n44    44  1590    3     2   0 183000 2020\n45    45  1230    3     2   0  69900 1010\n46    46  1510    2     2   0  60000 1640\n47    47  1450    2     2   0 127000  940\n48    48   970    3     2   0  86000 1580\n49    49   150    2     2   0  50000  860\n50    50  1470    3     2   0 137000 1420\n51    51  1850    3     2   0 121300 1270\n52    52   820    2     1   0  81000  980\n53    53  2050    4     2   0 188000 2300\n54    54   710    3     2   0  85000 1430\n55    55  1280    3     2   0 137000 1380\n56    56  1360    3     2   0 145000 1240\n57    57   830    3     2   0  69000 1120\n58    58   800    3     2   0 109300 1120\n59    59  1220    3     2   0 131500 1900\n60    60  3360    4     3   0 200000 2430\n61    61   210    3     2   0  81900 1080\n62    62   380    2     1   0  91200 1350\n63    63  1920    4     3   0 124500 1720\n64    64  4350    3     3   0 225000 4050\n65    65  1510    3     2   0 136500 1500\n66    66  4154    3     3   0 381000 2581\n67    67  1976    3     2   1 250000 2120\n68    68  3605    3     3   1 354900 2745\n69    69  1400    3     2   0 140000 1520\n70    70   790    2     2   0  89900 1280\n71    71  1210    3     2   0 137000 1620\n72    72  1550    3     2   0 103000 1520\n73    73  2800    3     2   0 183000 2030\n74    74  2560    3     2   0 140000 1390\n75    75  1390    4     2   0 160000 1880\n76    76  5443    3     2   0 434000 2891\n77    77  2850    2     1   0 130000 1340\n78    78  2230    2     2   0 123000  940\n79    79    20    2     1   0  21000  580\n80    80  1510    4     2   0  85000 1410\n81    81   710    3     2   0  69900 1150\n82    82  1540    3     2   0 125000 1380\n83    83  1780    3     2   1 162600 1470\n84    84  2920    2     2   1 156900 1590\n85    85  1710    3     2   1 105900 1200\n86    86  1880    3     2   0 167500 1920\n87    87  1680    3     2   0 151800 2150\n88    88  3690    5     3   0 118300 2200\n89    89   900    2     2   0  94300  860\n90    90   560    3     1   0  93900 1230\n91    91  2040    4     2   0 165000 1140\n92    92  4390    4     3   1 285000 2650\n93    93   690    3     1   0  45000 1060\n94    94  2100    3     2   0 124900 1770\n95    95  2880    4     2   0 147000 1860\n96    96   990    2     2   0 176000 1060\n97    97  3030    3     2   0 196500 1730\n98    98  1580    3     2   0 132200 1370\n99    99  1770    3     2   0  88400 1560\n100  100  1430    3     2   0 127200 1340\n\n\nCode\nhouse_lm &lt;- lm(Price ~ Size + New, data = house.selling.price)\nsummary(house_lm)\n\n\n\nCall:\nlm(formula = Price ~ Size + New, data = house.selling.price)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-205102  -34374   -5778   18929  163866 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -40230.867  14696.140  -2.738  0.00737 ** \nSize           116.132      8.795  13.204  &lt; 2e-16 ***\nNew          57736.283  18653.041   3.095  0.00257 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 53880 on 97 degrees of freedom\nMultiple R-squared:  0.7226,    Adjusted R-squared:  0.7169 \nF-statistic: 126.3 on 2 and 97 DF,  p-value: &lt; 2.2e-16\n\n\nThis model shows both size and new variables are statistically significant to price. A new house has a price of57736.283 more than a not new home and for each unit increase of size, price increases 116.132."
  },
  {
    "objectID": "posts/Homework4_AlexaPotter.html#b-2",
    "href": "posts/Homework4_AlexaPotter.html#b-2",
    "title": "Homework 4",
    "section": "B",
    "text": "B\nReport and interpret the prediction equation, and form separate equations relating selling price to size for new and for not new homes.\ny = -40230.867+116.132X1(size) + 57736.283X2(new)\nIf the house is not new, X2 is cancelled out by 0"
  },
  {
    "objectID": "posts/Homework4_AlexaPotter.html#c-2",
    "href": "posts/Homework4_AlexaPotter.html#c-2",
    "title": "Homework 4",
    "section": "C",
    "text": "C\nFind the predicted selling price for a home of 3000 square feet that is (i) new, (ii) not new.\ny = -40230.867+116.132X1(size) + 57736.283X2(new)\n\n\nCode\n#i\n\n-40230.867+(116.132*3000) + (57736.283*1)  \n\n\n[1] 365901.4\n\n\nCode\n#ii  \n-40230.867+(116.132*3000) + (57736.283*0)\n\n\n[1] 308165.1"
  },
  {
    "objectID": "posts/Homework4_AlexaPotter.html#d-1",
    "href": "posts/Homework4_AlexaPotter.html#d-1",
    "title": "Homework 4",
    "section": "D",
    "text": "D\nFit another model, this time with an interaction term allowing interaction between size and new, and report the regression results\n\n\nCode\nhouse_lm2 &lt;- lm(Price ~ Size*New, data = house.selling.price)\nsummary(house_lm2)\n\n\n\nCall:\nlm(formula = Price ~ Size * New, data = house.selling.price)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-175748  -28979   -6260   14693  192519 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -22227.808  15521.110  -1.432  0.15536    \nSize           104.438      9.424  11.082  &lt; 2e-16 ***\nNew         -78527.502  51007.642  -1.540  0.12697    \nSize:New        61.916     21.686   2.855  0.00527 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 52000 on 96 degrees of freedom\nMultiple R-squared:  0.7443,    Adjusted R-squared:  0.7363 \nF-statistic: 93.15 on 3 and 96 DF,  p-value: &lt; 2.2e-16\n\n\nThese results now show “new” is not statistically significant but “size” and “size:new” is."
  },
  {
    "objectID": "posts/Homework4_AlexaPotter.html#e-1",
    "href": "posts/Homework4_AlexaPotter.html#e-1",
    "title": "Homework 4",
    "section": "E",
    "text": "E\nReport the lines relating the predicted selling price to the size for homes that are (i) new, (ii) not new.\n\ny = -22227.808 + 104.438X1(size) - 78527.502X2(new) + 61.916X1(size)X2(new)\ny = -22227.808 + 104.438*X1(size)"
  },
  {
    "objectID": "posts/Homework4_AlexaPotter.html#f",
    "href": "posts/Homework4_AlexaPotter.html#f",
    "title": "Homework 4",
    "section": "F",
    "text": "F\nFind the predicted selling price for a home of 3000 square feet that is (i) new, (ii) not new.\n\n\nCode\n#i\n\n-22227.808 + (104.438*3000) - 78527.502 + (61.916*3000)  \n\n\n[1] 398306.7\n\n\nCode\n#ii\n-22227.808 + (104.438*3000)\n\n\n[1] 291086.2"
  },
  {
    "objectID": "posts/Homework4_AlexaPotter.html#g",
    "href": "posts/Homework4_AlexaPotter.html#g",
    "title": "Homework 4",
    "section": "G",
    "text": "G\nFind the predicted selling price for a home of 1500 square feet that is (i) new, (ii) not new. Comparing to (F), explain how the difference in predicted selling prices changes as the size of home increases.\n\n\nCode\n#i\n\n-22227.808 + (104.438*1500) - 78527.502 + (61.916*1500)  \n\n\n[1] 148775.7\n\n\nCode\n#ii\n-22227.808 + (104.438*1500)\n\n\n[1] 134429.2\n\n\nWith the interaction of size*new, predicted selling prices changes by an increase of over 200% as the size of home increases in these instances. As the size increases, the percentage also increases in predicted selling price."
  },
  {
    "objectID": "posts/Homework4_AlexaPotter.html#h",
    "href": "posts/Homework4_AlexaPotter.html#h",
    "title": "Homework 4",
    "section": "H",
    "text": "H\nDo you think the model with interaction or the one without it represents the relationship of size and new to the outcome price? What makes you prefer one model over another?\nR squared in the interaction model is higher as well as the standard error number is lower. BAsed on these two factors I would determine the interaction model is preferred."
  },
  {
    "objectID": "posts/FinalProject_Part2_AlexaPotter.html",
    "href": "posts/FinalProject_Part2_AlexaPotter.html",
    "title": "Final Project Part 2",
    "section": "",
    "text": "Greenery is widely recognized as a vital element to any public space. Plants and natural elements of all kinds can add immense benefits to urban areas, not only to the environment but to the humans who inhabit the space as well. Studies have shown that urban greening, specifically the planting of trees, can “combat challenges such as pollution, urban heat, and flooding, as well as to improve social cohesion, human health, and well-being.”1 The understanding and implementation of this information can lead cities, developers, and anyone with a stake in their community to consciously consider what elements they can incorporate into their own public spaces. The work does not end here though, greenery in public spaces requires maintenance and year-round management to make these efforts last long enough to see the benefits.\nNew York City, with a population of about 8.4 billion people and 300 square miles, is one of the largest urban spaces in the United States.2 In 1995, the New York City Department of Parks and Recreation conducted a city wide census of all the trees. They again conducted this survey in 2005 and 2015 to tackle their goal of enhancing and restoring urban forests.3 The information this survey collected has been used to create an interactive map of tree species around New York City. The Parks department then uses this data to calculate the related impacts and needs associated with the trees and tree maintenance.4\nWhile there is a large amount of data related to the tree census published, there is a gap in information on the relation between tree data to specific neighborhoods. Critically analyzing this tree data on a neighborhood level can lead to further community involvement within and stemming from neighborhoods themselves.5 Firsthand involvement can be used to develop connections between community members and foster ownership among members with the environment they inhabit.\nThis informs my research question:"
  },
  {
    "objectID": "posts/FinalProject_Part2_AlexaPotter.html#hypothesis-testing",
    "href": "posts/FinalProject_Part2_AlexaPotter.html#hypothesis-testing",
    "title": "Final Project Part 2",
    "section": "Hypothesis Testing",
    "text": "Hypothesis Testing\nFirst I’ll run an ANOVA test to review the population mean differences in key variables.\nH0 = There is no significant difference in means\nHa = The means are different in at least one of the groups\nThe first ANOVA is testing the independent variable borocode with dependent variable tree diameter.\n\n\nCode\nanova_treedbh &lt;- aov(tree_dbh ~ factor(borocode), data = treecensus_clean)\nsummary(anova_treedbh)\n\n\n                    Df  Sum Sq Mean Sq F value Pr(&gt;F)    \nfactor(borocode)     4   24916    6229   83.81 &lt;2e-16 ***\nResiduals        15437 1147306      74                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCode\nggplot(treecensus_clean, aes(x = factor(borocode), y = tree_dbh)) +\n    geom_boxplot(aes(color=factor(borocode)))+\n    ylab(\"Diameter\")+ \n    xlab(\"Borough\") + \n    ylim(0,50)+\n  scale_color_discrete(name = \"Borocode\")\n\n\nWarning: Removed 5 rows containing non-finite values (`stat_boxplot()`).\n\n\n\n\n\nThis ANOVA between tree_dbh ~ borocode shows a very small p-value, indicating the null hypothesis can be rejected at 0.001.\nNext I’ll test the ANOVA with the dependent variable health and independent variable borocde.\n\n\nCode\ntreecensus_clean$health &lt;- as.numeric(treecensus_clean$health)\nanova_treehealth &lt;- aov(health ~ factor(borocode), data = treecensus_clean)\nsummary(anova_treehealth)\n\n\n                    Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nfactor(borocode)     4     26   6.538    19.5 5.09e-16 ***\nResiduals        15437   5176   0.335                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCode\nggplot(treecensus_clean, aes(x = factor(borocode), y = health)) +\n    geom_boxplot(aes(color=factor(borocode)))+\n    ylab(\"Health\")+ \n    xlab(\"Borough\") + \n  scale_color_discrete(name = \"Borocode\")\n\n\n\n\n\nThis ANOVA between health ~ borocode shows a very small p-value, indicating the null hypothesis can be rejected at 0.001. From the graph, however, it seems there are two distinct patterns and not much variation."
  },
  {
    "objectID": "posts/FinalProject_Part2_AlexaPotter.html#model-comparisons",
    "href": "posts/FinalProject_Part2_AlexaPotter.html#model-comparisons",
    "title": "Final Project Part 2",
    "section": "Model Comparisons",
    "text": "Model Comparisons\nNext we’ll test different linear regression models.\n\n\nCode\nslrtree &lt;- lm(tree_dbh ~ borocode, data = treecensus_clean)\nsummary(slrtree)\n\nslrtree2 &lt;- lm(tree_dbh ~ borocode + steward, data = treecensus_clean)\nsummary(slrtree2)\n\nslrtree3 &lt;- lm(tree_dbh ~ borocode + steward + curb_loc, data = treecensus_clean)\nsummary(slrtree3)\n\nslrtreehealth &lt;- lm(health ~ borocode, data = treecensus_clean)\nsummary(slrtreehealth)\n\nslrtreehealth2 &lt;- lm(health ~ borocode + steward, data = treecensus_clean)\nsummary(slrtreehealth2)\n\nslrtreehealth3 &lt;- lm(health ~ borocode + steward + curb_loc, data = treecensus_clean)\nsummary(slrtreehealth3)\n\n\n\n\nCode\nstargazer(slrtree, slrtree2, slrtree3, type= 'text')\n\n\n\n====================================================================================================\n                                                  Dependent variable:                               \n                    --------------------------------------------------------------------------------\n                                                        tree_dbh                                    \n                               (1)                        (2)                        (3)            \n----------------------------------------------------------------------------------------------------\nborocode                     0.770***                   0.540***                   0.578***         \n                             (0.055)                    (0.056)                    (0.056)          \n                                                                                                    \nsteward                                                -1.776***                  -1.748***         \n                                                        (0.093)                    (0.093)          \n                                                                                                    \ncurb_loc                                                                          -1.885***         \n                                                                                   (0.314)          \n                                                                                                    \nConstant                     8.945***                  10.462***                  12.118***         \n                             (0.188)                    (0.202)                    (0.342)          \n                                                                                                    \n----------------------------------------------------------------------------------------------------\nObservations                  15,442                     15,442                     15,442          \nR2                            0.012                      0.035                      0.038           \nAdjusted R2                   0.012                      0.035                      0.037           \nResidual Std. Error     8.659 (df = 15440)         8.558 (df = 15439)         8.549 (df = 15438)    \nF Statistic         193.649*** (df = 1; 15440) 282.468*** (df = 2; 15439) 200.756*** (df = 3; 15438)\n====================================================================================================\nNote:                                                                    *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01\n\n\nWe’ll first exam the models with DV as tree diameter.\nIn the first model, Adjusted R2 only represents 1.2% of the variation in response variable tree diameter can be explained by the predictor variable borocode. Adjusted R2 does increase with each model to model 4’s 3.8%, however, this is still very low.\n\n\nCode\nstargazer(slrtree, slrtree2, slrtree3, type= 'text')\n\n\n\n====================================================================================================\n                                                  Dependent variable:                               \n                    --------------------------------------------------------------------------------\n                                                        tree_dbh                                    \n                               (1)                        (2)                        (3)            \n----------------------------------------------------------------------------------------------------\nborocode                     0.770***                   0.540***                   0.578***         \n                             (0.055)                    (0.056)                    (0.056)          \n                                                                                                    \nsteward                                                -1.776***                  -1.748***         \n                                                        (0.093)                    (0.093)          \n                                                                                                    \ncurb_loc                                                                          -1.885***         \n                                                                                   (0.314)          \n                                                                                                    \nConstant                     8.945***                  10.462***                  12.118***         \n                             (0.188)                    (0.202)                    (0.342)          \n                                                                                                    \n----------------------------------------------------------------------------------------------------\nObservations                  15,442                     15,442                     15,442          \nR2                            0.012                      0.035                      0.038           \nAdjusted R2                   0.012                      0.035                      0.037           \nResidual Std. Error     8.659 (df = 15440)         8.558 (df = 15439)         8.549 (df = 15438)    \nF Statistic         193.649*** (df = 1; 15440) 282.468*** (df = 2; 15439) 200.756*** (df = 3; 15438)\n====================================================================================================\nNote:                                                                    *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01\n\n\nCode\nstargazer(slrtreehealth, slrtreehealth2, slrtreehealth3, type= 'text')\n\n\n\n=================================================================================================\n                                                 Dependent variable:                             \n                    -----------------------------------------------------------------------------\n                                                       health                                    \n                               (1)                       (2)                       (3)           \n-------------------------------------------------------------------------------------------------\nborocode                    -0.018***                 -0.015***                 -0.015***        \n                             (0.004)                   (0.004)                   (0.004)         \n                                                                                                 \nsteward                                               0.022***                  0.021***         \n                                                       (0.006)                   (0.006)         \n                                                                                                 \ncurb_loc                                                                          0.011          \n                                                                                 (0.021)         \n                                                                                                 \nConstant                    2.737***                  2.718***                  2.708***         \n                             (0.013)                   (0.014)                   (0.023)         \n                                                                                                 \n-------------------------------------------------------------------------------------------------\nObservations                 15,442                    15,442                    15,442          \nR2                            0.001                     0.002                     0.002          \nAdjusted R2                   0.001                     0.002                     0.002          \nResidual Std. Error    0.580 (df = 15440)        0.580 (df = 15439)        0.580 (df = 15438)    \nF Statistic         22.936*** (df = 1; 15440) 17.409*** (df = 2; 15439) 11.698*** (df = 3; 15438)\n=================================================================================================\nNote:                                                                 *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01\n\n\nNext we’ll exam the models with DV as health.\nIn the first model, Adjusted R2 only represents 0.1% of the variation in response variable health can be explained by the predictor variable borocode. Adjusted R2 does increase with to 0.2% in Model 2 & 3, however, this is still very low. In Model 3, curb_loc is not statistically significant."
  },
  {
    "objectID": "posts/FinalProject_Part2_AlexaPotter.html#diagnostics",
    "href": "posts/FinalProject_Part2_AlexaPotter.html#diagnostics",
    "title": "Final Project Part 2",
    "section": "Diagnostics",
    "text": "Diagnostics\n\nTree Diameter\nWe’ll first run diagnostics on the linear regression for lm(tree_dbh ~ borocode + steward + curb_loc, data = treecensus_clean)\n\n\nCode\nplot(slrtree3, which = 1)\n\n\n\n\n\nIn this Residual vs. Fitted values plot, it does seem like it violates the constant variance assumption but does appear linear.\n\n\nCode\nplot(slrtree3, which = 2)\n\n\n\n\n\nThis next plot is a QQ-Plot. These points do generally seem to normally fall along the line.\n\n\nCode\nplot(slrtree3, which = 3)\n\n\n\n\n\nThe scale location plot also seems to violate constant variance as there are more plots between 10-14 range.\n\n\nCode\nplot(slrtree3, which = 4)\n\n\n\n\n\nThe Cook’s Distance plot also shows a violation of the influential observation assumption. Several observations are greater than 4/15,442=0.00026\n\n\nTree Health\nNext we’ll run dianogstics for the second dependent variable health, lm(health ~ borocode + steward, data = treecensus_clean)\n\n\nCode\nplot(slrtreehealth2, which = 1)\n\n\n\n\n\nThis Residuals vs. fitted plot appears to violate the assumption of constant variance.\n\n\nCode\nplot(slrtreehealth2, which = 2)\n\n\n\n\n\nThis QQ-plot shows the violation of normality as the points are not falling along the line.\n\n\nCode\nplot(slrtreehealth2, which = 3)\n\n\n\n\n\nThe Scale-Location plot also shows a violation of the constant variance assumption.\n\n\nCode\nplot(slrtreehealth2, which = 4)\n\n\n\n\n\nThe Cook’s Distance plot also shows a violation of the influential observation assumption. Many observations are greater than 4/15,442=0.00026"
  },
  {
    "objectID": "posts/FinalProject_Part2_AlexaPotter.html#footnotes",
    "href": "posts/FinalProject_Part2_AlexaPotter.html#footnotes",
    "title": "Final Project Part 2",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nCavender, N., & Donnelly, G. (2019). Intersecting Urban Forestry and botanical gardens to address big challenges for healthier trees, people, and cities. PLANTS, PEOPLE, PLANET, 1(4), 315–322. https://doi.org/10.1002/ppp3.38↩︎\nU.S. Census Bureau . (n.d.). U.S. Census Bureau quickfacts: New York City, New York. Retrieved March 20, 2023, from https://www.census.gov/quickfacts/newyorkcitynewyork↩︎\nMerriman, D. (2017) Volunteers count every street tree in New York City. US Forest Service. Retrieved March 20, 2023, from https://www.fs.usda.gov/features/volunteers-count-every-street-tree-new-york-city-0↩︎\nCochran, C., & Greer, B. (2016, June 29). Treescount! 2015: NYC’s Third Street-Tree Census. New York State Urban Forestry Council. Retrieved March 20, 2023, from https://nysufc.org/treescount/2016/04/26/↩︎\nMa, Q., Lin, J., Ju, Y. et al. Individual structure mapping over six million trees for New York City USA. Sci Data 10, 102 (2023). https://doi.org/10.1038/s41597-023-02000-w↩︎\nNeckerman, K., Lovasi, G., Davies, S. et al. Disparities in Urban Neighborhood Conditions: Evidence from GIS Measures and Field Observation in New York City. Public Health Pol 30 (Suppl 1), S264–S285 (2009). https://doi.org/10.1057/jphp.2008.47↩︎\nColleen E. Reid, Laura D. Kubzansky, Jiayue Li, Jessie L. Shmool, Jane E. Clougherty. It’s not easy assessing greenness: A comparison of NDVI datasets and neighborhood types and their associations with self-rated health in New York City. Health & Place 54, 92-101 (2018).https://doi.org/10.1016/j.healthplace.2018.09.005.↩︎\nJian Lin, Qiang Wang, Xiaojiang Li. Socioeconomic and spatial inequalities of street tree abundance, species diversity, and size structure in New York City. Landscape and Urban Planning, Volume 206. 2021. 103992. https://doi.org/10.1016/j.landurbplan.2020.103992.↩︎\nhttps://data.cityofnewyork.us/Environment/2015-Street-Tree-Census-Tree-Data/uvpi-gqnh↩︎"
  },
  {
    "objectID": "posts/abigailbalint_hw1.html",
    "href": "posts/abigailbalint_hw1.html",
    "title": "Homework 1",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(readxl)\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/abigailbalint_hw1.html#question-1---lung-capacity",
    "href": "posts/abigailbalint_hw1.html#question-1---lung-capacity",
    "title": "Homework 1",
    "section": "Question 1 - Lung Capacity",
    "text": "Question 1 - Lung Capacity\nReading in LungCapData –\n\n\nCode\nlung &lt;- read_excel(\"_data/LungCapData.xls\")\nhead(lung,2)\n\n\n# A tibble: 2 × 6\n  LungCap   Age Height Smoke Gender Caesarean\n    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;    \n1    6.48     6   62.1 no    male   no       \n2   10.1     18   74.7 yes   female no       \n\n\nLooking at some basic descriptive stats –\n\n\nCode\nglimpse(lung)\n\n\nRows: 725\nColumns: 6\n$ LungCap   &lt;dbl&gt; 6.475, 10.125, 9.550, 11.125, 4.800, 6.225, 4.950, 7.325, 8.…\n$ Age       &lt;dbl&gt; 6, 18, 16, 14, 5, 11, 8, 11, 15, 11, 19, 17, 12, 10, 10, 13,…\n$ Height    &lt;dbl&gt; 62.1, 74.7, 69.7, 71.0, 56.9, 58.7, 63.3, 70.4, 70.5, 59.2, …\n$ Smoke     &lt;chr&gt; \"no\", \"yes\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\",…\n$ Gender    &lt;chr&gt; \"male\", \"female\", \"female\", \"male\", \"male\", \"female\", \"male\"…\n$ Caesarean &lt;chr&gt; \"no\", \"no\", \"yes\", \"no\", \"no\", \"no\", \"yes\", \"no\", \"no\", \"no\"…\n\n\nCode\nmean(lung$LungCap, na.rm = T)\n\n\n[1] 7.863148\n\n\nCode\nvar(lung$LungCap, na.rm = T)\n\n\n[1] 7.086288\n\n\nCode\nsd(lung$LungCap, na.rm = T)\n\n\n[1] 2.662008\n\n\nCode\nrange(lung$LungCap, na.rm = T)\n\n\n[1]  0.507 14.675\n\n\n\nWhat does the distribution of LungCap look like?\n\n\n\nCode\nggplot(lung, aes(x = LungCap)) +\n  geom_histogram()\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nThe distribution looks relatively normal. There is a clear concentration of the sample around 7-8 and the outliers are only a very small portion of the sample.\n\nCompare the probability distribution of the LungCap with respect to Males and Females\n\n\n\nCode\nggplot(lung, aes(x=LungCap)) + \n    geom_boxplot(fill=\"slateblue\", alpha=0.2) + \n    xlab(\"Lung Capacity\") +\n  facet_wrap(\"Gender\")\n\n\n\n\n\nThe probability distribution is pretty similar between male and female, but males skew to a higher lung capacity overall and the median line is at around 8 whereas female is closer to 7.5.\n\nCompare the mean lung capacities for smokers and non-smokers. Does it make sense?\n\n\n\nCode\nlung %&gt;%\n  group_by(Smoke) %&gt;%\n  summarise(mean = mean(LungCap), n = n())\n\n\n# A tibble: 2 × 3\n  Smoke  mean     n\n  &lt;chr&gt; &lt;dbl&gt; &lt;int&gt;\n1 no     7.77   648\n2 yes    8.65    77\n\n\nWe would expect the lung capacities for non smokers to be higher but the mean for smokers is actually a little bit higher.\n\nExamine the relationship between Smoking and Lung Capacity within age groups: “less than or equal to 13”, “14 to 15”, “16 to 17”, and “greater than or equal to 18”.\n\nRecoding the age groups –\n\n\nCode\nlunggroup &lt;- lung %&gt;%\n  mutate(`AgeGroup` = dplyr::case_when(\n    `Age` &gt;= 0 & `Age` &lt; 14 ~ \"0-13\",\n    `Age` &gt;= 14 & `Age` &lt; 16 ~ \"14-15\",\n    `Age` &gt;= 16 & `Age` &lt; 18 ~ \"16-17\",\n    `Age` &gt;= 18 ~ \"18+\" ))\nhead(lunggroup)\n\n\n# A tibble: 6 × 7\n  LungCap   Age Height Smoke Gender Caesarean AgeGroup\n    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;     &lt;chr&gt;   \n1    6.48     6   62.1 no    male   no        0-13    \n2   10.1     18   74.7 yes   female no        18+     \n3    9.55    16   69.7 no    female yes       16-17   \n4   11.1     14   71   no    male   no        14-15   \n5    4.8      5   56.9 no    male   no        0-13    \n6    6.22    11   58.7 no    female no        0-13    \n\n\nMean lung capacity by age group –\n\n\nCode\nlunggroup %&gt;%\ngroup_by(Smoke, AgeGroup) %&gt;%\n  summarise(mean = mean(LungCap), n = n())\n\n\n`summarise()` has grouped output by 'Smoke'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 8 × 4\n# Groups:   Smoke [2]\n  Smoke AgeGroup  mean     n\n  &lt;chr&gt; &lt;chr&gt;    &lt;dbl&gt; &lt;int&gt;\n1 no    0-13      6.36   401\n2 no    14-15     9.14   105\n3 no    16-17    10.5     77\n4 no    18+      11.1     65\n5 yes   0-13      7.20    27\n6 yes   14-15     8.39    15\n7 yes   16-17     9.38    20\n8 yes   18+      10.5     15\n\n\nFor both smokers and non-smokers, the lung capacity goes up as the age increases with 18+ having the highest average capacity. In all age ranges besides 0-13 (the broadest range), the mean is higher for non-smokers than smokers.\n\nCompare the lung capacities for smokers and non-smokers within each age group. Is your answer different from the one in part c. What could possibly be going on here?\n\n\n\nCode\nggplot(lunggroup, aes(x=LungCap, fill=Smoke)) + \n    geom_boxplot() + \n    xlab(\"Lung Capacity\") +\n  facet_wrap(\"AgeGroup\")\n\n\n\n\n\nI’m seeing that the results by age group are slightly different than in part C. Above I can see that the average for all age ranges is higher for non-smokers, besides age group 0-13. I can see in my results in part D that the sample size for 0-13 non-smokers is extremely high, much higher than any other group of smokers or non-smokers, so with this higher sample size comes more variance. The median lines are actually pretty close but the outliers are probably affecting the mean."
  },
  {
    "objectID": "posts/abigailbalint_hw1.html#question-2",
    "href": "posts/abigailbalint_hw1.html#question-2",
    "title": "Homework 1",
    "section": "Question 2",
    "text": "Question 2\nCreating a data frame –\n\n\nCode\npriorconviction &lt;- c(0,1,2,3,4)\nprisoners &lt;- c(128,434,160,64,24)\nq2 &lt;- data.frame(priorconviction, prisoners)\nhead(q2)\n\n\n  priorconviction prisoners\n1               0       128\n2               1       434\n3               2       160\n4               3        64\n5               4        24\n\n\n\nWhat is the probability that a randomly selected inmate has exactly 2 prior convictions?\n\n\n\nCode\n160/810\n\n\n[1] 0.1975309\n\n\nI found it to be .1975 or 19.75%\n\nWhat is the probability that a randomly selected inmate has fewer than 2 prior convictions?\n\n\n\nCode\n(434+128)/810\n\n\n[1] 0.6938272\n\n\nTo get this I added the sample of 0 or 1 prior conviction and it comes out to .69 or 69%.\n\nWhat is the probability that a randomly selected inmate has 2 or fewer prior convictions?\n\n\n\nCode\n(128+434+160)/810\n\n\n[1] 0.891358\n\n\nTo get this I added the sample of 0 or 1 or 2 prior convictions and it comes out to .89 or 89%.\n\nWhat is the probability that a randomly selected inmate has more than 2 prior convictions?\n\n\n\nCode\n(64+24)/810\n\n\n[1] 0.108642\n\n\nTo get this I added the sample of 3 or 4 prior convictions and it comes out to .108 or 11%.\n\nWhat is the expected value1 for the number of prior convictions?\n\n\n\nCode\nsum(q2$priorconviction*prisoners)\n\n\n[1] 1042\n\n\nCode\n1042/810\n\n\n[1] 1.28642\n\n\nTo get this I summed all of the numbers of prior convictions by the amount of prisoners (1042) then divided this by total sample (810) to get a final expected value of 1.28 prior convictions.\n\nCalculate the variance and the standard deviation for the Prior Convictions.\n\n\n\nCode\nvar(q2$priorconviction)\n\n\n[1] 2.5\n\n\nCode\nvar(q2$priorconviction)*(5-1)/5\n\n\n[1] 2\n\n\nI used the above code to find a sample variance of 2.5 and a population variance of 2.\n\n\nCode\nsd(q2$priorconviction)\n\n\n[1] 1.581139\n\n\nI used the standard deviation function to calculate the above.\n:::"
  },
  {
    "objectID": "posts/HW3_AkhileshKumarMeghwal.html",
    "href": "posts/HW3_AkhileshKumarMeghwal.html",
    "title": "Homework 3",
    "section": "",
    "text": "United Nations (Data file: UN 1 lin alr4) The data in the file UN 11 contains several variables, including ppgdp, the gross national product per person in U.S. dollars, and fertility, the birth rate per 1000 females, both from the year 2009. The data are for 199 localities, mostly UN member countries, but also other areas such as Hong Kong that are not independent countries.The data were collected from the United Nations (2011). We will study the dependence of fertility on ppgdp.\n\n\nIdentify the predictor and the response?\n\n\nCode\ndata(UN11)\nhead(UN11)\n\n\n\n\n  \n\n\n\nThe predictor variable is ppgdp and the response variable is fertility. ‘ppgdp’ represents the gross national product per person, while ‘fertility’ refers to the birth rate per 1000 females. The objective of the study is to understand how changes in ‘ppgdp’ can influence the value of ‘fertility’. Positive or negative correlations between the two variables can lead to conclusions about how one variable impacts the other. The analysis of this relationship can provide insights into socio-economic factors that influence birth rates, which can then inform policies on population growth, family planning, and economic development.\n\n\n\nDraw the scatterplot of fertility on the vertical axis versus ppgdp on the horizontal axis and summarize the information in this graph. Does a straight-line mean function seem to be plausible for a summary of this graph?\n\n\nCode\nUN11 %&gt;%\n  select(c(ppgdp, fertility)) %&gt;%\n  ggplot(aes(x = ppgdp, y = fertility)) + \n  geom_point(color = \"#001F3F\", alpha = 0.8, size = 3) +\n  geom_smooth(method = lm, se = FALSE, color = \"#E41A1C\", size = 1) +\n  theme_minimal() +\n  labs(x = \"Gross National Product per Person (US$)\",\n       y = \"Birth Rate per 1000 Females\",\n       title = element_text(color = \"#006600\", \"Scatterplot of Fertility vs. PPGDP with Regression Line\")) +\n  theme(plot.title = element_text(size = 12, face = \"bold\", hjust = 0.5, color = \"#006600\"),\n        axis.title = element_text(size = 10, color = \"#001F3F\"),\n        axis.text = element_text(size = 8, color = \"#001F3F\"),\n        panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank(),\n        panel.border = element_blank(),\n        axis.line = element_line(color = \"#333333\", size = 0.5),\n        plot.background = element_blank(),\n        panel.background = element_blank(),\n        legend.position = \"none\")\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\nWarning: The `size` argument of `element_line()` is deprecated as of ggplot2 3.4.0.\nℹ Please use the `linewidth` argument instead.\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nThe scatterplot shows the relationship between two variables, fertility (number of live births per 1000 females) and ppgdp (gross national product per person in US dollars). Each point on the plot represents a country, with its fertility rate on the vertical axis and its ppgdp on the horizontal axis.\nFrom the scatterplot, we can see that there is a general negative relationship between fertility and ppgdp. This means that as a country’s income increases, its fertility rate tends to decrease. However, the relationship is not perfectly linear, and there is a lot of variation in fertility rates at each level of ppgdp.\nTo summarize the trend in the data, A straight-line mean function does seem to be plausible for summarizing the data, as the plotted regression line suggests a linear negative trend in the data. However, it is important to note that the relationship between ppgdp and fertility may not be entirely linear, and other factors may also contribute to the observed patterns in the data. Therefore, it may be appropriate to explore more complex statistical models to better understand the relationship between these variables.\nOverall, this scatterplot provides a visual representation of the relationship between fertility and ppgdp. It allows us to quickly identify trends and patterns in the data and can be used to inform further analysis and modeling.\n\n\n\nDraw the scatterplot of log(fertility) versus log(ppgdp) using natural logarithms. Does the simple linear regression model seem plausible for a summary of this graph? If you use a different base of logarithms, the shape of the graph won’t change, but the values on the axes will change.\n\n\nCode\nUN11 %&gt;%\n  select(c(ppgdp, fertility)) %&gt;%\n  ggplot(aes(x = log(ppgdp), y = log(fertility))) + \n  geom_point(color = \"#001F3F\", alpha = 0.8, size = 3) +\n  geom_smooth(method = lm, se = TRUE, color = \"#E41A1C\", size = 1) +\n  theme_minimal() +\n  labs(x = \"Log(Gross National Product per Person (US$))\",\n       y = \"Log(Birth Rate per 1000 Females)\",\n       title = element_text(color = \"#006600\", \"Scatterplot of Log(Fertility) vs. Log(PPGDP) with Regression Line\")) +\n  theme(plot.title = element_text(size = 12, face = \"bold\", hjust = 0.5, color = \"#006600\"),\n        axis.title = element_text(size = 10, color = \"#001F3F\"),\n        axis.text = element_text(size = 8, color = \"#001F3F\"),\n        panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank(),\n        panel.border = element_blank(),\n        axis.line = element_line(color = \"#333333\", size = 0.5),\n        plot.background = element_blank(),\n        panel.background = element_blank(),\n        legend.position = \"none\")\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nThe scatterplot of Log(Fertility) versus Log(PPGDP) using data from the UN11 dataset shows the relationship between birth rates per 1000 females and gross national product per person in U.S. dollars. The graph displays the logarithmic transformation of the variables to better visualize the relationship between them.\nThe graph shows a negative linear relationship between log(fertility) and log(ppgdp), indicating that as gross national product per person increases, birth rates per 1000 females decrease. The linear regression line, which is plotted using the method of least squares, indicates that the relationship is statistically significant.\nThe scatterplot also shows some degree of variability around the regression line, which is captured by the shaded area around the line. This variability could be due to other factors that are not accounted for in the linear regression model.\nOverall, the scatterplot suggests that there is a general negative relationship between fertility and gross national product per person, with higher levels of economic development associated with lower birth rates. However, the relationship does not appear to be strictly linear, as the points are not forming a tight linear pattern around the regression line. There is also significant scatter in the data, which suggests that there may be other factors that influence fertility rates besides gross national product per person.\nTherefore, while a simple linear regression model could provide a summary of the relationship between fertility and gross national product per person, it may not capture the full complexity of the relationship. A more sophisticated model that takes into account additional factors may be necessary to accurately model the relationship between these two variables.\n\n\nLog10\n\n\nCode\nUN11 %&gt;%\n  select(c(ppgdp, fertility)) %&gt;%\n  ggplot(aes(x = log10(ppgdp), y = log10(fertility))) + \n  geom_point(color = \"#001F3F\", alpha = 0.8, size = 3) +\n  geom_smooth(method = lm, se = TRUE, color = \"#E41A1C\", size = 1) +\n  theme_minimal() +\n  labs(x = \"Log10(Gross National Product per Person (US$))\",\n       y = \"Log10(Birth Rate per 1000 Females)\",\n       title = element_text(color = \"#006600\", \"Scatterplot of Log10(Fertility) vs. Log10(PPGDP) with Regression Line\")) +\n  theme(plot.title = element_text(size = 12, face = \"bold\", hjust = 0.5, color = \"#006600\"),\n        axis.title = element_text(size = 10, color = \"#001F3F\"),\n        axis.text = element_text(size = 8, color = \"#001F3F\"),\n        panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank(),\n        panel.border = element_blank(),\n        axis.line = element_line(color = \"#333333\", size = 0.5),\n        plot.background = element_blank(),\n        panel.background = element_blank(),\n        legend.position = \"none\")\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nLog 2\n\n\nCode\nUN11 %&gt;%\n  select(c(ppgdp, fertility)) %&gt;%\n  ggplot(aes(x = log2(ppgdp), y = log2(fertility))) + \n  geom_point(color = \"#001F3F\", alpha = 0.8, size = 3) +\n  geom_smooth(method = lm, se = TRUE, color = \"#E41A1C\", size = 1) +\n  theme_minimal() +\n  labs(x = \"Log2(Gross National Product per Person (US$))\",\n       y = \"Log2(Birth Rate per 1000 Females)\",\n       title = element_text(color = \"#006600\", \"Scatterplot of Log2(Fertility) vs. Log2(PPGDP) with Regression Line\")) +\n  theme(plot.title = element_text(size = 12, face = \"bold\", hjust = 0.5, color = \"#006600\"),\n        axis.title = element_text(size = 10, color = \"#001F3F\"),\n        axis.text = element_text(size = 8, color = \"#001F3F\"),\n        panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank(),\n        panel.border = element_blank(),\n        axis.line = element_line(color = \"#333333\", size = 0.5),\n        plot.background = element_blank(),\n        panel.background = element_blank(),\n        legend.position = \"none\")\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nChanging the base of logarithms will only change the values on the axes and not the shape of the graph.\nIn the case of the fertility vs. PPGDP graph, changing the base of logarithms will not change the overall pattern observed in the data, but it will change the values on the x and y axes. For example, using the natural logarithm (base e) instead of base 10 will result in different numerical values on the axes but the same underlying relationship between the two variables."
  },
  {
    "objectID": "posts/HW3_AkhileshKumarMeghwal.html#question-1",
    "href": "posts/HW3_AkhileshKumarMeghwal.html#question-1",
    "title": "Homework 3",
    "section": "",
    "text": "United Nations (Data file: UN 1 lin alr4) The data in the file UN 11 contains several variables, including ppgdp, the gross national product per person in U.S. dollars, and fertility, the birth rate per 1000 females, both from the year 2009. The data are for 199 localities, mostly UN member countries, but also other areas such as Hong Kong that are not independent countries.The data were collected from the United Nations (2011). We will study the dependence of fertility on ppgdp.\n\n\nIdentify the predictor and the response?\n\n\nCode\ndata(UN11)\nhead(UN11)\n\n\n\n\n  \n\n\n\nThe predictor variable is ppgdp and the response variable is fertility. ‘ppgdp’ represents the gross national product per person, while ‘fertility’ refers to the birth rate per 1000 females. The objective of the study is to understand how changes in ‘ppgdp’ can influence the value of ‘fertility’. Positive or negative correlations between the two variables can lead to conclusions about how one variable impacts the other. The analysis of this relationship can provide insights into socio-economic factors that influence birth rates, which can then inform policies on population growth, family planning, and economic development.\n\n\n\nDraw the scatterplot of fertility on the vertical axis versus ppgdp on the horizontal axis and summarize the information in this graph. Does a straight-line mean function seem to be plausible for a summary of this graph?\n\n\nCode\nUN11 %&gt;%\n  select(c(ppgdp, fertility)) %&gt;%\n  ggplot(aes(x = ppgdp, y = fertility)) + \n  geom_point(color = \"#001F3F\", alpha = 0.8, size = 3) +\n  geom_smooth(method = lm, se = FALSE, color = \"#E41A1C\", size = 1) +\n  theme_minimal() +\n  labs(x = \"Gross National Product per Person (US$)\",\n       y = \"Birth Rate per 1000 Females\",\n       title = element_text(color = \"#006600\", \"Scatterplot of Fertility vs. PPGDP with Regression Line\")) +\n  theme(plot.title = element_text(size = 12, face = \"bold\", hjust = 0.5, color = \"#006600\"),\n        axis.title = element_text(size = 10, color = \"#001F3F\"),\n        axis.text = element_text(size = 8, color = \"#001F3F\"),\n        panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank(),\n        panel.border = element_blank(),\n        axis.line = element_line(color = \"#333333\", size = 0.5),\n        plot.background = element_blank(),\n        panel.background = element_blank(),\n        legend.position = \"none\")\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\nWarning: The `size` argument of `element_line()` is deprecated as of ggplot2 3.4.0.\nℹ Please use the `linewidth` argument instead.\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nThe scatterplot shows the relationship between two variables, fertility (number of live births per 1000 females) and ppgdp (gross national product per person in US dollars). Each point on the plot represents a country, with its fertility rate on the vertical axis and its ppgdp on the horizontal axis.\nFrom the scatterplot, we can see that there is a general negative relationship between fertility and ppgdp. This means that as a country’s income increases, its fertility rate tends to decrease. However, the relationship is not perfectly linear, and there is a lot of variation in fertility rates at each level of ppgdp.\nTo summarize the trend in the data, A straight-line mean function does seem to be plausible for summarizing the data, as the plotted regression line suggests a linear negative trend in the data. However, it is important to note that the relationship between ppgdp and fertility may not be entirely linear, and other factors may also contribute to the observed patterns in the data. Therefore, it may be appropriate to explore more complex statistical models to better understand the relationship between these variables.\nOverall, this scatterplot provides a visual representation of the relationship between fertility and ppgdp. It allows us to quickly identify trends and patterns in the data and can be used to inform further analysis and modeling.\n\n\n\nDraw the scatterplot of log(fertility) versus log(ppgdp) using natural logarithms. Does the simple linear regression model seem plausible for a summary of this graph? If you use a different base of logarithms, the shape of the graph won’t change, but the values on the axes will change.\n\n\nCode\nUN11 %&gt;%\n  select(c(ppgdp, fertility)) %&gt;%\n  ggplot(aes(x = log(ppgdp), y = log(fertility))) + \n  geom_point(color = \"#001F3F\", alpha = 0.8, size = 3) +\n  geom_smooth(method = lm, se = TRUE, color = \"#E41A1C\", size = 1) +\n  theme_minimal() +\n  labs(x = \"Log(Gross National Product per Person (US$))\",\n       y = \"Log(Birth Rate per 1000 Females)\",\n       title = element_text(color = \"#006600\", \"Scatterplot of Log(Fertility) vs. Log(PPGDP) with Regression Line\")) +\n  theme(plot.title = element_text(size = 12, face = \"bold\", hjust = 0.5, color = \"#006600\"),\n        axis.title = element_text(size = 10, color = \"#001F3F\"),\n        axis.text = element_text(size = 8, color = \"#001F3F\"),\n        panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank(),\n        panel.border = element_blank(),\n        axis.line = element_line(color = \"#333333\", size = 0.5),\n        plot.background = element_blank(),\n        panel.background = element_blank(),\n        legend.position = \"none\")\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nThe scatterplot of Log(Fertility) versus Log(PPGDP) using data from the UN11 dataset shows the relationship between birth rates per 1000 females and gross national product per person in U.S. dollars. The graph displays the logarithmic transformation of the variables to better visualize the relationship between them.\nThe graph shows a negative linear relationship between log(fertility) and log(ppgdp), indicating that as gross national product per person increases, birth rates per 1000 females decrease. The linear regression line, which is plotted using the method of least squares, indicates that the relationship is statistically significant.\nThe scatterplot also shows some degree of variability around the regression line, which is captured by the shaded area around the line. This variability could be due to other factors that are not accounted for in the linear regression model.\nOverall, the scatterplot suggests that there is a general negative relationship between fertility and gross national product per person, with higher levels of economic development associated with lower birth rates. However, the relationship does not appear to be strictly linear, as the points are not forming a tight linear pattern around the regression line. There is also significant scatter in the data, which suggests that there may be other factors that influence fertility rates besides gross national product per person.\nTherefore, while a simple linear regression model could provide a summary of the relationship between fertility and gross national product per person, it may not capture the full complexity of the relationship. A more sophisticated model that takes into account additional factors may be necessary to accurately model the relationship between these two variables.\n\n\nLog10\n\n\nCode\nUN11 %&gt;%\n  select(c(ppgdp, fertility)) %&gt;%\n  ggplot(aes(x = log10(ppgdp), y = log10(fertility))) + \n  geom_point(color = \"#001F3F\", alpha = 0.8, size = 3) +\n  geom_smooth(method = lm, se = TRUE, color = \"#E41A1C\", size = 1) +\n  theme_minimal() +\n  labs(x = \"Log10(Gross National Product per Person (US$))\",\n       y = \"Log10(Birth Rate per 1000 Females)\",\n       title = element_text(color = \"#006600\", \"Scatterplot of Log10(Fertility) vs. Log10(PPGDP) with Regression Line\")) +\n  theme(plot.title = element_text(size = 12, face = \"bold\", hjust = 0.5, color = \"#006600\"),\n        axis.title = element_text(size = 10, color = \"#001F3F\"),\n        axis.text = element_text(size = 8, color = \"#001F3F\"),\n        panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank(),\n        panel.border = element_blank(),\n        axis.line = element_line(color = \"#333333\", size = 0.5),\n        plot.background = element_blank(),\n        panel.background = element_blank(),\n        legend.position = \"none\")\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nLog 2\n\n\nCode\nUN11 %&gt;%\n  select(c(ppgdp, fertility)) %&gt;%\n  ggplot(aes(x = log2(ppgdp), y = log2(fertility))) + \n  geom_point(color = \"#001F3F\", alpha = 0.8, size = 3) +\n  geom_smooth(method = lm, se = TRUE, color = \"#E41A1C\", size = 1) +\n  theme_minimal() +\n  labs(x = \"Log2(Gross National Product per Person (US$))\",\n       y = \"Log2(Birth Rate per 1000 Females)\",\n       title = element_text(color = \"#006600\", \"Scatterplot of Log2(Fertility) vs. Log2(PPGDP) with Regression Line\")) +\n  theme(plot.title = element_text(size = 12, face = \"bold\", hjust = 0.5, color = \"#006600\"),\n        axis.title = element_text(size = 10, color = \"#001F3F\"),\n        axis.text = element_text(size = 8, color = \"#001F3F\"),\n        panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank(),\n        panel.border = element_blank(),\n        axis.line = element_line(color = \"#333333\", size = 0.5),\n        plot.background = element_blank(),\n        panel.background = element_blank(),\n        legend.position = \"none\")\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nChanging the base of logarithms will only change the values on the axes and not the shape of the graph.\nIn the case of the fertility vs. PPGDP graph, changing the base of logarithms will not change the overall pattern observed in the data, but it will change the values on the x and y axes. For example, using the natural logarithm (base e) instead of base 10 will result in different numerical values on the axes but the same underlying relationship between the two variables."
  },
  {
    "objectID": "posts/HW3_AkhileshKumarMeghwal.html#question-2",
    "href": "posts/HW3_AkhileshKumarMeghwal.html#question-2",
    "title": "Homework 3",
    "section": "Question 2",
    "text": "Question 2\nAnnual income, in dollars, is an explanatory variable in a regression analysis. For a British version of the report on the analysis, all responses are converted to British pounds sterling (1 pound equals about 1.33 dollars, as of 2016).\n\n(a)\nHow, if at all, does the slope of the prediction equation change?\n\n\nCode\nUN11$ppgdp_pound &lt;- 1.33 * UN11$ppgdp\nsummary(lm(fertility ~ ppgdp_pound, UN11))\n\n\n\nCall:\nlm(formula = fertility ~ ppgdp_pound, data = UN11)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.9006 -0.8801 -0.3547  0.6749  3.7585 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  3.178e+00  1.048e-01  30.331  &lt; 2e-16 ***\nppgdp_pound -2.407e-05  3.500e-06  -6.877  7.9e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.206 on 197 degrees of freedom\nMultiple R-squared:  0.1936,    Adjusted R-squared:  0.1895 \nF-statistic: 47.29 on 1 and 197 DF,  p-value: 7.903e-11\n\n\nCode\nsummary(lm(fertility ~ ppgdp, UN11))\n\n\n\nCall:\nlm(formula = fertility ~ ppgdp, data = UN11)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.9006 -0.8801 -0.3547  0.6749  3.7585 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  3.178e+00  1.048e-01  30.331  &lt; 2e-16 ***\nppgdp       -3.201e-05  4.655e-06  -6.877  7.9e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.206 on 197 degrees of freedom\nMultiple R-squared:  0.1936,    Adjusted R-squared:  0.1895 \nF-statistic: 47.29 on 1 and 197 DF,  p-value: 7.903e-11\n\n\nThe analysis shows the results of fitting two linear regression models to the UN11 dataset. The first model predicts fertility rate based on the variable “ppgdp_pound,” which is the gross domestic product (GDP) per capita in British pounds. The second model predicts fertility rate based on the variable “ppgdp,” which is the GDP per capita in US dollars.\nBoth models show a statistically significant negative relationship between GDP per capita and fertility rate, with a p-value of 7.903e-11 for both. The coefficient estimates for ppgdp and ppgdp_pound are different, which is expected since the units are different. The coefficient for ppgdp_pound is -2.407e-05, which means that for every one pound increase in GDP per capita, the fertility rate decreases by 0.00002407. The coefficient for ppgdp is -3.201e-05, which means that for every one dollar increase in GDP per capita, the fertility rate decreases by 0.00003201.\nOverall, the results suggest that increasing economic development, as measured by GDP per capita, is associated with a decrease in fertility rate. However, the impact of the pound conversion on the slope of the regression line is negligible, with a difference of only 0.0000086 in the coefficient estimates.\n\n\n(b)\nHow, if at all, does the correlation change?\n\n\nCode\ncor(UN11$ppgdp, UN11$fertility)\n\n\n[1] -0.4399891\n\n\nCode\ncor(UN11$ppgdp_pound, UN11$fertility)\n\n\n[1] -0.4399891\n\n\nThe correlation coefficient between fertility and ppgdp (in US dollars) is the same as the correlation coefficient between fertility and ppgdp_pound (in British pounds). This suggests that the choice of currency used to measure gross domestic product per capita does not have a significant impact on the relationship between fertility and economic development.\nThe correlation coefficient of -0.4399891 indicates a moderate negative correlation between fertility and gross domestic product per capita. This suggests that as the gross domestic product per capita increases, the fertility rate tends to decrease, and vice versa.\nOverall, the analysis suggests that the choice of currency used to measure gross domestic product per capita does not have a significant impact on the relationship between fertility and economic development."
  },
  {
    "objectID": "posts/HW3_AkhileshKumarMeghwal.html#question-3",
    "href": "posts/HW3_AkhileshKumarMeghwal.html#question-3",
    "title": "Homework 3",
    "section": "Question 3",
    "text": "Question 3\nWater runoff in the Sierras (Data file: water in alr4) Can Southern California’s water supply in future years be predicted from past data? One factor affecting water availability is stream runoff. If runoff could be predicted, engineers, planners, and policy makers could do their jobs more efficiently. The data file contains 43 years’ worth of precipitation measurements taken at six sites in the Sierra Nevada mountains (labeled APMAM, APSAB, APSLAKE, OPBPC, OPRC, and OPSLAKE) and stream runoff volume at a site near Bishop, California, labeled BSAAM. Draw the scatterplot matrix for these data and summarize the information available from these plots.\n(Hint: Use the pairsQ function.)\n\n\nCode\ndata(water)\npairs(~APMAM + APSAB + APSLAKE + OPBPC + OPRC + OPSLAKE, data=water, main = \"Sierra Southern California Water Supply Runoff\",\n      pch = 21, bg = \"red\")\n\n\n\n\n\nCode\n# Set colors for variables\ncolorA &lt;- \"blue\"\ncolorB &lt;- \"red\"\npar(cex.axis = 0.8, cex.lab=0.8)\n\n# Create the plots with colored points and title\nplot(y=water$BSAAM,x=water$APMAM, col=ifelse(substr(names(water)[2],1,1) == \"A\", colorA, colorB),\n     main=\"Scatter plot of BSAAM and APMAM\", xlab=\"APMAM\", ylab=\"BSAAM\")\n\n\n\n\n\nCode\nplot(y=water$BSAAM,x=water$APSAB, col=ifelse(substr(names(water)[3],1,1) == \"A\", colorA, colorB),\n     main=\"Scatter plot of BSAAM and APSAB\", xlab=\"APSAB\", ylab=\"BSAAM\")\n\n\n\n\n\nCode\nplot(y=water$BSAAM,x=water$APSLAKE, col=ifelse(substr(names(water)[4],1,1) == \"A\", colorA, colorB),\n     main=\"Scatter plot of BSAAM and APSLAKE\", xlab=\"APSLAKE\", ylab=\"BSAAM\")\n\n\n\n\n\nCode\nplot(y=water$BSAAM,x=water$OPBPC, col=ifelse(substr(names(water)[5],1,1) == \"A\", colorA, colorB),\n     main=\"Scatter plot of BSAAM and OPBPC\", xlab=\"OPBPC\", ylab=\"BSAAM\")\n\n\n\n\n\nCode\nplot(y=water$BSAAM,x=water$OPRC, col=ifelse(substr(names(water)[6],1,1) == \"A\", colorA, colorB),\n     main=\"Scatter plot of BSAAM and OPRC\", xlab=\"OPRC\", ylab=\"BSAAM\")\n\n\n\n\n\nCode\nplot(y=water$BSAAM,x=water$OPSLAKE, col=ifelse(substr(names(water)[7],1,1) == \"A\", colorA, colorB),\n     main=\"Scatter plot of BSAAM and OPSLAKE\", xlab=\"OPSLAKE\", ylab=\"BSAAM\")\n\n\n\n\n\nI created output graph using ‘pairs’ to establish and read correlation between different variables given in the dataset, but I had trouble reading the correlation output; so I also ran each of the scatter-plots separately\nThe graph provided in the analysis shows the correlations between six variables: OPBPC, OPRC, OPSLAKE, APMAM, APSAB, and APSLAKE. The graph indicates that there are two groups of variables that are more closely related to each other than to the other variables in the set.\nThe first group consists of OPBPC, OPRC, and OPSLAKE, which are strongly correlated with each other. The correlation between these three variables and APMAM, APSAB, and APSLAKE is much weaker. This suggests that the first group of variables is related to each other in a way that is different from their relationship with the second group of variables.\nThe second group consists of APMAM, APSAB, and APSLAKE, which are also correlated with each other, although to a lesser extent than OPBPC, OPRC, and OPSLAKE. This suggests that there is indeed a correlation among these three variables, but it is not as strong as the correlation between the first group of variables.\nThe analysis also indicates that BSAAM is more closely related to OPBPC, OPRC, and OPSLAKE than to APMAM, APSAB, and APSLAKE. The results show that BSAAM has a stronger positive correlation with OPBPC, OPRC, and OPSLAKE. On the other hand, the correlation between BSAAM and APMAM, APSAB, and APSLAKE are weaker. This suggests that BSAAM is indeed more closely related to the first group of variables than to the second group of variables.\nIn summary, the analysis reveals that there are two groups of variables in the set, and they are more closely related to each other within their group. OPBPC, OPRC, and OPSLAKE are strongly correlated with each other, but not with APMAM, APSAB, and APSLAKE. APMAM, APSAB, and APSLAKE are also correlated with each other, but to a lesser extent than OPBPC, OPRC, and OPSLAKE. BSAAM is more closely related to the first group of variables than to the second group of variables. This information can be useful in identifying important variables for analysis or modeling."
  },
  {
    "objectID": "posts/HW3_AkhileshKumarMeghwal.html#question-4",
    "href": "posts/HW3_AkhileshKumarMeghwal.html#question-4",
    "title": "Homework 3",
    "section": "Question 4",
    "text": "Question 4\nProfessor ratings (Data fde: Rateprof in alr4) In the website and online forum RateMyProfessors.com, students rate and comment on their instructors. Launched in 1999, the site includes millions of ratings on thousands of instructors. The data file includes the summaries of the ratings of 364 instructors at a large campus in the Midwest (Bleske-Rechek and Fritsch, 2011). Each instructor included in the data had at least 10 ratings over a several year period. Students provided ratings of 1-5 on quality, helpfulness, clarity, easiness of instructor’s courses, and raterinterest in the subject matter covered in the instructor’s courses. The data file provides the averages of these five ratings. Create a scatterplot matrix of these five variables. Provide a brief description of the relationships between the five ratings.\n\n\nCode\nsummary(Rateprof)\n\n\n    gender       numYears        numRaters       numCourses     pepper   \n female:159   Min.   : 1.000   Min.   :10.00   Min.   : 1.000   no :320  \n male  :207   1st Qu.: 6.000   1st Qu.:15.00   1st Qu.: 3.000   yes: 46  \n              Median :10.000   Median :24.00   Median : 4.000            \n              Mean   : 8.347   Mean   :28.58   Mean   : 4.251            \n              3rd Qu.:11.000   3rd Qu.:37.00   3rd Qu.: 5.000            \n              Max.   :11.000   Max.   :86.00   Max.   :12.000            \n                                                                         \n    discipline          dept        quality       helpfulness   \n Hum     :134   English   : 49   Min.   :1.409   Min.   :1.364  \n SocSci  : 66   Math      : 34   1st Qu.:2.936   1st Qu.:3.069  \n STEM    :103   Biology   : 20   Median :3.612   Median :3.662  \n Pre-prof: 63   Chemistry : 20   Mean   :3.575   Mean   :3.631  \n                Psychology: 20   3rd Qu.:4.250   3rd Qu.:4.351  \n                Spanish   : 20   Max.   :4.981   Max.   :5.000  \n                (Other)   :203                                  \n    clarity         easiness     raterInterest     sdQuality      \n Min.   :1.333   Min.   :1.391   Min.   :1.098   Min.   :0.09623  \n 1st Qu.:2.871   1st Qu.:2.548   1st Qu.:2.934   1st Qu.:0.87508  \n Median :3.600   Median :3.148   Median :3.305   Median :1.15037  \n Mean   :3.525   Mean   :3.135   Mean   :3.310   Mean   :1.05610  \n 3rd Qu.:4.214   3rd Qu.:3.692   3rd Qu.:3.692   3rd Qu.:1.28730  \n Max.   :5.000   Max.   :4.900   Max.   :4.909   Max.   :1.67739  \n                                                                  \n sdHelpfulness      sdClarity        sdEasiness     sdRaterInterest \n Min.   :0.0000   Min.   :0.0000   Min.   :0.3162   Min.   :0.3015  \n 1st Qu.:0.9902   1st Qu.:0.9085   1st Qu.:0.9045   1st Qu.:1.0848  \n Median :1.2860   Median :1.1712   Median :1.0247   Median :1.2167  \n Mean   :1.1719   Mean   :1.0970   Mean   :1.0196   Mean   :1.1965  \n 3rd Qu.:1.4365   3rd Qu.:1.3328   3rd Qu.:1.1485   3rd Qu.:1.3326  \n Max.   :1.8091   Max.   :1.8091   Max.   :1.6293   Max.   :1.7246  \n                                                                    \n\n\nCode\nhead(Rateprof)\n\n\n\n\n  \n\n\n\nCode\n#subset of RateProf data\n\nrate_my_prof&lt;-select(Rateprof,c('quality','helpfulness','clarity','easiness','raterInterest'))\n\nhead(rate_my_prof)\n\n\n\n\n  \n\n\n\nCode\n#Scatterplot Matrix of five RateProf variables\n\npairs(rate_my_prof,\n      col = \"red3\",\n      pch = 20,\n      main = \"Rate my Professor Matrix ScatterPlot \")\n\n\n\n\n\nInterpreting the scatter plot output generated through ‘pairs’ for the average professor ratings on the topics of quality, clarity, helpfulness, easiness, and rater interest, the variables quality, clarity, and helpfulness appear to each have strong positive correlations with each other.\nThe relationship between some pairs of variables indicates better positive linear correlations than others.\n\nQuality-Clarity and Quality-Helpfulness indicate very strong positive linear correlations.\nQuality-Easiness and Quality-RaterInterest indicate weak positive linear correlations.\nHelpfulness-Easiness and Helpfulness-RaterInterest indicate weak linear correlations.\nClarity-Helpfulness indicates a positive correlation\nClarity-RaterInterest and Clarity-Easiness indicate weak positive correlations.\nEasiness and RaterInterest indicate a very weak positive correlation."
  },
  {
    "objectID": "posts/HW3_AkhileshKumarMeghwal.html#question-5",
    "href": "posts/HW3_AkhileshKumarMeghwal.html#question-5",
    "title": "Homework 3",
    "section": "Question 5",
    "text": "Question 5\nFor the student.survey data file in the smss package, conduct regression analysesrelating (by convention, y denotes the outcome variable, x denotes the explanatory variable) (i) y = political ideology and x = religiosity, (ii) y = high school GPA and x = hours of TV watching. (You can use ?student. survey in the R console, after loading the package, to see what each variable means.)\n\n(a)\nGraphically portray how the explanatory variable relates to the outcome variable in each of the two cases\n\n\nCode\ndata(student.survey)\nsummary(student.survey)\n\n\n      subj       ge           ag              hi              co       \n Min.   : 1.00   f:31   Min.   :22.00   Min.   :2.000   Min.   :2.600  \n 1st Qu.:15.75   m:29   1st Qu.:24.00   1st Qu.:3.000   1st Qu.:3.175  \n Median :30.50          Median :26.50   Median :3.350   Median :3.500  \n Mean   :30.50          Mean   :29.17   Mean   :3.308   Mean   :3.453  \n 3rd Qu.:45.25          3rd Qu.:31.00   3rd Qu.:3.625   3rd Qu.:3.725  \n Max.   :60.00          Max.   :71.00   Max.   :4.000   Max.   :4.000  \n                                                                       \n       dh             dr               tv               sp        \n Min.   :   0   Min.   : 0.200   Min.   : 0.000   Min.   : 0.000  \n 1st Qu.: 205   1st Qu.: 1.450   1st Qu.: 3.000   1st Qu.: 3.000  \n Median : 640   Median : 2.000   Median : 6.000   Median : 5.000  \n Mean   :1232   Mean   : 3.818   Mean   : 7.267   Mean   : 5.483  \n 3rd Qu.:1350   3rd Qu.: 5.000   3rd Qu.:10.000   3rd Qu.: 7.000  \n Max.   :8000   Max.   :20.000   Max.   :37.000   Max.   :16.000  \n                                                                  \n       ne               ah             ve          pa    \n Min.   : 0.000   Min.   : 0.000   Mode :logical   d:21  \n 1st Qu.: 2.000   1st Qu.: 0.000   FALSE:60        i:24  \n Median : 3.000   Median : 0.500                   r:15  \n Mean   : 4.083   Mean   : 1.433                         \n 3rd Qu.: 5.250   3rd Qu.: 2.000                         \n Max.   :14.000   Max.   :11.000                         \n                                                         \n                     pi                re         ab              aa         \n very liberal         : 8   never       :15   Mode :logical   Mode :logical  \n liberal              :24   occasionally:29   FALSE:60        FALSE:59       \n slightly liberal     : 6   most weeks  : 7                   NA's :1        \n moderate             :10   every week  : 9                                  \n slightly conservative: 6                                                    \n conservative         : 4                                                    \n very conservative    : 2                                                    \n     ld         \n Mode :logical  \n FALSE:44       \n NA's :16       \n                \n                \n                \n                \n\n\nCode\nhead(student.survey)\n\n\n\n\n  \n\n\n\n\n\nCode\nstudent.survey %&gt;%\n  \n  # Group by religiosity and political ideology and count the number of observations\n  group_by(re, pi) %&gt;%\n  summarize(count = n(), .groups = \"drop\") %&gt;%\n  # Create a stacked bar chart\n  ggplot() + \n  geom_col(aes(x = re, y = count, fill = pi), position = \"stack\") +\n  # Add labels to x and y axis\n  xlab(\"Religiosity\") +\n  ylab(\"Count\") +\n  # Add a legend to show the relationship between fill colors and political ideology\n  scale_fill_discrete(name = \"Political ideology\")\n\n\n\n\n\nConservatism and religiosity appear to be positively correlated, meaning that individuals who are more religious tend to be more conservative in their political beliefs.\n\n\nCode\nggplot(data=student.survey, aes(x=tv, y=hi))+\ngeom_point()+\ngeom_smooth(method=\"lm\", se=FALSE)+\nxlab(\"Average Number of Hours of TV watched per Week\") +\nylab(\"High School GPA\") \n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nHigh school GPA and TV-watching seem to have a negative relationship.\n\n\n(b)\nSummarize and interpret results of inferential analyses.\n\n\nCode\nsummary(lm(data = student.survey, formula = as.numeric(pi) ~ as.numeric(re)))\n\n\n\nCall:\nlm(formula = as.numeric(pi) ~ as.numeric(re), data = student.survey)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.81243 -0.87160  0.09882  1.12840  3.09882 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      0.9308     0.4252   2.189   0.0327 *  \nas.numeric(re)   0.9704     0.1792   5.416 1.22e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.345 on 58 degrees of freedom\nMultiple R-squared:  0.3359,    Adjusted R-squared:  0.3244 \nF-statistic: 29.34 on 1 and 58 DF,  p-value: 1.221e-06\n\n\nThis output is from a linear regression model that examines the relationship between religiousness (re) and political ideology (pi) in a sample of students. The model shows that there is a statistically significant positive relationship between religiousness and political ideology.\nThe intercept coefficient is 0.9308, which means that when religiousness is zero, the expected value of political ideology is 0.9308. The slope coefficient for religiousness is 0.9704, which means that for a one-unit increase in religiousness, the expected value of political ideology increases by 0.9704 units. The R-squared value is 0.3359, which indicates that the model explains 33.59% of the variation in political ideology.\nThe p-value for the slope coefficient is 1.221e-06, which is less than 0.05, indicating that the relationship between religiousness and political ideology is statistically significant. Therefore, we can conclude that there is a positive relationship between religiousness and political ideology in the sample of students.\n\n\nCode\nsummary(lm(data = student.survey, formula = hi ~ tv))\n\n\n\nCall:\nlm(formula = hi ~ tv, data = student.survey)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.2583 -0.2456  0.0417  0.3368  0.7051 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  3.441353   0.085345  40.323   &lt;2e-16 ***\ntv          -0.018305   0.008658  -2.114   0.0388 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4467 on 58 degrees of freedom\nMultiple R-squared:  0.07156,   Adjusted R-squared:  0.05555 \nF-statistic: 4.471 on 1 and 58 DF,  p-value: 0.03879\n\n\nThis analysis reports the results of a linear regression model to investigate the relationship between average hours of TV watched per week (TV) and high school GPA (hi) using data from student survey.\nThe model shows that the intercept is statistically significant at a p-value &lt; 2e-16, indicating that the predicted value of the high school GPA when the average number of hours of TV watched per week is zero is 3.44. This value can be interpreted as the expected high school GPA for students who do not watch any TV.\nFurthermore, the coefficient for TV is -0.018 with a p-value of 0.0388, which indicates that there is a statistically significant negative association between TV watching and high school GPA at the 0.05 level of significance. The negative coefficient suggests that as the average number of hours of TV watched per week increases by 1 hour, high school GPA decreases by 0.018.\nThe model’s R-squared value is 0.07156, indicating that only about 7% of the variance in high school GPA is explained by the variance in the average number of hours of TV watched per week. The adjusted R-squared value is 0.05555, which means that the model has a weak predictive power for high school GPA based on the average number of hours of TV watched per week.\nIn summary, the results suggest that there is a negative association between TV watching and high school GPA. However, the model’s low R-squared value suggests that other factors not included in the model might be more important in predicting high school GPA."
  },
  {
    "objectID": "posts/Homework3_solution_Pang.html",
    "href": "posts/Homework3_solution_Pang.html",
    "title": "Homework 3",
    "section": "",
    "text": "Please check your answers against the solutions."
  },
  {
    "objectID": "posts/Homework3_solution_Pang.html#question-1",
    "href": "posts/Homework3_solution_Pang.html#question-1",
    "title": "Homework 3",
    "section": "Question 1",
    "text": "Question 1\nLoad the necessary packages.\n\nlibrary(alr4)\nlibrary(smss)\nlibrary(ggplot2)\nlibrary(stargazer)\n\nLoad data:\n\ndata(UN11)\n\n\n(a)\nThe predictor is ppgdp, i.e. GDP per capita. The response is fertility, the birth rate per 1000 women.\n\n\n(b)\n\nggplot(data = UN11, aes(x = ppgdp, y = fertility)) +\n  geom_point()\n\n\n\n\nA straight line is not appropriate, because the relationship has an L-shaped structure (or the left half of a U-shape).\n\n\n(c)\n\nggplot(data = UN11, aes(x = log(ppgdp), y = log(fertility))) +\n  geom_point()\n\n\n\n\nYes, now a simple linear regression model is more plausible. We can imagine a negative-sloped straight line going through those points."
  },
  {
    "objectID": "posts/Homework3_solution_Pang.html#question-2",
    "href": "posts/Homework3_solution_Pang.html#question-2",
    "title": "Homework 3",
    "section": "Question 2",
    "text": "Question 2\n\n(a)\nThe conversion from USD to British pound will mean the numerical value of the response will be divided by 1.33. To offset that, the slope will also become divided by 1.33.\n\n\n(b)\nCorrelation will not change because it is a standardized measure that is not influenced by the unit of measurement.\nBoth outcomes can easily be shown via simulation."
  },
  {
    "objectID": "posts/Homework3_solution_Pang.html#question-3",
    "href": "posts/Homework3_solution_Pang.html#question-3",
    "title": "Homework 3",
    "section": "Question 3",
    "text": "Question 3\n\ndata(water)\npairs(water)\n\n\n\n\n\nYear appears to be largely unrelated to each of the other variables\nThe three variables starting with “O” seem to be correlated with each other, meaning that all the plot including two of these variables exhibit a dependence between the variables that is stronger than the dependence between the “O” variables and other variables. The three variables starting with “A” also seem to be another correlated group\nBSAAM is more closely related to the “O” variables than the “A” variables"
  },
  {
    "objectID": "posts/Homework3_solution_Pang.html#question-4",
    "href": "posts/Homework3_solution_Pang.html#question-4",
    "title": "Homework 3",
    "section": "Question 4",
    "text": "Question 4\n\ndata(Rateprof)\npairs(Rateprof[,c('quality', 'clarity', 'helpfulness',\n                  'easiness', 'raterInterest')])\n\n\n\n\nThe very strong pair-wise correlation among quality, clarity, and helpfulness is very striking. easiness is also correlated fairly highly with the other three. raterInterest is also moderately correlated, but raters almost always say they are at least moderately interested in the subject. Overall, the results might show that people don’t necessarily distinguish all these dimensions very well in their minds—or that professors that do one in one dimension tend to do well on the others too."
  },
  {
    "objectID": "posts/Homework3_solution_Pang.html#question-5",
    "href": "posts/Homework3_solution_Pang.html#question-5",
    "title": "Homework 3",
    "section": "Question 5",
    "text": "Question 5\n\n(a)\nOne way of visually representing the relationship between religiosity and political ideology is as follows (and there are other ways). As we go towards bars to the right (more religiousity), we see lighter colors pop up (more conservatism)\n\ndata(student.survey)\nggplot(data = student.survey, aes(x = re, fill = pi)) +\n    geom_bar(position = \"fill\")\n\n\n\n\nThe relationship between high school GPA and hours of watching TV can be shown with a good old scatter plot.\n\nggplot(data = student.survey, aes(x = tv, y = hi)) +\n  geom_point() \n\n\n\n\n\n\n(b)\nDealing with ordinal variables in linear regression is a difficult problem. We’ll just go ahead and assume that we can just convert them to numeric and use them. This would be done for political ideology and religiosity. High school GPA and hours of TV are already continuous.\n\nm1 &lt;- lm(as.numeric(pi) ~ as.numeric(re), \n         data = student.survey)\nm2 &lt;- lm(hi ~ tv, data = student.survey)\nstargazer(m1, m2, type = 'text', \n          dep.var.labels = c('Pol. Ideology', 'HS GPA'),\n          covariate.labels = c('Religiosity', 'Hours of TV')\n          )\n\n\n==========================================================\n                                  Dependent variable:     \n                              ----------------------------\n                               Pol. Ideology     HS GPA   \n                                    (1)            (2)    \n----------------------------------------------------------\nReligiosity                       0.970***                \n                                  (0.179)                 \n                                                          \nHours of TV                                     -0.018**  \n                                                 (0.009)  \n                                                          \nConstant                          0.931**       3.441***  \n                                  (0.425)        (0.085)  \n                                                          \n----------------------------------------------------------\nObservations                         60            60     \nR2                                 0.336          0.072   \nAdjusted R2                        0.324          0.056   \nResidual Std. Error (df = 58)      1.345          0.447   \nF Statistic (df = 1; 58)         29.336***       4.471**  \n==========================================================\nNote:                          *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01\n\n\nReligiosity is positively and statistically significantly (at the 0.01 significance level) associated with conservatism.\nHours of TV is negatively and statistically significantly (at the 0.05 significance level) associated with High School GPA. Watching an average of 1 more hour of TV per week is associated with a 0.018 decline in High School GPA."
  },
  {
    "objectID": "posts/Final_Project_Check1_GHTan.html",
    "href": "posts/Final_Project_Check1_GHTan.html",
    "title": "Final Project Check 1",
    "section": "",
    "text": "My final project will be a further investigation on digital devices in schools that I have submitted as the final project for DACSS 601. I still explore the data from the survey “Programme for International Student Assessment” in 2018. In this assignment, I will propose my hypothesis, and present the descriptive statistics with minor changes base on my last project.\n\n\nCode\nlibrary(tidyverse)\n\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.0      ✔ purrr   0.3.5 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.4.1 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\nCode\nlibrary(ggplot2)\nlibrary(dbplyr)\n\n\n\nAttaching package: 'dbplyr'\n\nThe following objects are masked from 'package:dplyr':\n\n    ident, sql\n\n\nCode\npisa &lt;- read_csv('_data/CY07_MSU_SCH_QQQ.csv')\n\n\nNew names:\nRows: 21903 Columns: 198\n── Column specification\n──────────────────────────────────────────────────────── Delimiter: \",\" chr\n(8): CNT, CYC, NatCen, STRATUM, SUBNATIO, SC053D11TA, PRIVATESCH, VER_DAT dbl\n(189): ...1, CNTRYID, CNTSCHID, Region, OECD, ADMINMODE, LANGTEST, SC001... lgl\n(1): BOOKID\nℹ Use `spec()` to retrieve the full column specification for this data. ℹ\nSpecify the column types or set `show_col_types = FALSE` to quiet this message.\n• `` -&gt; `...1`\n\n\n\nResearch Questions\nMy final project will probe into what factors contribute to the accessibility to and human resources’ support for digital devices in schools. Additionally, I will explore if there is a correlations between career guidance and digital devices? I will conduct this research based on the data “Programme for International Student Assessment” (PISA) collected by the The Organization for Economic Co-operation and Development (OECD) in 2018.\n\n\nHpyotheis\nI propose that the size of urban population primarily contributes to the conditions of digital device. “OECD or Non-OECD” and “public or private schools” may be two cofounders, which is suppose to be incorporated into the regression analysis. Also, I hypothesize that the higher score a school report regarding career guidance, the higher score a school reports in terms of digital divices.\n\n\nCode\n# create a data frame\n#view(pisa)\n# select related variable\npisa_selected &lt;- select(pisa,starts_with(c(\"SC001\", \"SC013\", \"SC016\", \"SC161\",\"SC155\")))\npisa2018_joint &lt;-cbind(pisa[, 1:12], pisa_selected)\n# pisa_SC155\npisa2018_joint$Accessibility=rowMeans(pisa2018_joint[,c(\"SC155Q01HA\",\"SC155Q02HA\",                                                  \"SC155Q03HA\",\"SC155Q04HA\")])\npisa2018_joint$Human_Resource_Support=rowMeans(pisa2018_joint[\n  ,c(\"SC155Q05HA\",\"SC155Q06HA\", \"SC155Q07HA\",\"SC155Q08HA\",\"SC155Q09HA\", \"SC155Q10HA\", \"SC155Q11HA\")])\npisa2018_joint$Career_Guidance=rowSums(pisa2018_joint[, c(\"SC161Q02SA\",\"SC161Q03SA\",\"SC161Q04SA\",\"SC161Q04SA\")])\npisa_SC155 &lt;- pisa2018_joint %&gt;%\n  select(CNT, STRATUM, OECD, Career_Guidance,Accessibility, Human_Resource_Support, SC001Q01TA, SC013Q01TA) %&gt;%\n  mutate(Urban=SC001Q01TA, Public_or_Private=SC013Q01TA) %&gt;%\n  select(-c(SC001Q01TA, SC013Q01TA)) %&gt;%\n  select(c(CNT,STRATUM,OECD,Urban, Public_or_Private,Career_Guidance,Accessibility,Human_Resource_Support))\npisa_SC155\n\n\n\n\nDescriptive Statistics\nThis original OECD PISA 2018 School Questionnaire Dataset is one part of PISA 2018 dataset with a focus on schools. It covers 80 countries and regions all over the world. The dataset documents 21,903 schools’ responses regarding 187 questions.After cleaning the data, the dataset includes 8 variables: CNT identifies countries. STRATUM identifies schools. OECD indicates if a school locates in a OECD country or not. Urban describes different conditions of urban communities where a school locates. Public_or_Private presents if a school is public or private. Career_Guidance demonstrates the score a school reports in terms of career guidance. Accessibility demonstrates the score a school reports in terms of accessibility to digital devices. Human_Resource_Support suggests the score a school reports in terms of human ressource support for digital devices.\nAfter using the summary function and visualization, I have already show the descriptive statistics. A large number of NA stands out. I will figure out how to deal with them properly.\n\n\nCode\nsummary(pisa_SC155)\n\n\n     CNT              STRATUM               OECD            Urban      \n Length:21903       Length:21903       Min.   :0.0000   Min.   :1.000  \n Class :character   Class :character   1st Qu.:0.0000   1st Qu.:2.000  \n Mode  :character   Mode  :character   Median :1.0000   Median :3.000  \n                                       Mean   :0.5171   Mean   :3.007  \n                                       3rd Qu.:1.0000   3rd Qu.:4.000  \n                                       Max.   :1.0000   Max.   :5.000  \n                                                        NA's   :1363   \n Public_or_Private Career_Guidance Accessibility   Human_Resource_Support\n Min.   :1.00      Min.   :0.000   Min.   :1.000   Min.   :1.000         \n 1st Qu.:1.00      1st Qu.:1.000   1st Qu.:2.000   1st Qu.:2.286         \n Median :1.00      Median :1.000   Median :2.750   Median :2.714         \n Mean   :1.19      Mean   :1.518   Mean   :2.674   Mean   :2.658         \n 3rd Qu.:1.00      3rd Qu.:2.000   3rd Qu.:3.250   3rd Qu.:3.000         \n Max.   :2.00      Max.   :4.000   Max.   :4.000   Max.   :4.000         \n NA's   :2092      NA's   :1499    NA's   :1185    NA's   :1236          \n\n\nCode\npisa_SC155_boxplot&lt;-pisa_SC155 %&gt;%\n  select(STRATUM, Career_Guidance, Accessibility, Human_Resource_Support) %&gt;% \n  pivot_longer(cols=c(Career_Guidance, Accessibility, Human_Resource_Support), \n               names_to = \"Group\", values_to = \"Evaluation\")\n\nggplot(pisa_SC155_boxplot,aes(Evaluation, fill=Group))+\n  stat_boxplot(geom = \"errorbar\", # Error bars\n               width = 0.2)+\n  geom_boxplot()+\n  facet_wrap(~Group)+\n  labs(title=\"Pisa2018 Evaluation\")+\n  coord_flip()\n\n\nWarning: Removed 3920 rows containing non-finite values (`stat_boxplot()`).\nRemoved 3920 rows containing non-finite values (`stat_boxplot()`)."
  },
  {
    "objectID": "posts/Rowley_Final_Project_Check-In_1.html",
    "href": "posts/Rowley_Final_Project_Check-In_1.html",
    "title": "Final Project - Check-In 1",
    "section": "",
    "text": "Backgroun and Research Question:\nMy research question will focus on the cross-country correlation between income and democracy. A 2008 study titled “Income and Democracy,” published in the American Economic Review, argues that existing studies that establish a strong cross-country correlation between income and democracy do not control for factors that simultaneously affect both variables. Accordingly, this study controls for certain country-fixed effects—such as date of independence, constraints on the executive, and religious affiliation—which thereby removes the statistical association between income per capita and various measures of democracy. This study is the source for my data set.\nIn contrast, a 1999 study written by Robert J. Barro asserted that improvements in the standard of living predict increase in democracy. However, similar to the argument in “Income and Democracy,” this study found that the allowance of certain economic variables weakens the interplay specifically between democracy and religious affiliation. Nevertheless, Barro claimed that the negative effects from Muslim and non‐religious affiliations remain regardless of control factors.\nThis incongruity led me to wonder whether we would see a correlation between income and democracy if the economic variables used in the two studies were both updated and more aligned. Specifically, I would like to examine how this would affect both studies’ claims on the role of religious affiliation; in other words, will adding control variables such as education shift the correlation between religious affiliation—though only for Islam and non-religious affiliations—and democracy?\nResearch question: How will adding education and non-religious affliation as a control variables impact the correlation between religious affiliation and democracy?\n\n\nHypothesis:\nAfter reviewing “Income and Democracy,” it does not appear that non-religious affiliation was integrated into the report. Additionally, the authors indicated that education was determined to be statistically insignificant as an independent country-fixed effect within the context of its causal effect on democracy. However, I am curious about how the inclusion of these two variables would affect Barro’s conclusion relative to the correlation between religious affiliation and democracy. As such, by adding these variables and updating existing variables with new data, I will be revisiting a previously-tested hypothesis.\nDespite my curiosity, I hypothesize that adding education and non-religious affiliation as control variables will not uncover any statistical significance between religious affiliation and democracy, even when narrowing the scope of religious affiliation to focus solely on Islam and non-religious affiliation.\n\n\nDescriptive Statistics:\nData for this study was collected from the Freedom House Political Rights Index, the Polity Composite Democracy Index, and data from other studies conducted by Barro and Kenneth A. Bollen.\nVariables I will be focusing on include:\n\nCountry;\nConstraint on the executive;\nYear of independence;\nSettler mortality;\nPopulation density;\nCatholic population;\nMuslim population;\nProtestant population;\nEducation;\nShift in per capita income; and\nShift in democracy.\n\n\n\nCode\n# load libraries:\n\nlibrary(tidyverse)\n\n\nWarning: package 'tidyverse' was built under R version 4.2.2\n\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.3.6      ✔ purrr   0.3.4 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.4.1 \n✔ readr   2.1.4      ✔ forcats 0.5.2 \n\n\nWarning: package 'readr' was built under R version 4.2.2\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\nCode\nlibrary(readxl)\n\n\nWarning: package 'readxl' was built under R version 4.2.2\n\n\n\n\nCode\n# read in data file:\n\ndata_file &lt;- read_excel(\"C:/Users/caitr/OneDrive/Documents/DACSS/DACSS 603/603_Spring_2023/posts/Final Project/Income-Democracy.xls\", sheet = \"500 Year Panel\") \nhead(data_file)\n\n\n# A tibble: 6 × 15\n  code  country     consf…¹ indcent indyear logem4 lpd15…² madid rel_c…³ rel_m…⁴\n  &lt;chr&gt; &lt;chr&gt;         &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1 ADO   Andorra      NA        18      1800  NA     NA      1001  NA      NA    \n2 AFG   Afghanistan   0        19.2    1919   4.54   2.12   3002   0       0.993\n3 AGO   Angola        0.333    19.8    1975   5.63   0.405  2011   0.687   0    \n4 ALB   Albania       0.667    19.1    1912  NA      1.99   2009  NA      NA    \n5 ARE   United Ara…   0.333    19.7    1971  NA      0      3002   0.004   0.949\n6 ARG   Argentina     0        18.2    1816   4.23  -2.21   5001   0.916   0.002\n# … with 5 more variables: rel_protmg80 &lt;dbl&gt;, growth &lt;dbl&gt;, democ &lt;dbl&gt;,\n#   world &lt;dbl&gt;, colony &lt;dbl&gt;, and abbreviated variable names ¹​consfirstaug,\n#   ²​lpd1500s, ³​rel_catho80, ⁴​rel_muslim80\n\n\n\n\nCode\n# remove dummy/unnecessary variables (as identified in study's variable key):\n\ndata_cln = subset(data_file, select = -c(code, world, colony, indcent, madid))\nhead(data_cln)\n\n\n# A tibble: 6 × 10\n  country   consf…¹ indyear logem4 lpd15…² rel_c…³ rel_m…⁴ rel_p…⁵  growth democ\n  &lt;chr&gt;       &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;\n1 Andorra    NA        1800  NA     NA      NA      NA      NA      3.46   NA   \n2 Afghanis…   0        1919   4.54   2.12    0       0.993   0     -0.0849  0.15\n3 Angola      0.333    1975   5.63   0.405   0.687   0       0.198  0.644   0.35\n4 Albania     0.667    1912  NA      1.99   NA      NA      NA      1.68    0.75\n5 United A…   0.333    1971  NA      0       0.004   0.949   0.003  3.37    0.1 \n6 Argentina   0        1816   4.23  -2.21    0.916   0.002   0.027  2.71    0.9 \n# … with abbreviated variable names ¹​consfirstaug, ²​lpd1500s, ³​rel_catho80,\n#   ⁴​rel_muslim80, ⁵​rel_protmg80\n\n\n\n\nCode\n# remove duplicates:\n\nduplicates &lt;- duplicated(data_cln)\nduplicates[\"TRUE\"]\n\n\n[1] NA\n\n\n\n\nCode\n# remove blank observations (observations with some NAs are not removed):\n\ndata_blank &lt;- data_cln[rowSums(is.na(data_cln)) != ncol(data_cln), ]\nhead(data_blank)\n\n\n# A tibble: 6 × 10\n  country   consf…¹ indyear logem4 lpd15…² rel_c…³ rel_m…⁴ rel_p…⁵  growth democ\n  &lt;chr&gt;       &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;\n1 Andorra    NA        1800  NA     NA      NA      NA      NA      3.46   NA   \n2 Afghanis…   0        1919   4.54   2.12    0       0.993   0     -0.0849  0.15\n3 Angola      0.333    1975   5.63   0.405   0.687   0       0.198  0.644   0.35\n4 Albania     0.667    1912  NA      1.99   NA      NA      NA      1.68    0.75\n5 United A…   0.333    1971  NA      0       0.004   0.949   0.003  3.37    0.1 \n6 Argentina   0        1816   4.23  -2.21    0.916   0.002   0.027  2.71    0.9 \n# … with abbreviated variable names ¹​consfirstaug, ²​lpd1500s, ³​rel_catho80,\n#   ⁴​rel_muslim80, ⁵​rel_protmg80\n\n\n\n\nCode\n# remove some NAs for description but not analysis:\n\ndata_NA &lt;- data_cln[rowSums(is.na(data_cln)) == 0, ]\ndim(data_NA)\n\n\n[1] 76 10\n\n\n\n\nCode\n# confirm data frame size of clean data set:\n\ndim(data_cln)\n\n\n[1] 173  10\n\n\nWe can see that this data set has 10 variables and 173 observations (though there will be 12 variables once I collect and add data related to education and non-religious affiliation). There are no duplicate observations, nor are there any blank observations. However, in the case that we remove observations with any missing values, the data set would only have 76 observations. Nonetheless, because the study’s authors elected to utilize incomplete observations, I will do the same.\n\n\nCode\n# summary of data (remove categorical variables):\n\nlibrary(summarytools)\n\n\nWarning: package 'summarytools' was built under R version 4.2.2\n\n\n\nAttaching package: 'summarytools'\n\n\nThe following object is masked from 'package:tibble':\n\n    view\n\n\nCode\nsummary &lt;- subset(data_cln, select = -c(country))\ndfSummary(summary)\n\n\nData Frame Summary  \nsummary  \nDimensions: 173 x 9  \nDuplicates: 1  \n\n--------------------------------------------------------------------------------------------------------------\nNo   Variable       Stats / Values            Freqs (% of Valid)    Graph                 Valid      Missing  \n---- -------------- ------------------------- --------------------- --------------------- ---------- ---------\n1    consfirstaug   Mean (sd) : 0.4 (0.4)     38 distinct values    :                     150        23       \n     [numeric]      min &lt; med &lt; max:                                :     :           .   (86.7%)    (13.3%)  \n                    0 &lt; 0.3 &lt; 1                                     :     :           :                       \n                    IQR (CV) : 0.7 (0.9)                            : .   :           :                       \n                                                                    : : . : . . .     :                       \n\n2    indyear        Mean (sd) : 1911.8 (67)   65 distinct values                    :     173        0        \n     [numeric]      min &lt; med &lt; max:                                .               : :   (100.0%)   (0.0%)   \n                    1800 &lt; 1947 &lt; 1984                              :               : :                       \n                    IQR (CV) : 134 (0)                              :           . . : :                       \n                                                                    : : . .   . : : : :                       \n\n3    logem4         Mean (sd) : 4.6 (1.3)     43 distinct values            :             88         85       \n     [numeric]      min &lt; med &lt; max:                                        :             (50.9%)    (49.1%)  \n                    0.9 &lt; 4.5 &lt; 8                                           : :                               \n                    IQR (CV) : 1.4 (0.3)                                    : :                               \n                                                                        : : : : : .                           \n\n4    lpd1500s       Mean (sd) : 1.1 (1.6)     98 distinct values            :             151        22       \n     [numeric]      min &lt; med &lt; max:                                        :             (87.3%)    (12.7%)  \n                    -3.8 &lt; 1.1 &lt; 5.6                                        : : :                             \n                    IQR (CV) : 2.2 (1.4)                                  : : : : :                           \n                                                                      .   : : : : : .                         \n\n5    rel_catho80    Mean (sd) : 0.3 (0.4)     107 distinct values   :                     152        21       \n     [numeric]      min &lt; med &lt; max:                                :                     (87.9%)    (12.1%)  \n                    0 &lt; 0.1 &lt; 1                                     :                                         \n                    IQR (CV) : 0.6 (1.1)                            :                 .                       \n                                                                    : : . : . .     . :                       \n\n6    rel_muslim80   Mean (sd) : 0.2 (0.4)     85 distinct values    :                     152        21       \n     [numeric]      min &lt; med &lt; max:                                :                     (87.9%)    (12.1%)  \n                    0 &lt; 0 &lt; 1                                       :                                         \n                    IQR (CV) : 0.4 (1.5)                            :                                         \n                                                                    : .     .       . :                       \n\n7    rel_protmg80   Mean (sd) : 0.1 (0.2)     80 distinct values    :                     151        22       \n     [numeric]      min &lt; med &lt; max:                                :                     (87.3%)    (12.7%)  \n                    0 &lt; 0 &lt; 1                                       :                                         \n                    IQR (CV) : 0.2 (1.7)                            :                                         \n                                                                    : . . . .                                 \n\n8    growth         Mean (sd) : 2 (1.1)       143 distinct values               :         172        1        \n     [numeric]      min &lt; med &lt; max:                                    .   :   :   .     (99.4%)    (0.6%)   \n                    -0.6 &lt; 1.9 &lt; 4.3                                    : : :   :   :                         \n                    IQR (CV) : 1.6 (0.5)                                : : : : : . :                         \n                                                                      : : : : : : : : .                       \n\n9    democ          Mean (sd) : 0.7 (0.3)     21 distinct values                    . :   135        38       \n     [numeric]      min &lt; med &lt; max:                                                : :   (78.0%)    (22.0%)  \n                    0 &lt; 0.8 &lt; 1                                       .             : :                       \n                    IQR (CV) : 0.6 (0.5)                            . :   .       : : :                       \n                                                                    : : : : . : . : : :                       \n--------------------------------------------------------------------------------------------------------------\n\n\nWe can see here a data frame containing summary statistics for the 9 variables with numeric data, as the categorical variable simply indicates the country name. These statistics will be more meaningful upon following the addition of data related to education and non-religious affiliation.\n\n\nSources:\n\nURL for data: https://www.openicpsr.org/openicpsr/project/113251/version/V1/view?path=/openicpsr/113251/fcr:versions/V1/Income-and-Democracy-Data-AER-adjustment.xls&type=file\nURL for study: http://homepage.ntu.edu.tw/~kslin/macro2009/Acemoglu%20et%20al%202008.pdf\nURL for external references: https://www.jstor.org/stable/10.1086/250107"
  },
  {
    "objectID": "posts/Derian-Toth_FinalProjectProposal.html",
    "href": "posts/Derian-Toth_FinalProjectProposal.html",
    "title": "Final Project Proposal Check-in1",
    "section": "",
    "text": "Code\n#reading in data\njobs_gender &lt;- readr::read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-03-05/jobs_gender.csv\")\n\n\nRows: 2088 Columns: 12\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): occupation, major_category, minor_category\ndbl (9): year, total_workers, workers_male, workers_female, percent_female, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nCode\nearnings_female &lt;- readr::read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-03-05/earnings_female.csv\") \n\n\nRows: 264 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): group\ndbl (2): Year, percent\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nCode\nemployed_gender &lt;- readr::read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-03-05/employed_gender.csv\") \n\n\nRows: 49 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (7): year, total_full_time, total_part_time, full_time_female, part_time...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nCode\n#View(employed_gender)\n#View(jobs_gender)\n#View(earnings_female)\n\n#bringing in libraries\nlibrary(dplyr)\n\n\n\nAttaching package: 'dplyr'\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nCode\nlibrary(vtable)\n\n\nLoading required package: kableExtra\n\nAttaching package: 'kableExtra'\n\nThe following object is masked from 'package:dplyr':\n\n    group_rows\n\n\nCode\nlibrary(ggplot2)\nlibrary(readr)\nlibrary(tidyverse)\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ lubridate 1.9.2     ✔ tibble    3.1.8\n✔ purrr     1.0.1     ✔ tidyr     1.3.0\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter()          masks stats::filter()\n✖ kableExtra::group_rows() masks dplyr::group_rows()\n✖ dplyr::lag()             masks stats::lag()\nℹ Use the \u001b]8;;http://conflicted.r-lib.org/\u0007conflicted package\u001b]8;;\u0007 to force all conflicts to become errors\n\n\nCode\nlibrary(stringr)\nlibrary(lubridate)"
  },
  {
    "objectID": "posts/Derian-Toth_FinalProjectProposal.html#introduction",
    "href": "posts/Derian-Toth_FinalProjectProposal.html#introduction",
    "title": "Final Project Proposal Check-in1",
    "section": "Introduction",
    "text": "Introduction\nThis research exploration takes a close look at the gender wage gap over the years. While the wage gap has decreased, women are still making on average, about 82 cents for every dollar men make (Aragao, 2023). Women are entering the workforce at a higher rate than in the past, yet are still not making the same salary as men who work the same jobs.\nFrom the two line graphs below we can see that women 16 years and older, working full time, are increasingly entering the workforce (Graph 1: Percent of Men and Women Working Full Time by Year); yet women are still not making 100% of the wage that men make (Graph 2: Female Salary Percent of Male Salary).\n\n\nCode\n#percent of women working full time compared to men\n#Still would like to add a legend to this graphs\nggplot(data = employed_gender, aes(x = year)) +\n  geom_line(aes(y = full_time_female), colour = \"red\") +\n  geom_point(aes(y = full_time_female), colour = \"red\") +\n  geom_line(aes(y = full_time_male), colour = \"blue\") +\n  geom_point(aes(y = full_time_male), colour = \"blue\") +\n  labs(title = \"Percent of Men and Women Working Full Time by Year\", \n       x=\"Year\", \n       y=\"Percent Working\") +\n  ylim(0,100)\n\n\n\n\n\nCode\n#wage gap by year\n#Would also like to add a legend and labels to the this graphs\nearnings_female%&gt;%\n  filter(str_detect(group,\"Total, 16 years and older\")) %&gt;%\n  ggplot(aes(x = Year)) +\n  geom_line(aes(y = percent)) +\n  geom_point(aes(y = percent)) +\n  labs(title = \"Female Salary Percent of Male Salary\", \n       x=\"Year\", \n       y=\"Percent of Salary\") +\n  ylim(0,100)\n\n\n\n\n\n\nThe Data set\nThe data used in this exploration and analysis come from the Bureau of Labor Statistics and the Census Bureau. These data describe different variables about women in the workforce across time. The three data frames are: (1) historical data, providing the percent of earnings women make compared to men, broken down by age group and ranging from 1979 - 2011, (2) Another historical dataset providing the workforce information (percent of women and men working full time and part time), by year, ranging from 1968 - 2016, (3) and lastly, detailed data regarding occupation (including two levels of categorization for the occupation), earnings for those occupations by gender, and percent of earnings women make compared to men, ranging from 2013 - 2016.\nThe tables below provide the descriptive statistics for the dependent variable, womens’ wage percent of male wage. The first table is the dependent variable across occupations and year (from 2013 to 2016). The second table below show the dependent variable for women who are 16 years or older, across many years (1979 - 2011).\n\n\nCode\n#Descriptive Statistics for the dependent variable: wage percent of male \nSumWage&lt;-data_frame(jobs_gender$year, jobs_gender$wage_percent_of_male)\n\n\nWarning: `data_frame()` was deprecated in tibble 1.1.0.\nℹ Please use `tibble()` instead.\n\n\nCode\nSumWage&lt;- SumWage%&gt;%\n  rename('Year'='jobs_gender$year',\n         'wage_percent_of_male' = 'jobs_gender$wage_percent_of_male')\nsumtable(SumWage)  \n\n\n\nSummary Statistics\n\n\nVariable\nN\nMean\nStd. Dev.\nMin\nPctl. 25\nPctl. 75\nMax\n\n\n\n\nYear\n2088\n2014\n1.1\n2013\n2014\n2015\n2016\n\n\nwage_percent_of_male\n1242\n84\n9.4\n51\n78\n91\n117\n\n\n\n\n\n\n\nCode\n#Descriptive Statistics for the historical data of the Dependent variable: wage percent of male\nearnings_female %&gt;%\n  group_by(group) %&gt;%\n  filter(str_detect(group,\"Total, 16 years and older\")) %&gt;%\n  sumtable()\n\n\n\nSummary Statistics\n\n\nVariable\nN\nMean\nStd. Dev.\nMin\nPctl. 25\nPctl. 75\nMax\n\n\n\n\nYear\n33\n1995\n9.7\n1979\n1987\n2003\n2011\n\n\ngroup\n33\n\n\n\n\n\n\n\n\n... Total, 16 years and older\n33\n100%\n\n\n\n\n\n\n\npercent\n33\n74\n5.7\n62\n70\n79\n82\n\n\n\n\n\n\n\n\n\nResearch Question\nThe purpose of the following exploration and analysis is to answer the question: What are the contributing factors that lead to the gender wage gap to be greater or smaller? The factors explored in this particular analysis are age and occupation. Occupation is categorized into broad and detailed categories, and will be explored by comparing female dominated fields compared to male dominated fields.\nThese factors have been explored in the literature, though the question regarding age (Aragao, 2023; Blau & Kahn, 2016; Kochhar, 2023) has been more thoroughly explored than occupation (Wrohlich 2017). Wrohlick, 2017 found that there is less of a wage gap in the public sector when compared to the private sector. The intention of this analysis is to go beyond comparing occupation by private versus public and into male dominated versus female dominated. This analysis has been done considerably less and is an important piece of knowing where to target closing the wage gap.\n\n\nHypothesis\nAccording to the research, the wage gap for a woman widens as she gets older (Aragao, 2023; Blau & Kahn, 2016; Kochhar, 2023), and the wage gap is wider for women in the private sector compared to the public sector (Wrohlich 2017). Based on this literature, we hypothesize that younger woman and women working in traditionally female-dominated fields will experience a smaller wage gap than older women and those working in traditionally male-dominated fields.\n\nWage Gap by Age\nThe graph below shows the average wage gap across age groups. These data show that as women age, the wage gap widens, it takes a particular dip around 35 - 44 years old.\n\n\nCode\n#wage gap by age\nearnings_female %&gt;%\n  group_by(group) %&gt;%\n  filter(!str_detect(group,\"Total, 16 years and older\")) %&gt;%\n  summarize(Mean_Percent_salary = mean(percent))%&gt;%\n  ggplot(aes(group,Mean_Percent_salary)) +\n  geom_col(aes(fill = group)) +\n  labs(title = \"Female Salary Percent of Male Salary\",\n       x=\"Age Group\", \n       y=\"Average Percent of Salary\") +\n  geom_text(aes(y=Mean_Percent_salary, \n                label=sprintf(\"%0.2f\", round(Mean_Percent_salary, digits = 2)))) +\n  theme(axis.text.x = element_text(angle = 45, vjust = 0.5))\n\n\n\n\n\nOne hypothesis for this is that the younger generation is experiencing a more narrow wage gap. And in that case, we would expect to the see wage gap decrease across time.\nThe graph and table below shows the the wage gap across time using the same data. From these visualizations we can conclude that the wage gap did not decrease, and that as women age, the wage gap widens.\n\n\nCode\n#wage gap by year\nSummaryYear &lt;- jobs_gender %&gt;%\n  group_by(year)%&gt;%\n  drop_na(wage_percent_of_male)%&gt;%\n  summarise(Average_Percent_of_Salary = mean(wage_percent_of_male))\n\nkable(SummaryYear)\n\n\n\n\n\nyear\nAverage_Percent_of_Salary\n\n\n\n\n2013\n83.77102\n\n\n2014\n83.70873\n\n\n2015\n84.45372\n\n\n2016\n84.19526\n\n\n\n\n\n\n\nCode\njobs_gender %&gt;%\n  group_by(year)%&gt;%\n  drop_na(wage_percent_of_male)%&gt;%\n  summarise(Mean_wage_percent_of_males = mean(wage_percent_of_male))%&gt;%\n  ggplot(aes(fill = year, y = `Mean_wage_percent_of_males`, x=year)) +\n  geom_bar(position = \"dodge\", stat = \"identity\") +\n  labs(title = \"Female Salary Percent of Male Salary by Year\",\n       x=\"Year\", \n       y=\"Average Percent of Salary\") +\n  geom_text(aes(y=Mean_wage_percent_of_males, \n                label=sprintf(\"%0.2f\", round(Mean_wage_percent_of_males, digits = 2)))) +\n  theme(axis.text.x = element_text(angle = 45, vjust = 0.5))\n\n\n\n\n\n\n\nWage Gap by Occupation\nThe visualization below shows the average number of total female workers versus total male workers averaged across 3 year (2013- 2016), by minor job category. The table is sorted in order to see the categories of which women tend to work at the top.\n\n\nCode\n#creating a table of just total workers by gender and category\nJobsCategoryGen&lt;- data_frame(jobs_gender$year, jobs_gender$minor_category, jobs_gender$workers_male, jobs_gender$workers_female)\nJobsCategoryGen&lt;- JobsCategoryGen%&gt;%\n  rename('Year'='jobs_gender$year',\n         'Occupation Category (Minor)' = 'jobs_gender$minor_category',\n         'Total Male Worker' = 'jobs_gender$workers_male',\n         'Total Female Workers' = 'jobs_gender$workers_female')\n\nSummaryJobsCatGen &lt;- JobsCategoryGen%&gt;%\n  group_by(`Occupation Category (Minor)`)%&gt;%\n  summarise(AverageMaleWorkers = mean(`Total Male Worker`), \n            AverageFemaleWorkers = mean(`Total Female Workers`))\n\nkable(SummaryJobsCatGen[order(SummaryJobsCatGen$AverageFemaleWorkers, decreasing=TRUE),])\n\n\n\n\n\nOccupation Category (Minor)\nAverageMaleWorkers\nAverageFemaleWorkers\n\n\n\n\nEducation, Training, and Library\n138707.95\n336849.955\n\n\nSales and Related\n324403.33\n226652.667\n\n\nBuilding and Grounds Cleaning and Maintenance\n377726.67\n188702.792\n\n\nOffice and Administrative Support\n73641.55\n182680.466\n\n\nHealthcare Support\n28363.98\n169199.227\n\n\nManagement\n269406.44\n167326.808\n\n\nCommunity and Social Service\n87166.78\n148718.469\n\n\nHealthcare Practitioners and Technical\n54026.68\n139826.234\n\n\nLegal\n137979.85\n138897.400\n\n\nFood Preparation and Serving Related\n149773.50\n128210.135\n\n\nBusiness and Financial Operations\n96380.12\n113663.170\n\n\nPersonal Care and Service\n33068.60\n97494.512\n\n\nComputer and mathematical\n169581.77\n56163.156\n\n\nArts, Design, Entertainment, Sports, and Media\n56255.49\n41269.278\n\n\nMaterial Moving\n146246.23\n32834.804\n\n\nProtective Service\n113246.58\n27095.264\n\n\nProduction\n58897.09\n20524.166\n\n\nLife, Physical, and Social Science\n25409.11\n19563.148\n\n\nTransportation\n171202.30\n18166.075\n\n\nArchitecture and Engineering\n96430.32\n15617.167\n\n\nFarming, Fishing, and Forestry\n66716.75\n13853.938\n\n\nInstallation, Maintenance, and Repair\n105593.08\n3813.965\n\n\nConstruction and Extraction\n137969.88\n3553.678\n\n\n\n\n\n\n\nFrom this visualization, we can see the following are the top 10 occupation categories that are on average more dominated by women:\n(1) Education, Training, and Library\n(2) Sales and Related\n(3) Building and Grounds Cleaning and Maintenance\n(4) Office and Administrative Support\n(5) Healthcare Support\n(6) Management\n(7) Community and Social Service\n(8) Healthcare Practitioners and Technical\n(9) Legal\n(10) Food Preparation and Serving Related\nThe following four visualizations below shows the average percent of salary women make compared to men across occupation categories. The first set of categories in more broad than the second, these are the “Major Categories”, while the more detailed categories are referred to as “Minor Categories”. These categories were created in order to compare the wage gap across occupations in a more digestible way than job title alone.\nBelow we can see that there is a higher wage gap for women working in “Management, Business, and Financial” with women making 80.5% of what men make, and those in”Production, Transportation, and Material Moving” with women making 79% of what men make.\n\n\nCode\n#wage gap by major occupation category\n \nSummaryMajorCat &lt;- jobs_gender%&gt;%\n  group_by(major_category)%&gt;%\n  drop_na(wage_percent_of_male) %&gt;%\n  summarise(Average_Percent_of_Salary = mean(wage_percent_of_male))\n\n#I would like to round these numbers by 2 digits past the decimal, as well as title the table and rename the variables.\nkable(SummaryMajorCat[order(SummaryMajorCat$Average_Percent_of_Salary, SummaryMajorCat$major_category, decreasing=TRUE),])\n\n\n\n\n\nmajor_category\nAverage_Percent_of_Salary\n\n\n\n\nComputer, Engineering, and Science\n86.80344\n\n\nService\n86.56902\n\n\nEducation, Legal, Community Service, Arts, and Media\n86.24398\n\n\nHealthcare Practitioners and Technical\n86.00865\n\n\nNatural Resources, Construction, and Maintenance\n85.41490\n\n\nSales and Office\n83.81083\n\n\nManagement, Business, and Financial\n80.48905\n\n\nProduction, Transportation, and Material Moving\n79.17389\n\n\n\n\n\n\n\nCode\n#This graph still needs to be reformatted so the title of the graph and the title of legand do not overlap. \njobs_gender %&gt;%\n  group_by(major_category)%&gt;%\n  drop_na(wage_percent_of_male)%&gt;%\n  summarise(Mean_wage_percent_of_males = mean(wage_percent_of_male))%&gt;%\n  ggplot(aes(fill = major_category, y = `Mean_wage_percent_of_males`, x=major_category)) +\n  geom_bar(position = \"dodge\", stat = \"identity\") +\n  labs(title = \"Female Salary Percent of Male Salary by Major Occupation Category\",\n       x=\"Occupation Cateogry\", \n       y=\"Average Percent of Salary\") +\n  geom_text(aes(y=Mean_wage_percent_of_males, \n                label=sprintf(\"%0.2f\", round(Mean_wage_percent_of_males, digits = 1)))) +\n  theme(axis.text.x = element_text(angle = 45, vjust = 0.5))\n\n\n\n\n\nWhen we break these categories down into more detail, we can see more instances of a wider wage gap. Women who work in occupations that fall into the “Community and Social Services” category make, on average, 91% of every dollar a man in their field makes. Whereas a woman working in Production” makes, on average, 77% of every dollar a man in their field makes. There seems to be a bigger increase in wage gap for the following occupational categories: “Management”, “Business and Financial Operations”, “Transportation”, “Farming, Fishing, and Forestry”, “Building and Grounds Cleaning and Maintenance”, “Sales and Related”, “Legal”, and “Production”.\n\n\nCode\n#wagegap by minor occupation category\nSummaryMinorCat &lt;- jobs_gender%&gt;%\n  group_by(minor_category)%&gt;%\n  drop_na(wage_percent_of_male) %&gt;%\n  summarise(Mean_wage_percent_of_males = mean(wage_percent_of_male))\n\nkable(SummaryMinorCat[order(SummaryMinorCat$Mean_wage_percent_of_males, SummaryMinorCat$minor_category, decreasing=FALSE),])\n\n\n\n\n\nminor_category\nMean_wage_percent_of_males\n\n\n\n\nProduction\n77.23993\n\n\nLegal\n77.81342\n\n\nSales and Related\n77.83065\n\n\nBuilding and Grounds Cleaning and Maintenance\n79.31545\n\n\nFarming, Fishing, and Forestry\n79.61618\n\n\nTransportation\n79.88060\n\n\nBusiness and Financial Operations\n80.16418\n\n\nManagement\n80.80490\n\n\nArts, Design, Entertainment, Sports, and Media\n84.79955\n\n\nLife, Physical, and Social Science\n84.94294\n\n\nInstallation, Maintenance, and Repair\n85.45982\n\n\nHealthcare Practitioners and Technical\n86.00865\n\n\nProtective Service\n86.02423\n\n\nOffice and Administrative Support\n86.18804\n\n\nPersonal Care and Service\n86.73739\n\n\nEducation, Training, and Library\n87.19622\n\n\nFood Preparation and Serving Related\n87.44786\n\n\nArchitecture and Engineering\n87.59434\n\n\nConstruction and Extraction\n87.94221\n\n\nComputer and mathematical\n88.12798\n\n\nMaterial Moving\n89.10787\n\n\nHealthcare Support\n90.31624\n\n\nCommunity and Social Service\n91.43559\n\n\n\n\n\n\n\nCode\n#The visualization below is in VERY draft form, and cannot provide helpful information until it is reformatted. \n#jobs_gender %&gt;%\n  #group_by(minor_category)%&gt;%\n  #drop_na(wage_percent_of_male)%&gt;%\n  #summarise(Mean_wage_percent_of_males = mean(wage_percent_of_male))%&gt;%\n  #ggplot(aes(fill = minor_category, y = `Mean_wage_percent_of_males`, x=minor_category)) +\n  #geom_bar(position = \"dodge\", stat = \"identity\") +\n  #labs(title = \"Female Salary Percent of Male Salary by Minor Occupation Category\",\n       #x=\"Occupation Cateogry\", \n       #y=\"Average Percent of Salary\") +\n  #theme(axis.text.x = element_text(angle = 45, vjust = 0.5))\n\n\nWhen comparing the top 10 minor occupation categories that are on average more dominated by women (“Occupation Categories Dominated by Women”) with the list of occupation categories with the smallest wage gap (“Occupation Categories with the Smallest Wage Gap”), we can see some overlap.\nOccupation Categories Dominated by Women\n\nEducation, Training, and Library\nSales and Related\nBuilding and Grounds Cleaning and Maintenance\nOffice and Administrative Support\nHealthcare Support\nManagement\nCommunity and Social Service\nHealthcare Practitioners and Technical\nLegal\nFood Preparation and Serving Related\n\nOccupation Categories with the Smallest Wage Gap\n\nProduction\nLegal\nSales and Related\nBuilding and Grounds Cleaning and Maintenance\nFarming, Fishing, and Forestry\nTransportation\nBusiness and Financial Operations\nManagement\nArts, Design, Entertainment, Sports, and Media\nLife, Physical, and Social Science\n\nFurther analysis is necessary to know if female dominated occupation categories is a statistically significant factor that contributes to the wage gap."
  },
  {
    "objectID": "posts/Homework1_AlexaPotter.html",
    "href": "posts/Homework1_AlexaPotter.html",
    "title": "Homework 1",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\n\n-- Attaching packages --------------------------------------- tidyverse 1.3.2 --\nv ggplot2 3.4.0      v purrr   0.3.5 \nv tibble  3.1.8      v dplyr   1.0.10\nv tidyr   1.2.1      v stringr 1.5.0 \nv readr   2.1.3      v forcats 0.5.2 \n-- Conflicts ------------------------------------------ tidyverse_conflicts() --\nx dplyr::filter() masks stats::filter()\nx dplyr::lag()    masks stats::lag()\n\n\nCode\nlibrary(ggplot2)\nlibrary(formattable)"
  },
  {
    "objectID": "posts/Homework1_AlexaPotter.html#a",
    "href": "posts/Homework1_AlexaPotter.html#a",
    "title": "Homework 1",
    "section": "a",
    "text": "a\nFirst, let’s read in the data from the Excel file:\n\n\nCode\nlibrary(readxl)\ndf &lt;- read_excel(\"_data/LungCapData.xls\")\n\n\nThe distribution of LungCap looks as follows:\n\n\nCode\nhist(df$LungCap, xlab= 'LungCap', main = 'Histogram of LungCap Distribution')\n\n\n\n\n\nThe histogram suggests that the distribution is close to a normal distribution. Most of the observations are close to the mean. Very few observations are close to the margins (0 and 15).\n##b\nCompare the probability distribution of the LungCap with respect to Males and Females:\n\n\nCode\nboxplot(df$LungCap~df$Gender, xlab = 'Gender', ylab = 'LungCap')\n\n\n\n\n\n##c\nCompare the mean lung capacities for smokers and non-smokers. Does it make sense?\n\n\nCode\nLungCap_mean &lt;- df %&gt;% \n  group_by(Smoke) %&gt;%\n  summarise(Lungcap_Mean = mean(LungCap))\n\nLungCap_mean\n\n\n# A tibble: 2 x 2\n  Smoke Lungcap_Mean\n  &lt;chr&gt;        &lt;dbl&gt;\n1 no            7.77\n2 yes           8.65\n\n\nThese results show the lung capacity for non-smokers is lower than the lung capacity for smokers. With no background knowledge of lung capacity, I would think these are not standard results.\n##d)\nExamine the relationship between Smoking and Lung Capacity within age groups: “less than or equal to 13”, “14 to 15”, “16 to 17”, and “greater than or equal to 18”.\n\n\nCode\ndf_Agegroup &lt;- df %&gt;% \n  mutate(\n    Age_group = dplyr::case_when(\n      Age &lt;= 13            ~ \"0-13\",\n      Age &gt; 13 & Age &lt;= 15 ~ \"14-15\",\n      Age &gt; 15 & Age &lt;= 17 ~ \"16-17\",\n      Age &gt;= 18             ~ \"18+\"))\n\n\nggplot(data = df_Agegroup, aes(x=Age_group, y=LungCap)) + \n  geom_boxplot(aes(fill=Gender))\n\n\n\n\n\n##e)\nCompare the lung capacities for smokers and non-smokers within each age group. Is your answer different from the one in part c. What could possibly be going on here?\n\n\nCode\nggplot(data = df_Agegroup, aes(x=Age_group, y=LungCap)) + \n  geom_boxplot(aes(fill=Smoke))\n\n\n\n\n\nThis data is different from what was seen in part C. The only age group displaying greater lung capacity for smokers is the group 0-13. This would suggest this sample of the population is influencing the overall lung capacity mean. This by far is the largest sample of age groups with 428 participants\n\n\nCode\ncount(df_Agegroup, Age_group, Smoke)\n\n\n# A tibble: 8 x 3\n  Age_group Smoke     n\n  &lt;chr&gt;     &lt;chr&gt; &lt;int&gt;\n1 0-13      no      401\n2 0-13      yes      27\n3 14-15     no      105\n4 14-15     yes      15\n5 16-17     no       77\n6 16-17     yes      20\n7 18+       no       65\n8 18+       yes      15"
  },
  {
    "objectID": "posts/SidPost.html",
    "href": "posts/SidPost.html",
    "title": "Blog Post Template",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\nknitr::opts_chunk$set(echo = TRUE)\n\n\nTEST"
  },
  {
    "objectID": "posts/HW2_Saisrinivas_ Ambatipudi.html",
    "href": "posts/HW2_Saisrinivas_ Ambatipudi.html",
    "title": "Homework 2",
    "section": "",
    "text": "Code\ns_p &lt;- c(\"Bypass\", \"Angiography\")\ns_s &lt;- c(539, 847)\nm_w_t &lt;- c(19, 18)\ns_d &lt;- c(10, 9)\n\nsurgery &lt;- data.frame(s_p, s_s, m_w_t, s_d)\n\n# 90% confidence interval\n\nc_l &lt;- 0.90\n\n# Tail area\n\nt_a &lt;- (1-c_l)/2\n\n# t_score Calculation\n\nt_s &lt;- qt(p = 1-t_a, df = s_s-1)\n\n# Standard Error Calculation\n\ns_e &lt;- s_d/sqrt(s_s)\n\n# Confidence Interval Calculation for Bypass and Angiography\n\nc_i &lt;- c(m_w_t - t_s * s_e,\n        m_w_t + t_s * s_e)\n\nc_i\n\n\n[1] 18.29029 17.49078 19.70971 18.50922\n\n\nThe Cardiac Care Network analyzed wait times for cardiac procedures in Ontario, constructing a 90% confidence interval using the t-distribution. For bypass surgery, the true mean wait time is estimated to be between 18.29 and 19.71 days, while for angiography it is between 17.49 and 18.51 days. The bypass surgery interval is slightly wider, indicating more uncertainty in the estimate. These confidence intervals help inform decisions on resource allocation and patient care by indicating the likely range of true mean wait times."
  },
  {
    "objectID": "posts/HW2_Saisrinivas_ Ambatipudi.html#q1",
    "href": "posts/HW2_Saisrinivas_ Ambatipudi.html#q1",
    "title": "Homework 2",
    "section": "",
    "text": "Code\ns_p &lt;- c(\"Bypass\", \"Angiography\")\ns_s &lt;- c(539, 847)\nm_w_t &lt;- c(19, 18)\ns_d &lt;- c(10, 9)\n\nsurgery &lt;- data.frame(s_p, s_s, m_w_t, s_d)\n\n# 90% confidence interval\n\nc_l &lt;- 0.90\n\n# Tail area\n\nt_a &lt;- (1-c_l)/2\n\n# t_score Calculation\n\nt_s &lt;- qt(p = 1-t_a, df = s_s-1)\n\n# Standard Error Calculation\n\ns_e &lt;- s_d/sqrt(s_s)\n\n# Confidence Interval Calculation for Bypass and Angiography\n\nc_i &lt;- c(m_w_t - t_s * s_e,\n        m_w_t + t_s * s_e)\n\nc_i\n\n\n[1] 18.29029 17.49078 19.70971 18.50922\n\n\nThe Cardiac Care Network analyzed wait times for cardiac procedures in Ontario, constructing a 90% confidence interval using the t-distribution. For bypass surgery, the true mean wait time is estimated to be between 18.29 and 19.71 days, while for angiography it is between 17.49 and 18.51 days. The bypass surgery interval is slightly wider, indicating more uncertainty in the estimate. These confidence intervals help inform decisions on resource allocation and patient care by indicating the likely range of true mean wait times."
  },
  {
    "objectID": "posts/HW2_Saisrinivas_ Ambatipudi.html#q2",
    "href": "posts/HW2_Saisrinivas_ Ambatipudi.html#q2",
    "title": "Homework 2",
    "section": "Q2",
    "text": "Q2\n\n\nCode\nprop.test(567, 1031, conf.level = .95)\n\n\n\n    1-sample proportions test with continuity correction\n\ndata:  567 out of 1031, null probability 0.5\nX-squared = 10.091, df = 1, p-value = 0.00149\nalternative hypothesis: true p is not equal to 0.5\n95 percent confidence interval:\n 0.5189682 0.5805580\nsample estimates:\n        p \n0.5499515 \n\n\nThe National Center for Public Policy surveyed 1031 adult Americans, finding that 54.99% believe a college education is essential for success. A 95% confidence interval for this proportion is [0.5189682, 0.5805580], suggesting that between 51.90% and 58.06% of adult Americans hold this belief. Since the interval doesn’t include 0.5, we can conclude that the proportion is significantly different from 0.5 at the 0.05 significance level, indicating a majority of adult Americans believe in the importance of a college education."
  },
  {
    "objectID": "posts/HW2_Saisrinivas_ Ambatipudi.html#q3",
    "href": "posts/HW2_Saisrinivas_ Ambatipudi.html#q3",
    "title": "Homework 2",
    "section": "Q3",
    "text": "Q3\n\n\nCode\n# Define variables\nM_E &lt;- 5\ns_d &lt;- (200-30)/4\na &lt;- 0.05\nz_a&lt;- qnorm(p = 1-a/2, lower.tail = FALSE)\n\n\n# Calculate required sample size\n\ns_s &lt;- ceiling(((z_a * s_d) / M_E)^2)\n\n\n# Required Sample Size, Round up to nearest integer\n\ncat(\"The required sample size is:\", s_s)\n\n\nThe required sample size is: 278\n\n\nUMass Amherst’s financial aid office aims to estimate the mean cost of textbooks per semester within $5, using a $10 or less confidence interval. Assuming a population standard deviation of a quarter of the range ($30-$200), they calculate a required sample size of 278 students for a 95% confidence interval. Accurate estimates are vital for determining appropriate financial assistance for textbooks, ensuring student academic success and financial well-being."
  },
  {
    "objectID": "posts/HW2_Saisrinivas_ Ambatipudi.html#q4",
    "href": "posts/HW2_Saisrinivas_ Ambatipudi.html#q4",
    "title": "Homework 2",
    "section": "Q4",
    "text": "Q4\n\nQ4-A\nWe assume that the sample is random and that the population has a normal distribution. Null hypothesis: H0: μ = 500 Alternative hypothesis: Ha: μ ≠ 500 We will reject the null hypothesis at a p-value, p &lt;= 0.05\n\n\nCode\ns_m &lt;- 410 # sample mean\nmu &lt;- 500 # population mean\ns_d &lt;- 90 # standard deviation\ns_s &lt;- 9 # sample size\n\n# Calculating test-statistic\n\nt_s &lt;- (s_m-mu)/(s_d/sqrt(s_s))\n\ncat(\"test-statistic:\", t_s, '\\n')\n\n\ntest-statistic: -3 \n\n\nCode\np_v &lt;- 2 * pt(t_s, df = s_s - 1, lower.tail = TRUE)\n\ncat(\"p value :\", p_v)\n\n\np value : 0.01707168\n\n\nWe investigate if the mean income of nine randomly selected female employees in a large service company differs from the $500 per week union agreement. With a sample mean of $410 and a standard deviation of $90, we conduct a hypothesis test at a 0.05 significance level. The resulting p-value of 0.01707168 is less than 0.05, leading us to reject the null hypothesis and conclude that the mean income of female employees significantly differs from $500 per week.\n\n\nQ4-B\nWe assume that the sample is random and that the population has a normal distribution. Null hypothesis: H0: μ = 500 Alternative hypothesis: Ha: μ &lt; 500 We will reject the null hypothesis at a p-value less than 0.05\n\n\nCode\np &lt;- pt(t_s, s_s-1, lower.tail = TRUE)\np\n\n\n[1] 0.008535841\n\n\nA one-tailed t-test yielded a p-value of 0.008535841, which is less than the 0.05 significance level. We reject the null hypothesis, concluding that female employees in this service company earn less than $500 per week, implying they are paid less than senior-level workers per the union agreement.\n\n\nQ4-C\nWe assume that the sample is random and that the population has a normal distribution. Null hypothesis: H0: μ = 500 Alternative hypothesis: Ha: μ &gt; 500 We will reject the null hypothesis at a p-value less than 0.05\n\n\nCode\np &lt;- pt(t_s, s_s-1, lower.tail = FALSE)\np\n\n\n[1] 0.9914642\n\n\nWith a p-value of 0.9914642, greater than the 0.05 significance level, we fail to reject the null hypothesis. There is insufficient evidence to support the claim that female employees earn more than $500 per week, meaning we cannot conclude they earn significantly more than the agreed-upon mean income for senior-level workers."
  },
  {
    "objectID": "posts/HW2_Saisrinivas_ Ambatipudi.html#q5",
    "href": "posts/HW2_Saisrinivas_ Ambatipudi.html#q5",
    "title": "Homework 2",
    "section": "Q5",
    "text": "Q5\n\nQ5-A\nWe assume that the sample is random and that the population has a normal distribution. Null hypothesis: H0: μ = 500 Alternative hypothesis: Ha: μ ≠ 500 We will reject the null hypothesis at a p-value less than 0.05\n\n\nCode\ns_m &lt;- 519.5\nmu &lt;- 500\ns_e &lt;- 10\ns_s &lt;- 1000\n\nt_s_j &lt;- (s_m-mu)/(s_e)\nt_s_j\n\n\n[1] 1.95\n\n\nCode\np &lt;- 2*pt(t_s_j, s_s-1, lower.tail = FALSE)\np\n\n\n[1] 0.05145555\n\n\n\n\nCode\ns_m &lt;- 519.7\nmu &lt;- 500\ns_e &lt;- 10\ns_s &lt;- 1000\n\nt_s_s &lt;- (s_m-mu)/(s_e)\nt_s_s\n\n\n[1] 1.97\n\n\nCode\np &lt;- 2*pt(t_s_s, s_s-1, lower.tail = FALSE)\np\n\n\n[1] 0.04911426\n\n\nJones obtained a test-statistic of 1.95 and a p-value of 0.05145555, whereas Smith achieved a test-statistic of 1.97 with the same p-value of 0.05145555.\n\n\nQ5-B\nBased on the given information, the result is statistically significant for Smith, but not for Jones.\nFor Jones, the p-value (0.05145555) is greater than the significance level (α = 0.05), which means that we fail to reject the null hypothesis. This indicates that the result is not statistically significant for Jones.\nFor Smith, the p-value (0.04911426) is less than the significance level (α = 0.05), which means that we reject the null hypothesis. This indicates that the result is statistically significant for Smith.\n\n\nQ5-C\nReporting results as “P ≤ 0.05” or “P &gt; 0.05” without providing the actual P-value can mask the degree of uncertainty in the findings. In the Jones/Smith example, such reporting could lead to different conclusions for very similar results. It is important to report the actual P-value to accurately interpret the results and understand the strength of the conclusion."
  },
  {
    "objectID": "posts/HW2_Saisrinivas_ Ambatipudi.html#q6",
    "href": "posts/HW2_Saisrinivas_ Ambatipudi.html#q6",
    "title": "Homework 2",
    "section": "Q6",
    "text": "Q6\nTo examine if the proportion of students selecting healthy snacks varies by grade level, we employ the chi-squared test of independence. The null hypothesis assumes the same proportion across all grades, while the alternative hypothesis posits differing proportions based on grade level.\n\n\nCode\n# Create the contingency table\ns_t &lt;- matrix(c(31, 43, 51, 69, 57, 49), nrow = 2, byrow = TRUE)\nrownames(s_t) &lt;- c(\"Healthy snack\", \"Unhealthy snack\")\ncolnames(s_t) &lt;- c(\"6th grade\", \"7th grade\", \"8th grade\")\ns_t\n\n\n                6th grade 7th grade 8th grade\nHealthy snack          31        43        51\nUnhealthy snack        69        57        49\n\n\nCode\n# Conduct the chi-square test of independence\nchisq.test(s_t)\n\n\n\n    Pearson's Chi-squared test\n\ndata:  s_t\nX-squared = 8.3383, df = 2, p-value = 0.01547\n\n\nThe chi-square test statistic is 8.3383 with 2 degrees of freedom, and a p-value of 0.01547, which is less than the 0.05 significance level. We reject the null hypothesis, concluding that there is a significant association between snack choice and grade level among surveyed students.\nMore 6th-graders chose healthy snacks compared to 7th and 8th-graders, while more 8th-graders chose unhealthy snacks. This suggests grade level influences snack choice, and health promotion efforts should target specific grades with higher unhealthy snack choices. These findings may also guide further research on factors affecting middle school students’ snack choices."
  },
  {
    "objectID": "posts/HW2_Saisrinivas_ Ambatipudi.html#q7",
    "href": "posts/HW2_Saisrinivas_ Ambatipudi.html#q7",
    "title": "Homework 2",
    "section": "Q7",
    "text": "Q7\nTo test the claim that there is a difference in means for the three areas, we will use a one-way ANOVA test. The null hypothesis is that the means for the three areas are the same. The alternative hypothesis is that at least one area’s mean is different from the others.\n\n\nCode\n# Data in vectors\narea1 &lt;- c(6.2, 9.3, 6.8, 6.1, 6.7, 7.5)\narea2 &lt;- c(7.5, 8.2, 8.5, 8.2, 7.0, 9.3)\narea3 &lt;- c(5.8, 6.4, 5.6, 7.1, 3.0, 3.5)\n\n# One-way ANOVA test\ntest &lt;- aov(c(area1, area2, area3) ~ factor(c(rep(\"Area 1\", length(area1)), rep(\"Area 2\", length(area2)), rep(\"Area 3\", length(area3)))))\nsummary(test)\n\n\n                                                                                                    Df\nfactor(c(rep(\"Area 1\", length(area1)), rep(\"Area 2\", length(area2)), rep(\"Area 3\", length(area3))))  2\nResiduals                                                                                           15\n                                                                                                    Sum Sq\nfactor(c(rep(\"Area 1\", length(area1)), rep(\"Area 2\", length(area2)), rep(\"Area 3\", length(area3))))  25.66\nResiduals                                                                                            23.54\n                                                                                                    Mean Sq\nfactor(c(rep(\"Area 1\", length(area1)), rep(\"Area 2\", length(area2)), rep(\"Area 3\", length(area3))))  12.832\nResiduals                                                                                             1.569\n                                                                                                    F value\nfactor(c(rep(\"Area 1\", length(area1)), rep(\"Area 2\", length(area2)), rep(\"Area 3\", length(area3))))   8.176\nResiduals                                                                                                  \n                                                                                                     Pr(&gt;F)\nfactor(c(rep(\"Area 1\", length(area1)), rep(\"Area 2\", length(area2)), rep(\"Area 3\", length(area3)))) 0.00397\nResiduals                                                                                                  \n                                                                                                      \nfactor(c(rep(\"Area 1\", length(area1)), rep(\"Area 2\", length(area2)), rep(\"Area 3\", length(area3)))) **\nResiduals                                                                                             \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nWith a p-value of 0.00397, which is below the significance level (α = 0.05), we reject the null hypothesis, concluding that there is a difference in means among the three areas."
  },
  {
    "objectID": "posts/Kristin_Abijaoude_HW1.html",
    "href": "posts/Kristin_Abijaoude_HW1.html",
    "title": "Hw 1 by Kristin Abijaoude",
    "section": "",
    "text": "Code\nlibrary(ggplot2)\nlibrary(readxl)\nlibrary(dplyr)\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nCode\nlibrary(summarytools)\nlibrary(stats)\n\n\n\nLung Capacity\n\n\nCode\nLungCapData &lt;- read_excel(\"~/Documents/GitHub/Github Help/603_Spring_2023/posts/_data/LungCapData.xls\")\nLungCapData\n\n\n# A tibble: 725 × 6\n   LungCap   Age Height Smoke Gender Caesarean\n     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;    \n 1    6.48     6   62.1 no    male   no       \n 2   10.1     18   74.7 yes   female no       \n 3    9.55    16   69.7 no    female yes      \n 4   11.1     14   71   no    male   no       \n 5    4.8      5   56.9 no    male   no       \n 6    6.22    11   58.7 no    female no       \n 7    4.95     8   63.3 no    male   yes      \n 8    7.32    11   70.4 no    male   no       \n 9    8.88    15   70.5 no    male   no       \n10    6.8     11   59.2 no    male   no       \n# … with 715 more rows\n\n\n\n\nCode\nprint(dfSummary(LungCapData,\n                        varnumbers = FALSE,\n                        plain.ascii  = FALSE, \n                        style        = \"grid\", \n                        graph.magnif = 0.70, \n                        valid.col    = FALSE),\n      method = 'render',\n      table.classes = 'table-condensed')\n\n\n\nData Frame Summary\nLungCapData\nDimensions: 725 x 6\n  Duplicates: 0\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nStats / Values\nFreqs (% of Valid)\nGraph\nMissing\n\n\n\n\nLungCap [numeric]\n\n\n\nMean (sd) : 7.9 (2.7)\n\n\nmin ≤ med ≤ max:\n\n\n0.5 ≤ 8 ≤ 14.7\n\n\nIQR (CV) : 3.7 (0.3)\n\n\n\n342 distinct values\n\n0 (0.0%)\n\n\nAge [numeric]\n\n\n\nMean (sd) : 12.3 (4)\n\n\nmin ≤ med ≤ max:\n\n\n3 ≤ 13 ≤ 19\n\n\nIQR (CV) : 6 (0.3)\n\n\n\n17 distinct values\n\n0 (0.0%)\n\n\nHeight [numeric]\n\n\n\nMean (sd) : 64.8 (7.2)\n\n\nmin ≤ med ≤ max:\n\n\n45.3 ≤ 65.4 ≤ 81.8\n\n\nIQR (CV) : 10.4 (0.1)\n\n\n\n274 distinct values\n\n0 (0.0%)\n\n\nSmoke [character]\n\n\n\n1. no\n\n\n2. yes\n\n\n\n\n\n\n648\n(\n89.4%\n)\n\n\n77\n(\n10.6%\n)\n\n\n\n\n0 (0.0%)\n\n\nGender [character]\n\n\n\n1. female\n\n\n2. male\n\n\n\n\n\n\n358\n(\n49.4%\n)\n\n\n367\n(\n50.6%\n)\n\n\n\n\n0 (0.0%)\n\n\nCaesarean [character]\n\n\n\n1. no\n\n\n2. yes\n\n\n\n\n\n\n561\n(\n77.4%\n)\n\n\n164\n(\n22.6%\n)\n\n\n\n\n0 (0.0%)\n\n\n\n\nGenerated by summarytools 1.0.1 (R version 4.2.2)2023-03-16\n\n\n\n\n\nCode\ncolnames(LungCapData)\n\n\n[1] \"LungCap\"   \"Age\"       \"Height\"    \"Smoke\"     \"Gender\"    \"Caesarean\"\n\n\nCode\ndim(LungCapData)\n\n\n[1] 725   6\n\n\n\n\nCode\nhist(LungCapData$LungCap)\n\n\n\n\n\n1a. The distribution looks pretty normal to me, with capacity between 6 and 9 being the most frequent.\n\n\nCode\nboxplot(LungCap ~ Gender, data=LungCapData)\n\n\n\n\n\n1b. Separating the two genders, it looks like men have a higher lung capacity rate in comparison to women.\n\n\nCode\nLungCapData %&gt;%\n  group_by(Smoke) %&gt;%\n  summarise(mean = mean(LungCap), n = n())\n\n\n# A tibble: 2 × 3\n  Smoke  mean     n\n  &lt;chr&gt; &lt;dbl&gt; &lt;int&gt;\n1 no     7.77   648\n2 yes    8.65    77\n\n\n1c. The average lung capacity for a non-smoker is around 7.78, while for smokers it’s 8.65. In other words, on average, the smokers have a higher lung capacity rate than non-smokers… this doesn’t make sense because smoking is supposed to be bad for your lungs.\n\n\nCode\nagegroup &lt;- LungCapData %&gt;%\n  mutate(agegroup = case_when(Age &lt;= 13 ~ \"Less than 13 years old\",\n                              Age == 14| Age == 15 ~ \"14 to 15 years old\",\n                              Age == 16 | Age == 17 ~ \"16 to 17 years old\",\n                              Age &gt;= 18 ~ \"18 years old and older\"))\nagegroup %&gt;%\n  ggplot(aes(x=LungCap, fill=Smoke)) +\n  geom_histogram() +\n  facet_wrap(~agegroup)\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n1d. Obviously, older teens are more likely to be smokers, as well as have higher lung capacity, than younger teens. The vast majority of teens 13 years and younger are non-smoker (I would be horrified at the sight of a kid smoking).\n\n\nCode\nagegroup &lt;- agegroup %&gt;%\n  mutate(AgeGroup = factor(agegroup, level= c(\"Less than 13 years old\", \n                                              \"14 to 15 years old\",\n                                              \"16 to 17 years old\",\n                                              \"18 years old and older\")))\n\nboxplot(LungCap ~ AgeGroup, data=agegroup)\n\n\n\n\n\n1e. There is a correlation between age and lung capacity. The lung capacity rate increases as the person gets older.\n\n\nPrior Convictions\nAnother dataset I created here deals with prison convictions. The sample size is 810 prisoners in a state prison, some of the prisoners are there for the first time, while others have been imprison as many as 4 times, or have 4 prior convictions in other words. prior means numbers of prior convictions. freq means how many prisoners have a set of convictions (434 prisoners have 1 prior convictions, 160 prisoners have 2 prior convictions etc.). Finally, I created a new variable called probability, where I divided the freq variable by the total number of prisoners, to denote the probability that a prisoner had a certain number of prior convictions.\n\n\nCode\ndf &lt;- data.frame(prior = c(0:4), \n                 freq = c(128, 434, 160, 64, 24)\n                 )\n\ndf &lt;- df %&gt;%\n  mutate(probability = freq/810)\ndf\n\n\n  prior freq probability\n1     0  128  0.15802469\n2     1  434  0.53580247\n3     2  160  0.19753086\n4     3   64  0.07901235\n5     4   24  0.02962963\n\n\n\n\nCode\n# alternatively\n(dbinom(x = 1, size = 1, prob = 160/810))*100\n\n\n[1] 19.75309\n\n\n2a. There is a less than 20% probability that a randomly selected inmate has exactly 2 prior convictions.\n\n\nCode\n128 + 434\n\n\n[1] 562\n\n\nCode\n(dbinom(x = 1, size = 1, prob = 562/810))*100\n\n\n[1] 69.38272\n\n\n2b. There is a 69% probability that a randomly selected inmate has fewer than 2 prior convictions.\n\n\nCode\n128 + 434 + 160\n\n\n[1] 722\n\n\nCode\n(dbinom(x = 1, size = 1, prob = 722/810))*100\n\n\n[1] 89.1358\n\n\n2c. There is a 89% probability that a randomly selected inmate has 2 or fewer prior convictions.\n\n\nCode\n64 + 24\n\n\n[1] 88\n\n\nCode\n(dbinom(x = 1, size = 1, prob = 88/810))*100\n\n\n[1] 10.8642\n\n\n2d. There is a 10% probability that a randomly selected inmate has more than 2 prior convictions.\n\n\nCode\nprior &lt;- df$prior\nprob &lt;- df$probability\nfreq &lt;- df$freq\n\nexval &lt;- sum(prior*prob)\nexval\n\n\n[1] 1.28642\n\n\n2e. The expected value exval, or long term mean, is 1.28642. I separated the variables into its own set and multiplied prior (# of prior convictions) and prob (the probability a given prisoner has a certain number of prior convictions).\n\n\nCode\n# variance\nvar(rep(df$prior, df$freq))\n\n\n[1] 0.8572937\n\n\nCode\n# standard deviation\nsd(rep(df$prior, df$freq))\n\n\n[1] 0.9259016\n\n\nThe variance is 0.86, which mean the data is close to one another.\nThe standard deviation is 0.93, which means the data is more clustered around the mean.\n\n\nCode\nrender(\"Kristin_Abijaoude_HW1.qmd\", output_format = \"pdf_document\", output_file = \"Kristin_Abijaoude_HW1.pdf\")\n\n\nError in render(\"Kristin_Abijaoude_HW1.qmd\", output_format = \"pdf_document\", : could not find function \"render\""
  },
  {
    "objectID": "posts/AlexisGamez_Project_Proposal.html",
    "href": "posts/AlexisGamez_Project_Proposal.html",
    "title": "Project Proposal",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(readr)\n\nknitr::opts_chunk$set(echo = T)"
  },
  {
    "objectID": "posts/AlexisGamez_Project_Proposal.html#description-and-summary-of-the-data",
    "href": "posts/AlexisGamez_Project_Proposal.html#description-and-summary-of-the-data",
    "title": "Project Proposal",
    "section": "Description and Summary of the Data",
    "text": "Description and Summary of the Data\nThis data set was pulled from the Kaggle online database and it’s description reads as follows, This data set contains a list of video games with sales greater than 100,000 copies along with critic and user ratings.\n\n\nCode\n# reading in our data set\nVideo_Game_Sales &lt;- read_csv(\"_data/final_project/Video_Game_Sales_as_of_Jan_2017.csv\")\nhead(Video_Game_Sales)\n\n\n# A tibble: 6 × 15\n  Name       Platform Year_of_Release Genre Publisher NA_Sales EU_Sales JP_Sales\n  &lt;chr&gt;      &lt;chr&gt;              &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;        &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 Wii Sports Wii                 2006 Spor… Nintendo      41.4    29.0      3.77\n2 Super Mar… NES                 1985 Plat… Nintendo      29.1     3.58     6.81\n3 Mario Kar… Wii                 2008 Raci… Nintendo      15.7    12.8      3.79\n4 Wii Sport… Wii                 2009 Spor… Nintendo      15.6    11.0      3.28\n5 Pokemon R… G                   1996 Role… Nintendo      11.3     8.89    10.2 \n6 Tetris     G                   1989 Puzz… Nintendo      23.2     2.26     4.22\n# … with 7 more variables: Other_Sales &lt;dbl&gt;, Global_Sales &lt;dbl&gt;,\n#   Critic_Score &lt;dbl&gt;, Critic_Count &lt;dbl&gt;, User_Score &lt;dbl&gt;, User_Count &lt;dbl&gt;,\n#   Rating &lt;chr&gt;\n\n\nWith this updated data set provided by the collector, we are given 15 variables and approximately 17,500 entries. The variables are as follows:\n\nName [game’s name]\nPlatform [platform of game release]\nYear of Release [game’s release date]\nGenre [genre of game]\nPublisher [publisher of game]\nNA Sales [sales in North America in millions]\nEU Sales [sales in Europe in millions]\nJPN Sales [sales in Japan in millions]\nOther Sales [sales in rest of the world in millions]\nGlobal Sales [total worldwide sales in millions]\nCritic Score [aggregate score compiled by Metacritic staff]\nCritic Count [the number of critis used in creating the critic score]\nUser Score [score according to Metacritic subscribers]\nUser Count [number of users who gave the user score]\nRating [ESRB rating for the game]"
  },
  {
    "objectID": "posts/AlexisGamez_Project_Proposal.html#how-was-the-data-collected",
    "href": "posts/AlexisGamez_Project_Proposal.html#how-was-the-data-collected",
    "title": "Project Proposal",
    "section": "How was the data collected?",
    "text": "How was the data collected?\nReferencing the data set’s description once again, it states that, It is a combined web scrape from VGChartz and Metacritic along with manually entered year of release values for most games with a missing year of release.\nThe original code the collector utilized was created by Rush Kirubi, but it’s made apparent that the original set limited the data to only include a subset of video game platforms. Additionally, not all the listed video games have information on Metacritic, so there are a significant amount of missing values under the critic & user scores/counts variables.\nThis provides valuable context concerning Metacritic, the forum utilized by critics and users to rate their favorite games, and the numerous missing values within the data set. Metacritic was established in 1999. As a result, all entries pre-dating early 2000 lack critic and user scores, as it had not been as well established at the time.\n\n\nCode\n# summarizing our data\nsummary(Video_Game_Sales)\n\n\n     Name             Platform         Year_of_Release    Genre          \n Length:17416       Length:17416       Min.   :1976    Length:17416      \n Class :character   Class :character   1st Qu.:2003    Class :character  \n Mode  :character   Mode  :character   Median :2008    Mode  :character  \n                                       Mean   :2007                      \n                                       3rd Qu.:2011                      \n                                       Max.   :2017                      \n                                       NA's   :8                         \n  Publisher            NA_Sales          EU_Sales          JP_Sales       \n Length:17416       Min.   : 0.0000   Min.   : 0.0000   Min.   : 0.00000  \n Class :character   1st Qu.: 0.0000   1st Qu.: 0.0000   1st Qu.: 0.00000  \n Mode  :character   Median : 0.0700   Median : 0.0200   Median : 0.00000  \n                    Mean   : 0.2545   Mean   : 0.1407   Mean   : 0.07502  \n                    3rd Qu.: 0.2300   3rd Qu.: 0.1000   3rd Qu.: 0.03000  \n                    Max.   :41.3600   Max.   :28.9600   Max.   :10.22000  \n                                                                          \n  Other_Sales        Global_Sales      Critic_Score    Critic_Count   \n Min.   : 0.00000   Min.   : 0.0100   Min.   :13.00   Min.   :  3.00  \n 1st Qu.: 0.00000   1st Qu.: 0.0500   1st Qu.:60.00   1st Qu.: 11.00  \n Median : 0.01000   Median : 0.1600   Median :71.00   Median : 21.00  \n Mean   : 0.04591   Mean   : 0.5165   Mean   :68.91   Mean   : 26.19  \n 3rd Qu.: 0.03000   3rd Qu.: 0.4500   3rd Qu.:79.00   3rd Qu.: 36.00  \n Max.   :10.57000   Max.   :82.5400   Max.   :98.00   Max.   :113.00  \n                                      NA's   :9080    NA's   :9080    \n   User_Score      User_Count         Rating         \n Min.   :0.000   Min.   :    4.0   Length:17416      \n 1st Qu.:6.400   1st Qu.:   10.0   Class :character  \n Median :7.500   Median :   25.0   Mode  :character  \n Mean   :7.117   Mean   :  162.7                     \n 3rd Qu.:8.200   3rd Qu.:   81.0                     \n Max.   :9.700   Max.   :10766.0                     \n NA's   :9618    NA's   :9618                        \n\n\nSummarizing our data shows that 9,080 entries lack critic scores and 9,618 of them lack user scores. Even with 9,618 entries omitted, there are still over 7,700 complete entries to analyze and I do not fear that the omission will negatively impact the analysis."
  },
  {
    "objectID": "posts/AlexisGamez_Project_Proposal.html#what-are-the-important-variables-of-interest",
    "href": "posts/AlexisGamez_Project_Proposal.html#what-are-the-important-variables-of-interest",
    "title": "Project Proposal",
    "section": "What are the important variables of interest?",
    "text": "What are the important variables of interest?\nOf the 15 variables provided, 11 will be heavily utilized throughout the scope of this project. 6 are to be considered independent variables and the remaining 5 will be dependent.\nThe 6 independent variables are as follows:\n\nPlatform\nGenre\nPublisher\nRating\nCritic Scores\nUser Scores\n\nThe 5 dependent variables are:\n\nNA Sales\nEU Sales\nJPN Sales\nOther Sales\nGlobal Sales\n\nAs stated previously, of the 6 independent variables, I believe that Genre and Platform will have the most significant impact on commercial success than any other of 4 remaining independent variables. However, I’d also like to add that I believe that the Critic Score variable will have little to no correlation with the commercial success of a video game."
  },
  {
    "objectID": "posts/HW_4_Diana_Rinker.html#a.",
    "href": "posts/HW_4_Diana_Rinker.html#a.",
    "title": "HW_4_Diana_Rinker",
    "section": "A.",
    "text": "A.\n\nx1&lt;- 1240\nx2 &lt;- 18000 \npredicted.price &lt;- (-10536) + 53.8*x1 + 2.84*x2\nresidual &lt;- predicted.price - 145000\ntable &lt;- cbind( c(\"prediciton\", \"residual\"), c(predicted.price,residual))\nknitr::kable(table) \n\n\n\n\nprediciton\n107296\n\n\nresidual\n-37704\n\n\n\n\n\nThe residual is negative, meaning that the model underestimated house price."
  },
  {
    "objectID": "posts/HW_4_Diana_Rinker.html#b.",
    "href": "posts/HW_4_Diana_Rinker.html#b.",
    "title": "HW_4_Diana_Rinker",
    "section": "B.",
    "text": "B.\nŷ = −10,536 + 53.8x1 + 2.84x2. If X2 is fixed, Y increases by 53.8 dollars per each square foot increase. The coeeficient of X1 indicates increase in Y with the change of X1, if the other variables kept the same."
  },
  {
    "objectID": "posts/HW_4_Diana_Rinker.html#c.",
    "href": "posts/HW_4_Diana_Rinker.html#c.",
    "title": "HW_4_Diana_Rinker",
    "section": "C.",
    "text": "C.\nTo increase Y by 53.8, X2 would need to increase by 53.8/2.84 = 8.94366.\n\nx2.incr&lt;- 53.8/2.84\nx2.incr\n\n[1] 18.94366"
  },
  {
    "objectID": "posts/HW_4_Diana_Rinker.html#a.-1",
    "href": "posts/HW_4_Diana_Rinker.html#a.-1",
    "title": "HW_4_Diana_Rinker",
    "section": "A.",
    "text": "A.\nTest the hypothesis that the mean salary for men and women is the same, without regard to any other variable but sex. Explain your findings.\n\ndata(salary)\nstr(salary)\n\n'data.frame':   52 obs. of  6 variables:\n $ degree: Factor w/ 2 levels \"Masters\",\"PhD\": 1 1 1 1 2 1 2 1 2 2 ...\n $ rank  : Factor w/ 3 levels \"Asst\",\"Assoc\",..: 3 3 3 3 3 3 3 3 3 3 ...\n $ sex   : Factor w/ 2 levels \"Male\",\"Female\": 1 1 1 2 1 1 2 1 1 1 ...\n $ year  : int  25 13 10 7 19 16 0 16 13 13 ...\n $ ysdeg : int  35 22 23 27 30 21 32 18 30 31 ...\n $ salary: int  36350 35350 28200 26775 33696 28516 24900 31909 31850 32850 ...\n\n\nFirst, I will make sure that variances in male and female groups are equal:\n\nsalary$sex\n\n [1] Male   Male   Male   Female Male   Male   Female Male   Male   Male  \n[11] Male   Male   Male   Male   Male   Male   Male   Male   Male   Male  \n[21] Male   Male   Male   Female Male   Male   Male   Female Male   Male  \n[31] Female Male   Male   Female Female Male   Female Male   Male   Male  \n[41] Male   Male   Male   Female Male   Male   Female Female Male   Female\n[51] Female Female\nLevels: Male Female\n\nMale &lt;- subset(salary, sex == \"Male\")\nFemale&lt;- subset(salary, sex == \"Female\")\nvar.test(Male$salary, Female$salary)\n\n\n    F test to compare two variances\n\ndata:  Male$salary and Female$salary\nF = 0.84242, num df = 37, denom df = 13, p-value = 0.6525\nalternative hypothesis: true ratio of variances is not equal to 1\n95 percent confidence interval:\n 0.3015275 1.9189474\nsample estimates:\nratio of variances \n         0.8424225 \n\n\nP&gt;0.05 means that the variances of thwo samples can be considered equal.\n\nsalary$sex\n\n [1] Male   Male   Male   Female Male   Male   Female Male   Male   Male  \n[11] Male   Male   Male   Male   Male   Male   Male   Male   Male   Male  \n[21] Male   Male   Male   Female Male   Male   Male   Female Male   Male  \n[31] Female Male   Male   Female Female Male   Female Male   Male   Male  \n[41] Male   Male   Male   Female Male   Male   Female Female Male   Female\n[51] Female Female\nLevels: Male Female\n\nMale &lt;- subset(salary, sex == \"Male\")\nFemale&lt;- subset(salary, sex == \"Female\")\nt.test(Male$salary, Female$salary, var.equal=T, Alternative=\"two.soded\")\n\n\n    Two Sample t-test\n\ndata:  Male$salary and Female$salary\nt = 1.8474, df = 50, p-value = 0.0706\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -291.257 6970.550\nsample estimates:\nmean of x mean of y \n 24696.79  21357.14 \n\n\nThe p-value of a t-test is above 0.05, which means that we accept H0 that the means of these two groups ere the same."
  },
  {
    "objectID": "posts/HW_4_Diana_Rinker.html#b.-1",
    "href": "posts/HW_4_Diana_Rinker.html#b.-1",
    "title": "HW_4_Diana_Rinker",
    "section": "B.",
    "text": "B.\nRun a multiple linear regression with salary as the outcome variable and everything else as predictors, including sex. Assuming no interactions between sex and the other predictors, obtain a 95% confidence interval for the difference in salary between males and females.\n\nstr(salary)\n\n'data.frame':   52 obs. of  6 variables:\n $ degree: Factor w/ 2 levels \"Masters\",\"PhD\": 1 1 1 1 2 1 2 1 2 2 ...\n $ rank  : Factor w/ 3 levels \"Asst\",\"Assoc\",..: 3 3 3 3 3 3 3 3 3 3 ...\n $ sex   : Factor w/ 2 levels \"Male\",\"Female\": 1 1 1 2 1 1 2 1 1 1 ...\n $ year  : int  25 13 10 7 19 16 0 16 13 13 ...\n $ ysdeg : int  35 22 23 27 30 21 32 18 30 31 ...\n $ salary: int  36350 35350 28200 26775 33696 28516 24900 31909 31850 32850 ...\n\nsummary ( lm(salary ~ degree + rank + sex + year +ysdeg , data = salary ))\n\n\nCall:\nlm(formula = salary ~ degree + rank + sex + year + ysdeg, data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4045.2 -1094.7  -361.5   813.2  9193.1 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 15746.05     800.18  19.678  &lt; 2e-16 ***\ndegreePhD    1388.61    1018.75   1.363    0.180    \nrankAssoc    5292.36    1145.40   4.621 3.22e-05 ***\nrankProf    11118.76    1351.77   8.225 1.62e-10 ***\nsexFemale    1166.37     925.57   1.260    0.214    \nyear          476.31      94.91   5.018 8.65e-06 ***\nysdeg        -124.57      77.49  -1.608    0.115    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2398 on 45 degrees of freedom\nMultiple R-squared:  0.855, Adjusted R-squared:  0.8357 \nF-statistic: 44.24 on 6 and 45 DF,  p-value: &lt; 2.2e-16\n\n\nConfidence interval for the difference in salary between males and females:\n\nconfint(model, \"sexMale\", level = 0.95)\n\nError in confint(model, \"sexMale\", level = 0.95): object 'model' not found"
  },
  {
    "objectID": "posts/HW_4_Diana_Rinker.html#c.-1",
    "href": "posts/HW_4_Diana_Rinker.html#c.-1",
    "title": "HW_4_Diana_Rinker",
    "section": "C.",
    "text": "C.\nInterpret your finding for each predictor variable; discuss (a) statistical significance, (b) interpretation of the coefficient / slope in relation to the outcome variable and other variables\n\nstr(salary)\n\n'data.frame':   52 obs. of  6 variables:\n $ degree: Factor w/ 2 levels \"Masters\",\"PhD\": 1 1 1 1 2 1 2 1 2 2 ...\n $ rank  : Factor w/ 3 levels \"Asst\",\"Assoc\",..: 3 3 3 3 3 3 3 3 3 3 ...\n $ sex   : Factor w/ 2 levels \"Male\",\"Female\": 1 1 1 2 1 1 2 1 1 1 ...\n $ year  : int  25 13 10 7 19 16 0 16 13 13 ...\n $ ysdeg : int  35 22 23 27 30 21 32 18 30 31 ...\n $ salary: int  36350 35350 28200 26775 33696 28516 24900 31909 31850 32850 ...\n\nsummary ( lm(salary ~ degree + rank + sex + year +ysdeg , data = salary ))\n\n\nCall:\nlm(formula = salary ~ degree + rank + sex + year + ysdeg, data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4045.2 -1094.7  -361.5   813.2  9193.1 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 15746.05     800.18  19.678  &lt; 2e-16 ***\ndegreePhD    1388.61    1018.75   1.363    0.180    \nrankAssoc    5292.36    1145.40   4.621 3.22e-05 ***\nrankProf    11118.76    1351.77   8.225 1.62e-10 ***\nsexFemale    1166.37     925.57   1.260    0.214    \nyear          476.31      94.91   5.018 8.65e-06 ***\nysdeg        -124.57      77.49  -1.608    0.115    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2398 on 45 degrees of freedom\nMultiple R-squared:  0.855, Adjusted R-squared:  0.8357 \nF-statistic: 44.24 on 6 and 45 DF,  p-value: &lt; 2.2e-16\n\n\ndegree: shows no significance in the model (p-value is above 0.05), meaning that this variable cannot explain or predict salary. Rank variable shows high level of significance for all of its three levels (p-value lower than 0.0001), meaning that the rank influences the salary. The rank of the professor increases the salary twice more than the rank of the Associate professor.\nSex variable with two levels (male and female) has no significance and therefore does not contribute to the salary value.\nYear shows high significance (low p-value), but has the lowest coefficient to the salary value.\nYsdeg shows no significance and therefore does not contribute to the salary."
  },
  {
    "objectID": "posts/HW_4_Diana_Rinker.html#d.",
    "href": "posts/HW_4_Diana_Rinker.html#d.",
    "title": "HW_4_Diana_Rinker",
    "section": "D.",
    "text": "D.\nChange the baseline category for the rank variable. Interpret the coefficients related to rank again.\n\nstr(salary)\n\n'data.frame':   52 obs. of  6 variables:\n $ degree: Factor w/ 2 levels \"Masters\",\"PhD\": 1 1 1 1 2 1 2 1 2 2 ...\n $ rank  : Factor w/ 3 levels \"Asst\",\"Assoc\",..: 3 3 3 3 3 3 3 3 3 3 ...\n $ sex   : Factor w/ 2 levels \"Male\",\"Female\": 1 1 1 2 1 1 2 1 1 1 ...\n $ year  : int  25 13 10 7 19 16 0 16 13 13 ...\n $ ysdeg : int  35 22 23 27 30 21 32 18 30 31 ...\n $ salary: int  36350 35350 28200 26775 33696 28516 24900 31909 31850 32850 ...\n\nlevels(salary$rank)\n\n[1] \"Asst\"  \"Assoc\" \"Prof\" \n\nsalary$rank &lt;- relevel(salary$rank, ref = \"Prof\")\nsummary ( lm(salary ~ degree + rank + sex + year +ysdeg , data = salary ))\n\n\nCall:\nlm(formula = salary ~ degree + rank + sex + year + ysdeg, data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4045.2 -1094.7  -361.5   813.2  9193.1 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  26864.81    1375.29  19.534  &lt; 2e-16 ***\ndegreePhD     1388.61    1018.75   1.363    0.180    \nrankAsst    -11118.76    1351.77  -8.225 1.62e-10 ***\nrankAssoc    -5826.40    1012.93  -5.752 7.28e-07 ***\nsexFemale     1166.37     925.57   1.260    0.214    \nyear           476.31      94.91   5.018 8.65e-06 ***\nysdeg         -124.57      77.49  -1.608    0.115    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2398 on 45 degrees of freedom\nMultiple R-squared:  0.855, Adjusted R-squared:  0.8357 \nF-statistic: 44.24 on 6 and 45 DF,  p-value: &lt; 2.2e-16\n\n\nI used relevel() function tochange base cathegory for Rank from Asssistant to Professor. Now we can see that ssostant professor earns on average $11118 less than professor, and Associate makes $5826 less than professor."
  },
  {
    "objectID": "posts/HW_4_Diana_Rinker.html#e.",
    "href": "posts/HW_4_Diana_Rinker.html#e.",
    "title": "HW_4_Diana_Rinker",
    "section": "E.",
    "text": "E.\nFinkelstein (1980), in a discussion of the use of regression in discrimination cases, wrote, “[a] variable may reflect a position or status bestowed by the employer, in which case if there is discrimination in the award of the position or status, the variable may be ‘tainted.’” Thus, for example, if discrimination is at work in promotion of faculty to higher ranks, using rank to adjust salaries before comparing the sexes may not be acceptable to the courts.\nExclude the variable rank, refit, and summarize how your findings changed, if they did.\n\n# first, I return value \"Asst\" to be baseline, because it has smallest salaries. \nlevels(salary$rank)\n\n[1] \"Prof\"  \"Asst\"  \"Assoc\"\n\nsalary$rank &lt;- relevel(salary$rank, ref = \"Asst\")\n\n#excluding rank from the model \nsummary ( lm(salary ~ degree + sex + year +ysdeg , data = salary ))\n\n\nCall:\nlm(formula = salary ~ degree + sex + year + ysdeg, data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8146.9 -2186.9  -491.5  2279.1 11186.6 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 17183.57    1147.94  14.969  &lt; 2e-16 ***\ndegreePhD   -3299.35    1302.52  -2.533 0.014704 *  \nsexFemale   -1286.54    1313.09  -0.980 0.332209    \nyear          351.97     142.48   2.470 0.017185 *  \nysdeg         339.40      80.62   4.210 0.000114 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3744 on 47 degrees of freedom\nMultiple R-squared:  0.6312,    Adjusted R-squared:  0.5998 \nF-statistic: 20.11 on 4 and 47 DF,  p-value: 1.048e-09\n\n\nOnce I excluded rank from the model, the variable sex became significant. I interpret it, as the variable rank was taking “covering” variable sex. It the previous model rank had very high significance, and now “sex” became significant, supporting Finkelstein’s hypothesis."
  },
  {
    "objectID": "posts/HW_4_Diana_Rinker.html#f.",
    "href": "posts/HW_4_Diana_Rinker.html#f.",
    "title": "HW_4_Diana_Rinker",
    "section": "F.",
    "text": "F.\nEveryone in this dataset was hired the year they earned their highest degree.\nysdeg = time since hired\nIt is also known that a new Dean was appointed 15 years ago, and everyone in the dataset who earned their highest degree 15 years ago or less than that has been hired by the new Dean.\nseparate thos who been hired by him ysdeg &lt;=15 yrs - &gt; two groups compare\nSome people have argued that the new Dean has been making offers that are a lot more generous to newly hired faculty than the previous one and that this might explain some of the variation in Salary.\nCreate a new variable that would allow you to test this hypothesis and run another multiple regression model to test this.\nSelect variables carefully to make sure there is no multicollinearity.\nExplain why multicollinearity would be a concern in this case and how you avoided it.\nDo you find support for the hypothesis that the people hired by the new Dean are making higher than those that were not?\n\n# ysdeg = time since hired\nsalary&lt;-salary%&gt;%\n  mutate (new.hire = ifelse (ysdeg &gt;15, 0, 1))\n\nSince new.hire variable is derived from ysdeg, I will remove ysdeg from the model to avoid multicollinearity:\n\nsummary ( lm(salary ~ degree + sex +  rank + year + new.hire, data = salary ))\n\n\nCall:\nlm(formula = salary ~ degree + sex + rank + year + new.hire, \n    data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3403.3 -1387.0  -167.0   528.2  9233.8 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 13328.38    1483.38   8.985 1.33e-11 ***\ndegreePhD     818.93     797.48   1.027   0.3100    \nsexFemale     907.14     840.54   1.079   0.2862    \nrankProf    11096.95    1191.00   9.317 4.54e-12 ***\nrankAssoc    4972.66     997.17   4.987 9.61e-06 ***\nyear          434.85      78.89   5.512 1.65e-06 ***\nnew.hire     2163.46    1072.04   2.018   0.0496 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2362 on 45 degrees of freedom\nMultiple R-squared:  0.8594,    Adjusted R-squared:  0.8407 \nF-statistic: 45.86 on 6 and 45 DF,  p-value: &lt; 2.2e-16\n\n# old summary ( lm(salary ~ degree + sex + year +ysdeg , data = salary ))\n\nThe model is showing significance for the new.hire variable, supporting the hypothesis tha tpeople hired by the new Dean are paid better."
  },
  {
    "objectID": "posts/HW_4_Diana_Rinker.html#a.-2",
    "href": "posts/HW_4_Diana_Rinker.html#a.-2",
    "title": "HW_4_Diana_Rinker",
    "section": "A.",
    "text": "A.\nUsing the house.selling.price data, run and report regression results modeling y = selling price (in dollars) in terms of size of home (in square feet) and whether the home is new (1 = yes; 0 = no). In particular, for each variable; discuss statistical significance and interpret the meaning of the coefficient.\n\ndata(house.selling.price)\nstr(house.selling.price)\n\n'data.frame':   100 obs. of  7 variables:\n $ case : int  1 2 3 4 5 6 7 8 9 10 ...\n $ Taxes: int  3104 1173 3076 1608 1454 2997 4054 3002 6627 320 ...\n $ Beds : int  4 2 4 3 3 3 3 3 5 3 ...\n $ Baths: int  2 1 2 2 3 2 2 2 4 2 ...\n $ New  : int  0 0 0 0 0 1 0 1 0 0 ...\n $ Price: int  279900 146500 237700 200000 159900 499900 265500 289900 587000 70000 ...\n $ Size : int  2048 912 1654 2068 1477 3153 1355 2075 3990 1160 ...\n\n\n\nsummary (lm(Price~ Size + New, data = house.selling.price))\n\n\nCall:\nlm(formula = Price ~ Size + New, data = house.selling.price)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-205102  -34374   -5778   18929  163866 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -40230.867  14696.140  -2.738  0.00737 ** \nSize           116.132      8.795  13.204  &lt; 2e-16 ***\nNew          57736.283  18653.041   3.095  0.00257 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 53880 on 97 degrees of freedom\nMultiple R-squared:  0.7226,    Adjusted R-squared:  0.7169 \nF-statistic: 126.3 on 2 and 97 DF,  p-value: &lt; 2.2e-16\n\n\nThe model is demostrating statisticsl significance of both independent variable, with the size variable having a better significance level. Size coefficient of 115 indicates that with an increase of size by square foot, the price of the house would increase by 116 dollars.\nAt the same time, the coefficient of “New” variable is much bgger than Size, meaning that the the new house would be 57736 dollars more expencive than the old one of the same size."
  },
  {
    "objectID": "posts/HW_4_Diana_Rinker.html#b.-2",
    "href": "posts/HW_4_Diana_Rinker.html#b.-2",
    "title": "HW_4_Diana_Rinker",
    "section": "B.",
    "text": "B.\nReport and interpret the prediction equation, and form separate equations relating selling price to size for new and for not new homes.\nPrediction equation is: Price = (116 * Size) + (57736 * New)\nto form separate equations for new and not new houses, I will split dataset into two by the “New” variable:\n\nnew.houses&lt;- house.selling.price%&gt;%\n  filter(New==1)\nold.houses&lt;- house.selling.price%&gt;%\n  filter(New==0)\nnew.model&lt;- (lm(Price~ Size , data = new.houses))\nnew.model\n\n\nCall:\nlm(formula = Price ~ Size, data = new.houses)\n\nCoefficients:\n(Intercept)         Size  \n  -100755.3        166.4  \n\nold.model&lt;- (lm(Price~ Size , data = old.houses))\nold.model\n\n\nCall:\nlm(formula = Price ~ Size, data = old.houses)\n\nCoefficients:\n(Intercept)         Size  \n   -22227.8        104.4  \n\n\nModel for New houses: price = 166* Size\nModel for Old houses: price = 104* Size"
  },
  {
    "objectID": "posts/HW_4_Diana_Rinker.html#c.-find-the-predicted-selling-price-for-a-home-of-3000-square-feet-that-is-i-new-ii-not-new.",
    "href": "posts/HW_4_Diana_Rinker.html#c.-find-the-predicted-selling-price-for-a-home-of-3000-square-feet-that-is-i-new-ii-not-new.",
    "title": "HW_4_Diana_Rinker",
    "section": "C. Find the predicted selling price for a home of 3000 square feet that is (i) new, (ii) not new.",
    "text": "C. Find the predicted selling price for a home of 3000 square feet that is (i) new, (ii) not new.\nNew house of size 3000\n\nSize&lt;- 3000\nnew.house&lt;- 166*Size \nnew.house\n\n[1] 498000\n\n\nOld house of size 3000\n\nold.house &lt;- 104*Size\nold.house\n\n[1] 312000"
  },
  {
    "objectID": "posts/HW_4_Diana_Rinker.html#d.-fit-another-model-this-time-with-an-interaction-term-allowing-interaction-between-size-and-new-and-report-the-regression-results",
    "href": "posts/HW_4_Diana_Rinker.html#d.-fit-another-model-this-time-with-an-interaction-term-allowing-interaction-between-size-and-new-and-report-the-regression-results",
    "title": "HW_4_Diana_Rinker",
    "section": "D. Fit another model, this time with an interaction term allowing interaction between size and new, and report the regression results",
    "text": "D. Fit another model, this time with an interaction term allowing interaction between size and new, and report the regression results\n\nsummary (lm(Price~ Size + New +Size*New, data = house.selling.price))\n\n\nCall:\nlm(formula = Price ~ Size + New + Size * New, data = house.selling.price)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-175748  -28979   -6260   14693  192519 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -22227.808  15521.110  -1.432  0.15536    \nSize           104.438      9.424  11.082  &lt; 2e-16 ***\nNew         -78527.502  51007.642  -1.540  0.12697    \nSize:New        61.916     21.686   2.855  0.00527 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 52000 on 96 degrees of freedom\nMultiple R-squared:  0.7443,    Adjusted R-squared:  0.7363 \nF-statistic: 93.15 on 3 and 96 DF,  p-value: &lt; 2.2e-16\n\n\nOnce we added interaction term between New and size, the significance of “New” variable disappeared. Instead, the interaction term appears to explain the change in price better, i.e. combination of size and newness of the house impacts the price."
  },
  {
    "objectID": "posts/HW_4_Diana_Rinker.html#e.-report-the-lines-relating-the-predicted-selling-price-to-the-size-for-homes-that-are-i-new-ii-not-new.",
    "href": "posts/HW_4_Diana_Rinker.html#e.-report-the-lines-relating-the-predicted-selling-price-to-the-size-for-homes-that-are-i-new-ii-not-new.",
    "title": "HW_4_Diana_Rinker",
    "section": "E. Report the lines relating the predicted selling price to the size for homes that are (i) new, (ii) not new.",
    "text": "E. Report the lines relating the predicted selling price to the size for homes that are (i) new, (ii) not new.\n\nhouse.selling.price&lt;- house.selling.price%&gt;%\n  mutate (predicted.price = predict(lm(Price~ Size + New +Size*New)))\n\nnew.houses&lt;- house.selling.price%&gt;%\n  filter(New==1)\nggplot(new.houses, mapping=aes(x=Size, y=predicted.price))+\n  geom_smooth(method=\"lm\")+\n  labs(title=\" Predicted selling price to the size for  new homes\", x=\"Size\", y=\"Predicted price\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\nold.houses&lt;- house.selling.price%&gt;%\n  filter(New==0)\nggplot(old.houses, mapping=aes(x=Size, y=predicted.price))+\n  geom_smooth(method=\"lm\")+\n  labs(title=\" Predicted selling price to the size for old homes\", x=\"Size\", y=\"Predicted price\")\n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "posts/HW_4_Diana_Rinker.html#f.-find-the-predicted-selling-price-for-a-home-of-3000-square-feet-that-is-i-new-ii-not-new.",
    "href": "posts/HW_4_Diana_Rinker.html#f.-find-the-predicted-selling-price-for-a-home-of-3000-square-feet-that-is-i-new-ii-not-new.",
    "title": "HW_4_Diana_Rinker",
    "section": "F. Find the predicted selling price for a home of 3000 square feet that is (i) new, (ii) not new.",
    "text": "F. Find the predicted selling price for a home of 3000 square feet that is (i) new, (ii) not new.\nNew:\n\nmodel &lt;-lm(Price~ Size + New +Size*New, data = house.selling.price)\n\npredicted1.y.new &lt;- predict(model, newdata = data.frame(Size = 3000, New = 1))\npredicted1.y.new \n\n       1 \n398307.5 \n\n\nOld:\n\npredicted1.y.old &lt;- predict(model, newdata = data.frame(Size = 3000, New = 0))\npredicted1.y.old \n\n       1 \n291087.4"
  },
  {
    "objectID": "posts/HW_4_Diana_Rinker.html#g.-find-the-predicted-selling-price-for-a-home-of-1500-square-feet-that-is-i-new-ii-not-new.-comparing-to-f-explain-how-the-difference-in-predicted-selling-prices-changes-as-the-size-of-home-increases.",
    "href": "posts/HW_4_Diana_Rinker.html#g.-find-the-predicted-selling-price-for-a-home-of-1500-square-feet-that-is-i-new-ii-not-new.-comparing-to-f-explain-how-the-difference-in-predicted-selling-prices-changes-as-the-size-of-home-increases.",
    "title": "HW_4_Diana_Rinker",
    "section": "G. Find the predicted selling price for a home of 1500 square feet that is (i) new, (ii) not new. Comparing to (F), explain how the difference in predicted selling prices changes as the size of home increases.",
    "text": "G. Find the predicted selling price for a home of 1500 square feet that is (i) new, (ii) not new. Comparing to (F), explain how the difference in predicted selling prices changes as the size of home increases.\n\npredicted2.y.new &lt;- predict(model, newdata = data.frame(Size = 1500, New = 1))\npredicted2.y.new \n\n       1 \n148776.1 \n\npredicted2.y.old &lt;- predict(model, newdata = data.frame(Size = 1500, New = 0))\npredicted2.y.old\n\n       1 \n134429.8 \n\n\nThe difference in prices between old and new house for F (3000sq.f) is\n\ndif.1 &lt;- predicted1.y.new -predicted1.y.old \ndif.1\n\n       1 \n107220.1 \n\n\nThe difference in prices between old and new house for G (1500sq.f) is\n\ndif.2 &lt;- predicted2.y.new -predicted2.y.old \ndif.2\n\n       1 \n14346.32 \n\n\nApparently the difference between old and new houses of the same square footage is increasing with increase of size."
  },
  {
    "objectID": "posts/HW_4_Diana_Rinker.html#h.-do-you-think-the-model-with-interaction-or-the-one-without-it-represents-the-relationship-of-size-and-new-to-the-outcome-price-what-makes-you-prefer-one-model-over-another",
    "href": "posts/HW_4_Diana_Rinker.html#h.-do-you-think-the-model-with-interaction-or-the-one-without-it-represents-the-relationship-of-size-and-new-to-the-outcome-price-what-makes-you-prefer-one-model-over-another",
    "title": "HW_4_Diana_Rinker",
    "section": "H. Do you think the model with interaction or the one without it represents the relationship of size and new to the outcome price? What makes you prefer one model over another?",
    "text": "H. Do you think the model with interaction or the one without it represents the relationship of size and new to the outcome price? What makes you prefer one model over another?\nClearly the model with interaction represents relationship better, because difference in prices of larger houses is bigger than difference in prices for smaller houses."
  },
  {
    "objectID": "posts/HW_3_Diana_Rinker.html",
    "href": "posts/HW_3_Diana_Rinker.html",
    "title": "Homework 3",
    "section": "",
    "text": "DACSS 603, spring 2023\n\n\nHomework 3, Diana Rinker.\nLoading necessary libraries:\n\n\nCode\nlibrary(dplyr)\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nCode\nlibrary(ggplot2)\nlibrary(alr4)\n\n\nLoading required package: car\n\n\nLoading required package: carData\n\n\n\nAttaching package: 'car'\n\n\nThe following object is masked from 'package:dplyr':\n\n    recode\n\n\nLoading required package: effects\n\n\nlattice theme set by effectsTheme()\nSee ?effectsTheme for details.\n\n\nCode\nlibrary(smss)\n\n\n\n\nQuestion 1\nUnited Nations (Data file: UN11 in alr4) The data in the file UN11 contains several variables, including ppgdp, the gross national product per person in U.S. dollars, and fertility, the birth rate per 1000 females, both from the year 2009. The data are for 199 localities, mostly UN member countries, but also other areas such as Hong Kong that are not independent countries. The data were collected from the United Nations (2011). We will study the dependence of fertility on ppgdp.\n\nIdentify the predictor and the response.\nDV/response: fertility\nIV/predictor: ppgdp\nDraw the scatterplot of fertility on the vertical axis versus ppgdp on the horizontal axis and summarize the information in this graph. Does a straight-line mean function seem to be plausible for a summary of this graph?\n\n\n\nCode\ndata(UN11)\nstr(UN11)\n\n\n'data.frame':   199 obs. of  6 variables:\n $ region   : Factor w/ 8 levels \"Africa\",\"Asia\",..: 2 4 1 1 3 5 2 3 8 4 ...\n $ group    : Factor w/ 3 levels \"oecd\",\"other\",..: 2 2 3 3 2 2 2 2 1 1 ...\n $ fertility: num  5.97 1.52 2.14 5.13 2 ...\n $ ppgdp    : num  499 3677 4473 4322 13750 ...\n $ lifeExpF : num  49.5 80.4 75 53.2 81.1 ...\n $ pctUrban : num  23 53 67 59 100 93 64 47 89 68 ...\n - attr(*, \"na.action\")= 'omit' Named int [1:34] 4 5 8 28 41 67 68 72 79 83 ...\n  ..- attr(*, \"names\")= chr [1:34] \"Am Samoa\" \"Andorra\" \"Antigua and Barbuda\" \"Br Virigin Is\" ...\n\n\nCode\n    ggplot(data=UN11, mapping= aes(x=ppgdp, y =fertility ))+\n      geom_point()+\n      stat_smooth(method = \"lm\", se = T)+\n      geom_hline(yintercept = mean(UN11$fertility), color = \"red\")+\n      labs(title=\"Scatterplot of fertility by ppgdp\")+\n      stat_smooth(method = \"lm\", se = T)\n\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nFrom the graph above it is obvious, that the straight line mean is not representative of the data. Even if be draw a linear model with “stat_smooth(method =”lm”, se = T)“, the line is still far from most datapoints ( which indicate high value of error in the model)\n\nDraw the scatterplot of log(fertility) versus log(ppgdp) using natural logarithms. Does the simple linear regression model seem plausible for a summary of this graph? If you use a different base of logarithms, the shape of the graph won’t change, but the values on the axes will change.\n\n\n\nCode\n   ggplot(data=UN11, mapping= aes(x=log(ppgdp), y =log(fertility) ))+\n      geom_point()+\n      stat_smooth(method = \"lm\", se = T)+\n      geom_hline(yintercept = mean(log(UN11$fertility)), color = \"red\")+\n      labs(title=\"Scatterplot of log(fertility) by log(ppgdp) \")\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nSimple linear regression seems much more plausible on this logged scatter plot.\nBelow, I am using different bases (5 and 7) of the logarithms. The shape of a line doesn’t change, only x-scale limits change:\n\n\nCode\nggplot(data=UN11, mapping= aes(x=log(ppgdp, base =5), y =log(fertility), base =5 ))+\n      geom_point()+\n      stat_smooth(method = \"lm\", se = T)+\n      geom_hline(yintercept = mean(log(UN11$fertility),  base =5), color = \"red\")+\n      labs(title=\"Scatterplot of log(fertility) by log(ppgdp) base =5 \")\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nCode\nggplot(data=UN11, mapping= aes(x=log(ppgdp, base =7), y =log(fertility), base =7))+\n      geom_point()+\n      stat_smooth(method = \"lm\", se = T)+\n      geom_hline(yintercept = mean(log(UN11$fertility),  base =7), color = \"red\")+\n      labs(title=\"Scatterplot of log(fertility) by log(ppgdp), base =7 \")\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\nQuestion 2\nAnnual income, in dollars, is an explanatory variable in a regression analysis. For a British version of the report on the analysis, all responses are converted to British pounds sterling (1 pound equals about 1.33 dollars, as of 2016).\n(a) How, if at all, does the slope of the prediction equation change?\n(b) How, if at all, does the correlation change?\nTo answer these questions, I will run linear regressions for the ppgpd in dollars and pounds and compare results.\n\n\nCode\nUN11$ppgdp.pound&lt;-UN11$ppgdp*1.33\nfit1&lt;- lm(lifeExpF ~ ppgdp, data = UN11)\nsummary(fit1)\n\n\n\nCall:\nlm(formula = lifeExpF ~ ppgdp, data = UN11)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-20.825  -4.889   2.618   6.619  11.299 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 6.837e+01  7.370e-01  92.762   &lt;2e-16 ***\nppgdp       3.018e-04  3.274e-05   9.218   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.483 on 197 degrees of freedom\nMultiple R-squared:  0.3014,    Adjusted R-squared:  0.2978 \nF-statistic: 84.98 on 1 and 197 DF,  p-value: &lt; 2.2e-16\n\n\nCode\nfit2&lt;- lm(lifeExpF ~ ppgdp.pound, data = UN11)\nsummary(fit2)\n\n\n\nCall:\nlm(formula = lifeExpF ~ ppgdp.pound, data = UN11)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-20.825  -4.889   2.618   6.619  11.299 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 6.837e+01  7.370e-01  92.762   &lt;2e-16 ***\nppgdp.pound 2.269e-04  2.462e-05   9.218   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.483 on 197 degrees of freedom\nMultiple R-squared:  0.3014,    Adjusted R-squared:  0.2978 \nF-statistic: 84.98 on 1 and 197 DF,  p-value: &lt; 2.2e-16\n\n\nVisualizing the model with both variables (ppgdp and ppgdp.pound ) produces the same model:\n\n\nCode\nggplot( data=UN11, mapping=aes(x= ppgdp , y=lifeExpF))+\n  geom_point() +\n  geom_smooth(method = 'lm')+\n      labs(title=\"Scatterplot of Female life expectancy by annual income \", x=\"PPGDP in US dollars\", y=\"Life expectancy\")\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nCode\nggplot( data=UN11, mapping=aes(x= ppgdp.pound , y=lifeExpF))+\n  geom_point() +\n  geom_smooth(method = 'lm')+\n      labs(title=\"Scatterplot of Female life expectancy by annual income \", x=\"PPGDP in British pounds\", y=\"Life expectancy\")\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nVisualizing logged variables ( for the US dollars and British pounds) :\n\n\nCode\nggplot( data=UN11, mapping=aes(x= log(ppgdp) , y=lifeExpF   )     )+\n  geom_point() +\n  geom_smooth(method = 'lm')+\n      labs(title=\"Scatterplot of Female life expectancy by annual income \", x=\"Logged PPGDP in US dollars\", y=\"Life expectancy\")\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nCode\nggplot( data=UN11, mapping=aes(x= log(ppgdp.pound) , y=lifeExpF    )     )+\n  geom_point() +\n  geom_smooth(method = 'lm')+\n      labs(title=\"Scatterplot of Female life expectancy by annual income \", x=\"Logged PPGDP in British pounds\", y=\"Life expectancy\")\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nThere is no change in slope and correlation between variablle in US dollars and British pounds.\n\n\nCode\nfit_logged&lt;- lm(lifeExpF~ log(ppgdp), data = UN11)\n\nsummary(fit_logged)\n\n\n\nCall:\nlm(formula = lifeExpF ~ log(ppgdp), data = UN11)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-25.749  -2.879   1.280   3.987  12.345 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  29.8148     2.5314   11.78   &lt;2e-16 ***\nlog(ppgdp)    5.0188     0.2942   17.06   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.448 on 197 degrees of freedom\nMultiple R-squared:  0.5964,    Adjusted R-squared:  0.5943 \nF-statistic: 291.1 on 1 and 197 DF,  p-value: &lt; 2.2e-16\n\n\nHowever, using log() function on x variable allowed us to build a better fitted model with R^2 increased from 0.30 to 0.60. I can conclude that 1% change in PPGDP correspons with 5 % increase in Life expectancy.\n\n\nQuestion 3\nWater runoff in the Sierras (Data file: water in alr4) Can Southern California’s water supply in future years be predicted from past data? One factor affecting water availability is stream runoff. If runoff could be predicted, engineers, planners, and policy makers could do their jobs\nmore efficiently. The data file contains 43 years’ worth of precipitation measurements taken at six sites in the Sierra Nevada mountains (labeled APMAM, APSAB, APSLAKE, OPBPC, OPRC, and OPSLAKE) and stream runoff volume at a site near Bishop, California, labeled BSAAM. Draw the scatterplot matrix for these data and summarize the information available from these plots. (Hint: Use the pairs() function.)\n\n\nCode\nlibrary(alr4)\ndata(water)\nstr(water)\n\n\n'data.frame':   43 obs. of  8 variables:\n $ Year   : int  1948 1949 1950 1951 1952 1953 1954 1955 1956 1957 ...\n $ APMAM  : num  9.13 5.28 4.2 4.6 7.15 9.7 5.02 6.7 10.5 9.1 ...\n $ APSAB  : num  3.58 4.82 3.77 4.46 4.99 5.65 1.45 7.44 5.85 6.13 ...\n $ APSLAKE: num  3.91 5.2 3.67 3.93 4.88 4.91 1.77 6.51 3.38 4.08 ...\n $ OPBPC  : num  4.1 7.55 9.52 11.14 16.34 ...\n $ OPRC   : num  7.43 11.11 12.2 15.15 20.05 ...\n $ OPSLAKE: num  6.47 10.26 11.35 11.13 22.81 ...\n $ BSAAM  : int  54235 67567 66161 68094 107080 67594 65356 67909 92715 70024 ...\n\n\nCode\nhead(water)\n\n\n  Year APMAM APSAB APSLAKE OPBPC  OPRC OPSLAKE  BSAAM\n1 1948  9.13  3.58    3.91  4.10  7.43    6.47  54235\n2 1949  5.28  4.82    5.20  7.55 11.11   10.26  67567\n3 1950  4.20  3.77    3.67  9.52 12.20   11.35  66161\n4 1951  4.60  4.46    3.93 11.14 15.15   11.13  68094\n5 1952  7.15  4.99    4.88 16.34 20.05   22.81 107080\n6 1953  9.70  5.65    4.91  8.88  8.15    7.41  67594\n\n\nCode\nwater$Year &lt;-as.factor(water$Year)\nlevels(water$Year)\n\n\n [1] \"1948\" \"1949\" \"1950\" \"1951\" \"1952\" \"1953\" \"1954\" \"1955\" \"1956\" \"1957\"\n[11] \"1958\" \"1959\" \"1960\" \"1961\" \"1962\" \"1963\" \"1964\" \"1965\" \"1966\" \"1967\"\n[21] \"1968\" \"1969\" \"1970\" \"1971\" \"1972\" \"1973\" \"1974\" \"1975\" \"1976\" \"1977\"\n[31] \"1978\" \"1979\" \"1980\" \"1981\" \"1982\" \"1983\" \"1984\" \"1985\" \"1986\" \"1987\"\n[41] \"1988\" \"1989\" \"1990\"\n\n\nCode\npairs(subset (water, select = c(APMAM : BSAAM)))\n\n\n\n\n\nFrom the scatter plot matrix above I can see that some sources of water well correlated to each other, as their scatter plots are very close to a straight line. Moreover, there are two groups of water sources that demonstrate being inter-connected. The first group is APMAM, APSAB and APSLAKE. The second group is OPBPC, OPRC, OPLAKE and BSAAM. It is possible that the sites within a grouop share a water source or separated from each other (may be by mountains).\n\n\nCode\nhead(water)\n\n\n  Year APMAM APSAB APSLAKE OPBPC  OPRC OPSLAKE  BSAAM\n1 1948  9.13  3.58    3.91  4.10  7.43    6.47  54235\n2 1949  5.28  4.82    5.20  7.55 11.11   10.26  67567\n3 1950  4.20  3.77    3.67  9.52 12.20   11.35  66161\n4 1951  4.60  4.46    3.93 11.14 15.15   11.13  68094\n5 1952  7.15  4.99    4.88 16.34 20.05   22.81 107080\n6 1953  9.70  5.65    4.91  8.88  8.15    7.41  67594\n\n\nCode\npairs(subset (water, select = c(APMAM : APSLAKE)))\n\n\n\n\n\n\n\nCode\nhead(water)\n\n\n  Year APMAM APSAB APSLAKE OPBPC  OPRC OPSLAKE  BSAAM\n1 1948  9.13  3.58    3.91  4.10  7.43    6.47  54235\n2 1949  5.28  4.82    5.20  7.55 11.11   10.26  67567\n3 1950  4.20  3.77    3.67  9.52 12.20   11.35  66161\n4 1951  4.60  4.46    3.93 11.14 15.15   11.13  68094\n5 1952  7.15  4.99    4.88 16.34 20.05   22.81 107080\n6 1953  9.70  5.65    4.91  8.88  8.15    7.41  67594\n\n\nCode\npairs(subset (water, select = c(OPBPC : BSAAM)))\n\n\n\n\n\n\n\nQuestion 4\nProfessor ratings (Data file: Rateprof in alr4) In the website and online forum RateMyProfessors.com, students rate and comment on their instructors. Launched in 1999, the site includes millions of ratings on thousands of instructors. The data file includes the summaries of the ratings of 364 instructors at a large campus in the Midwest (Bleske-Rechek and Fritsch, 2011). Each instructor included in the data had at least 10 ratings over a several year period. Students provided ratings of 1–5 on quality, helpfulness, clarity, easiness of instructor’s courses, and rater Interest in the subject matter covered in the instructor’s courses. The data file provides the averages of these five ratings. Create a scatter plot matrix of these five variables. Provide a brief description of the relationships between the five ratings.\n\n\nCode\ndata(Rateprof)\nstr(Rateprof)\n\n\n'data.frame':   366 obs. of  17 variables:\n $ gender         : Factor w/ 2 levels \"female\",\"male\": 2 2 2 2 2 2 2 2 2 2 ...\n $ numYears       : int  7 6 10 11 11 10 7 11 11 7 ...\n $ numRaters      : int  11 11 43 24 19 15 17 16 12 18 ...\n $ numCourses     : int  5 5 2 5 7 9 3 3 4 4 ...\n $ pepper         : Factor w/ 2 levels \"no\",\"yes\": 1 1 1 1 1 1 1 1 1 1 ...\n $ discipline     : Factor w/ 4 levels \"Hum\",\"SocSci\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ dept           : Factor w/ 48 levels \"Accounting\",\"Anthropology\",..: 17 42 3 17 45 45 45 17 34 17 ...\n $ quality        : num  4.64 4.32 4.79 4.25 4.68 ...\n $ helpfulness    : num  4.64 4.55 4.72 4.46 4.68 ...\n $ clarity        : num  4.64 4.09 4.86 4.04 4.68 ...\n $ easiness       : num  4.82 4.36 4.6 2.79 4.47 ...\n $ raterInterest  : num  3.55 4 3.43 3.18 4.21 ...\n $ sdQuality      : num  0.552 0.902 0.453 0.933 0.65 ...\n $ sdHelpfulness  : num  0.674 0.934 0.666 0.932 0.82 ...\n $ sdClarity      : num  0.505 0.944 0.413 0.999 0.582 ...\n $ sdEasiness     : num  0.405 0.505 0.541 0.588 0.612 ...\n $ sdRaterInterest: num  1.128 1.074 1.237 1.332 0.975 ...\n\n\nCode\nhead(Rateprof)\n\n\n  gender numYears numRaters numCourses pepper discipline              dept\n1   male        7        11          5     no        Hum           English\n2   male        6        11          5     no        Hum Religious Studies\n3   male       10        43          2     no        Hum               Art\n4   male       11        24          5     no        Hum           English\n5   male       11        19          7     no        Hum           Spanish\n6   male       10        15          9     no        Hum           Spanish\n   quality helpfulness  clarity easiness raterInterest sdQuality sdHelpfulness\n1 4.636364    4.636364 4.636364 4.818182      3.545455 0.5518564     0.6741999\n2 4.318182    4.545455 4.090909 4.363636      4.000000 0.9020179     0.9341987\n3 4.790698    4.720930 4.860465 4.604651      3.432432 0.4529343     0.6663898\n4 4.250000    4.458333 4.041667 2.791667      3.181818 0.9325048     0.9315329\n5 4.684211    4.684211 4.684211 4.473684      4.214286 0.6500112     0.8200699\n6 4.233333    4.266667 4.200000 4.533333      3.916667 0.8632717     1.0327956\n  sdClarity sdEasiness sdRaterInterest\n1 0.5045250  0.4045199       1.1281521\n2 0.9438798  0.5045250       1.0744356\n3 0.4129681  0.5407021       1.2369438\n4 0.9990938  0.5882300       1.3322506\n5 0.5823927  0.6117753       0.9749613\n6 0.7745967  0.6399405       0.6685579\n\n\nCode\npairs(subset (Rateprof, select =c(quality : raterInterest)))\n\n\n\n\n\nAs we can see from the scatter plot above, three variables are clearly highly correlated: quality, helpfulness and clarity. Particularly, quality highly correlated with both helpfulness and clarity. I suspect, these variables might create multicollinearity problem if used together in multiple regression model.\nConnection between clarity and helpfulness is also very distinct, but the variability seems bigger.\nEasiness variable appears positively correlated with the first three (quality, helpfulness and clarity), however there is much higher variability.\nRater’s interest appears less connected with the other variables, and might demonstrate a smaller slope of a regression line when calculated.\n\n\n\nQuestion 5\nFor the student.survey data file in the smss package, conduct regression analyses relating (by convention, y denotes the outcome variable, x denotes the explanatory variable)\n(i) y = political ideology and x = religiosity,\n(ii) y = high school GPA and x = hours of TV watching. (You can use ?student.survey in the R console, after loading the package, to see what each variable means.)\n(a) Graphically portray how the explanatory variable relates to the outcome variable in each of the two cases\n(b) Summarize and interpret results of inferential analyses.\n\ni. y = political ideology and x = religiosity\nBy using ?student.survey function, I found out that : Political ideology is stored as PI variable, Religiosity doesn’t have its own variable, but can be represented as “how often you attend religious services” from this data set, variable RE\nTherefore , my model can be presented as\n\\[\nPI =\\beta_o +\\beta_1*RE\n\\]\n\n\nCode\ndata(student.survey)\n?student.survey\n\n\nstarting httpd help server ... done\n\n\nCode\nstr(student.survey)\n\n\n'data.frame':   60 obs. of  18 variables:\n $ subj: int  1 2 3 4 5 6 7 8 9 10 ...\n $ ge  : Factor w/ 2 levels \"f\",\"m\": 2 1 1 1 2 2 2 1 2 2 ...\n $ ag  : int  32 23 27 35 23 39 24 31 34 28 ...\n $ hi  : num  2.2 2.1 3.3 3.5 3.1 3.5 3.6 3 3 4 ...\n $ co  : num  3.5 3.5 3 3.2 3.5 3.5 3.7 3 3 3.1 ...\n $ dh  : int  0 1200 1300 1500 1600 350 0 5000 5000 900 ...\n $ dr  : num  5 0.3 1.5 8 10 3 0.2 1.5 2 2 ...\n $ tv  : num  3 15 0 5 6 4 5 5 7 1 ...\n $ sp  : int  5 7 4 5 6 5 12 3 5 1 ...\n $ ne  : int  0 5 3 6 3 7 4 3 3 2 ...\n $ ah  : int  0 6 0 3 0 0 2 1 0 1 ...\n $ ve  : logi  FALSE FALSE FALSE FALSE FALSE FALSE ...\n $ pa  : Factor w/ 3 levels \"d\",\"i\",\"r\": 3 1 1 2 2 1 2 2 2 2 ...\n $ pi  : Ord.factor w/ 7 levels \"very liberal\"&lt;..: 6 2 2 4 1 2 2 2 1 3 ...\n $ re  : Ord.factor w/ 4 levels \"never\"&lt;\"occasionally\"&lt;..: 3 2 3 2 1 2 2 2 2 1 ...\n $ ab  : logi  FALSE FALSE FALSE FALSE FALSE FALSE ...\n $ aa  : logi  FALSE FALSE FALSE FALSE FALSE FALSE ...\n $ ld  : logi  FALSE NA NA FALSE FALSE NA ...\n\n\nCode\n# head(student.survey)\nclass(student.survey$pi)\n\n\n[1] \"ordered\" \"factor\" \n\n\nCode\nlevels(student.survey$pi)\n\n\n[1] \"very liberal\"          \"liberal\"               \"slightly liberal\"     \n[4] \"moderate\"              \"slightly conservative\" \"conservative\"         \n[7] \"very conservative\"    \n\n\nCode\nclass(student.survey$re)\n\n\n[1] \"ordered\" \"factor\" \n\n\nWe can see that both variables are ordered factor variables. We will need to convert factor variables into the numeric variables for regression, and then use them in linear model:\n\n\nCode\nstudent.survey$pi &lt;-as.numeric (student.survey$pi)\nstudent.survey$re &lt;-as.numeric (student.survey$re)\nsummary(lm(pi~re, data=student.survey))\n\n\n\nCall:\nlm(formula = pi ~ re, data = student.survey)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.81243 -0.87160  0.09882  1.12840  3.09882 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   0.9308     0.4252   2.189   0.0327 *  \nre            0.9704     0.1792   5.416 1.22e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.345 on 58 degrees of freedom\nMultiple R-squared:  0.3359,    Adjusted R-squared:  0.3244 \nF-statistic: 29.34 on 1 and 58 DF,  p-value: 1.221e-06\n\n\nCode\nggplot(data = student.survey, aes(x = re, y = pi)) +\n  geom_point() +\n  geom_smooth(method = 'lm', se=F)+\n  labs(title=\"Political ideology and religiosity\", x=\"Frequency of going to church\", y = \"level of consrvatism\")\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nBased on the summary above, we can see that religiosity and political ideology are positively related, but the religiosity only contributes about 30 % of political ideology (R^2 =0.34). Overal w can say that higher level of conservatism can be somewhat (30%) predicted by frequency of church attendance.\n\n\nii. y = high school GPA and x = hours of TV watching\nHigh school GPA is presented as HI variable, and hours of TV watching - by TV variable.\n\n\nCode\nclass(student.survey$hi)\n\n\n[1] \"numeric\"\n\n\nCode\nclass(student.survey$tv)\n\n\n[1] \"numeric\"\n\n\nBoth variables are numeric, which allows us to run a simple linear regression:\n\n\nCode\nsummary(lm(data=student.survey, hi ~tv))\n\n\n\nCall:\nlm(formula = hi ~ tv, data = student.survey)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.2583 -0.2456  0.0417  0.3368  0.7051 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  3.441353   0.085345  40.323   &lt;2e-16 ***\ntv          -0.018305   0.008658  -2.114   0.0388 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4467 on 58 degrees of freedom\nMultiple R-squared:  0.07156,   Adjusted R-squared:  0.05555 \nF-statistic: 4.471 on 1 and 58 DF,  p-value: 0.03879\n\n\nWe can see that the the slope of Independent variable (tv) is very small and negative. It’s statistical significance is also very low and depends on alpha level that we select. R squared, which indicates the proportion of the variance that can be explained by x (tv watching time), is very low (0.072). Taking this into account, I conclude there is no contribution of TV watching to GPA level.\nTo visualize these relationship, we can build a scatter plot and a fitted linear line:\n\n\nCode\nggplot (data=student.survey, mapping=aes(x=tv, y=hi))+\n  geom_point()+\n  stat_smooth(method = \"lm\", se = T)+\n  labs(title=\"Sctterplot between hours of TV watching and high school GPA\", y=\"High school GPA\", x=\"hours of TV watching\")\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nFrom the graph above we can see, that the standard error band is getting wider towards the right side of the chart, due to low amount of observations in that part of the sample. It also adds uncertainty to a conclusion and calls for more data and further investigation. Since the overall slope does go down, I would suggest that there may be another factor that contributes to the relationship between TV watching and GPA."
  },
  {
    "objectID": "posts/HW2_young.html",
    "href": "posts/HW2_young.html",
    "title": "Homework 2",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.1     ✔ purrr   1.0.1\n✔ tibble  3.1.8     ✔ dplyr   1.1.0\n✔ tidyr   1.3.0     ✔ stringr 1.5.0\n✔ readr   2.1.4     ✔ forcats 1.0.0\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\nCode\nlibrary(dplyr)"
  },
  {
    "objectID": "posts/HW2_young.html#more",
    "href": "posts/HW2_young.html#more",
    "title": "Homework 2",
    "section": "More",
    "text": "More\nRandom data were generated using the given statistics and each confidence interval was also obtained.\n\n\nCode\nset.seed(1)\n\nbypass.s&lt;-rnorm(539, mean=19, sd=10)\nangio.s&lt;-rnorm(847, mean=18, sd=9)\n\nt.test(bypass.s, conf.level = .9)\n\n\n\n    One Sample t-test\n\ndata:  bypass.s\nt = 44.184, df = 538, p-value &lt; 2.2e-16\nalternative hypothesis: true mean is not equal to 0\n90 percent confidence interval:\n 18.44913 19.87842\nsample estimates:\nmean of x \n 19.16377 \n\n\nCode\nt.test(angio.s, conf.level = .9)\n\n\n\n    One Sample t-test\n\ndata:  angio.s\nt = 54.954, df = 846, p-value &lt; 2.2e-16\nalternative hypothesis: true mean is not equal to 0\n90 percent confidence interval:\n 17.15787 18.21788\nsample estimates:\nmean of x \n 17.68788 \n\n\nThe statistics of the generated data did not exactly match the statistics given by the problem, so the exact same confidence interval was not obtained, but they are similar."
  },
  {
    "objectID": "posts/HW2_young.html#a",
    "href": "posts/HW2_young.html#a",
    "title": "Homework 2",
    "section": "A",
    "text": "A\nFirst, assume that s in the population is equal to the sample in question. And the significance level is set at 5%.\nNull Hypothesis: μ = 500 Alternative Hypothesis: μ ≠ 500\nSince it is a test that estimates whether the parameter is a specific value, a two-side t-test should be performed. At this time, t is ȳ - μ / s/(n^1/2).\n\n\nCode\nt.a&lt;-((410-500)/(90/3))\nt.a\n\n\n[1] -3\n\n\nt is -3. And since it is a two-side test, the p-value of this hypothesis can be obtained by multiplying the p-value at t=-3.\n\n\nCode\np.a&lt;-(pt(t.a, df=8))*2\np.a\n\n\n[1] 0.01707168\n\n\np-value is 0.017. And this is smaller than our significant level(0.05). So, we can reject our null hypothesis. This means that At the 95% confidence level, μ is not 500."
  },
  {
    "objectID": "posts/HW2_young.html#b",
    "href": "posts/HW2_young.html#b",
    "title": "Homework 2",
    "section": "B",
    "text": "B\nHere, the null hypothesis is μ &gt;= 500. Then, the alternative hypothesis is μ &lt; 500. In this case, a one-sided test shall be performed.\n\n\nCode\nt.b&lt;--3\np.b&lt;-pt(t.b, df=8)\np.b\n\n\n[1] 0.008535841\n\n\nThe p-value can be obtained in the same way as A. Here, the value of p is 0.009. This means that at the 95% confidence level, μ is not greater than 500."
  },
  {
    "objectID": "posts/HW2_young.html#c",
    "href": "posts/HW2_young.html#c",
    "title": "Homework 2",
    "section": "C",
    "text": "C\nHere, contrary to B, the the null hypothesis is μ =&lt; 500. Since the direction of the inequality has changed, the area of the p value in the t-distribution must be opposite to B.\n\n\nCode\nt.c&lt;--3\np.c&lt;-(1-pt(t.c, df=8))\np.c\n\n\n[1] 0.9914642\n\n\nUsing these facts, the p-value is 0.9914. Now, at the 5% significance level, the null hypothesis cannot be rejected. That is, at a 95% confidence level, μ is less than 500."
  },
  {
    "objectID": "posts/HW2_young.html#a-1",
    "href": "posts/HW2_young.html#a-1",
    "title": "Homework 2",
    "section": "A",
    "text": "A\nt is (ȳ-μ) / (sd/sqrt(n)). And sd/sqrt(n) is se. So we can compute t through this equation.\n\n\nCode\n# Jones's t and p-value\njones.t&lt;-(519.5-500)/10\njones.p&lt;-(1-pt(jones.t, df=999))*2\njones.t\n\n\n[1] 1.95\n\n\nCode\njones.p\n\n\n[1] 0.05145555\n\n\n\n\nCode\n# Smith's t and p-value\nsmith.t&lt;-(519.7-500)/10\nsmith.p&lt;-(1-pt(smith.t, df=999))*2\nsmith.t\n\n\n[1] 1.97\n\n\nCode\nsmith.p\n\n\n[1] 0.04911426"
  },
  {
    "objectID": "posts/HW2_young.html#b-1",
    "href": "posts/HW2_young.html#b-1",
    "title": "Homework 2",
    "section": "B",
    "text": "B\nα is a level arbitrarily set according to the researcher’s judgment, and does not affect the t or p-value of each hypothesis.\nThe p value means the probability that t will have such a value when t is a normal distribution, that is, even if ȳ=μ=500, and se is 10, the probability of ȳ=519.5 can be 5.14%. Therefore, p greater than α means that even if ȳ=μ is true, the probability of accidentally ȳ=519.5 is greater than the type 1 error set by the researcher (false positive), which means that the null hypothesis cannot be rejected.\nSo when α=0.05, Jones’s null hypothesis cannot be rejected, and Smith’s null hypothesis can be rejected. In other words, only Smith’s research results are statistically significant."
  },
  {
    "objectID": "posts/HW2_young.html#c-1",
    "href": "posts/HW2_young.html#c-1",
    "title": "Homework 2",
    "section": "C",
    "text": "C\nAs seen in B, the p-value itself means the probability of a specific t, and not revealing a specific p-value does not tell how statistically significant this study will be at the significance level (type 1 error), so information loss occurs."
  },
  {
    "objectID": "posts/HW3_solutions_Pang.html",
    "href": "posts/HW3_solutions_Pang.html",
    "title": "Homework 3",
    "section": "",
    "text": "Please check your answers against the solutions."
  },
  {
    "objectID": "posts/HW3_solutions_Pang.html#question-1",
    "href": "posts/HW3_solutions_Pang.html#question-1",
    "title": "Homework 3",
    "section": "Question 1",
    "text": "Question 1\nLoad the necessary packages.\n\nlibrary(alr4)\n\nError in library(alr4): there is no package called 'alr4'\n\nlibrary(smss)\n\nError in library(smss): there is no package called 'smss'\n\nlibrary(ggplot2)\nlibrary(stargazer)\n\nError in library(stargazer): there is no package called 'stargazer'\n\n\nLoad data:\n\ndata(UN11)\n\nWarning in data(UN11): data set 'UN11' not found\n\n\n\n(a)\nThe predictor is ppgdp, i.e. GDP per capita. The response is fertility, the birth rate per 1000 women.\n\n\n(b)\n\nggplot(data = UN11, aes(x = ppgdp, y = fertility)) +\n  geom_point()\n\nError in eval(expr, envir, enclos): object 'UN11' not found\n\n\nA straight line is not appropriate, because the relationship has an L-shaped structure (or the left half of a U-shape).\n\n\n(c)\n\nggplot(data = UN11, aes(x = log(ppgdp), y = log(fertility))) +\n  geom_point()\n\nError in eval(expr, envir, enclos): object 'UN11' not found\n\n\nYes, now a simple linear regression model is more plausible. We can imagine a negative-sloped straight line going through those points."
  },
  {
    "objectID": "posts/HW3_solutions_Pang.html#question-2",
    "href": "posts/HW3_solutions_Pang.html#question-2",
    "title": "Homework 3",
    "section": "Question 2",
    "text": "Question 2\n\n(a)\nThe conversion from USD to British pound will mean the numerical value of the response will be divided by 1.33. To offset that, the slope will also become divided by 1.33.\n\n\n(b)\nCorrelation will not change because it is a standardized measure that is not influenced by the unit of measurement.\nBoth outcomes can easily be shown via simulation."
  },
  {
    "objectID": "posts/HW3_solutions_Pang.html#question-3",
    "href": "posts/HW3_solutions_Pang.html#question-3",
    "title": "Homework 3",
    "section": "Question 3",
    "text": "Question 3\n\ndata(water)\n\nWarning in data(water): data set 'water' not found\n\npairs(water)\n\nError in eval(expr, envir, enclos): object 'water' not found\n\n\n\nYear appears to be largely unrelated to each of the other variables\nThe three variables starting with “O” seem to be correlated with each other, meaning that all the plot including two of these variables exhibit a dependence between the variables that is stronger than the dependence between the “O” variables and other variables. The three variables starting with “A” also seem to be another correlated group\nBSAAM is more closely related to the “O” variables than the “A” variables"
  },
  {
    "objectID": "posts/HW3_solutions_Pang.html#question-4",
    "href": "posts/HW3_solutions_Pang.html#question-4",
    "title": "Homework 3",
    "section": "Question 4",
    "text": "Question 4\n\ndata(Rateprof)\n\nWarning in data(Rateprof): data set 'Rateprof' not found\n\npairs(Rateprof[,c('quality', 'clarity', 'helpfulness',\n                  'easiness', 'raterInterest')])\n\nError in eval(expr, envir, enclos): object 'Rateprof' not found\n\n\nThe very strong pair-wise correlation among quality, clarity, and helpfulness is very striking. easiness is also correlated fairly highly with the other three. raterInterest is also moderately correlated, but raters almost always say they are at least moderately interested in the subject. Overall, the results might show that people don’t necessarily distinguish all these dimensions very well in their minds—or that professors that do one in one dimension tend to do well on the others too."
  },
  {
    "objectID": "posts/HW3_solutions_Pang.html#question-5",
    "href": "posts/HW3_solutions_Pang.html#question-5",
    "title": "Homework 3",
    "section": "Question 5",
    "text": "Question 5\n\n(a)\nOne way of visually representing the relationship between religiosity and political ideology is as follows (and there are other ways). As we go towards bars to the right (more religiousity), we see lighter colors pop up (more conservatism)\n\ndata(student.survey)\n\nWarning in data(student.survey): data set 'student.survey' not found\n\nggplot(data = student.survey, aes(x = re, fill = pi)) +\n    geom_bar(position = \"fill\")\n\nError in eval(expr, envir, enclos): object 'student.survey' not found\n\n\nThe relationship between high school GPA and hours of watching TV can be shown with a good old scatter plot.\n\nggplot(data = student.survey, aes(x = tv, y = hi)) +\n  geom_point() \n\nError in eval(expr, envir, enclos): object 'student.survey' not found\n\n\n\n\n(b)\nDealing with ordinal variables in linear regression is a difficult problem. We’ll just go ahead and assume that we can just convert them to numeric and use them. This would be done for political ideology and religiosity. High school GPA and hours of TV are already continuous.\n\nm1 &lt;- lm(as.numeric(pi) ~ as.numeric(re), \n         data = student.survey)\n\nError in eval(mf, parent.frame()): object 'student.survey' not found\n\nm2 &lt;- lm(hi ~ tv, data = student.survey)\n\nError in eval(mf, parent.frame()): object 'student.survey' not found\n\nstargazer(m1, m2, type = 'text', \n          dep.var.labels = c('Pol. Ideology', 'HS GPA'),\n          covariate.labels = c('Religiosity', 'Hours of TV')\n          )\n\nError in stargazer(m1, m2, type = \"text\", dep.var.labels = c(\"Pol. Ideology\", : could not find function \"stargazer\"\n\n\nReligiosity is positively and statistically significantly (at the 0.01 significance level) associated with conservatism.\nHours of TV is negatively and statistically significantly (at the 0.05 significance level) associated with High School GPA. Watching an average of 1 more hour of TV per week is associated with a 0.018 decline in High School GPA."
  },
  {
    "objectID": "posts/HW1_LTucksmith.html",
    "href": "posts/HW1_LTucksmith.html",
    "title": "HW1",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tidyr)\nknitr::opts_chunk$set(echo = TRUE)\n\n\n\n\nCode\ndf &lt;- read_excel(\"_data/LungCapData.xls\")\n\n\n\n\n\n\nWhat does the distribution of LungCap look like?\n\n\n\nCode\nhist(df$LungCap)\n\n\n\n\n\nThe distribution looks normal, the observations are centered around the mean with minimal outliers.\n\nCompare the probability distribution of the LungCap with respect to Males and Females?\n\n\n\nCode\nlungCapP &lt;- pnorm(df$LungCap, mean(df$LungCap), sd(df$LungCap))\nboxplot(lungCapP~df$Gender)\n\n\n\n\n\nFor both male and female, the probability distribution of the LungCap is similiar. For male, the median line is slightly higher than female, suggesting that their average lungCap is slightly higher for males than females. Also, for male the median is directly in the middle, suggesting that the data isn’t particularly skewed. For female the split appears to have more values fall below the mean than above, telling us that the data skews moreso than for male.\n\nCompare the mean lung capacities for smokers and non-smokers. Does it make sense?\n\n\n\nCode\ndfSmokeLungCap &lt;- df %&gt;% group_by(Smoke) %&gt;% \n    summarise(mean_LungCap=mean(LungCap))\n\nprint(dfSmokeLungCap)\n\n\n# A tibble: 2 × 2\n  Smoke mean_LungCap\n  &lt;chr&gt;        &lt;dbl&gt;\n1 no            7.77\n2 yes           8.65\n\n\nNo, the lungCap average does not make sense. We would expect that the lung capacity for smokers would be lower than it is for non-smokers, but we are seeing the opposite, as the lungCap for smokers is higher than it is for non-smokers.\n\nExamine the relationship between Smoking and Lung Capacity within age groups: “less than or equal to 13”, “14 to 15”, “16 to 17”, and “greater than or equal to 18”.\n\n\n\nCode\ndfAgeRanges &lt;- df %&gt;% \n  mutate(ageRange = case_when(\n    df$Age&lt;=13 ~ \"A\",\n    between(df$Age, 14, 15) ~ \"B\",\n    between(df$Age, 16, 17) ~ \"C\",\n    df$Age&gt;=18 ~ \"D\"\n  ))\n\nprint(dfAgeRanges)\n\n\n# A tibble: 725 × 7\n   LungCap   Age Height Smoke Gender Caesarean ageRange\n     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;     &lt;chr&gt;   \n 1    6.48     6   62.1 no    male   no        A       \n 2   10.1     18   74.7 yes   female no        D       \n 3    9.55    16   69.7 no    female yes       C       \n 4   11.1     14   71   no    male   no        B       \n 5    4.8      5   56.9 no    male   no        A       \n 6    6.22    11   58.7 no    female no        A       \n 7    4.95     8   63.3 no    male   yes       A       \n 8    7.32    11   70.4 no    male   no        A       \n 9    8.88    15   70.5 no    male   no        B       \n10    6.8     11   59.2 no    male   no        A       \n# … with 715 more rows\n\n\n\nCompare the lung capacities for smokers and non-smokers within each age group. Is your answer different from the one in part c. What could possibly be going on here?\n\n\n\nCode\ndfMeanAgeRanges &lt;- dfAgeRanges %&gt;% group_by(ageRange, Smoke) %&gt;% \n    summarise(mean_LungCap=mean(LungCap))\n\n\n`summarise()` has grouped output by 'ageRange'. You can override using the\n`.groups` argument.\n\n\nCode\nprint(dfMeanAgeRanges)\n\n\n# A tibble: 8 × 3\n# Groups:   ageRange [4]\n  ageRange Smoke mean_LungCap\n  &lt;chr&gt;    &lt;chr&gt;        &lt;dbl&gt;\n1 A        no            6.36\n2 A        yes           7.20\n3 B        no            9.14\n4 B        yes           8.39\n5 C        no           10.5 \n6 C        yes           9.38\n7 D        no           11.1 \n8 D        yes          10.5 \n\n\nHere, there is a more varied relationship between smoking and lung capacity due to age as a mitigating variable. Here, the lung capacity for non-smokers is higher than the lung capacity for smokers in youngest age range, but the other age ranges display the opposite relationship. This may be due to the possibility that smoking has a stronger influence on lungCapacity the longer you smoke. So younger folks may have only smoked for a year or so, while folks in the older age ranges have spent many more years smoking. While there still isn’t a clear indirect or direct relationship between smoking and lung capacity with this view, adding the ageRange variable showed that there is more to the relationship than was found in part C.\n\n\n\n\n\nCode\nx &lt;- c(0,1,2,3,4)\nfrequency &lt;- c(128,434,160,64,24)\ndf_2 &lt;- data.frame(x, frequency)\n\n\n\nWhat is the probability that a randomly selected inmate has exactly 2 prior convictions?\n\n\n\nCode\nprob_a = dpois(df_2[3,1], df_2[3,2]/(sum(df_2$frequency)))\nprint(prob_a)\n\n\n[1] 0.01601229\n\n\n\nWhat is the probability that a randomly selected inmate has fewer than 2 prior convictions?\n\n\n\nCode\nprob_b = ppois(df_2[2,1], (df_2[2,2]+df_2[1,2])/(sum(df_2$frequency)))\nprint(prob_b)\n\n\n[1] 0.8463379\n\n\n\nWhat is the probability that a randomly selected inmate has 2 or fewer prior convictions?\n\n\n\nCode\nprob_c = ppois(df_2[3,1], (df_2[3,2]+df_2[2,2]+df_2[1,2])/(sum(df_2$frequency)))\nprint(prob_c)\n\n\n[1] 0.9385585\n\n\n\nWhat is the probability that a randomly selected inmate has more than 2 prior convictions?\n\n\n\nCode\ndf_2d &lt;- df_2[4:5,]\nprob_d = 1 - (ppois(df_2d[2,1],(df_2[4,2]+df_2[5,2])/(sum(df_2$frequency))))\nprint(prob_d)              \n\n\n[1] 1.15223e-07\n\n\n\nWhat is the expected value1 for the number of prior convictions?\n\n\n\nCode\nprobs_e = dpois(df_2$x, df_2$frequency/(sum(df_2$frequency)))\neval = sum(df_2$x*probs_e)\nprint(eval)\n\n\n[1] 0.3458039\n\n\n\nCalculate the variance and the standard deviation for the Prior Convictions.\n\n\n\nCode\nvar_f &lt;- var(rep(df_2$x, df_2$frequency))\nprint(var_f)\n\n\n[1] 0.8572937\n\n\nCode\nsd_f &lt;- sd(rep(df_2$x, df_2$frequency))\nprint(sd_f)\n\n\n[1] 0.9259016"
  },
  {
    "objectID": "posts/HW2_MiguelCuriel.html",
    "href": "posts/HW2_MiguelCuriel.html",
    "title": "Homework 2",
    "section": "",
    "text": "Question 1\nThe time between the date a patient was recommended for heart surgery and the surgery date for cardiac patients in Ontario was collected by the Cardiac Care Network (“Wait Times Data Guide,” Ministry of Health and Long-Term Care, Ontario, Canada, 2006). The sample mean and sample standard deviation for wait times (in days) of patients for two cardiac procedures are given in the accompanying table. Assume that the sample is representative of the Ontario population.\n\n\n\n\n\n\n\n\n\nSurgical Procedure\nSample Size\nMean Wait Time\nStandard Deviation\n\n\n\n\nBypass\n539\n19\n10\n\n\nAngiography\n847\n18\n9\n\n\n\nConstruct the 90% confidence interval to estimate the actual mean wait time for each of the two procedures. Is the confidence interval narrower for angiography or bypass surgery?\n\n\nCode\n# setting seed for reproducible results\nset.seed(0)\n\n# 90% confidence interval for bypass\nbypass_ci &lt;- t.test(x=c(rnorm(539, mean=19, sd=10)), conf.level=0.90)$conf.int\n\n# 90% confidence interval for bypass\nangiography_ci &lt;- t.test(x=c(rnorm(847, mean=18, sd=9)), conf.level=0.90)$conf.int\n\n# print results\ncat(\"Bypass 90% confidence interval:\", bypass_ci, \"\\n\"\n    , \"Angiography 90% confidence interval:\", angiography_ci, \"\\n\")\n\n\nBypass 90% confidence interval: 18.32586 19.74194 \n Angiography 90% confidence interval: 17.09532 18.10881 \n\n\nAs we can see from the results, bypass’ 90% confidence interval is 18.6-20.1, while angiography’s is 17.5-18.5. Therefore, angiography’s confidence interval is narrower.\n\n\nQuestion 2\nA survey of 1031 adult Americans was carried out by the National Center for Public Policy. Assume that the sample is representative of adult Americans. Among those surveyed, 567 believed that college education is essential for success. Find the point estimate, p, of the proportion of all adult Americans who believe that a college education is essential for success. Construct and interpret a 95% confidence interval for p.\n\n\nCode\n# setting seedd for reproducible results\nset.seed(0)\n\n# point estimate\npoint_estimate &lt;- 567/1031\n\n# 95% confidence interval\nproportion_test &lt;- prop.test(567, 1031, conf.level = 0.95)$conf.int\n\n# print results\ncat(\"Point estimate:\", point_estimate, \"\\n\"\n    , \"95% confidence interval:\", proportion_test, \"\\n\")\n\n\nPoint estimate: 0.5499515 \n 95% confidence interval: 0.5189682 0.580558 \n\n\nThe results indicate that we are 95% confident that the true proportion of all adult Americans who believe that college education is essential for success lies between .52-.58. In other words, we can say with 95% certainty that 52%-58% of adult Americans believe that college is essential for success.\n\n\nQuestion 3\nSuppose that the financial aid office of UMass Amherst seeks to estimate the mean cost of textbooks per semester for students. The estimate will be useful if it is within $5 of the true population mean (i.e. they want the confidence interval to have a length of $10 or less). The financial aid office is pretty sure that the amount spent on books varies widely, with most values between $30 and $200. They think that the population standard deviation is about a quarter of this range (in other words, you can assume they know the population standard deviation). Assuming the significance level to be 5%, what should be the size of the sample?\n\n\nCode\n# margin of error formula\n## ME = z * (sigma / sqrt(n))\n### solving for n\n#### n = (z * sigma/ME)^2\n##### replacing values\n###### z = significance level 5% = critical z value for 95% conf int = 1.96\n###### sigma = 1/4 of range of textbook costs = (200-30)/4 = 42.5\n###### ME = estimate within 5 of the true population mean = 5\n\n# calculate result\nn &lt;- round((1.96*42.5/5)^2)\n\n# print result\ncat(\"Ideal sample size:\", n, \"\\n\")\n\n\nIdeal sample size: 278 \n\n\nUsing the margin of error formula for confidence intervals and solving for n (sample size), we see that the ideal sample size (rounded to the nearest integer) is 278.\n\n\nQuestion 4\nAccording to a union agreement, the mean income for all senior-level workers in a large service company equals $500 per week. A representative of a women’s group decides to analyze whether the mean income μ for female employees matches this norm. For a random sample of nine female employees, ȳ = $410 and s = 90.\n\nTest whether the mean income of female employees differs from $500 per week. Include assumptions, hypotheses, test statistic, and P-value. Interpret the result.\n\nAssumptions:\n\nThe data is normally distributed.\nThe sample is a simple random sample.\nThe standard deviation of the population is unknown.\n\nHypotheses:\n\nH0: μ = 500\nH1: Ha: μ ≠ 500\n\nTest statistic:\n\nt = (ȳ - μ) / (s / sqrt(n)) = (410 - 500) / (90 / sqrt(9)) = -3\n\nP-value:\n\nThe P-value is 0.01707168, which is less than the level of significance of 0.05. Therefore, we reject the null hypothesis and conclude that there is sufficient evidence to suggest that the mean income of female employees differs from $500 per week.\n\n\nReport the P-value for Ha: μ &lt; 500. Interpret.\n\nThe P-value is 0.008535841, which is less than the level of significance of 0.05. Therefore, we reject the null hypothesis and conclude that there is sufficient evidence to suggest that the mean income of female employees is less than $500 per week.\n\nReport and interpret the P-value for Ha: μ &gt; 500.\n\nThe P-value is 0.9914642, which is more than the level of significance of 0.05. Therefore, we fail to reject the null hypothesis and conclude that there is sufficient evidence to suggest that the mean income of female employees is greater than $500 per week.\n\n\n\n\nCode\n# A.3.a. Test statistic\n(410 - 500) / (90 / sqrt(9))\n\n# A.4.a. P-value two-sided\npt(-3, 8) * 2\n\n# B.1. P-value μ &lt; 500\npt(-3, 8)\n\n# C.1. P-value μ &gt; 500\npt(-3, 8, lower.tail=FALSE)\n\n\n\n\nQuestion 5\nJones and Smith separately conduct studies to test H0: μ = 500 against Ha: μ ≠ 500, each with n = 1000. Jones gets ȳ = 519.5, with se = 10.0. Smith gets ȳ = 519.7, with se = 10.0.\n\nShow that t = 1.95 and P-value = 0.051 for Jones. Show that t = 1.97 and P-value = 0.049 for Smith.\n\nJones:\n\nt = (ȳ - μ) / se = (519.5 - 500) / 10 = 19.5 / 10 = 1.95\nP-value = round(2 * pt(q=1.95, df=999, lower.tail=FALSE), 3) = 0.051\n\nSmith:\n\nt = (ȳ - μ) / se = (519.7 - 500) / 10 = 19.7 / 10 = 1.97\nP-value = round(2 * pt(q=1.97, df=999, lower.tail=FALSE), 3) = 0.049\n\n\nUsing α = 0.05, for each study indicate whether the result is “statistically significant.”\n\nSince Jones’ P-value is greater than 0.05, we fail to reject the null hypothesis, indicating that his results are not statistically significant. In contrast, Smith’s P-value is less than 0.05, therefore we reject the null hypothesis and find his results statistically significant.\n\nUsing this example, explain the misleading aspects of reporting the result of a test as “P ≤ 0.05” versus “P &gt; 0.05,” or as “reject H0” versus “Do not reject H0,” without reporting the actual P-value.\n\n“P ≤ 0.05” or “reject H0” without reporting the actual P-value can be misleading because it doesn’t provide information on how strong the evidence is against the null hypothesis. For example, both Jones and Smith barely pass the 0.05 threshold, having 0.049 and 0.051, respectively. Reporting this would help readers and analysts to take the strength of the evidence in consideration (in this case, “rejecting H0” or “failing to reject H0” should be taken with a grain of salt).\n\n\n\n\nCode\n# A.1.b. Proving Jones' p-value\nround(2 * pt(q=1.95, df=999, lower.tail=FALSE), 3)\n\n# A.2.b. Proving Smith's p-value\nround(2 * pt(q=1.97, df=999, lower.tail=FALSE), 3)\n\n\n\n\nQuestion 6\nA school nurse wants to determine whether age is a factor in whether children choose a healthy snack after school. She conducts a survey of 300 middle school students, with the results below. Test at α = 0.05 the claim that the proportion who choose a healthy snack differs by grade level. What is the null hypothesis? Which test should we use? What is the conclusion?\n\n\n\nGrade level\n6th grade\n7th grade\n8th grade\n\n\n\n\nHealthy snack\n31\n43\n51\n\n\nUnhealthy snack\n69\n57\n49\n\n\n\n\nNull hypothesis: There is no difference in the proportion of students who choose a healthy snack based on grade level.\nTest: Chi-squared test because we are assessing whether proportions of outcomes (choosing healthy versus unhealthy snacks) in each grade are equal or different.\nConclusion: Since the p-value is 0.01547, we reject the null hypothesis at the 0.05 level of significance and conclude that there is a significant difference in the proportion of healthy snack choices among the different grade levels.\n\n\n\nCode\n# create a matrix of the observed values\nobserved &lt;- matrix(c(31, 43, 51, 69, 57, 49), nrow = 2, byrow = TRUE)\n\n# perform the chi-squared test\nresult &lt;- chisq.test(observed)\n\n# print the results\nprint(observed)\n\n\n     [,1] [,2] [,3]\n[1,]   31   43   51\n[2,]   69   57   49\n\n\nCode\nprint(result)\n\n\n\n    Pearson's Chi-squared test\n\ndata:  observed\nX-squared = 8.3383, df = 2, p-value = 0.01547\n\n\n\n\nQuestion 7\nPer-pupil costs (in thousands of dollars) for cyber charter school tuition for school districts in three areas are shown. Test the claim that there is a difference in means for the three areas, using an appropriate test. What is the null hypothesis? Which test should we use? What is the conclusion?\n\n\n\nArea 1\n6.2\n9.3\n6.8\n6.1\n6.7\n7.5\n\n\nArea 2\n7.5\n8.2\n8.5\n8.2\n7.0\n9.3\n\n\nArea 3\n5.8\n6.4\n5.6\n7.1\n3.0\n3.5\n\n\n\n\nNull hypothesis: There is no difference in means for the three areas.\nTest: Analysis of Variance (ANOVA) because we are computing the difference between the means of three or more groups.\nConclusion: Given that the P-value associated to the F-statistic is 0.00397, we reject the null hypothesis and conclude that there is a significant difference in means for the three areas.\n\n\n\nCode\narea1 &lt;- c(6.2, 9.3, 6.8, 6.1, 6.7, 7.5)\narea2 &lt;- c(7.5, 8.2, 8.5, 8.2, 7.0, 9.3)\narea3 &lt;- c(5.8, 6.4, 5.6, 7.1, 3.0, 3.5)\n\nanova_result &lt;- aov(c(area1, area2, area3) ~ rep(c(\"Area 1\", \"Area 2\", \"Area 3\")\n                                                 , c(6, 6, 6)))\nprint(summary(anova_result))\n\n\n                                                 Df Sum Sq Mean Sq F value\nrep(c(\"Area 1\", \"Area 2\", \"Area 3\"), c(6, 6, 6))  2  25.66  12.832   8.176\nResiduals                                        15  23.54   1.569        \n                                                  Pr(&gt;F)   \nrep(c(\"Area 1\", \"Area 2\", \"Area 3\"), c(6, 6, 6)) 0.00397 **\nResiduals                                                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "posts/DerianToth_HW2.html",
    "href": "posts/DerianToth_HW2.html",
    "title": "Homework 2",
    "section": "",
    "text": "Question 1: Construct the 90% confidence interval to estimate the actual mean wait time for each of the two procedures. Is the confidence interval narrower for angiography or bypass surgery?\n\n\n\n\n\n\n\n\n\nSurgical Procedure\nSample Size\nMean Wait Time\nStandard Deviation\n\n\nBypass\n539\n19\n10\n\n\nAngiography\n847\n18\n9\n\n\n\n\n\nCode\n#90% CI for Bypass\n\nBy_mean&lt;-19\nBy_sd&lt;-10\nBy_size&lt;-539\n\nBySE&lt;-By_sd/sqrt(By_size)\n\nConfidence_level1&lt;-0.90\n\ntail_area&lt;-(1-Confidence_level1)/2\n\nBy_t_score&lt;-qt(p = 1 - tail_area, df = By_size - 1)\n\nBy_CI &lt;- c(By_mean - By_t_score * BySE, By_mean + By_t_score * BySE)\nBy_CI\n\n\n[1] 18.29029 19.70971\n\n\nCode\n#Difference between the range average wait time for Bypass\n19.70971-18.29029\n\n\n[1] 1.41942\n\n\nThe average wait time for a Bypass ranges from 18.29 minutes to 19.71 minutes.\n\n\nCode\n#CI Angiography\n\nAng_mean&lt;-18\nAng_sd&lt;-9\nAng_size&lt;-847\n\nAng_SE&lt;- Ang_sd/sqrt(Ang_size)\n\n#Condifence level and tail area calculated in the code chunk above\n\nAng_t_score &lt;- qt(p = 1 - tail_area, df = Ang_size)\n\nAng_CI &lt;- c(Ang_mean - Ang_t_score * Ang_SE, Ang_mean + Ang_t_score * Ang_SE)\nAng_CI\n\n\n[1] 17.49078 18.50922\n\n\nCode\n#Difference between the range average wait time for Angiography\n18.50922-17.49078\n\n\n[1] 1.01844\n\n\nThe average wait time for an Angiography ranges from 17.5 minutes to 18.51 minutes.\nThe difference in wait time for Bypass is 1.42 minutes, whereas the difference in wait time for Angiography is 1.02 minutes. Therefor the confidence interval is narrower for the Angiography wait time.\n\n\nQuestion 2: Find the point estimate, p, of the proportion of all adult Americans who believe that a college education is essential for success. Construct and interpret a 95% confidence interval for p.\nA survey of 1031 adult Americans was carried out by the National Center for Public Policy. Assume that the sample is representative of adult Americans. Among those surveyed, 567 believed that college education is essential for success.\n\n\nCode\nNCPP_size&lt;-1031\nCollegeSuc&lt;-567\n#Calculating those who do not necessarily believe you need a college degree to be successful (1031-567)\nNotCollegeSuc&lt;-464\n\n#Creating a dataframe to run a t-test to find the P value and confidence interval\nNCPP_DF&lt;-data.frame(CollegeSuc, NotCollegeSuc)\n\nprop.test(NCPP_DF$CollegeSuc,1031)\n\n\n\n    1-sample proportions test with continuity correction\n\ndata:  NCPP_DF$CollegeSuc out of 1031, null probability 0.5\nX-squared = 10.091, df = 1, p-value = 0.00149\nalternative hypothesis: true p is not equal to 0.5\n95 percent confidence interval:\n 0.5189682 0.5805580\nsample estimates:\n        p \n0.5499515 \n\n\nThe point of estimate (p value) for adult Americans who believe that you need a college degree to be successful is 0.5499515, meaning about 55% of adult Americans believe you need a college degree to be successful. The confidence interval (95%) shows a range of 0.5189682 to 0.5805580, meaning this belief could range from about 52% of adult Americans to 58% of adult Americans.\n\n\nQuestion 3: Assuming the significance level to be 5%, what should be the size of the sample?\nSuppose that the financial aid office of UMass Amherst seeks to estimate the mean cost of textbooks per semester for students. The estimate will be useful if it is within $5 of the true population mean (i.e. they want the confidence interval to have a length of $10 or less). The financial aid office is pretty sure that the amount spent on books varies widely, with most values between $30 and $200. They think that the population standard deviation is about a quarter of this range (in other words, you can assume they know the population standard deviation).\n\n\nCode\n#Brainstorming:\n#.25*30\n#Pop_sd_Low&lt;-7.5\n#.25*200\n#Pop_sd_High&lt;-50\n#Confidence_level2&lt;-0.95\n\nPop_sd&lt;-.25*(200-30)\nPop_sd\n\n\n[1] 42.5\n\n\nCode\n#Based on the formula for a 95% confidence interval:\n\nz &lt;- qnorm(.95)\ns &lt;- Pop_sd\nn &lt;- ((z*s)/5)^2\nprint(n)\n\n\n[1] 195.4755\n\n\nCode\n#Question - this drops the sample mean, how can we use this formula without the sample mean?\n\n\nDue to the fact that the financial aid office believes that the amount spent on books varies widely, we can assume that the estimator value is not efficient. This means our sample mean should have a smaller variance than our sample median. Therefore, we need a higher sample size in order to reduce the variance.\n\n\nQuestion 4A - 4C:\nAccording to a union agreement, the mean income for all senior-level workers in a large service company equals $500 per week. A representative of a women’s group decides to analyze whether the mean income μ for female employees matches this norm. For a random sample of nine female employees, ȳ = $410 and s = 90\nHint: The P-values for the two possible one-sided tests must sum to 1.\n\nA. Test whether the mean income of female employees differs from $500 per week. Include assumptions, hypotheses, test statistic, and P-value. Interpret the result.\n\n\nCode\n#pop mean = 500\n#sample mean = 410\n#sample standard deviation = 90\n#sample size = 9\n\nSEM&lt;- 90/sqrt(9)\nSEM\n\n\n[1] 30\n\n\nCode\n#SEM = 30\n\nqnorm(0.025, mean = 500, sd = SEM)\n\n\n[1] 441.2011\n\n\nCode\n#qnorm(0.025)=$441.2011\nqnorm(0.975, mean = 500, sd = SEM)\n\n\n[1] 558.7989\n\n\nCode\n#qnorm(0.975)=$558.7989\n\n#Also\n500-1.96*SEM\n\n\n[1] 441.2\n\n\nCode\n500+1.96*SEM\n\n\n[1] 558.8\n\n\nCode\ns_mean&lt;- 410\ns_sd&lt;- 90\ns_size&lt;-9\nconfidence_level3&lt;-0.975\ntail_area2&lt;-(1-confidence_level3)/2\nt_score&lt;-qt(p = 1-tail_area2, df = s_size-1)\nCI&lt;- c(s_mean-t_score*s_sd/sqrt(s_size),\ns_mean+t_score*s_sd/sqrt(s_size))\nprint(CI)\n\n\n[1] 327.4543 492.5457\n\n\nCode\nt_statistic&lt;-410-500/(30/sqrt(9))\nt_statistic\n\n\n[1] 360\n\n\nCode\np_value&lt;- (1 - pt(t_statistic, df=8))*2\np_value\n\n\n[1] 0\n\n\nHere we are working with a two-tailed test, because we want to know if female employees are making more or less than and average of $500/ week. A normal distribution tells us that only 5% of the observations would be less than $441.20 and more than $558.80, 95% of the observations are expected to to be between this range (about $441 - $559).\nSignificance Testing\nIn this test, the null hypothesis is that female employees make on average $500/ week.\nThe p value is: 0\nThe t value is: 360\nWith a t value of 360, greater than the critical value of 5, we reject the null hypothesis that the population mean is 500.\n\n\nB. Report the P-value for Ha: μ &lt; 500. Interpret\n\n\nCode\n#pop mean = 500\n#sample mean = 410\n#sample standard deviation = 90\n#sample size = 9\n\nqnorm(0.05, mean = 500, sd = SEM)\n\n\n[1] 450.6544\n\n\nCode\n#qnorm(0.05)=$450.6544\n\ns_mean&lt;- 410\ns_sd&lt;- 90\ns_size&lt;-9\nconfidence_level4&lt;-0.05\ntail_area3&lt;-(1-confidence_level4)/1\nt_score&lt;-qt(p = 1-tail_area3, df = s_size-1)\nCI&lt;- c(s_mean-t_score*s_sd/sqrt(s_size),\ns_mean+t_score*s_sd/sqrt(s_size))\nprint(CI)\n\n\n[1] 465.7864 354.2136\n\n\nCode\nt_statistic2&lt;-410-500/(30/sqrt(9))\nt_statistic2\n\n\n[1] 360\n\n\nCode\np_value2&lt;- (1 - pt(t_statistic, df=8))*1\np_value2\n\n\n[1] 0\n\n\nHere we are working with a one-tailed test, because we are asking whether female employees make an average of less than $500/ week. A normal distribution tells us that 5% of the observations would be less than $450.6544 and 95% of the observations would be more than that.\nSignificance Testing\np value = 0\nt value = 360\nWith a critical value of 0, and a t value of 360 (larger than the critical value) we reject the null hypothesis that the population mean is less than 500.\n\n\nC. Report and interpret the P-value for Ha: μ &gt; 500.\n\n\nCode\n#pop mean = 500\n#sample mean = 410\n#sample standard deviation = 90\n#sample size = 9\n\nqnorm(0.95, mean = 500, sd = SEM)\n\n\n[1] 549.3456\n\n\nCode\n#qnorm(0.025)=$549.3456\n\ns_mean&lt;- 410\ns_sd&lt;- 90\ns_size&lt;-9\nconfidence_level5&lt;-0.95\ntail_area4&lt;-(1-confidence_level5)/1\nt_score&lt;-qt(p = 1-tail_area4, df = s_size-1)\nCI&lt;- c(s_mean-t_score*s_sd/sqrt(s_size),\ns_mean+t_score*s_sd/sqrt(s_size))\nprint(CI)\n\n\n[1] 354.2136 465.7864\n\n\nCode\nt_statistic3&lt;-410+500/(30/sqrt(9))\nt_statistic3\n\n\n[1] 460\n\n\nCode\np_value3&lt;- (1 - pt(t_statistic, df=8))*1\np_value3\n\n\n[1] 0\n\n\nHere we are working with a one-tailed test, because we are asking whether female employees make an average of more than $500/ week. A normal distribution tells us that 5% of the observations would be more than $549.34 and 95% of the observations would be less than that.\nSignificance Testing\np value = 0\nt value = 460\nWith a critical value of 0, and a t value of 460 (larger than the critical value) we reject the null hypothesis that the population mean is greater than 500.\n\n\n\nQuestion 5A - 5C:\nJones and Smith separately conduct studies to test H0: μ = 500 against Ha: μ ≠ 500, each with n = 1000. Jones gets ȳ = 519.5, with se = 10.0. Smith gets ȳ = 519.7, with se = 10.0.\n\nA. Show that t = 1.95 and P-value = 0.051 for Jones. Show that t = 1.97 and P-value = 0.049 for Smith.\n\n\nCode\n#H0: μ = 500\n#Ha: μ ≠ 500\n\n#Jones: \n#n = 1000; y = 519.5; se = 10\n#t = 1.95 and P-value = 0.051\n#t_value = x1-x2/se\nJones_t_value&lt;-(519.5-500)/10\nJones_t_value\n\n\n[1] 1.95\n\n\nCode\nJones_p_value&lt;-(2*pt(Jones_t_value, df = 999, lower.tail=FALSE))\nJones_p_value\n\n\n[1] 0.05145555\n\n\nCode\n#Smith: \n#n = 1000; y = 519.7; se = 10\n#t = 1.97 and P-value = 0.049\nSmith_t_value&lt;-(519.7-500)/10\nSmith_t_value\n\n\n[1] 1.97\n\n\nCode\nSmith_p_value&lt;-(2*pt(Smith_t_value, df=999, lower.tail = FALSE))\nSmith_p_value\n\n\n[1] 0.04911426\n\n\n\n\nB. Using α = 0.05, for each study indicate whether the result is “statistically significant.”\nSmith’s results are statistically significant, as the p value is less than .05. However, Jone’s results are not statistically significant since they are greater than .05.\n\n\nC. Using this example, explain the misleading aspects of reporting the result of a test as “P ≤ 0.05” versus “P &gt; 0.05,” or as “reject H0” versus “Do not reject H0,” without reporting the actual P-value.\nReporting the actual p value provides context for how strong the results of the statistical analysis are. Without the context of the actual p value, it hides this context, and does not provide enough information to understand the validity of the results.\n\n\n\nQuestion 6: Test at α = 0.05 the claim that the proportion who choose a healthy snack differs by grade level. What is the null hypothesis? Which test should we use? What is the conclusion?\nA school nurse wants to determine whether age is a factor in whether children choose a healthy snack after school. She conducts a survey of 300 middle school students, with the results below.\n\n\n\nGrade level\n6th grade\n7th grade\n8th grade\n\n\nHealthy snack\n31\n43\n51\n\n\nUnhealthy snack\n69\n57\n49\n\n\n\n\n\nCode\n#n = 300\n\ngrade_level&lt;-(c(rep(\"6th Grade\", 100), \n                rep(\"7th Grade\", 100), \n                rep(\"8th Grade\", 100)))\nsnack_choice&lt;-(c(rep(\"healthy snack\", 31), \n                 rep(\"unhealthy snack\", 69), \n                 rep(\"healthy snack\", 43), \n                 rep(\"unhealthy snack\", 57), \n                 rep(\"healthy snack\", 51), \n                 rep(\"unhealthy snack\", 49)))\nMS_snack_choice&lt;-data.frame(grade_level, snack_choice)\n\nchisq.test(MS_snack_choice$snack_choice, MS_snack_choice$grade_level, correct = FALSE)\n\n\n\n    Pearson's Chi-squared test\n\ndata:  MS_snack_choice$snack_choice and MS_snack_choice$grade_level\nX-squared = 8.3383, df = 2, p-value = 0.01547\n\n\nI chose to use a chi-square test because we are comparing a categorical IV with a categorical DV. The results show a significant relationship between grade level and snack choice. We know it is significant because the p-value is less than .05 (p = 0.015).\n\n\nQuestion 7: Test the claim that there is a difference in means for the three areas, using an appropriate test. What is the null hypothesis? Which test should we use? What is the conclusion?\nPer-pupil costs (in thousands of dollars) for cyber charter school tuition for school districts in three areas are shown.\n\n\n\nArea 1\n6.2\n9.3\n6.8\n6.1\n6.7\n7.5\n\n\nArea 2\n7.5\n8.2\n8.5\n8.2\n7.0\n9.3\n\n\nArea 3\n5.8\n6.4\n5.6\n7.1\n3.0\n3.5\n\n\n\n\n\nCode\nArea&lt;-c(rep(\"Area1\", 6), \n        rep(\"Area2\", 6), \n        rep(\"Area3\", 6))\n\nCost&lt;-c(6.2, 9.3, 6.8, 6.1, 6.7, 7.5, 7.5, 8.2, 8.5, 8.2, 7.0, 9.3,\n          5.8, 6.4, 5.6, 7.1, 3.0, 3.5)\nArea_cost&lt;-data.frame(Area, Cost)\n\nmy.anova &lt;- aov(Cost ~ Area, data = Area_cost)\nsummary(my.anova)\n\n\n            Df Sum Sq Mean Sq F value  Pr(&gt;F)   \nArea         2  25.66  12.832   8.176 0.00397 **\nResiduals   15  23.54   1.569                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nI chose to run an ANOVA because we have a continuous DV and more than one categorical IV. According to the results, there is a significant relationship between area and cost because the p value is less than .05 (p = 0.00397). The null hypothesis for this test is that the means for each Area are equal. We reject the null hypothesis with a clear difference between the means."
  },
  {
    "objectID": "posts/HW3_XiaoyanHu.html",
    "href": "posts/HW3_XiaoyanHu.html",
    "title": "Homework 3",
    "section": "",
    "text": "Code\nlibrary(tidyr)\nlibrary(dplyr)\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nCode\nlibrary(readxl)\nlibrary(ggplot2)\nlibrary(alr4)\n\n\nLoading required package: car\n\n\nLoading required package: carData\n\n\n\nAttaching package: 'car'\n\n\nThe following object is masked from 'package:dplyr':\n\n    recode\n\n\nLoading required package: effects\n\n\nlattice theme set by effectsTheme()\nSee ?effectsTheme for details.\n\n\nCode\nlibrary(smss)\nlibrary(stargazer)\n\n\n\nPlease cite as: \n\n\n Hlavac, Marek (2022). stargazer: Well-Formatted Regression and Summary Statistics Tables.\n\n\n R package version 5.2.3. https://CRAN.R-project.org/package=stargazer \n\n\n\nQuestion 1\n\nUnited Nations (Data file: UN11in alr4) The data in the file UN11 contains several variables, including ppgdp, the gross national product per person in U.S. dollars, and fertility, the birth rate per 1000 females, both from the year 2009. The data are for 199 localities, mostly UN member countries, but also other areas such as Hong Kong that are not independent countries. The data were collected from the United Nations (2011). We will study the dependence of fertility on ppgdp.\n\n\nIdentify the predictor and the response.\n\n\n\nCode\nhead(UN11)\n\n\n                region  group fertility   ppgdp lifeExpF pctUrban\nAfghanistan       Asia  other     5.968   499.0    49.49       23\nAlbania         Europe  other     1.525  3677.2    80.40       53\nAlgeria         Africa africa     2.142  4473.0    75.00       67\nAngola          Africa africa     5.135  4321.9    53.17       59\nAnguilla     Caribbean  other     2.000 13750.1    81.10      100\nArgentina   Latin Amer  other     2.172  9162.1    79.89       93\n\n\npredictor is ppgdp and response is fertility and birth rate.\n\nDraw the scatterplot of fertility on the vertical axis versus ppgdp on the horizontal axis and summarize the information in this graph. Does a straight-line mean function seem to be plausible for a summary of this graph?\n\n\n\nCode\nggplot(UN11, aes(ppgdp, fertility)) +\n  geom_point()\n\n\n\n\n\nstraight line doesn’t fit for summary this graph.\n\nDraw the scatterplot of log(fertility) versus log(ppgdp) using natural logarithms. Does the simple linear regression model seem plausible for a summary of this graph? If you use a different base of logarithms, the shape of the graph won’t change, but the values on the axes will change.\n\n\n\nCode\nggplot(UN11, aes(ppgdp, fertility)) +\n  geom_point()+scale_y_log10()+scale_x_log10()\n\n\n\n\n\n\n\nQuestion 2\nAnnual income, in dollars, is an explanatory variable in a regression analysis. For a British version of the report on the analysis, all responses are converted to British pounds sterling (1 pound equals about 1.33 dollars, as of 2016). (a) How, if at all, does the slope of the prediction equation change? the slope will also divided by 1.33 (b) How, if at all, does the correlation change? correlation will not change # Question 3 Water runoff in the Sierras (Data file: water in alr4) Can Southern California’s water supply in future years be predicted from past data? One factor affecting water availability is stream runoff. If runoff could be predicted, engineers, planners, and policy makers could do their jobs more efficiently. The data file contains 43 years’ worth of precipitation measurements taken at six sites in the Sierra Nevada mountains (labeled APMAM, APSAB, APSLAKE, OPBPC, OPRC, and OPSLAKE) and stream runoff volume at a site near Bishop, California, labeled BSAAM. Draw the scatterplot matrix for these data and summarize the information available from these plots. (Hint: Use the pairs() function.)\n\n\nCode\nhead(water)\n\n\n  Year APMAM APSAB APSLAKE OPBPC  OPRC OPSLAKE  BSAAM\n1 1948  9.13  3.58    3.91  4.10  7.43    6.47  54235\n2 1949  5.28  4.82    5.20  7.55 11.11   10.26  67567\n3 1950  4.20  3.77    3.67  9.52 12.20   11.35  66161\n4 1951  4.60  4.46    3.93 11.14 15.15   11.13  68094\n5 1952  7.15  4.99    4.88 16.34 20.05   22.81 107080\n6 1953  9.70  5.65    4.91  8.88  8.15    7.41  67594\n\n\nCode\npairs(water)\n\n\n\n\n\n\n\nQuestion 4\nProfessor ratings (Data file: Rateprof in alr4) In the website and online forum RateMyProfessors.com, students rate and comment on their instructors. Launched in 1999, the site includes millions of ratings on thousands of instructors. The data file includes the summaries of the ratings of 364 instructors at a large campus in the Midwest (Bleske-Rechek and Fritsch, 2011). Each instructor included in the data had at least 10 ratings over a several year period. Students provided ratings of 1–5 on quality, helpfulness, clarity, easiness of instructor’s courses, and raterInterest in the subject matter covered in the instructor’s courses. The data file provides the averages of these five ratings. Create a scatterplot matrix of these five variables. Provide a brief description of the relationships between the five ratings.\n\n\nCode\nhead(Rateprof)\n\n\n  gender numYears numRaters numCourses pepper discipline              dept\n1   male        7        11          5     no        Hum           English\n2   male        6        11          5     no        Hum Religious Studies\n3   male       10        43          2     no        Hum               Art\n4   male       11        24          5     no        Hum           English\n5   male       11        19          7     no        Hum           Spanish\n6   male       10        15          9     no        Hum           Spanish\n   quality helpfulness  clarity easiness raterInterest sdQuality sdHelpfulness\n1 4.636364    4.636364 4.636364 4.818182      3.545455 0.5518564     0.6741999\n2 4.318182    4.545455 4.090909 4.363636      4.000000 0.9020179     0.9341987\n3 4.790698    4.720930 4.860465 4.604651      3.432432 0.4529343     0.6663898\n4 4.250000    4.458333 4.041667 2.791667      3.181818 0.9325048     0.9315329\n5 4.684211    4.684211 4.684211 4.473684      4.214286 0.6500112     0.8200699\n6 4.233333    4.266667 4.200000 4.533333      3.916667 0.8632717     1.0327956\n  sdClarity sdEasiness sdRaterInterest\n1 0.5045250  0.4045199       1.1281521\n2 0.9438798  0.5045250       1.0744356\n3 0.4129681  0.5407021       1.2369438\n4 0.9990938  0.5882300       1.3322506\n5 0.5823927  0.6117753       0.9749613\n6 0.7745967  0.6399405       0.6685579\n\n\nCode\npairs(Rateprof[,c('quality', 'clarity', 'helpfulness',\n                  'easiness', 'raterInterest')])\n\n\n\n\n\nIn this five parameters, quality, Clarity and helpfulness are positively correlated. while easiness and raterinterest were not significantly related. # Question 5 For the student.survey data file in the smss package, conduct regression analyses relating (by convention, y denotes the outcome variable, x denotes the explanatory variable) (i) y = political ideology and x = religiosity, (ii) y = high school GPA and x = hours of TV watching. (You can use ?student.survey in the R console, after loading the package, to see what each variable means.) (a) Graphically portray how the explanatory variable relates to the outcome variable in each of the two cases\n\n\nCode\nhead(student.survey)\n\n\nError in head(student.survey): object 'student.survey' not found\n\n\nCode\nggplot(data = student.survey, aes(x = re,fill=pi)) +\n    geom_bar(position = \"dodge\" )\n\n\nError in ggplot(data = student.survey, aes(x = re, fill = pi)): object 'student.survey' not found\n\n\n\n\nCode\nggplot(data = student.survey, aes(x = hi, y = tv)) +\n    geom_point()\n\n\nError in ggplot(data = student.survey, aes(x = hi, y = tv)): object 'student.survey' not found\n\n\n\nSummarize and interpret results of inferential analyses."
  },
  {
    "objectID": "posts/Homework2_AlexaPotter.html",
    "href": "posts/Homework2_AlexaPotter.html",
    "title": "Homework 2",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\n\n-- Attaching packages --------------------------------------- tidyverse 1.3.2 --\nv ggplot2 3.4.0      v purrr   0.3.5 \nv tibble  3.1.8      v dplyr   1.0.10\nv tidyr   1.2.1      v stringr 1.5.0 \nv readr   2.1.3      v forcats 0.5.2 \n-- Conflicts ------------------------------------------ tidyverse_conflicts() --\nx dplyr::filter() masks stats::filter()\nx dplyr::lag()    masks stats::lag()"
  },
  {
    "objectID": "posts/Homework2_AlexaPotter.html#a.-test-whether-the-mean-income-of-female-employees-differs-from-500-per-week.-include-assumptions-hypotheses-test-statistic-and-p-value.-interpret-the-result.",
    "href": "posts/Homework2_AlexaPotter.html#a.-test-whether-the-mean-income-of-female-employees-differs-from-500-per-week.-include-assumptions-hypotheses-test-statistic-and-p-value.-interpret-the-result.",
    "title": "Homework 2",
    "section": "A. Test whether the mean income of female employees differs from $500 per week. Include assumptions, hypotheses, test statistic, and P-value. Interpret the result.",
    "text": "A. Test whether the mean income of female employees differs from $500 per week. Include assumptions, hypotheses, test statistic, and P-value. Interpret the result.\nHypothesis: H0: μ = 500 HA: μ ≠ 500\nAssumptions: - The data is collected from a representative & randomly selected portion of the total population. - The data is a normal distribution - The two groups have the same population variance (homoskedasticity)\nFormula for t-test: t = (X‾ - μ0) / (s / √n) X‾ = 410 μ0 = 500 s = 90 n = 9\n\n\nCode\nt_stat &lt;- (410 - 500) / (90 / sqrt(9))\nt_stat\n\n\n[1] -3\n\n\np-value:\n\n\nCode\n2*pt(q=t_stat, df=8)\n\n\n[1] 0.01707168\n\n\nSince the p-value is less than 0.05 we can reject the null hypothesis at significance level 0.05."
  },
  {
    "objectID": "posts/Homework2_AlexaPotter.html#b.-report-the-p-value-for-ha-μ-500.-interpret.",
    "href": "posts/Homework2_AlexaPotter.html#b.-report-the-p-value-for-ha-μ-500.-interpret.",
    "title": "Homework 2",
    "section": "B. Report the P-value for Ha: μ < 500. Interpret.",
    "text": "B. Report the P-value for Ha: μ &lt; 500. Interpret.\n\n\nCode\npt(q= t_stat, df=8, lower.tail=TRUE)\n\n\n[1] 0.008535841\n\n\nSince the p value is less than 0.05 we can reject the null hypothesis at 0.05 significance level."
  },
  {
    "objectID": "posts/Homework2_AlexaPotter.html#c.-report-and-interpret-the-p-value-for-ha-μ-500.",
    "href": "posts/Homework2_AlexaPotter.html#c.-report-and-interpret-the-p-value-for-ha-μ-500.",
    "title": "Homework 2",
    "section": "C. Report and interpret the P-value for Ha: μ > 500.",
    "text": "C. Report and interpret the P-value for Ha: μ &gt; 500.\n(Hint: The P-values for the two possible one-sided tests must sum to 1.)\n\n\nCode\npt(q= t_stat, df=8, lower.tail=FALSE)\n\n\n[1] 0.9914642\n\n\nSince the p value is greater than 0.05 we fail to reject the null hypothesis at 0.05 significance level."
  },
  {
    "objectID": "posts/Homework2_AlexaPotter.html#a.-show-that-t-1.95-and-p-value-0.051-for-jones.-show-that-t-1.97-and-p-value-0.049-for-smith.",
    "href": "posts/Homework2_AlexaPotter.html#a.-show-that-t-1.95-and-p-value-0.051-for-jones.-show-that-t-1.97-and-p-value-0.049-for-smith.",
    "title": "Homework 2",
    "section": "A. Show that t = 1.95 and P-value = 0.051 for Jones. Show that t = 1.97 and P-value = 0.049 for Smith.",
    "text": "A. Show that t = 1.95 and P-value = 0.051 for Jones. Show that t = 1.97 and P-value = 0.049 for Smith.\nJones\nt = (ȳ - μ)/se\n\n\nCode\njones_t &lt;- (519.5-500)/10\nprint(jones_t)\n\n\n[1] 1.95\n\n\nCode\n#p-value:\n2*pt(-abs(jones_t),df=1000-1)\n\n\n[1] 0.05145555\n\n\nSmith\nt = (ȳ - μ)/se\n\n\nCode\nsmith_t &lt;- (519.7-500)/10\nprint(smith_t)\n\n\n[1] 1.97\n\n\nCode\n#p-value:\n2*pt(-abs(smith_t),df=1000-1)\n\n\n[1] 0.04911426"
  },
  {
    "objectID": "posts/Homework2_AlexaPotter.html#b.-using-α-0.05-for-each-study-indicate-whether-the-result-is-statistically-significant.",
    "href": "posts/Homework2_AlexaPotter.html#b.-using-α-0.05-for-each-study-indicate-whether-the-result-is-statistically-significant.",
    "title": "Homework 2",
    "section": "B. Using α = 0.05, for each study indicate whether the result is “statistically significant.”",
    "text": "B. Using α = 0.05, for each study indicate whether the result is “statistically significant.”\nThe result is statistically significant when the p-value is less than or equal to the alpha level.\nAt α = 0.05, Jones’ p-value of 0.051 is not statistically significant. At α = 0.05, Smith’s p-value of 0.049 is statistically significant."
  },
  {
    "objectID": "posts/Homework2_AlexaPotter.html#c.-using-this-example-explain-the-misleading-aspects-of-reporting-the-result-of-a-test-as-p-0.05-versus-p-0.05-or-as-reject-h0-versus-do-not-reject-h0-without-reporting-the-actual-p-value.",
    "href": "posts/Homework2_AlexaPotter.html#c.-using-this-example-explain-the-misleading-aspects-of-reporting-the-result-of-a-test-as-p-0.05-versus-p-0.05-or-as-reject-h0-versus-do-not-reject-h0-without-reporting-the-actual-p-value.",
    "title": "Homework 2",
    "section": "C. Using this example, explain the misleading aspects of reporting the result of a test as “P ≤ 0.05” versus “P > 0.05,” or as “reject H0” versus “Do not reject H0,” without reporting the actual P-value.",
    "text": "C. Using this example, explain the misleading aspects of reporting the result of a test as “P ≤ 0.05” versus “P &gt; 0.05,” or as “reject H0” versus “Do not reject H0,” without reporting the actual P-value.\nUsing “P ≤ 0.05” versus “P &gt; 0.05,” can leave a gap in the understanding of the full analysis. While one value is statistically significant, these two are extremely close. It’s important to state at what value you reject or fail to reject the null hypothesis, because both of these would not be statistically significant at α = 0.01."
  },
  {
    "objectID": "posts/ZhiyuanZhou_HW1.html",
    "href": "posts/ZhiyuanZhou_HW1.html",
    "title": "Homework 1",
    "section": "",
    "text": "Code\nlibrary(dplyr)\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union"
  },
  {
    "objectID": "posts/ZhiyuanZhou_HW1.html#a",
    "href": "posts/ZhiyuanZhou_HW1.html#a",
    "title": "Homework 1",
    "section": "a",
    "text": "a\nFirst, let’s read in the data from the Excel file:\n\n\nCode\nlibrary(readxl)\ndf &lt;- read_excel(\"_data/LungCapData.xls\")\n\n\nThe distribution of LungCap looks as follows:\n\n\nCode\nhist(df$LungCap)\n\n\n\n\n\nThe histogram suggests that the distribution is close to a normal distribution. Most of the observations are close to the mean. Very few observations are close to the margins (0 and 15).\n##b\n\n\nCode\nboxplot(df$LungCap~df$Gender,\nmain = \"Lung Capacity by Gender\",\nxlab = \"Gender\",\nylab = \"Lung Capacity\",\n)\n\n\n\n\n\n##c\n\n\nCode\ndf %&gt;%\n  group_by(Smoke) %&gt;%\n  summarize(mean = mean(LungCap))\n\n\n# A tibble: 2 × 2\n  Smoke  mean\n  &lt;chr&gt; &lt;dbl&gt;\n1 no     7.77\n2 yes    8.65\n\n\nThis result surprised me that smokers have more lung capacity than non-smokers.\n##d\n\n\nCode\ndf[\"AgeGroup\"] = \n  cut(df$Age,\n      c(0, 13, 15, 17, Inf),\n      c(\"&lt;=13\", \"14-15\",\"16-17\", \"&gt;=18\"),\n      right = T\n  )\n\ndf%&gt;%\n  group_by(AgeGroup, Smoke)%&gt;%\n  summarize(meanLungCap = mean(LungCap), meanAge = mean(Age), count = n())\n\n\n`summarise()` has grouped output by 'AgeGroup'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 8 × 5\n# Groups:   AgeGroup [4]\n  AgeGroup Smoke meanLungCap meanAge count\n  &lt;fct&gt;    &lt;chr&gt;       &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt;\n1 &lt;=13     no           6.36    9.49   401\n2 &lt;=13     yes          7.20   11.7     27\n3 14-15    no           9.14   14.5    105\n4 14-15    yes          8.39   14.6     15\n5 16-17    no          10.5    16.4     77\n6 16-17    yes          9.38   16.6     20\n7 &gt;=18     no          11.1    18.5     65\n8 &gt;=18     yes         10.5    18.1     15\n\n\n##e In age group “0-13”, smokers have higher lung capacity than non-smokers. In all other groups, smokers have less lung capacity than non-smokers. The number of samples under 13 gave it a clue about the interesting finding in 1c. And the mean age difference among smokers and non-smokers pointed out that the age difference is more likely to be the reason of higher lung capacity instead of smoking.\n#Question 2\n##a\n\n\nCode\nprob_2 &lt;- (160 / 810)\nprob_2\n\n\n[1] 0.1975309\n\n\n##b\n\n\nCode\nprob_fewer2 &lt;- (128 + 434) / 810\nprob_fewer2\n\n\n[1] 0.6938272\n\n\n##c\n\n\nCode\nprob_2OrFewer &lt;- (128 + 434 + 160) / 810\nprob_2OrFewer\n\n\n[1] 0.891358\n\n\n##d\n\n\nCode\nprob_more2 &lt;- (64 + 24) / 810\nprob_more2\n\n\n[1] 0.108642\n\n\n##e\n\n\nCode\nexpectation &lt;- (0 * 128 + 1 * 434 + 2 * 160 + 3 * 64 + 4 * 24) / 810\nexpectation\n\n\n[1] 1.28642\n\n\n##f\n\n\nCode\nvariance &lt;- sum(128 * (0 - expectation) ^ 2,\n                434 * (1 - expectation) ^ 2,\n                160 * (2 - expectation) ^ 2,\n                64 * (3 - expectation) ^ 2,\n                24 * (4 - expectation) ^ 2) / 810\nvariance\n\n\n[1] 0.8562353\n\n\nCode\nsd &lt;- sqrt(variance)\nsd\n\n\n[1] 0.9253298"
  },
  {
    "objectID": "posts/HW4_XiaoyanHu.html",
    "href": "posts/HW4_XiaoyanHu.html",
    "title": "Homework 4",
    "section": "",
    "text": "Code\nlibrary(tidyr)\nlibrary(dplyr)\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nCode\nlibrary(readxl)\nlibrary(ggplot2)\n\n\n\nQuestion 1\nFor recent data in Jacksonville, Florida, on y = selling price of home (in dollars), x1 = size of home (in square feet), and x2 = lot size (in square feet), the prediction equation is ŷ = −10,536 + 53.8x1 + 2.84x2.\nA. A particular home of 1240 square feet on a lot of 18,000 square feet sold for $145,000. Find the predicted selling price and the residual, and interpret.\nB. For fixed lot size, how much is the house selling price predicted to increase for each square- foot increase in home size? Why?\nC. According to this prediction equation, for fixed home size, how much would lot size need to increase to have the same impact as a one-square-foot increase in home size?\n\n\nQuestion 2\n(Data file: salary in alr4 R package). The data file concerns salary and other characteristics of all faculty in a small Midwestern college collected in the early 1980s for presentation in legal proceedings for which discrimination against women in salary was at issue. All persons in the data hold tenured or tenure track positions; temporary faculty are not included. The variables include degree, a factor with levels PhD and MS; rank, a factor with levels Asst, Assoc, and Prof; sex, a factor with levels Male and Female; Year, years in current rank; ysdeg, years since highest degree, and salary, academic year salary in dollars.\nA. Test the hypothesis that the mean salary for men and women is the same, without regard to any other variable but sex. Explain your findings.\nB. Run a multiple linear regression with salary as the outcome variable and everything else as predictors, including sex. Assuming no interactions between sex and the other predictors, obtain a 95% confidence interval for the difference in salary between males and females.\nC. Interpret your finding for each predictor variable; discuss (a) statistical significance, (b) interpretation of the coefficient / slope in relation to the outcome variable and other variables\nD. Change the baseline category for the rank variable. Interpret the coefficients related to rank again.\nE. Finkelstein (1980), in a discussion of the use of regression in discrimination cases, wrote, “[a] variable may reflect a position or status bestowed by the employer, in which case if there is discrimination in the award of the position or status, the variable may be ‘tainted.’” Thus, for example, if discrimination is at work in promotion of faculty to higher ranks, using rank to adjust salaries before comparing the sexes may not be acceptable to the courts. Exclude the variable rank, refit, and summarize how your findings changed, if they did.\nF. Everyone in this dataset was hired the year they earned their highest degree. It is also known that a new Dean was appointed 15 years ago, and everyone in the dataset who earned their highest degree 15 years ago or less than that has been hired by the new Dean. Some people have argued that the new Dean has been making offers that are a lot more generous to newly hired faculty than the previous one and that this might explain some of the variation in Salary. Create a new variable that would allow you to test this hypothesis and run another multiple regression model to test this. Select variables carefully to make sure there is no multicollinearity. Explain why multicollinearity would be a concern in this case and how you avoided it. Do you find support for the hypothesis that the people hired by the new Dean are making higher than those that were not? # Question 3 (Data file: house.selling.price in smss R package)\nA. Using the house.selling.price data, run and report regression results modeling y = selling price (in dollars) in terms of size of home (in square feet) and whether the home is new (1 = yes; 0 = no). In particular, for each variable; discuss statistical significance and interpret the meaning of the coefficient.\nB. Report and interpret the prediction equation, and form separate equations relating selling price to size for new and for not new homes.\nC. Find the predicted selling price for a home of 3000 square feet that is (i) new, (ii) not new.\nD. Fit another model, this time with an interaction term allowing interaction between size and new, and report the regression results\nE. Report the lines relating the predicted selling price to the size for homes that are (i) new, (ii) not new.\nF. Find the predicted selling price for a home of 3000 square feet that is (i) new, (ii) not new.\nG. Find the predicted selling price for a home of 1500 square feet that is (i) new, (ii) not new. Comparing to (F), explain how the difference in predicted selling prices changes as the size of home increases.\nH. Do you think the model with interaction or the one without it represents the relationship of size and new to the outcome price? What makes you prefer one model over another?"
  },
  {
    "objectID": "posts/HW3_SaisrinivasAmbatipudi.html",
    "href": "posts/HW3_SaisrinivasAmbatipudi.html",
    "title": "603Homework_Saisrinivas_Ambatipudi",
    "section": "",
    "text": "library(tidyverse)\nlibrary(ggplot2)\nlibrary(stats)\nlibrary(alr4)\nlibrary(smss)\n\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/HW3_SaisrinivasAmbatipudi.html#question-1",
    "href": "posts/HW3_SaisrinivasAmbatipudi.html#question-1",
    "title": "603Homework_Saisrinivas_Ambatipudi",
    "section": "Question 1:",
    "text": "Question 1:\nUnited Nations (Data file: UN11in alr4) The data in the file UN11 contains several variables, including ppgdp, the gross national product per person in U.S. dollars, and fertility, the birth rate per 1000 females, both from the year 2009. The data are for 199 localities, mostly UN member countries, but also other areas such as Hong Kong that are not independent countries. The data were collected from the United Nations (2011). We will study the dependence of fertility on ppgdp. (a) Identify the predictor and the response. (b) Draw the scatterplot of fertility on the vertical axis versus ppgdp on the horizontal axis and summarize the information in this graph. Does a straight-line mean function seem to be plausible for a summary of this graph? (c) Draw the scatterplot of log(fertility) versus log(ppgdp) using natural logarithms. Does the simple linear regression model seem plausible for a summary of this graph? If you use a different base of logarithms, the shape of the graph won’t change, but the values on the axes will change."
  },
  {
    "objectID": "posts/HW3_SaisrinivasAmbatipudi.html#answer",
    "href": "posts/HW3_SaisrinivasAmbatipudi.html#answer",
    "title": "603Homework_Saisrinivas_Ambatipudi",
    "section": "Answer:",
    "text": "Answer:"
  },
  {
    "objectID": "posts/HW3_SaisrinivasAmbatipudi.html#a",
    "href": "posts/HW3_SaisrinivasAmbatipudi.html#a",
    "title": "603Homework_Saisrinivas_Ambatipudi",
    "section": "(a)",
    "text": "(a)\nThe dependence of fertility is examined in this study. Thus, the predictor (independent variable) is ppgdp (gross national product per person), and the response (dependent variable) is fertility (birth rate per 1000 females)."
  },
  {
    "objectID": "posts/HW3_SaisrinivasAmbatipudi.html#b",
    "href": "posts/HW3_SaisrinivasAmbatipudi.html#b",
    "title": "603Homework_Saisrinivas_Ambatipudi",
    "section": "(b)",
    "text": "(b)\n\ndata(UN11)\nUN11\n\n                                        region  group fertility    ppgdp\nAfghanistan                               Asia  other  5.968000    499.0\nAlbania                                 Europe  other  1.525000   3677.2\nAlgeria                                 Africa africa  2.142000   4473.0\nAngola                                  Africa africa  5.135000   4321.9\nAnguilla                             Caribbean  other  2.000000  13750.1\nArgentina                           Latin Amer  other  2.172000   9162.1\nArmenia                                   Asia  other  1.735000   3030.7\nAruba                                Caribbean  other  1.671000  22851.5\nAustralia                              Oceania   oecd  1.949000  57118.9\nAustria                                 Europe   oecd  1.346000  45158.8\nAzerbaijan                                Asia  other  2.148000   5637.6\nBahamas                              Caribbean  other  1.877000  22461.6\nBahrain                                   Asia  other  2.430000  18184.1\nBangladesh                                Asia  other  2.157000    670.4\nBarbados                             Caribbean  other  1.575000  14497.3\nBelarus                                 Europe  other  1.479000   5702.0\nBelgium                                 Europe   oecd  1.835000  43814.8\nBelize                              Latin Amer  other  2.679000   4495.8\nBenin                                   Africa africa  5.078000    741.1\nBermuda                              Caribbean  other  1.760000  92624.7\nBhutan                                    Asia  other  2.258000   2047.2\nBolivia                             Latin Amer  other  3.229000   1977.9\nBosnia and Herzegovina                  Europe  other  1.134000   4477.7\nBotswana                                Africa africa  2.617000   7402.9\nBrazil                              Latin Amer  other  1.800000  10715.6\nBrunei Darussalam                         Asia  other  1.984000  32647.6\nBulgaria                                Europe  other  1.546000   6365.1\nBurkina Faso                            Africa africa  5.750000    519.7\nBurundi                                 Africa africa  4.051000    176.6\nCambodia                                  Asia  other  2.422000    797.2\nCameroon                                Africa africa  4.287000   1206.6\nCanada                           North America   oecd  1.691000  46360.9\nCape Verde                              Africa africa  2.279000   3244.0\nCayman Islands                       Caribbean  other  1.600000  57047.9\nCentral African Republic                Africa africa  4.423000    450.8\nChad                                    Africa africa  5.737000    727.4\nChile                               Latin Amer   oecd  1.832000  11887.7\nChina                                     Asia  other  1.559000   4354.0\nColombia                            Latin Amer  other  2.293000   6222.8\nComoros                                 Africa africa  4.742000    736.6\nCongo                                   Africa africa  4.442000   2665.1\nCook Islands                           Oceania  other  2.530806  12212.1\nCosta Rica                          Latin Amer  other  1.812000   7703.8\nCote dIvoire                            Africa africa  4.224000   1154.1\nCroatia                                 Europe  other  1.501000  13819.5\nCuba                                 Caribbean  other  1.451000   5704.4\nCyprus                                    Asia  other  1.458000  28364.3\nCzech Republic                          Europe   oecd  1.501000  18838.8\nDemocratic Republic of the Congo        Africa africa  5.485000    200.6\nDenmark                                 Europe   oecd  1.885000  55830.2\nDjibouti                                Africa africa  3.589000   1282.6\nDominica                             Caribbean  other  3.000000   7020.8\nDominican Republic                   Caribbean  other  2.490000   5195.4\nEast Timor                                Asia  other  5.918000    706.1\nEcuador                             Latin Amer  other  2.393000   4072.6\nEgypt                                   Africa africa  2.636000   2653.7\nEl Salvador                         Latin Amer  other  2.171000   3425.6\nEquatorial Guinea                       Africa africa  4.980000  16852.4\nEritrea                                 Africa africa  4.243000    429.1\nEstonia                                 Europe   oecd  1.702000  14135.4\nEthiopia                                Africa africa  3.848000    324.6\nFiji                                   Oceania  other  2.602000   3545.7\nFinland                                 Europe   oecd  1.875000  44501.7\nFrance                                  Europe   oecd  1.987000  39545.9\nFrench Polynesia                       Oceania  other  2.033000  24669.0\nGabon                                   Africa africa  3.195000  12468.8\nGambia                                  Africa africa  4.689000    579.1\nGeorgia                                   Asia  other  1.528000   2680.3\nGermany                                 Europe   oecd  1.457000  39857.1\nGhana                                   Africa africa  3.988000   1333.2\nGreece                                  Europe   oecd  1.540000  26503.8\nGreenland                        NorthAtlantic  other  2.217000  35292.7\nGrenada                              Caribbean  other  2.171000   7429.0\nGuatemala                           Latin Amer  other  3.840000   2882.3\nGuinea                                  Africa africa  5.032000    427.5\nGuinea-Bissau                           Africa africa  4.877000    539.4\nGuyana                              Latin Amer  other  2.190000   2996.0\nHaiti                                Caribbean  other  3.159000    612.7\nHonduras                            Latin Amer  other  2.996000   2026.2\nHong Kong                                 Asia  other  1.137000  31823.7\nHungary                                 Europe   oecd  1.430000  12884.0\nIceland                                 Europe  other  2.098000  39278.0\nIndia                                     Asia  other  2.538000   1406.4\nIndonesia                                 Asia  other  2.055000   2949.3\nIran                                      Asia  other  1.587000   5227.1\nIraq                                      Asia  other  4.535000    888.5\nIreland                                 Europe   oecd  2.097000  46220.3\nIsrael                                    Asia   oecd  2.909000  29311.6\nItaly                                   Europe   oecd  1.476000  33877.1\nJamaica                              Caribbean  other  2.262000   4899.0\nJapan                                     Asia   oecd  1.418000  43140.9\nJordan                                    Asia  other  2.889000   4445.3\nKazakhstan                                Asia  other  2.481000   9166.7\nKenya                                   Africa africa  4.623000    801.8\nKiribati                               Oceania  other  3.500000   1468.2\nKuwait                                    Asia  other  2.251000  45430.4\nKyrgyzstan                                Asia  other  2.621000    865.4\nLaos                                      Asia  other  2.543000   1047.6\nLatvia                                  Europe  other  1.506000  10663.0\nLebanon                                   Asia  other  1.764000   9283.7\nLesotho                                 Africa africa  3.051000    980.7\nLiberia                                 Africa africa  5.038000    218.6\nLibya                                   Africa africa  2.410000  11320.8\nLithuania                               Europe  other  1.495000  10975.5\nLuxembourg                              Europe   oecd  1.683000 105095.4\nMacao                                     Asia  other  1.163000  49990.2\nMadagascar                              Africa africa  4.493000    421.9\nMalawi                                  Africa africa  5.968000    357.4\nMalaysia                                  Asia  other  2.572000   8372.8\nMaldives                                  Asia  other  1.668000   4684.5\nMali                                    Africa africa  6.117000    598.8\nMalta                                   Europe  other  1.284000  19599.2\nMarshall Islands                       Oceania  other  4.384466   3069.4\nMauritania                              Africa africa  4.361000   1131.1\nMauritius                               Africa africa  1.590000   7488.3\nMexico                              Latin Amer   oecd  2.227000   9100.7\nMicronesia                             Oceania  other  3.307000   2678.2\nMoldova                                 Europe  other  1.450000   1625.8\nMongolia                                  Asia  other  2.446000   2246.7\nMontenegro                              Europe  other  1.630000   6509.8\nMorocco                                 Africa africa  2.183000   2865.0\nMozambique                              Africa africa  4.713000    407.5\nMyanmar                                   Asia  other  1.939000    876.2\nNamibia                                 Africa africa  3.055000   5124.7\nNauru                                  Oceania  other  3.300000   6190.1\nNepal                                     Asia  other  2.587000    534.7\nNeth Antilles                        Caribbean  other  1.900000  20321.1\nNetherlands                             Europe   oecd  1.794000  46909.7\nNew Caledonia                          Oceania  other  2.091000  35319.5\nNew Zealand                            Oceania   oecd  2.135000  32372.1\nNicaragua                           Latin Amer  other  2.500000   1131.9\nNiger                                   Africa africa  6.925000    357.7\nNigeria                                 Africa africa  5.431000   1239.8\nNorth Korea                               Asia  other  1.988000    504.0\nNorway                                  Europe   oecd  1.948000  84588.7\nOman                                      Asia  other  2.146000  20791.0\nPakistan                                  Asia  other  3.201000   1003.2\nPalau                                  Oceania  other  2.000000  10821.8\nPalestinian Territory                     Asia  other  4.270000   1819.5\nPanama                              Latin Amer  other  2.409000   7614.0\nPapua New Guinea                       Oceania  other  3.799000   1428.4\nParaguay                            Latin Amer  other  2.858000   2771.1\nPeru                                Latin Amer  other  2.410000   5410.7\nPhilippines                               Asia  other  3.050000   2140.1\nPoland                                  Europe   oecd  1.415000  12263.2\nPortugal                                Europe   oecd  1.312000  21437.6\nPuerto Rico                          Caribbean  other  1.757000  26461.0\nQatar                                     Asia  other  2.204000  72397.9\nRepublic of Korea                         Asia  other  1.389000  21052.2\nRomania                                 Europe  other  1.428000   7522.4\nRussian Federation                      Europe  other  1.529000  10351.4\nRwanda                                  Africa africa  5.282000    532.3\nSaint Lucia                          Caribbean  other  1.907000   6677.1\nSamoa                                  Oceania  other  3.763000   3343.3\nSao Tome and Principe                   Africa africa  3.488000   1283.3\nSaudi Arabia                              Asia  other  2.639000  15835.9\nSenegal                                 Africa africa  4.605000   1032.7\nSerbia                                  Europe  other  1.562000   5123.2\nSeychelles                              Africa africa  2.340000  11450.6\nSierra Leone                            Africa africa  4.728000    351.7\nSingapore                                 Asia  other  1.367000  43783.1\nSlovakia                                Europe   oecd  1.372000  15976.0\nSlovenia                                Europe   oecd  1.477000  23109.8\nSolomon Islands                        Oceania  other  4.041000   1193.5\nSomalia                                 Africa africa  6.283000    114.8\nSouth Africa                            Africa africa  2.383000   7254.8\nSpain                                   Europe  other  1.504000  30542.8\nSri Lanka                                 Asia  other  2.235000   2375.3\nSt Vincent and Grenadines            Caribbean  other  1.995000   6171.7\nSudan                                   Africa africa  4.225000   1824.9\nSuriname                            Latin Amer  other  2.266000   7018.0\nSwaziland                               Africa africa  3.174000   3311.2\nSweden                                  Europe   oecd  1.925000  48906.2\nSwitzerland                             Europe   oecd  1.536000  68880.2\nSyria                                     Asia  other  2.772000   2931.5\nTajikistan                                Asia  other  3.162000    816.0\nTanzania                                Africa africa  5.499000    516.0\nTFYR Macedonia                          Europe  other  1.397000   4434.5\nThailand                                  Asia  other  1.528000   4612.8\nTogo                                    Africa africa  3.864000    524.6\nTonga                                  Oceania  other  3.783000   3543.1\nTrinidad and Tobago                  Caribbean  other  1.632000  15205.1\nTunisia                                 Africa africa  1.909000   4222.1\nTurkey                                    Asia   oecd  2.022000  10095.1\nTurkmenistan                              Asia  other  2.316000   4587.5\nTuvalu                                 Oceania  other  3.700000   3187.2\nUganda                                  Africa africa  5.901000    509.0\nUkraine                                 Europe  other  1.483000   3035.0\nUnited Arab Emirates                      Asia  other  1.707000  39624.7\nUnited Kingdom                          Europe   oecd  1.867000  36326.8\nUnited States                    North America   oecd  2.077000  46545.9\nUruguay                             Latin Amer  other  2.043000  11952.4\nUzbekistan                                Asia  other  2.264000   1427.3\nVanuatu                                Oceania  other  3.750000   2963.5\nVenezuela                           Latin Amer  other  2.391000  13502.7\nViet Nam                                  Asia  other  1.750000   1182.7\nYemen                                     Asia  other  4.938000   1437.2\nZambia                                  Africa africa  6.300000   1237.8\nZimbabwe                                Africa africa  3.109000    573.1\n                                 lifeExpF pctUrban\nAfghanistan                      49.49000       23\nAlbania                          80.40000       53\nAlgeria                          75.00000       67\nAngola                           53.17000       59\nAnguilla                         81.10000      100\nArgentina                        79.89000       93\nArmenia                          77.33000       64\nAruba                            77.75000       47\nAustralia                        84.27000       89\nAustria                          83.55000       68\nAzerbaijan                       73.66000       52\nBahamas                          78.85000       84\nBahrain                          76.06000       89\nBangladesh                       70.23000       29\nBarbados                         80.26000       45\nBelarus                          76.37000       75\nBelgium                          82.81000       97\nBelize                           77.81000       53\nBenin                            58.66000       42\nBermuda                          82.30000      100\nBhutan                           69.84000       35\nBolivia                          69.40000       67\nBosnia and Herzegovina           78.40000       49\nBotswana                         51.34000       62\nBrazil                           77.41000       87\nBrunei Darussalam                80.64000       76\nBulgaria                         77.12000       72\nBurkina Faso                     57.02000       27\nBurundi                          52.58000       11\nCambodia                         65.10000       20\nCameroon                         53.56000       59\nCanada                           83.49000       81\nCape Verde                       77.70000       62\nCayman Islands                   83.80000      100\nCentral African Republic         51.30000       39\nChad                             51.61000       28\nChile                            82.35000       89\nChina                            75.61000       48\nColombia                         77.69000       75\nComoros                          63.18000       28\nCongo                            59.33000       63\nCook Islands                     76.24547       76\nCosta Rica                       81.99000       65\nCote dIvoire                     57.71000       51\nCroatia                          80.37000       58\nCuba                             81.33000       75\nCyprus                           82.14000       71\nCzech Republic                   81.00000       74\nDemocratic Republic of the Congo 50.56000       36\nDenmark                          81.37000       87\nDjibouti                         60.04000       76\nDominica                         78.20000       67\nDominican Republic               76.57000       70\nEast Timor                       64.20000       29\nEcuador                          78.91000       68\nEgypt                            75.52000       44\nEl Salvador                      77.09000       65\nEquatorial Guinea                52.91000       40\nEritrea                          64.41000       22\nEstonia                          79.95000       70\nEthiopia                         61.59000       17\nFiji                             72.27000       52\nFinland                          83.28000       85\nFrance                           84.90000       86\nFrench Polynesia                 78.07000       51\nGabon                            64.32000       86\nGambia                           60.30000       59\nGeorgia                          77.31000       53\nGermany                          82.99000       74\nGhana                            65.80000       52\nGreece                           82.58000       62\nGreenland                        71.60000       84\nGrenada                          77.72000       40\nGuatemala                        75.10000       50\nGuinea                           56.39000       36\nGuinea-Bissau                    50.40000       30\nGuyana                           73.45000       29\nHaiti                            63.87000       54\nHonduras                         75.92000       52\nHong Kong                        86.35000      100\nHungary                          78.47000       68\nIceland                          83.77000       94\nIndia                            67.62000       30\nIndonesia                        71.80000       45\nIran                             75.28000       71\nIraq                             72.60000       66\nIreland                          83.17000       62\nIsrael                           84.19000       92\nItaly                            84.62000       69\nJamaica                          75.98000       52\nJapan                            87.12000       67\nJordan                           75.17000       79\nKazakhstan                       72.84000       59\nKenya                            59.16000       23\nKiribati                         63.10000       44\nKuwait                           75.89000       98\nKyrgyzstan                       72.36000       35\nLaos                             69.42000       34\nLatvia                           78.51000       68\nLebanon                          75.07000       87\nLesotho                          48.11000       28\nLiberia                          58.59000       48\nLibya                            77.86000       78\nLithuania                        78.28000       67\nLuxembourg                       82.67000       85\nMacao                            83.80000      100\nMadagascar                       68.61000       31\nMalawi                           55.17000       20\nMalaysia                         76.86000       73\nMaldives                         78.70000       41\nMali                             53.14000       37\nMalta                            82.29000       95\nMarshall Islands                 70.60000       72\nMauritania                       60.95000       42\nMauritius                        76.89000       42\nMexico                           79.64000       78\nMicronesia                       70.17000       23\nMoldova                          73.48000       48\nMongolia                         72.83000       63\nMontenegro                       77.37000       61\nMorocco                          74.86000       59\nMozambique                       51.81000       39\nMyanmar                          67.87000       34\nNamibia                          63.04000       39\nNauru                            57.10000      100\nNepal                            70.05000       19\nNeth Antilles                    79.86000       93\nNetherlands                      82.79000       83\nNew Caledonia                    80.49000       57\nNew Zealand                      82.77000       86\nNicaragua                        77.45000       58\nNiger                            55.77000       17\nNigeria                          53.38000       51\nNorth Korea                      72.12000       60\nNorway                           83.47000       80\nOman                             76.44000       73\nPakistan                         66.88000       36\nPalau                            72.10000       84\nPalestinian Territory            74.81000       74\nPanama                           79.07000       75\nPapua New Guinea                 65.52000       13\nParaguay                         74.91000       62\nPeru                             76.90000       77\nPhilippines                      72.57000       49\nPoland                           80.56000       61\nPortugal                         82.76000       61\nPuerto Rico                      83.20000       99\nQatar                            78.24000       96\nRepublic of Korea                83.95000       83\nRomania                          77.95000       58\nRussian Federation               75.01000       73\nRwanda                           57.13000       19\nSaint Lucia                      77.54000       28\nSamoa                            76.02000       20\nSao Tome and Principe            66.48000       63\nSaudi Arabia                     75.57000       82\nSenegal                          60.92000       43\nSerbia                           77.05000       56\nSeychelles                       78.00000       56\nSierra Leone                     48.87000       39\nSingapore                        83.71000      100\nSlovakia                         79.53000       55\nSlovenia                         82.84000       49\nSolomon Islands                  70.00000       19\nSomalia                          53.38000       38\nSouth Africa                     54.09000       62\nSpain                            84.76000       78\nSri Lanka                        78.40000       14\nSt Vincent and Grenadines        74.73000       50\nSudan                            63.82000       41\nSuriname                         74.18000       70\nSwaziland                        48.54000       21\nSweden                           83.65000       85\nSwitzerland                      84.71000       74\nSyria                            77.72000       56\nTajikistan                       71.23000       26\nTanzania                         60.31000       27\nTFYR Macedonia                   77.14000       59\nThailand                         77.76000       34\nTogo                             59.40000       44\nTonga                            75.38000       24\nTrinidad and Tobago              73.82000       14\nTunisia                          77.05000       68\nTurkey                           76.61000       70\nTurkmenistan                     69.40000       50\nTuvalu                           65.10000       51\nUganda                           55.44000       13\nUkraine                          74.58000       69\nUnited Arab Emirates             78.02000       84\nUnited Kingdom                   82.42000       80\nUnited States                    81.31000       83\nUruguay                          80.66000       93\nUzbekistan                       71.90000       36\nVanuatu                          73.58000       26\nVenezuela                        77.73000       94\nViet Nam                         77.44000       31\nYemen                            67.66000       32\nZambia                           50.04000       36\nZimbabwe                         52.72000       39\n\n\n\nplot(UN11$ppgdp, UN11$fertility,\n     main=\"Fertility vs PPGDP\",\n     xlab=\"PPGDP (Gross National Product per Person)\",\n     ylab=\"Fertility (Birth Rate per 1000 Females)\")\n\nmodel &lt;- lm(fertility ~ ppgdp, data = UN11)\n\nabline(model, col = \"red\", lwd = 2)\n\n\n\n\nThe graph first displays a severe negative association between a country’s gross national product per person and fertility rate, but after this point, there appears to be no change in fertility in relation to ppgdp. A straight-line mean function does not appear to be an adequate metric for this graph’s summary."
  },
  {
    "objectID": "posts/HW3_SaisrinivasAmbatipudi.html#c",
    "href": "posts/HW3_SaisrinivasAmbatipudi.html#c",
    "title": "603Homework_Saisrinivas_Ambatipudi",
    "section": "(c)",
    "text": "(c)\n\nplot(log(UN11$ppgdp), log(UN11$fertility),\n     main=\"Log(Fertility) vs Log(PPGDP)\",\n     xlab=\"Log(PPGDP)\",\n     ylab=\"Log(Fertility)\")\n\nmodel &lt;- lm(log(fertility) ~ log(ppgdp), data = UN11)\n\nabline(model, col = \"red\", lwd = 2)\n\n\n\n\nThroughout the graph, the connection between the variables appears to be negative. For a description of this graph, basic linear regression is feasible."
  },
  {
    "objectID": "posts/HW3_SaisrinivasAmbatipudi.html#question-2",
    "href": "posts/HW3_SaisrinivasAmbatipudi.html#question-2",
    "title": "603Homework_Saisrinivas_Ambatipudi",
    "section": "Question 2:",
    "text": "Question 2:\nAnnual income, in dollars, is an explanatory variable in a regression analysis. For a British version of the report on the analysis, all responses are converted to British pounds sterling (1 pound equals about 1.33 dollars, as of 2016).\n\nHow, if at all, does the slope of the prediction equation change?\nHow, if at all, does the correlation change?"
  },
  {
    "objectID": "posts/HW3_SaisrinivasAmbatipudi.html#answer-1",
    "href": "posts/HW3_SaisrinivasAmbatipudi.html#answer-1",
    "title": "603Homework_Saisrinivas_Ambatipudi",
    "section": "Answer:",
    "text": "Answer:"
  },
  {
    "objectID": "posts/HW3_SaisrinivasAmbatipudi.html#a-1",
    "href": "posts/HW3_SaisrinivasAmbatipudi.html#a-1",
    "title": "603Homework_Saisrinivas_Ambatipudi",
    "section": "(a)",
    "text": "(a)\n\nUN11$british &lt;- 1.33 * UN11$ppgdp\nsummary(lm(fertility ~ british, UN11))\n\n\nCall:\nlm(formula = fertility ~ british, data = UN11)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.9006 -0.8801 -0.3547  0.6749  3.7585 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  3.178e+00  1.048e-01  30.331  &lt; 2e-16 ***\nbritish     -2.407e-05  3.500e-06  -6.877  7.9e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.206 on 197 degrees of freedom\nMultiple R-squared:  0.1936,    Adjusted R-squared:  0.1895 \nF-statistic: 47.29 on 1 and 197 DF,  p-value: 7.903e-11\n\nsummary(lm(fertility ~ ppgdp, UN11))\n\n\nCall:\nlm(formula = fertility ~ ppgdp, data = UN11)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.9006 -0.8801 -0.3547  0.6749  3.7585 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  3.178e+00  1.048e-01  30.331  &lt; 2e-16 ***\nppgdp       -3.201e-05  4.655e-06  -6.877  7.9e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.206 on 197 degrees of freedom\nMultiple R-squared:  0.1936,    Adjusted R-squared:  0.1895 \nF-statistic: 47.29 on 1 and 197 DF,  p-value: 7.903e-11\n\n\nThe slope of the prediction equation will change when income is converted from dollars to pounds sterling. Since 1 pound equals about 133. dollars (as of 2016), to convert the annual income to pounds, you will need to divide the annual income in dollars by 133. The slope of the equation in pounds will be the same as the original slope in dollars. The magnitude of the slope changes due to the change in units, but the relationship between the variables remains the same."
  },
  {
    "objectID": "posts/HW3_SaisrinivasAmbatipudi.html#b-1",
    "href": "posts/HW3_SaisrinivasAmbatipudi.html#b-1",
    "title": "603Homework_Saisrinivas_Ambatipudi",
    "section": "(b)",
    "text": "(b)\n\ncor(UN11$ppgdp, UN11$fertility)\n\n[1] -0.4399891\n\ncor(UN11$british, UN11$fertility)\n\n[1] -0.4399891\n\n\nWhen converting from dollars to pounds sterling, the correlation between variables will not change. The correlation coefficients are only a measure of the relative relationship between the two variables, not the units they are measured in. The correlation coefficients of the variables remain the same since the conversion from dollars to pounds only involves a linear transformation."
  },
  {
    "objectID": "posts/HW3_SaisrinivasAmbatipudi.html#question-3",
    "href": "posts/HW3_SaisrinivasAmbatipudi.html#question-3",
    "title": "603Homework_Saisrinivas_Ambatipudi",
    "section": "Question 3:",
    "text": "Question 3:\nWater runoff in the Sierras (Data file: water in alr4) Can Southern California’s water supply in future years be predicted from past data? One factor affecting water availability is stream runoff. If runoff could be predicted, engineers, planners, and policy makers could do their jobs more efficiently. The data file contains 43 years’ worth of precipitation measurements taken at six sites in the Sierra Nevada mountains (labeled APMAM, APSAB, APSLAKE, OPBPC, OPRC, and OPSLAKE) and stream runoff volume at a site near Bishop, California, labeled BSAAM. Draw the scatterplot matrix for these data and summarize the information available from these plots. (Hint: Use the pairs() function.)"
  },
  {
    "objectID": "posts/HW3_SaisrinivasAmbatipudi.html#answer-2",
    "href": "posts/HW3_SaisrinivasAmbatipudi.html#answer-2",
    "title": "603Homework_Saisrinivas_Ambatipudi",
    "section": "Answer:",
    "text": "Answer:\n\ndata(water)\nwater\n\n   Year APMAM APSAB APSLAKE OPBPC  OPRC OPSLAKE  BSAAM\n1  1948  9.13  3.58    3.91  4.10  7.43    6.47  54235\n2  1949  5.28  4.82    5.20  7.55 11.11   10.26  67567\n3  1950  4.20  3.77    3.67  9.52 12.20   11.35  66161\n4  1951  4.60  4.46    3.93 11.14 15.15   11.13  68094\n5  1952  7.15  4.99    4.88 16.34 20.05   22.81 107080\n6  1953  9.70  5.65    4.91  8.88  8.15    7.41  67594\n7  1954  5.02  1.45    1.77 13.57 12.45   13.32  65356\n8  1955  6.70  7.44    6.51  9.28  9.65    9.80  67909\n9  1956 10.50  5.85    3.38 21.20 18.55   17.42  92715\n10 1957  9.10  6.13    4.08  9.55  9.20    8.25  70024\n11 1958  8.75  5.23    5.90 15.25 14.80   17.48  99216\n12 1959  8.10  3.77    4.56  9.05  6.85    9.56  55786\n13 1960  3.75  1.47    1.78  4.57  6.10    7.65  46153\n14 1961 10.15  5.09    4.86  8.90  7.15    9.00  47947\n15 1962  6.15  3.52    3.30 16.90 14.75   17.68  76877\n16 1963 12.75  8.17   10.16 16.75 11.55   15.53  88443\n17 1964  7.35  4.33    4.85  5.25  7.45    8.20  54634\n18 1965 11.25  6.56    7.60  8.40 13.20   13.29  78806\n19 1966  4.05  1.90    2.00 10.85  8.25   12.56  56542\n20 1967 12.65  6.62    7.14 23.25 17.00   23.66 116244\n21 1968  4.65  3.84    3.34  7.10  6.80    8.28  60857\n22 1969  5.35  3.62    4.62 43.37 24.85   33.07 146345\n23 1970  4.05  1.98    2.94  8.95 11.25   11.00  73726\n24 1971  5.90  5.72    5.42  8.45 10.90   10.82  65530\n25 1972  9.45  4.82    6.79  7.90  7.60    8.06  60772\n26 1973  3.45  2.63    2.88 14.80 14.70   15.86  91696\n27 1974  4.25  2.54    2.36 18.05 16.90   16.42  87377\n28 1975  7.90  4.42    6.78 11.50  9.55   12.56  77306\n29 1976  9.38  8.30    9.70  6.80  5.25    4.73  44756\n30 1977  7.08  4.40    3.90  4.05  4.35    4.60  41785\n31 1978 11.92  5.78    6.70 25.30 20.55   21.94 112653\n32 1979  3.88  2.26    3.10 15.97 11.83   13.88  79975\n33 1980  5.80  3.10    3.34 24.40 19.15   23.78 106821\n34 1981  2.70  2.22    2.48  8.99  9.45   12.14  69177\n35 1982 18.08 11.96   13.02 18.55 18.40   19.45 120463\n36 1983  8.20  4.98    5.76 19.25 22.90   23.86 135043\n37 1984  7.65  5.30    5.74 14.45 13.15   14.42 102001\n38 1985  5.22  4.42    4.04 11.45 10.16   13.06  77790\n39 1986  4.93  3.26    4.58 26.47 15.33   26.46 118144\n40 1987  5.99  2.76    3.98  4.80  6.85    6.36  61229\n41 1988  6.83  6.82    5.18  7.20  9.01    9.88  58942\n42 1989  8.80  5.06    4.92  8.05  9.60    9.58  53965\n43 1990  7.10  5.06    6.05  5.80  6.50    8.41  49774\n\n\n\npairs(water)\n\n\n\n\nAccording to the above figure, the stream run-off variable has a link with the ‘O’ named lakes but no significant association with the ’A named lakes."
  },
  {
    "objectID": "posts/HW3_SaisrinivasAmbatipudi.html#question-4",
    "href": "posts/HW3_SaisrinivasAmbatipudi.html#question-4",
    "title": "603Homework_Saisrinivas_Ambatipudi",
    "section": "Question 4:",
    "text": "Question 4:\nProfessor ratings (Data file: Rateprof in alr4) In the website and online forum RateMyProfessors.com, students rate and comment on their instructors. Launched in 1999, the site includes millions of ratings on thousands of instructors. The data file includes the summaries of the ratings of 364 instructors at a large campus in the Midwest (Bleske-Rechek and Fritsch, 2011). Each instructor included in the data had at least 10 ratings over a several year period. Students provided ratings of 1–5 on quality, helpfulness, clarity, easiness of instructor’s courses, and raterInterest in the subject matter covered in the instructor’s courses. The data file provides the averages of these five ratings. Create a scatterplot matrix of these five variables. Provide a brief description of the relationships between the five ratings."
  },
  {
    "objectID": "posts/HW3_SaisrinivasAmbatipudi.html#answer-3",
    "href": "posts/HW3_SaisrinivasAmbatipudi.html#answer-3",
    "title": "603Homework_Saisrinivas_Ambatipudi",
    "section": "Answer:",
    "text": "Answer:\n\ndata(Rateprof)\nRateprof\n\n    gender numYears numRaters numCourses pepper discipline\n1     male        7        11          5     no        Hum\n2     male        6        11          5     no        Hum\n3     male       10        43          2     no        Hum\n4     male       11        24          5     no        Hum\n5     male       11        19          7     no        Hum\n6     male       10        15          9     no        Hum\n7     male        7        17          3     no        Hum\n8     male       11        16          3     no        Hum\n9     male       11        12          4     no        Hum\n10    male        7        18          4     no        Hum\n11    male       11        11          4     no        Hum\n12    male        6        33          4    yes        Hum\n13    male        4        23          5     no        Hum\n14    male       10        34          2    yes        Hum\n15    male       11        23          8    yes        Hum\n16    male        3        27          5    yes        Hum\n17    male        1        14          2    yes        Hum\n18    male       11        19          7     no        Hum\n19    male        6        11          5     no        Hum\n20    male       11        14          5     no        Hum\n21    male        7        59          9     no        Hum\n22    male       10        28          5     no        Hum\n23    male        1        13          3     no        Hum\n24    male       10        15          5     no        Hum\n25    male        7        16          2     no        Hum\n26    male       11        35          6     no        Hum\n27    male        7        11          2     no        Hum\n28    male       11        23         10     no        Hum\n29    male       11        27          6     no        Hum\n30    male       11        42          5    yes        Hum\n31    male       11        38          4     no        Hum\n32    male       11        20          8     no        Hum\n33    male       11        44          6     no        Hum\n34    male       11        57          3    yes        Hum\n35    male       10        15          3     no        Hum\n36    male        9        46          5     no        Hum\n37    male       11        10          4     no        Hum\n38    male        3        18          2     no        Hum\n39    male       11        79          7     no        Hum\n40    male       11        18          2     no        Hum\n41    male        3        26          5     no        Hum\n42    male       11        52          2     no        Hum\n43    male       11        10          2     no        Hum\n44    male        7        26          5     no        Hum\n45    male       10        15          3     no        Hum\n46    male       11        45          8     no        Hum\n47    male        3        12          3    yes        Hum\n48    male        7        12          2     no        Hum\n49    male       11        16          3     no        Hum\n50    male        9        36          8     no        Hum\n51    male       11        24          5     no        Hum\n52    male        3        29          4     no        Hum\n53    male       11        65          4     no        Hum\n54    male       11        11          4     no        Hum\n55    male        2        14          4    yes        Hum\n56    male       11        14          7     no        Hum\n57    male       11        29          6     no        Hum\n58    male       11        54          6    yes        Hum\n59    male        4        12          3     no        Hum\n60    male       11        57          5     no        Hum\n61    male        9        13          6     no        Hum\n62    male        5        33          3     no        Hum\n63    male        8        46          7     no        Hum\n64    male       11        19          4     no        Hum\n65  female       11        18          6    yes        Hum\n66  female        9        28          3     no        Hum\n67  female       11        21          7     no        Hum\n68  female        8        26          8    yes        Hum\n69  female       11        10          2     no        Hum\n70  female        2        14          4     no        Hum\n71  female        7        27          6     no        Hum\n72  female       10        22          3     no        Hum\n73  female        4        36          6     no        Hum\n74  female        2        14          4    yes        Hum\n75  female       11        19          6     no        Hum\n76  female       11        86          8     no        Hum\n77  female        8        25          8     no        Hum\n78  female        8        26          7    yes        Hum\n79  female       10        16          2     no        Hum\n80  female        9        20          3     no        Hum\n81  female       11        30          4     no        Hum\n82  female        7        12          3     no        Hum\n83  female       10        21          3     no        Hum\n84  female        1        19          4    yes        Hum\n85  female       11        12          3    yes        Hum\n86  female        4        31          3    yes        Hum\n87  female        4        22          2    yes        Hum\n88  female        4        13          5     no        Hum\n89  female       11        22          5     no        Hum\n90  female       11        26          8     no        Hum\n91  female       11        15          3     no        Hum\n92  female        2        20          5     no        Hum\n93  female       11        24          7     no        Hum\n94  female       11        69          3     no        Hum\n95  female       10        37          6     no        Hum\n96  female        9        36          7     no        Hum\n97  female        7        32          3     no        Hum\n98  female        2        11          3     no        Hum\n99  female       11        41          2     no        Hum\n100 female        7        34          7     no        Hum\n101 female        5        15          4     no        Hum\n102 female       11        38          3     no        Hum\n103 female        2        10          4    yes        Hum\n104 female       11        29          5     no        Hum\n105 female        1        10          4    yes        Hum\n106 female       11        27          5     no        Hum\n107 female       11        36          4     no        Hum\n108 female        9        19          3     no        Hum\n109 female       11        14          2     no        Hum\n110 female        8        17          5     no        Hum\n111 female       10        54          3     no        Hum\n112 female       11        21          5     no        Hum\n113 female        5        33          6     no        Hum\n114 female       11        47          8     no        Hum\n115 female       11        58          5     no        Hum\n116 female        7        14          4    yes        Hum\n117 female       11        10          5     no        Hum\n118 female        9        36          6     no        Hum\n119 female        7        14          5    yes        Hum\n120 female       11        26          5     no        Hum\n121 female        8        39          5     no        Hum\n122 female       11        16          6     no        Hum\n123 female        5        67          3     no        Hum\n124 female        9        29          4     no        Hum\n125 female       11        26          6     no        Hum\n126 female        6        24          7     no        Hum\n127 female        7        16          5    yes        Hum\n128 female        5        20          2     no        Hum\n129 female       10        35          6     no        Hum\n130 female        5        36          9     no        Hum\n131 female        4        28          3     no        Hum\n132 female       11        10          4     no        Hum\n133 female        9        53          7     no        Hum\n134 female        2        13          2     no        Hum\n135   male        7        10          2     no     SocSci\n136   male        7        21          2     no     SocSci\n137   male        2        33          2     no     SocSci\n138   male        7        11          2     no     SocSci\n139   male        6        18          3     no     SocSci\n140   male       11        30          2     no     SocSci\n141   male        7        14          2     no     SocSci\n142   male        4        15          5    yes     SocSci\n143   male       11        50          4     no     SocSci\n144   male        3        10          3     no     SocSci\n145   male       11        30          5     no     SocSci\n146   male       11        53          3     no     SocSci\n147   male       11        46          4    yes     SocSci\n148   male       11        30          5     no     SocSci\n149   male       11        42          7     no     SocSci\n150   male       11        74          8     no     SocSci\n151   male       10        51          4     no     SocSci\n152   male       11        31          4     no     SocSci\n153   male       11        19          2     no     SocSci\n154   male       11        56          4     no     SocSci\n155   male       11        72          7     no     SocSci\n156   male        8        67          4     no     SocSci\n157   male       11        85          7     no     SocSci\n158   male        9        14          1     no     SocSci\n159   male        9        47          7     no     SocSci\n160   male       10        69          2     no     SocSci\n161   male        2        11          2     no     SocSci\n162   male       11        56          6     no     SocSci\n163   male       10        14          5     no     SocSci\n164   male        7        23          7     no     SocSci\n165   male        6        36          5     no     SocSci\n166   male       11        44          8     no     SocSci\n167 female        6        19          4     no     SocSci\n168 female        4        11          5     no     SocSci\n169 female       11        10          1    yes     SocSci\n170 female       10        56          2     no     SocSci\n171 female       11        85          3     no     SocSci\n172 female        2        16          1    yes     SocSci\n173 female        7        39          5    yes     SocSci\n174 female       11        31          4     no     SocSci\n175 female        8        24          3     no     SocSci\n176 female       11        67          3     no     SocSci\n177 female        2        15          3     no     SocSci\n178 female        6        46          3     no     SocSci\n179 female        9        13          6     no     SocSci\n180 female        4        27          3     no     SocSci\n181 female       11        35          3     no     SocSci\n182 female        1        32          4     no     SocSci\n183 female        3        22          2    yes     SocSci\n184 female       11        51          3     no     SocSci\n185 female        8        11          2     no     SocSci\n186 female       10        52          3     no     SocSci\n187 female       10        62          5     no     SocSci\n188 female        3        13          1     no     SocSci\n189 female        2        20          2     no     SocSci\n190 female       11        32          4     no     SocSci\n191 female        5        23          3     no     SocSci\n192 female       11        62          1     no     SocSci\n193 female        5        42          6    yes     SocSci\n194 female       11        24          2     no     SocSci\n195 female        5        26          6     no     SocSci\n196 female        3        12          4    yes     SocSci\n197 female       11        32          4     no     SocSci\n198 female        9        50          5     no     SocSci\n199 female       11        57          4     no     SocSci\n200 female       11        15          4     no     SocSci\n201   male        8        32          4    yes       STEM\n202   male       11        17          9     no       STEM\n203   male       11        23          5     no       STEM\n204   male       11        13          4     no       STEM\n205   male        1        13          2     no       STEM\n206   male        6        18          7     no       STEM\n207   male        1        11          3    yes       STEM\n208   male       10        26          4     no       STEM\n209   male        6        15          4     no       STEM\n210   male       11        25          2     no       STEM\n211   male       11        21          3     no       STEM\n212   male       11        43          3     no       STEM\n213   male       11        51          6     no       STEM\n214   male       11        11          2     no       STEM\n215   male       11        27          2     no       STEM\n216   male       11        37          5     no       STEM\n217   male       11        35          3     no       STEM\n218   male       11        29          2     no       STEM\n219   male        4        30          2     no       STEM\n220   male       11        28          6    yes       STEM\n221   male       11        33          3     no       STEM\n222   male       11        28          6     no       STEM\n223   male       11        62          5     no       STEM\n224   male        5        38          7     no       STEM\n225   male       11        14          1     no       STEM\n226   male        5        25          3     no       STEM\n227   male       11        53          8     no       STEM\n228   male        7        20          3     no       STEM\n229   male       11        21         10     no       STEM\n230   male        6        40          6     no       STEM\n231   male       11        13          3     no       STEM\n232   male       11        67          4     no       STEM\n233   male       11        19          5     no       STEM\n234   male       10        17          2     no       STEM\n235   male       11        22          7     no       STEM\n236   male       11        16          3     no       STEM\n237   male        6        35          6     no       STEM\n238   male        8        38          5     no       STEM\n239   male       11        67          6     no       STEM\n240   male       11        35          4     no       STEM\n241   male       11        32          5     no       STEM\n242   male       11        58          4     no       STEM\n243   male       11        21          3     no       STEM\n244   male       10        12          4     no       STEM\n245   male       11        45          7     no       STEM\n246   male        3        14          2     no       STEM\n247   male        9        41          5     no       STEM\n248   male        1        10          3     no       STEM\n249   male       11        30          3     no       STEM\n250   male       11        25         12     no       STEM\n251   male       11        37          7     no       STEM\n252   male       11        23          4     no       STEM\n253   male       11        65          3     no       STEM\n254   male       11        54          7     no       STEM\n255   male        8        41          5     no       STEM\n256   male        9        13          6     no       STEM\n257   male       11        52          6     no       STEM\n258   male        4        33          8    yes       STEM\n259   male        8        20          6     no       STEM\n260   male        2        13          2     no       STEM\n261   male       11        52          5     no       STEM\n262   male        6        20          3     no       STEM\n263   male        5        39          3     no       STEM\n264   male       11        53          8     no       STEM\n265   male       11        49          7     no       STEM\n266   male       11        49          6     no       STEM\n267   male       11        57          7     no       STEM\n268   male       11        17          5     no       STEM\n269   male       11        30         10     no       STEM\n270   male       11        13          3     no       STEM\n271   male       11        29          6     no       STEM\n272   male       11        29          3     no       STEM\n273   male       11        11          6     no       STEM\n274   male       10        12          2     no       STEM\n275   male       11        18          5     no       STEM\n276   male        3        15          3     no       STEM\n277 female       11        25          3     no       STEM\n278 female        8        10          4     no       STEM\n279 female       11        13          3    yes       STEM\n280 female       11        47          8     no       STEM\n281 female        5        11          6     no       STEM\n282 female       11        18          5     no       STEM\n283 female        4        21          1     no       STEM\n284 female        7        42          2     no       STEM\n285 female        9        33          1     no       STEM\n286 female        3        27          3     no       STEM\n287 female       11        12          3     no       STEM\n288 female       11        45          4     no       STEM\n289 female       11        49          3     no       STEM\n290 female       11        36          3    yes       STEM\n291 female       11        54          4     no       STEM\n292 female       10        31          6     no       STEM\n293 female        6        17          4     no       STEM\n294 female        7        30          4     no       STEM\n295 female        4        11          3     no       STEM\n296 female        2        17          5     no       STEM\n297 female        8        80          8     no       STEM\n298 female       11        39          4     no       STEM\n299 female       11        26          4     no       STEM\n300 female       11        29          4     no       STEM\n301 female       11        60          7     no       STEM\n302 female        1        16          3     no       STEM\n303 female        3        10          5     no       STEM\n304   male        3        14          2     no   Pre-prof\n305   male        6        14          7     no   Pre-prof\n306   male        7        12          4     no   Pre-prof\n307   male       11        13          3     no   Pre-prof\n308   male       11        11          5     no   Pre-prof\n309   male        8        39          2     no   Pre-prof\n310   male        8        32          3    yes   Pre-prof\n311   male        2        10          2     no   Pre-prof\n312   male       10        21          5    yes   Pre-prof\n313   male       11        10          3     no   Pre-prof\n314   male        7        12          3     no   Pre-prof\n315   male        5        15          3    yes   Pre-prof\n316   male       11        53          6     no   Pre-prof\n317   male        8        68          3     no   Pre-prof\n318   male        7        48          4     no   Pre-prof\n319   male        2        10          1     no   Pre-prof\n320   male       11        36          3     no   Pre-prof\n321   male        8        15          2    yes   Pre-prof\n322   male        3        32          3    yes   Pre-prof\n323   male        9        23          5     no   Pre-prof\n324   male        4        14          4     no   Pre-prof\n325   male        8        12          4     no   Pre-prof\n326   male       11        31          2     no   Pre-prof\n327   male       11        24          3    yes   Pre-prof\n328   male        8        33          4     no   Pre-prof\n329   male        6        50          2    yes   Pre-prof\n330   male        8        11          6     no   Pre-prof\n331   male       11        19          3     no   Pre-prof\n332   male        6        19          1     no   Pre-prof\n333   male       11        57          5     no   Pre-prof\n334   male       11        34          3     no   Pre-prof\n335   male        6        16          2     no   Pre-prof\n336   male        8        29          3     no   Pre-prof\n337   male       11        45          6     no   Pre-prof\n338   male       11        21          4     no   Pre-prof\n339 female        9        10          1     no   Pre-prof\n340 female        7        10          4     no   Pre-prof\n341 female       11        24          3     no   Pre-prof\n342 female        5        10          2     no   Pre-prof\n343 female        7        10          2     no   Pre-prof\n344 female       11        16          4     no   Pre-prof\n345 female       11        16          2     no   Pre-prof\n346 female       11        10          5     no   Pre-prof\n347 female        2        15          1     no   Pre-prof\n348 female        2        29          2    yes   Pre-prof\n349 female        3        24          2     no   Pre-prof\n350 female        2        11          4     no   Pre-prof\n351 female       11        22          9    yes   Pre-prof\n352 female       11        62          3     no   Pre-prof\n353 female       11        21          3     no   Pre-prof\n354 female        9        53          4     no   Pre-prof\n355 female        5        12          2     no   Pre-prof\n356 female       11        11          2     no   Pre-prof\n357 female       11        38          2     no   Pre-prof\n358 female       11        46          4     no   Pre-prof\n359 female       11        10          3     no   Pre-prof\n360 female        3        24          4     no   Pre-prof\n361 female       11        27          4     no   Pre-prof\n362 female        5        21          3     no   Pre-prof\n363 female        4        15          2     no   Pre-prof\n364 female        2        10          3     no   Pre-prof\n365 female        9        11          5     no   Pre-prof\n366 female        2        11          4     no   Pre-prof\n                           dept  quality helpfulness  clarity easiness\n1                       English 4.636364    4.636364 4.636364 4.818182\n2             Religious Studies 4.318182    4.545455 4.090909 4.363636\n3                           Art 4.790698    4.720930 4.860465 4.604651\n4                       English 4.250000    4.458333 4.041667 2.791667\n5                       Spanish 4.684211    4.684211 4.684211 4.473684\n6                       Spanish 4.233333    4.266667 4.200000 4.533333\n7                       Spanish 4.382353    4.352941 4.411765 4.117647\n8                       English 2.062500    2.062500 2.062500 1.437500\n9                         Music 2.041667    2.166667 2.000000 1.750000\n10                      English 4.111111    4.222222 4.000000 3.666667\n11                   Philosophy 4.727273    4.909091 4.545455 4.000000\n12                   Philosophy 3.724242    3.848485 3.606060 4.242424\n13                        Music 2.804348    2.695652 2.913043 2.217391\n14                        Music 4.838235    4.823529 4.852941 4.676471\n15                      Spanish 4.565217    4.565217 4.565217 2.826087\n16            Religious Studies 4.944444    4.962963 4.925926 3.703704\n17                      English 4.464286    4.714286 4.214286 3.214286\n18                      History 4.184211    4.368421 4.000000 3.631579\n19                          Art 3.909091    4.090909 3.727273 2.272727\n20                          Art 3.500000    3.285714 3.714286 3.285714\n21                   Philosophy 3.474576    3.542373 3.406780 2.355932\n22                      English 3.696429    3.714286 3.678571 3.642857\n23                      English 3.576923    3.461538 3.692308 4.615385\n24                   Philosophy 1.633333    1.600000 1.666667 2.266667\n25                      History 3.531250    3.312500 3.750000 2.562500\n26                      History 3.114286    3.057143 3.171429 3.857143\n27                        Music 4.909091    5.000000 4.818182 4.181818\n28                      English 4.239130    4.434783 4.043478 2.826087\n29                      Spanish 3.981481    4.037037 3.925926 3.148148\n30                      English 4.392857    4.500000 4.285714 3.166667\n31                      History 4.526316    4.473684 4.578947 3.131579\n32                       German 4.075000    4.150000 4.000000 3.250000\n33                   Philosophy 2.829546    3.022727 2.636364 2.045454\n34            Religious Studies 4.552632    4.561404 4.543860 3.614035\n35                        Music 3.700000    3.600000 3.800000 3.133333\n36                      English 1.891304    2.239130 1.543478 3.869565\n37                       French 4.277778    4.222222 4.333333 3.888889\n38                      English 4.416667    4.611111 4.222222 3.111111\n39                      History 2.569620    2.696203 2.443038 2.658228\n40                      Theater 3.472222    3.111111 3.833333 3.388889\n41                      English 2.442308    2.692308 2.192308 2.230769\n42            Religious Studies 3.067308    3.403846 2.692308 3.076923\n43            Religious Studies 3.600000    3.600000 3.600000 3.200000\n44                      Spanish 4.115385    4.230769 4.000000 2.884615\n45                      English 2.900000    3.200000 2.600000 3.800000\n46                      English 3.388889    3.711111 3.111111 2.844444\n47                   Philosophy 4.708334    4.750000 4.666666 3.666667\n48                         FLTR 3.291667    3.333333 3.250000 3.333333\n49                      History 2.250000    2.062500 2.437500 2.812500\n50                      English 3.555556    3.722222 3.388889 2.777778\n51                       French 3.854167    3.875000 3.833333 3.000000\n52                      History 3.913793    3.965517 3.862069 2.965517\n53                      History 3.469231    3.353846 3.584615 3.061538\n54                          Art 3.045455    3.090909 3.000000 3.363636\n55                      Spanish 3.464286    3.285714 3.642857 3.357143\n56                       German 4.000000    4.000000 4.000000 2.500000\n57                        Music 3.982759    3.862069 4.103448 3.310345\n58                      History 4.518519    4.518519 4.518519 4.074074\n59                      Theater 2.958333    2.833333 3.083333 2.500000\n60                      History 2.085965    2.280702 1.894737 2.631579\n61                      Theater 3.538462    3.461538 3.615385 3.230769\n62                      English 2.924242    2.696970 3.151515 3.454545\n63                      English 2.293478    2.282609 2.304348 2.456522\n64                      English 3.684211    3.684211 3.684211 3.157895\n65                       German 4.666666    4.888889 4.444445 3.055556\n66               Womens Studies 4.732143    4.857143 4.607143 4.500000\n67               Womens Studies 4.880952    4.952381 4.809524 4.809524\n68                      English 4.653846    4.615385 4.692308 4.076923\n69                      History 3.100000    3.500000 2.700000 3.000000\n70                      Spanish 2.107143    1.928571 2.285714 2.000000\n71                      Spanish 4.180000    4.320000 4.040000 3.720000\n72                      English 3.500000    3.500000 3.500000 2.272727\n73                      English 4.625000    4.666667 4.583333 4.527778\n74                      Spanish 4.428571    4.642857 4.214286 4.000000\n75                      English 2.394737    2.263158 2.526316 1.736842\n76                      English 1.674419    1.686047 3.360465 1.558140\n77                      English 3.760000    3.840000 3.680000 2.920000\n78                      English 4.538462    4.769231 4.307692 3.500000\n79            Religious Studies 1.875000    2.187500 1.562500 2.687500\n80            Religious Studies 4.025000    3.950000 4.100000 2.650000\n81                      Spanish 3.550000    3.666667 3.433333 1.966667\n82                      Spanish 3.166667    3.333333 3.000000 3.916667\n83                      English 2.523810    2.428571 2.619048 1.857143\n84                      Spanish 3.710526    3.894737 3.526316 2.789474\n85                          Art 4.458333    4.500000 4.416667 4.166667\n86                      Spanish 4.709677    4.806452 4.612903 3.645161\n87                      English 4.500000    4.545455 4.454545 3.636364\n88                       French 3.461538    3.307692 3.615385 2.923077\n89                       French 4.545455    4.545455 4.545455 3.500000\n90                      English 3.500000    3.807692 3.192308 3.692308\n91                      English 2.200000    2.000000 2.400000 2.400000\n92                      English 3.925000    4.000000 3.500000 3.850000\n93                      History 3.958333    3.916667 4.000000 2.750000\n94            Religious Studies 2.782609    3.000000 2.565218 3.768116\n95                      English 1.756757    1.810811 1.702703 1.783784\n96                      English 4.851351    4.945946 4.756757 3.729730\n97                      English 3.218750    3.343750 3.093750 2.156250\n98                      Spanish 3.312500    3.416667 3.208333 2.458333\n99               Womens Studies 3.695122    3.731707 3.658537 3.390244\n100                 Art History 3.794118    3.794118 3.794118 4.088235\n101                     Spanish 3.233333    3.266667 3.200000 3.400000\n102                     Spanish 3.236842    3.631579 2.842105 3.421053\n103                     English 4.700000    4.700000 4.700000 3.700000\n104                     English 1.482759    1.517241 1.448276 2.068966\n105                     Spanish 3.950000    3.900000 4.000000 3.400000\n106                     English 3.537037    3.629630 3.444444 2.814815\n107                     English 3.208333    3.222222 3.194444 2.083333\n108                     English 2.394737    2.684211 2.105263 1.789474\n109                       Dance 3.500000    3.714286 3.285714 3.500000\n110                     English 3.470588    3.529412 3.411765 3.294118\n111                     English 4.305556    4.259259 4.351852 3.203704\n112                     English 3.095238    3.142857 3.047619 3.857143\n113                  Philosophy 2.469697    2.454545 2.484848 2.666667\n114                     English 2.904255    3.021277 2.787234 3.361702\n115                     History 3.189655    3.206897 3.172414 3.672414\n116                     Spanish 4.678571    4.714286 4.642857 4.071429\n117              Art and design 3.200000    2.800000 3.600000 2.000000\n118                     History 3.833333    3.805556 3.861111 2.638889\n119                       Music 3.785714    3.500000 4.071429 3.285714\n120                       Music 3.442308    3.346154 3.500000 4.038462\n121                     History 2.589744    2.641026 2.538462 3.923077\n122                     History 2.437500    2.375000 2.500000 3.187500\n123                     English 2.977612    2.955224 3.000000 2.373134\n124                     English 3.224138    3.172414 3.275862 2.965517\n125                      German 2.750000    2.961539 2.538461 3.038461\n126                     English 3.729167    4.041666 3.416667 3.166667\n127                    Japanese 4.625000    4.812500 4.437500 3.062500\n128                     English 3.825000    4.000000 3.650000 2.550000\n129                     History 3.200000    3.171429 3.228571 3.028571\n130                     English 3.347222    3.333333 3.361111 3.500000\n131                     English 3.000000    3.074074 2.851852 3.592593\n132                       Music 4.100000    4.100000 4.100000 2.600000\n133                     English 2.792453    2.698113 2.886792 2.735849\n134                     Spanish 3.076923    3.076923 3.076923 3.461538\n135                  Psychology 4.800000    4.900000 4.700000 4.400000\n136               Communication 4.571429    4.571429 4.571429 4.238095\n137                  Psychology 4.757576    4.818182 4.696970 3.969697\n138               Communication 3.636364    3.545455 3.727273 3.727273\n139               Communication 3.583333    3.222222 3.944444 3.833333\n140                  Psychology 4.400000    4.366667 4.433333 4.133333\n141               Communication 4.071429    4.071429 4.142857 3.142857\n142                  Psychology 4.333333    4.333333 4.333333 2.933333\n143                   Sociology 4.250000    4.220000 4.280000 2.480000\n144                   Geography 4.150000    4.100000 4.200000 2.800000\n145           Political Science 3.366667    3.566667 3.166667 2.366667\n146                Anthropology 4.490566    4.283019 4.698113 3.603774\n147           Political Science 4.130435    3.934783 4.326087 3.217391\n148                  Psychology 2.700000    2.866667 2.533333 1.633333\n149           Political Science 3.142857    3.119048 3.166667 2.547619\n150                   Geography 2.621622    2.662162 2.581081 2.175676\n151                  Psychology 4.372549    4.176471 4.568627 3.588235\n152                  Psychology 1.467742    1.483871 1.451613 2.483871\n153                  Psychology 3.421053    3.578947 3.263158 4.368421\n154                Anthropology 2.419643    2.375000 2.464286 2.142857\n155           Political Science 3.833333    3.805556 3.861111 2.611111\n156           Political Science 4.216418    4.104477 4.328358 2.537314\n157           Political Science 2.494118    2.764706 2.223529 2.082353\n158                  Psychology 4.178571    4.142857 4.214286 2.428571\n159                   Sociology 2.872340    3.340426 2.404255 3.808511\n160                   Sociology 3.086957    3.420290 2.753623 3.579710\n161                Anthropology 3.409091    3.454545 3.363636 3.818182\n162           Political Science 2.535714    2.446429 2.625000 2.500000\n163               Communication 1.604167    1.500000 1.708333 2.083333\n164               Communication 2.630435    3.173913 2.086957 3.173913\n165               Communication 2.916667    3.138889 2.694444 3.500000\n166                   Sociology 2.733333    2.622222 2.844444 3.177778\n167     Communication Disorders 4.684211    4.894737 4.473684 4.210526\n168               Communication 4.000000    4.272727 3.727273 3.636364\n169               Communication 4.600000    4.700000 4.500000 4.500000\n170                   Sociology 4.544643    4.517857 4.571429 4.303571\n171                  Psychology 4.311765    4.305882 4.317647 4.435294\n172                  Psychology 4.937500    4.937500 4.937500 4.000000\n173                  Psychology 4.115385    4.179487 4.051282 2.410256\n174               Communication 2.435484    2.451613 2.419355 1.870968\n175                  Psychology 4.574074    4.703704 4.444444 2.888889\n176                   Sociology 3.880597    3.746269 4.029851 3.910448\n177                   Sociology 3.666667    3.866667 3.466667 3.533333\n178                   Sociology 4.195652    4.413043 3.978261 3.869565\n179                Anthropology 4.153846    4.384615 3.923077 4.076923\n180               Communication 2.000000    1.962963 2.037037 3.185185\n181               Communication 3.742857    3.714286 3.771429 2.885714\n182                   Sociology 2.171875    2.437500 1.906250 4.093750\n183                  Psychology 4.295455    4.318182 4.272727 3.954545\n184                  Psychology 3.686275    3.686275 3.686275 2.764706\n185               Communication 3.636364    4.090909 3.181818 3.363636\n186                  Psychology 4.076923    4.076923 4.076923 3.423077\n187                Anthropology 3.435484    3.274194 3.596774 2.322581\n188               Communication 2.730769    2.230769 3.230769 3.461538\n189                  Psychology 3.625000    3.550000 3.700000 3.500000\n190                  Psychology 4.562500    4.468750 4.656250 3.875000\n191                   Sociology 3.173913    3.130435 3.217391 3.173913\n192               Communication 3.741935    3.612903 3.870968 3.483871\n193                  Psychology 4.535714    4.547619 4.523810 2.500000\n194               Communication 4.083333    3.958333 4.208333 3.333333\n195     Communication Disorders 2.807692    2.730769 2.884615 3.576923\n196                   Geography 3.791667    3.500000 4.083334 3.000000\n197               Communication 3.343750    3.218750 3.468750 2.718750\n198                  Psychology 2.580000    2.600000 2.560000 2.860000\n199           Political Science 2.728070    2.526316 2.929825 2.684210\n200               Communication 3.966667    3.800000 4.133333 3.400000\n201                        Math 4.921875    5.000000 4.843750 3.812500\n202                        Math 2.529412    2.411765 2.647059 2.235294\n203                   Chemistry 2.565218    2.565218 2.565218 1.391304\n204                   Chemistry 4.269231    4.461538 4.076923 2.615385\n205                        Math 3.730769    3.692308 3.769231 3.846154\n206                        Math 2.638889    2.833333 2.444444 2.055556\n207                     Geology 4.954545    4.909091 5.000000 4.363636\n208                   Chemistry 4.480769    4.384615 4.576923 3.038462\n209                     Biology 2.646667    2.800000 2.533333 2.000000\n210                     Physics 4.720000    4.760000 4.680000 2.560000\n211                   Chemistry 2.619048    2.904762 2.333333 1.571429\n212           Astronomy/Physics 4.918605    5.000000 4.837209 3.279070\n213                        Math 4.549020    4.627451 4.470588 4.039216\n214                     Biology 4.045455    4.090909 4.000000 3.000000\n215                   Chemistry 4.796296    4.925926 4.666667 3.148148\n216                   Chemistry 3.067568    3.108108 3.027027 2.189189\n217                     Biology 3.385714    3.028571 3.742857 1.628571\n218                   Chemistry 4.000000    3.655172 4.344828 2.137931\n219                   Chemistry 1.750000    2.166667 1.333333 2.666667\n220            Computer Science 4.321429    4.392857 4.250000 2.714286\n221                     Physics 4.666667    4.757576 4.575758 3.212121\n222                        Math 2.714286    2.892857 2.535714 2.750000\n223                        Math 3.338710    3.645161 3.048387 3.193548\n224                        Math 3.986842    4.026316 3.947368 3.842105\n225       Physics and Astronomy 3.821429    4.071429 3.571429 3.214286\n226                     Geology 3.482759    3.482759 3.482759 3.206897\n227                   Chemistry 3.490566    3.924528 3.056604 3.000000\n228                     Geology 4.625000    4.600000 4.650000 3.700000\n229         Physics & Astronomy 3.452381    3.523810 3.380952 2.238095\n230                     Biology 2.287500    2.450000 2.125000 1.900000\n231                     Biology 3.038462    3.384615 2.692308 2.230769\n232                     Biology 4.365672    4.447761 4.283582 1.820896\n233                     Physics 3.684211    3.947368 3.421053 2.157895\n234                     Geology 3.500000    3.294118 3.705882 2.823529\n235                     Physics 4.159091    4.363636 3.954545 3.090909\n236                        Math 3.562500    3.437500 3.687500 3.375000\n237                   Chemistry 3.562500    3.437500 3.687500 3.375000\n238                     Geology 3.960526    4.000000 3.921053 2.578947\n239                        Math 4.231343    4.164179 4.298507 4.164179\n240                     Biology 2.671429    2.885714 2.457143 1.971429\n241                        Math 4.015625    3.937500 4.093750 4.406250\n242                     Biology 3.103448    3.275862 2.931034 2.224138\n243       Physics and Astronomy 4.166667    4.428571 3.904762 3.095238\n244                     Biology 2.791667    2.750000 2.833333 1.750000\n245                        Math 3.966667    4.088889 3.844444 2.533333\n246                     Geology 4.107143    4.142857 4.142857 3.071429\n247                   Chemistry 3.329268    3.512195 3.146341 3.000000\n248                     Geology 2.800000    2.900000 2.700000 2.400000\n249                     Biology 4.066667    3.900000 4.233333 2.933333\n250            Computer Science 4.620000    4.720000 4.520000 2.480000\n251                        Math 3.000000    3.081081 2.918919 3.054054\n252                   Chemistry 2.586957    2.608696 2.565217 2.695652\n253                        Math 1.915385    2.076923 1.723077 2.107692\n254                     Geology 4.527778    4.537037 4.518519 2.888889\n255                   Chemistry 3.890244    4.073171 3.707317 2.634146\n256                        Math 4.384615    4.461538 4.307692 3.846154\n257                        Math 3.451923    3.596154 3.269231 3.692308\n258                        Math 3.878788    4.030303 3.727273 3.515152\n259                     Physics 3.950000    3.900000 4.000000 3.200000\n260                   Chemistry 4.846154    4.923077 4.769231 3.769231\n261                        Math 2.865385    2.826923 2.903846 2.250000\n262                   Chemistry 3.000000    3.400000 2.600000 2.300000\n263                     Biology 3.166667    3.333333 3.000000 2.153846\n264                        Math 3.264151    3.339623 3.188679 3.528302\n265                        Math 2.928571    3.081633 2.775510 3.591837\n266         Physics & Astronomy 4.265306    4.265306 4.265306 3.204082\n267                        Math 2.508772    2.561404 2.473684 3.017544\n268                   Chemistry 4.235294    4.235294 4.235294 2.705882\n269            Computer Science 2.850000    2.933333 2.766667 2.200000\n270                     Geology 3.307692    3.461539 3.153846 2.769231\n271                        Math 2.620690    2.758621 2.482759 3.034483\n272                   Chemistry 3.103448    3.172414 3.034483 2.758621\n273            Computer Science 3.727273    4.000000 3.454545 3.272727\n274                     Geology 2.375000    2.416667 2.333333 2.750000\n275                        Math 3.722222    3.944444 3.500000 3.500000\n276                   Chemistry 2.500000    2.733333 2.266667 2.733333\n277                        Math 4.940000    5.000000 4.880000 4.800000\n278                     Biology 2.900000    3.100000 2.700000 1.500000\n279                     Biology 4.692308    4.769231 4.615385 4.230769\n280                        Math 3.436170    3.531915 3.340425 4.744681\n281                        Math 3.772727    3.818182 3.727273 3.727273\n282                   Chemistry 2.750000    2.777778 2.722222 2.444444\n283                   Chemistry 2.619048    3.190476 2.047619 2.523810\n284                     Geology 4.285714    4.452381 4.119048 3.619048\n285                     Biology 4.015152    4.181818 3.848485 1.878788\n286                        Math 4.981481    5.000000 4.962963 3.962963\n287                        Math 4.583333    4.500000 4.666667 3.666667\n288                     Biology 2.422222    2.533333 2.311111 2.022222\n289                        Math 2.336735    2.591837 2.081633 2.183673\n290                     Biology 4.500000    4.555556 4.444444 3.305556\n291                     Geology 3.555556    3.555556 3.555556 2.500000\n292                        Math 4.064516    4.096774 4.032258 3.258065\n293                     Biology 3.529412    3.470588 3.588235 2.117647\n294                     Biology 4.233333    4.366667 4.100000 2.933333\n295                     Biology 3.454545    3.363636 3.545455 2.636364\n296                        Math 2.352941    2.411765 2.294118 2.000000\n297                        Math 2.087500    2.375000 1.800000 2.387500\n298                     Physics 3.653846    4.076923 3.230769 2.794872\n299                     Biology 2.153846    2.346154 1.961538 2.115385\n300            Computer Science 2.655172    2.689655 2.620690 2.655172\n301                        Math 2.608333    2.716667 2.466667 3.483333\n302       Physics and Astronomy 2.406250    2.375000 2.437500 1.937500\n303                        Math 2.900000    3.200000 2.600000 3.100000\n304                        Kins 2.392857    2.142857 2.642857 1.428572\n305                        Kins 4.892857    4.928571 4.857143 4.857143\n306                        Kins 2.958333    3.000000 2.916667 3.833333\n307                        Kins 2.000000    2.000000 2.000000 2.846154\n308                    Business 3.954545    4.363636 3.545455 3.636364\n309                   Economics 4.294872    4.384615 4.205128 3.769231\n310                        Kins 4.453125    4.343750 4.562500 4.625000\n311                  Accounting 4.200000    4.600000 4.200000 2.400000\n312                        Kins 4.904762    4.857143 4.952381 4.809524\n313                     Finance 3.100000    3.500000 2.700000 1.900000\n314 Environmental Public Health 3.166667    3.166667 3.250000 3.583333\n315         Information Systems 3.433333    3.533333 3.333333 3.600000\n316                   Economics 4.811321    4.849057 4.773585 4.132075\n317                   Economics 3.919118    4.250000 3.588235 2.911765\n318                  Accounting 4.062500    4.229167 3.895833 1.750000\n319                    Business 2.600000    3.100000 2.200000 4.300000\n320                    Business 2.791667    2.750000 2.777778 2.194444\n321                     Finance 4.166667    4.266667 4.066667 3.000000\n322         Information Systems 4.250000    4.218750 4.281250 4.125000\n323                   Economics 4.260870    4.478261 4.043478 4.130435\n324                    Business 4.178571    4.071429 4.285714 3.142857\n325          Managerial Science 2.916667    3.000000 2.833333 2.333333\n326                  Accounting 2.919355    2.645161 3.193548 2.322581\n327                   Marketing 4.562500    4.791667 4.333333 3.666667\n328                   Economics 4.393939    4.424242 4.363636 3.454545\n329            Criminal Justice 4.640000    4.560000 4.720000 3.940000\n330  Curriculum and Instruction 2.590909    2.636364 2.545454 2.272727\n331             Library Science 4.394737    4.473684 4.315789 3.368421\n332                     Finance 3.447368    3.105263 3.789474 2.473684\n333                   Economics 3.719298    3.912281 3.526316 2.561404\n334                    Business 2.720588    2.764706 2.676471 2.647059\n335                    Business 3.687500    3.812500 3.562500 3.437500\n336           Special Education 3.136364    3.000000 3.272727 2.090909\n337                   Economics 4.288889    4.466667 4.111111 3.155556\n338                 Social Work 3.095238    2.809524 3.380952 2.666667\n339                    Business 4.050000    3.900000 4.200000 4.900000\n340           Special Education 4.850000    4.900000 4.800000 4.900000\n341                  Accounting 4.861111    4.833333 4.888889 4.722222\n342                        Kins 4.850000    4.800000 4.900000 4.000000\n343                  Management 3.350000    3.600000 3.100000 3.700000\n344                   Marketing 3.562500    3.687500 3.437500 4.125000\n345                    Business 4.687500    4.625000 4.750000 3.500000\n346                   Marketing 3.600000    3.600000 3.600000 2.700000\n347          Managerial Science 3.433333    3.666667 3.200000 3.200000\n348                   Economics 4.120690    4.344828 3.896552 4.034483\n349                   Economics 4.083330    4.583330 3.583330 4.166670\n350                 Social Work 3.181818    3.272727 3.090909 3.545455\n351                        Kins 4.340909    4.545455 4.090909 4.454545\n352                  Accounting 3.491935    3.612903 3.370968 2.467742\n353                  Management 4.500000    4.428571 4.571429 3.619048\n354                   Economics 3.584906    3.415094 3.754717 3.037736\n355                        Kins 3.708333    3.583333 3.833333 4.250000\n356                  Accounting 3.318182    3.090909 3.545455 2.181818\n357                   Economics 3.513158    3.657895 3.368421 3.868421\n358                   Economics 4.228261    4.304348 4.152174 3.369565\n359                        Kins 1.900000    1.900000 1.900000 3.800000\n360                   Economics 1.937500    2.250000 1.625000 2.333333\n361                    Business 3.462963    3.333333 3.592593 3.111111\n362                 Social Work 2.619048    2.714286 2.523810 3.619048\n363                  Accounting 2.966667    3.066667 2.866667 2.666667\n364                  Accounting 3.250000    3.200000 3.300000 3.000000\n365                     Nursing 1.909091    1.909091 1.909091 2.272727\n366                   Marketing 1.409091    1.363636 1.454545 2.636364\n    raterInterest  sdQuality sdHelpfulness sdClarity sdEasiness sdRaterInterest\n1        3.545455 0.55185640     0.6741999 0.5045250  0.4045199       1.1281521\n2        4.000000 0.90201795     0.9341987 0.9438798  0.5045250       1.0744356\n3        3.432432 0.45293432     0.6663898 0.4129681  0.5407021       1.2369438\n4        3.181818 0.93250483     0.9315329 0.9990938  0.5882300       1.3322506\n5        4.214286 0.65001124     0.8200699 0.5823927  0.6117753       0.9749613\n6        3.916667 0.86327170     1.0327956 0.7745967  0.6399405       0.6685579\n7        3.812500 0.94421613     0.9963167 0.9393364  0.6966305       1.2230427\n8        2.937500 1.19547760     1.3400871 1.1236103  0.7274384       1.6111590\n9        3.750000 1.07573090     1.3371159 1.0444659  0.7537783       1.2880570\n10       4.176471 0.90025413     1.0032627 0.9074852  0.7669650       1.5381123\n11       2.900000 0.34377584     0.3015113 0.5222330  0.7745967       1.1972190\n12       3.333333 1.23870660     1.3257359 1.2975793  0.7917663       1.2909944\n13       3.217391 0.98556780     1.0632191 1.1246431  0.7952428       1.1660548\n14       4.718750 0.51816386     0.6262243 0.4357058  0.8060600       0.5811210\n15       3.952381 0.99206334     1.0368697 0.9920634  0.8340577       1.0712698\n16       3.592593 0.16012815     0.1924501 0.2668803  0.8688992       1.1522306\n17       2.357143 0.60333323     0.4688072 0.8925824  0.8925824       1.1507284\n18       3.526316 0.69142620     0.6839856 0.8164966  0.8950808       1.1239030\n19       4.000000 1.26131250     1.4459976 1.1908744  0.9045340       1.0000000\n20       3.230769 1.25575600     1.3827827 1.2666473  0.9138736       1.2351685\n21       3.537037 1.59585600     1.6433989 1.5878705  0.9240599       1.3418231\n22       3.250000 1.23482860     1.2724180 1.3348206  0.9511898       1.3228756\n23       2.923077 1.57911040     1.6641006 1.6012815  0.9607689       1.4978617\n24       3.400000 0.71879530     0.9102590 1.0465362  0.9611501       1.2983506\n25       3.800000 0.93930380     1.1383468 0.9309493  0.9639329       0.9411239\n26       3.000000 1.27236300     1.3271566 1.3823619  0.9744639       1.5811388\n27       4.909091 0.20225996     0.0000000 0.4045199  0.9816498       0.3015113\n28       3.333333 1.12683750     1.1609591 1.2605288  0.9840627       1.3284223\n29       4.153846 1.08735290     1.1596247 1.1410496  0.9885383       0.8338972\n30       3.516129 1.00932060     1.1529390 0.9947598  1.0101115       1.3384311\n31       4.029412 0.62544435     0.6872130 0.7580765  1.0179750       1.0294245\n32       3.631579 1.38862860     1.3484884 1.4867839  1.0195458       1.1160708\n33       3.475000 1.35519330     1.5017607 1.3655378  1.0332731       1.3772417\n34       3.901961 0.72999640     0.8867586 0.8252743  1.0480319       1.1533413\n35       3.933333 1.26491100     1.2983506 1.3201731  1.0600988       0.7988086\n36       2.347826 1.03769530     1.4634022 0.8084697  1.0668478       1.1588767\n37       3.882353 0.84404874     1.0602750 0.7669650  1.0786096       0.9926198\n38       3.500000 0.57522374     0.9785276 0.7320845  1.0786096       0.9851844\n39       2.956522 1.17597260     1.3141676 1.2273260  1.0846963       1.3109806\n40       4.000000 1.18162680     1.4095844 1.0981267  1.0921586       1.0954451\n41       2.692308 1.37351320     1.5942203 1.2967415  1.1066234       1.2890068\n42       3.442308 1.14630200     1.2408033 1.2452020  1.1175280       1.1784580\n43       4.300000 1.50554530     1.5055453 1.5776213  1.1352924       0.4830459\n44       4.320000 0.86380196     0.8629110 1.0954451  1.1428709       0.9882645\n45       2.733333 1.28452320     1.3732131 1.3522468  1.1464230       1.3870146\n46       3.185185 1.42577290     1.4866408 1.5109031  1.1472409       1.3877773\n47       3.416667 0.54181236     0.4522670 0.6513389  1.1547005       1.4433757\n48       3.416667 1.33923880     1.3026779 1.5447860  1.1547005       1.4433757\n49       3.200000 0.96609180     1.1236103 1.0935416  1.1672617       1.2649110\n50       3.290322 1.40802690     1.4660171 1.4595063  1.1737878       1.2556325\n51       4.227273 1.27244340     1.2958965 1.3405601  1.1795356       0.9725675\n52       4.320000 1.40196910     1.4010904 1.4814119  1.1796718       0.9451631\n53       3.280702 1.28049350     1.3855366 1.3450407  1.1842313       1.3059532\n54       3.636364 1.42222620     1.6403991 1.2649111  1.2060454       0.9244163\n55       2.785714 1.51231200     1.7288756 1.3926810  1.2157393       1.4238934\n56       3.875000 1.19292780     1.4142136 1.0377490  1.2247449       0.6408699\n57       4.333333 1.24987690     1.4814119 1.2633523  1.2846191       1.2038585\n58       4.340909 0.70015470     0.7458246 0.8182065  1.2863852       1.0330173\n59       3.500000 1.23322070     1.4034589 1.3113722  1.3142575       1.0000000\n60       3.166667 1.15177480     1.2642670 1.2054076  1.3447196       1.4241846\n61       3.800000 1.16299780     1.5607362 0.8697185  1.3634421       1.6193277\n62       2.696970 1.28160800     1.3803271 1.3257359  1.3713795       1.2370542\n63       3.806452 1.33988430     1.4089089 1.3311576  1.3777038       1.3271361\n64       2.933333 1.18098970     1.2495613 1.2495613  1.3849652       1.1629192\n65       3.937500 0.42008403     0.3233808 0.7047922  1.0556415       0.7719024\n66       3.333333 0.44058344     0.4483951 0.5669467  0.5091751       1.2089410\n67       3.428571 0.21821790     0.2182179 0.4023739  0.5117663       1.2478553\n68       3.550000 0.62879616     0.6972473 0.6793662  0.6275716       1.0505954\n69       2.800000 1.12546290     1.3540064 1.0593499  0.6666667       1.2292726\n70       3.166667 1.04105290     1.2066665 1.1387288  0.6793662       1.4034589\n71       3.840000 0.77567180     0.8524475 0.8888194  0.7371115       0.9865766\n72       3.000000 1.38873020     1.5039630 1.4392458  0.7672969       1.4832397\n73       3.722222 0.59009683     0.6324555 0.7319251  0.7740842       1.0852547\n74       3.214286 0.26726124     0.4972452 0.5789342  0.7844645       1.1217138\n75       3.066667 1.51454940     1.6276126 1.6113632  0.8056816       1.4375906\n76       2.814815 1.07585110     1.1808435 1.0584168  0.8059288       1.2258784\n77       3.631579 1.20864940     1.2330483 1.2489996  0.8124038       1.2115429\n78       3.730769 0.54631630     0.4296689 0.7358930  0.8602325       1.1156233\n79       2.666667 0.82663980     1.1086779 0.8139410  0.8732125       1.2909944\n80       3.812500 1.15251450     1.2343760 1.1652874  0.8750940       1.1672618\n81       4.000000 1.15482500     1.0933445 1.2507469  0.8899180       1.1126973\n82       3.777778 1.05169420     1.0730867 1.2060454  0.9003366       1.7159384\n83       2.450000 1.25972410     1.4342743 1.3219754  0.9102590       1.3562720\n84       4.157895 1.26178660     1.2864567 1.3485968  0.9176629       1.2588865\n85       3.750000 0.68947720     0.9045340 0.6685579  0.9374369       1.1381804\n86       3.870968 0.49622230     0.4774484 0.6672041  0.9503819       1.4081178\n87       3.545455 0.80178374     0.8578641 0.9116846  0.9534626       1.0107646\n88       3.153846 0.98871840     1.1094004 1.1929279  0.9540736       1.2142318\n89       3.818182 0.87163080     1.0568269 0.8004328  0.9636241       1.2960145\n90       3.153846 1.20830460     1.3272296 1.2006409  0.9703290       1.3473621\n91       2.769231 1.34695420     1.3093073 1.4540584  0.9856108       1.4232502\n92       3.250000 1.05475120     1.1697953 1.0894228  0.9880869       1.2926920\n93       3.666667 1.13172300     1.4116492 0.9780193  0.9890707       1.3904436\n94       2.903226 1.24699130     1.3612279 1.2541491  1.0021291       1.3755437\n95       2.545455 1.06472230     1.1263964 1.1514451  1.0037467       1.5429458\n96       3.750000 0.30878040     0.2292434 0.4947168  1.0178586       1.2181424\n97       2.937500 1.24393690     1.3102419 1.4223872  1.0194678       1.1341474\n98       3.125000 1.46563900     1.5012072 1.5316705  1.0206207       1.2618999\n99       3.358974 1.24939010     1.3968606 1.2571745  1.0217154       1.1582014\n100      2.794118 1.14228400     1.2499554 1.2739681  1.0259555       1.2739681\n101      3.400000 1.34783780     1.4864467 1.3732131  1.0555973       1.3522468\n102      3.457143 1.42722960     1.4597492 1.5338754  1.0560400       1.2912114\n103      3.400000 0.78881060     0.9486833 0.6749486  1.0593499       1.0749677\n104      2.400000 0.66120505     0.7847060 0.6316762  1.0667385       1.3844373\n105      3.900000 0.89597870     1.4491377 0.6666667  1.0749677       0.8755950\n106      3.166667 1.31504550     1.3414650 1.3681355  1.0754976       1.4345630\n107      2.647059 1.15495820     1.3117297 1.0907257  1.0790207       1.2524486\n108      2.842105 1.03519920     1.2495613 1.1002392  1.0841765       1.3849652\n109      3.846154 1.17669680     1.2666474 1.2043876  1.0919284       1.7246330\n110      2.875000 1.15204420     1.3284223 1.1757351  1.1048024       1.6683325\n111      2.920000 1.01133510     1.1190492 0.9935150  1.1053836       1.2590894\n112      2.850000 1.51343190     1.6212870 1.5961263  1.1084094       1.0021622\n113      3.363636 1.36324180     1.4809395 1.4603341  1.1086779       1.0552897\n114      3.380952 1.35389260     1.5671819 1.3011061  1.1117071       1.1032630\n115      3.333333 1.21694030     1.3861295 1.2860647  1.1299423       0.9712859\n116      4.071429 0.63872296     0.6112498 0.8418974  1.1411388       1.1411388\n117      3.000000 1.41813650     1.6193277 1.3498971  1.1547005       1.2472191\n118      3.166667 1.06904500     1.2608261 1.0994226  1.1748016       1.2761549\n119      3.714286 0.87077080     1.0919285 0.7300459  1.2043875       1.2043875\n120      1.098039 1.57052170     1.7191232 1.7191232  1.2159200       1.1972190\n121      3.105263 1.33215200     1.4232502 1.3542556  1.2222631       1.4292216\n122      3.062500 1.22304260     1.5438048 1.0954451  1.2230427       1.4361407\n123      2.552239 1.47307360     1.6554120 1.4142136  1.2288923       1.1452614\n124      2.827586 1.36660560     1.5369024 1.3600565  1.2387424       1.4159541\n125      3.277778 1.19373370     1.3705698 1.1740791  1.2483835       1.2307339\n126      3.333333 1.33025940     1.3376712 1.5012072  1.2740442       1.1671841\n127      4.533333 0.64549720     0.5439056 0.8920949  1.2893797       0.6399405\n128      3.235294 1.29039980     1.3764944 1.3869694  1.3168943       1.1472473\n129      3.657143 1.31842150     1.3823619 1.3303187  1.3169866       1.1867617\n130      3.500000 1.53910660     1.5856499 1.6062873  1.3416408       1.3471506\n131      3.148148 1.27097790     1.3846945 1.3503192  1.3660516       1.0635103\n132      3.300000 1.24275680     1.3703203 1.1972190  1.4298407       1.6363917\n133      2.913043 1.35317990     1.3948381 1.4366143  1.4431662       1.2441166\n134      2.615385 1.32045050     1.4978617 1.3204506  1.4500221       1.0439078\n135      4.500000 0.25819890     0.3162278 0.4830459  0.5163978       0.5477226\n136      4.095238 0.63807750     0.6761234 0.6761234  0.7003401       0.9952267\n137      4.000000 0.28287620     0.3916747 0.4666937  0.7699370       0.7905694\n138      3.333333 1.22659910     1.3684763 1.1908744  0.7862454       1.3228757\n139      3.000000 1.15363870     1.2628425 1.2113300  0.8574929       1.2649111\n140      4.000000 0.75885576     0.7648905 0.8583598  0.8603661       0.9325048\n141      3.500000 1.10692130     1.3847680 1.0271052  0.8644378       1.0801235\n142      3.933333 0.91936830     1.0465362 0.8997354  0.8837151       0.5936168\n143      3.312500 0.66432410     0.7899884 0.8091316  0.8861750       1.1697654\n144      3.800000 1.17968920     1.1972190 1.2292726  0.9189366       1.3165612\n145      3.296296 1.22427560     1.3047217 1.3412124  0.9278575       1.0308628\n146      3.428571 0.62396500     0.9482242 0.5401177  0.9474586       1.3844373\n147      3.057143 0.87834935     0.9752988 0.9440892  0.9640895       1.4939655\n148      3.192308 1.36836170     1.5698305 1.2793677  0.9643055       1.2335066\n149      3.428571 1.23607220     1.2916690 1.3954048  0.9678334       1.5324595\n150      3.218750 1.30806460     1.4456077 1.3447692  0.9702155       1.2657735\n151      4.083334 0.76709280     1.0675591 0.6963624  0.9900296       0.7810788\n152      3.225806 0.88445354     0.9956896 0.8500474  0.9956896       1.5429339\n153      3.277778 1.49316180     1.6095475 1.5578513  1.0116283       1.6379886\n154      3.111111 1.40057380     1.4962073 1.4008346  1.0344708       1.3963114\n155      3.606557 1.17485390     1.2631159 1.1904024  1.0421478       1.2685778\n156      3.500000 0.91793525     1.0020332 0.9752709  1.0777130       1.2460464\n157      3.222222 1.24521490     1.4528056 1.2571365  1.0824549       1.3862730\n158      3.785714 1.01160850     1.0994504 1.1217138  1.0894096       1.3688047\n159      2.857143 1.13476720     1.4337030 1.0766206  1.0962049       1.2010448\n160      3.019608 1.21557450     1.3438947 1.1931890  1.1298854       1.2726381\n161      3.090909 1.39316510     1.5075567 1.4333686  1.1677484       1.3003496\n162      2.365854 1.39106610     1.4259879 1.5203170  1.1908744       1.3182583\n163      2.454545 0.90864410     0.9780193 0.9990938  1.2128539       1.4384937\n164      3.650000 1.17953560     1.5270939 1.0406748  1.2303796       1.0399899\n165      2.861111 1.33897610     1.5147424 1.3484265  1.2305632       1.2224747\n166      3.097561 1.33399600     1.4027390 1.4027390  1.3973279       1.0721450\n167      4.368421 0.34199280     0.3153018 0.5129892  0.6306035       0.7608859\n168      3.636364 0.80622580     0.9045340 0.9045340  0.6741999       0.9244163\n169      3.400000 0.65828060     0.6749486 0.7071068  0.7071068       1.5776213\n170      3.285714 0.51589980     0.6321988 0.6566344  0.7608522       1.0738933\n171      3.705882 0.91604894     0.9883070 0.9905718  0.7935313       1.1070847\n172      2.187500 0.17078250     0.2500000 0.2500000  0.8164966       1.5585784\n173      3.828571 1.23789690     1.2746887 1.2343487  0.8181477       1.0427823\n174      3.357143 1.19542130     1.2606535 1.2589465  0.8462441       1.3666473\n175      3.833333 0.58348596     0.5417078 0.7510676  0.8473185       1.0494995\n176      3.566667 1.06986810     1.2103504 1.0726652  0.8656989       1.1404232\n177      3.466667 1.34518540     1.5055453 1.4573296  0.9154754       1.1872337\n178      3.369565 0.98589080     1.1070699 0.9997584  0.9335403       1.1226694\n179      3.692308 1.16161910     1.1929279 1.2557560  0.9540736       1.1094004\n180      2.740741 1.05611770     1.2854655 1.0912759  0.9622504       1.1298654\n181      3.483871 1.24482120     1.4465258 1.2622509  0.9631880       1.1509697\n182      3.281250 1.29271770     1.6051831 1.2536238  0.9954534       1.3009767\n183      3.590909 0.95940320     1.0413528 0.9847319  0.9989172       0.7341397\n184      3.500000 0.96416175     1.0486219 1.0860975  1.0116963       1.1832160\n185      4.714286 1.00227010     1.4459976 1.0787198  1.0269106       0.4879500\n186      3.659574 1.21826220     1.3112807 1.2342522  1.0355666       1.1087909\n187      3.241935 1.17516610     1.3203248 1.2208527  1.0366117       1.1878850\n188      2.923077 1.12944280     1.3634421 1.0127394  1.0500305       1.3204506\n189      3.950000 1.26569100     1.5035047 1.2607433  1.0513150       0.9986833\n190      4.266667 0.83037110     1.0467885 0.7006621  1.0701221       0.9802650\n191      4.086957 1.54925720     1.5463843 1.6502485  1.0724727       0.6683115\n192      2.870968 1.07787720     1.2460408 1.0937059  1.0825279       1.1232837\n193      3.761905 0.66619470     0.8025077 0.7066960  1.0876244       0.8499505\n194      2.708333 1.26548390     1.3014763 1.2846643  1.1293194       1.1970677\n195      4.272727 1.64970860     1.7101507 1.6811168  1.2057554       1.1621744\n196      3.833333 0.81067690     1.2431631 0.5149286  1.2060454       1.2673044\n197      2.586207 1.33463480     1.4081416 1.3674647  1.2504032       1.0527936\n198      3.100000 1.30290100     1.4568627 1.3425532  1.2779128       1.2152872\n199      2.957447 1.24308870     1.3771770 1.2936610  1.3114591       1.3180593\n200      3.307692 1.21694387     1.3732131 1.2459458  1.5946339       1.4366985\n201      2.900000 0.18445101     0.0000000 0.3689020  0.5922892       1.3222238\n202      3.333333 1.21797200     1.2776357 1.2718675  0.6642112       0.8164966\n203      3.650000 1.38419940     1.3968564 1.4405203  0.7223151       1.2258188\n204      3.333333 0.63296210     0.7762500 0.6405126  0.7679476       1.3228757\n205      2.846154 1.21818480     1.1821319 1.3008873  0.8006408       0.9870962\n206      2.444444 1.30390310     1.4652846 1.2935233  0.8023658       1.2472191\n207      3.181818 0.15075567     0.3015113 0.0000000  0.8090398       1.4012981\n208      4.043478 0.84238670     1.0228166 0.8086075  0.8236878       1.2239378\n209      3.200000 1.46671000     1.6124516 1.3557637  0.8451543       1.3201731\n210      3.320000 0.43493295     0.5228129 0.4760952  0.8698659       1.2819256\n211      3.142857 1.46547570     1.6704718 1.3540064  0.8701396       1.1952286\n212      3.047619 0.21630646     0.0000000 0.4326129  0.8817078       1.2484601\n213      3.000000 0.59375840     0.7472827 0.6435197  0.8935499       1.3070323\n214      2.909091 1.12815210     1.3003496 1.0954451  0.8944272       1.2210279\n215      2.904762 0.31802452     0.2668803 0.5547002  0.9073929       1.3749459\n216      3.028571 1.42965700     1.5773357 1.4431156  0.9079231       1.3169866\n217      3.228571 1.20101500     1.4242793 1.2209653  0.9102590       1.2622509\n218      3.107143 0.91612536     1.1733914 0.9364012  0.9151166       1.3968029\n219      2.961538 0.80676025     1.2058288 0.6064784  0.9222661       1.1482428\n220      3.642857 0.91504186     0.9164863 1.0408330  0.9371803       0.9440892\n221      3.354839 0.60810500     0.6139169 0.7084447  0.9603898       1.4270806\n222      3.346154 1.22798060     1.4991179 1.1379690  0.9670497       1.1642099\n223      2.796610 1.15508210     1.2557641 1.2337789  0.9723844       1.1563865\n224      3.894737 1.19394220     1.4423487 1.0640912  0.9733285       1.1806886\n225      3.357143 0.91161615     1.1411388 0.9376145  0.9749613       1.2774459\n226      3.896552 1.27813170     1.6822750 1.1838403  0.9775812       1.2913124\n227      2.851064 1.00320000     1.1715584 1.0413763  0.9784634       1.3984265\n228      3.882353 0.55901700     0.6805570 0.5871429  0.9787210       1.1114379\n229      2.722222 1.42218820     1.5039630 1.5321942  0.9952267       1.2274103\n230      3.375000 1.18693470     1.4667249 1.1137256  1.0076629       1.3716451\n231      4.000000 1.39136530     1.5566236 1.4366985  1.0127394       1.2472191\n232      3.611940 1.03199600     1.0770834 1.1120002  1.0139239       1.5270810\n233      3.000000 1.01667380     1.2681432 1.2163602  1.0145145       1.3228757\n234      2.875000 1.03077640     0.9195587 1.3585243  1.0145993       1.1474610\n235      3.363636 0.89157915     0.9021379 1.0455016  1.0192944       1.3471507\n236      2.812500 1.22304260     1.3647344 1.2500000  1.0246951       1.0468206\n237      2.812500 1.22304260     1.3647344 1.2500000  1.0246951       1.0468206\n238      3.200000 1.01596430     1.1624764 1.0496223  1.0301293       1.2319282\n239      2.650794 0.97440100     1.0814835 1.0152612  1.0386036       1.2202423\n240      3.645161 1.24819200     1.3233520 1.3578282  1.0427823       1.1704241\n241      2.281250 1.31053040     1.3897667 1.3526408  1.0429293       1.1139692\n242      3.810345 1.23102660     1.4605106 1.1826534  1.0436626       1.4244112\n243      3.350000 1.01653000     1.1212238 1.0910895  1.0442587       1.2258187\n244      3.000000 1.21465170     1.2880570 1.1934163  1.0552897       1.1832160\n245      3.333333 1.35847510     1.4588635 1.3809673  1.0574412       1.2841818\n246      3.000000 0.62568640     0.9492623 0.5345225  1.0716117       1.0377490\n247      3.028571 1.21248590     1.5020311 1.1524100  1.0723805       1.3169866\n248      2.700000 1.22927260     1.2866839 1.2516656  1.0749677       1.2516656\n249      3.413793 0.76263310     1.0618786 0.7738544  1.0806554       0.9826074\n250      4.117647 0.72571800     0.6782330 0.9183318  1.0847427       1.2187264\n251      2.343750 1.33853150     1.4601822 1.3617468  1.1041826       1.1247760\n252      2.333333 1.29379800     1.4996706 1.3082287  1.1051443       1.0846523\n253      2.122807 0.97855616     1.2413160 0.9438567  1.1057107       1.2965638\n254      3.078431 0.84032700     0.8403270 0.7948120  1.1271381       1.2139710\n255      3.341463 1.20162490     1.2326850 1.2892615  1.1348149       1.2963363\n256      3.076923 1.13933180     1.1266014 1.1821319  1.1435437       1.4411534\n257      2.787234 1.20562260     1.2873383 1.2225878  1.1468364       1.2146957\n258      3.303030 1.17944840     1.3342800 1.1256311  1.1489455       1.3574988\n259      2.950000 1.30686450     1.5525870 1.1697953  1.1516578       1.1459310\n260      4.153846 0.31521258     0.2773501 0.4385290  1.1657506       1.2810252\n261      2.285714 1.25680650     1.3535886 1.2408033  1.1694644       1.3385315\n262      3.050000 1.15849270     1.2732057 1.1876558  1.1742859       1.1459310\n263      3.615385 1.20488770     1.3245324 1.2139540  1.1818465       1.1148610\n264      2.978723 1.13765860     1.3147594 1.1938400  1.1865233       1.1700866\n265      2.950000 1.17703720     1.2883571 1.2460823  1.2062056       1.1972190\n266      3.891304 1.15064310     1.3034491 1.1686087  1.2412962       1.0160547\n267      2.350877 1.35781870     1.5355030 1.2970470  1.2746320       1.2318863\n268      4.090909 1.14724730     1.2004901 1.1472473  1.3117119       1.0444659\n269      3.724138 1.34645130     1.5297810 1.3565507  1.3235272       1.2217245\n270      3.692308 1.31558700     1.3301244 1.3445045  1.3634421       1.1094004\n271      3.173913 1.25822410     1.4054784 1.1838403  1.3753638       1.0724727\n272      2.962963 1.30506780     1.3645765 1.3491468  1.4054784       1.2241632\n273      4.181818 1.64869094     1.6124516 1.8090681  1.4206273       0.9816498\n274      2.090909 1.13066760     1.4433757 1.3026779  1.4222262       0.8312094\n275      2.444444 1.36362650     1.4741786 1.4652846  1.4245742       1.3382263\n276      3.000000 1.06904500     1.4864467 1.0997835  1.4375906       1.4142136\n277      4.240000 0.16583124     0.0000000 0.3316625  0.5000000       0.8793937\n278      3.000000 1.02198060     0.8755950 1.3374935  0.5270463       1.3333333\n279      3.250000 0.59646390     0.4385290 0.8697185  0.7250111       1.1381804\n280      2.914894 1.11129090     1.2131716 1.1661270  0.7362680       1.4269120\n281      3.000000 1.45539750     1.6624188 1.3483997  0.9045340       1.4832397\n282      3.588235 1.37466570     1.5550886 1.4061025  0.9217772       0.9393364\n283      2.904762 1.19557330     1.4359334 1.1169687  0.9283883       1.0442587\n284      3.459459 0.99885650     0.9160461 1.1305596  0.9358023       1.1924437\n285      3.961538 0.79534630     0.8083372 0.9721501  0.9603898       0.8708970\n286      3.407407 0.09622505     0.0000000 0.1924501  0.9798541       1.3376000\n287      3.000000 0.73340220     0.6741999 0.8876254  0.9847319       1.6514456\n288      3.850000 1.27009100     1.4238233 1.2581050  1.0110501       1.1220403\n289      2.302326 1.20064120     1.3527498 1.2047948  1.0139335       0.9888638\n290      3.656250 0.70710677     0.6946508 0.8432740  1.0642085       1.0957211\n291      2.607843 1.06251160     1.2539247 1.0757475  1.0772081       1.2973578\n292      2.320000 1.40084460     1.4225526 1.4255729  1.0944631       1.3453624\n293      3.352941 1.23073390     1.3284223 1.2277430  1.1114379       1.2718675\n294      3.366667 1.08860350     1.2452207 1.0618786  1.1121068       1.2726116\n295      3.272727 1.40453820     1.5666989 1.3684763  1.1200649       1.4893562\n296      2.470588 0.91454744     0.9393364 1.1599949  1.1726039       1.2307339\n297      2.733333 1.09883430     1.3442329 1.0482052  1.2169036       1.2339054\n298      3.421053 1.27296710     1.2852322 1.4227760  1.2178386       1.0813300\n299      2.884615 1.02731920     1.3547637 0.9583640  1.2434443       1.4234303\n300      4.068966 1.34365870     1.4418106 1.3993313  1.2614012       1.3074248\n301      2.250000 1.35637750     1.4390636 1.4078071  1.2688132       1.0823902\n302      2.875000 1.14336860     1.3102163 1.0935416  1.3400871       1.3102163\n303      1.900000 1.07496770     1.3165612 0.9660918  1.3703203       1.1005049\n304      2.714286 1.19580300     1.2924124 1.1507283  0.5135526       1.2043875\n305      4.000000 0.28946713     0.2672612 0.3631365  0.5345225       1.1766968\n306      3.416667 1.49443412     1.4298407 1.6329932  0.6992059       1.3540064\n307      2.769231 1.24163870     1.5275252 1.0801234  0.8006408       1.4232502\n308      2.909091 1.15009880     1.2060454 1.2135598  0.8090398       1.1361818\n309      3.055556 0.76706850     0.8148421 0.8938234  0.8098583       1.1697239\n310      3.593750 0.87399140     1.0957211 0.8775883  0.8327955       1.5420844\n311      3.900000 0.78881064     0.6992059 0.6324555  0.8432740       1.2866839\n312      3.894737 0.20118695     0.3585686 0.2182179  0.8728716       1.1969747\n313      2.500000 1.32916010     1.5811388 1.4181365  0.8755950       0.9258201\n314      3.500000 1.11464080     1.6422453 1.0552897  0.9003366       1.1677484\n315      3.666667 1.26585190     1.3557637 1.2909944  0.9102590       1.5886502\n316      3.133333 0.34287268     0.4112002 0.4658123  0.9206548       1.2720778\n317      2.919355 1.06368230     1.1638203 1.2366939  0.9261587       1.1205734\n318      3.348837 1.09459510     1.0766296 1.2070640  0.9339917       1.3252802\n319      2.600000 1.02198060     1.3703203 1.3984118  0.9486833       1.3498971\n320      3.114286 1.38551180     1.5743479 1.3333333  0.9803627       1.2312459\n321      3.357143 0.79432510     0.9611501 0.7988086  1.0000000       1.4468609\n322      3.093750 1.03954090     1.2374369 1.0544644  1.0080323       1.4448881\n323      2.952381 0.91539320     0.8979555 1.0650762  1.0137396       1.2835961\n324      4.000000 1.11987540     1.1411388 1.2043876  1.0271052       1.0377490\n325      3.750000 1.41153260     1.8090681 1.1934162  1.0730867       0.7537783\n326      2.741935 1.17564420     1.2529535 1.2759142  1.0766335       1.2101719\n327      3.863636 0.86366886     0.8329709 1.0494995  1.0901403       1.0371873\n328      3.285714 0.94998010     1.0615526 0.9623598  1.0923286       1.1818737\n329      4.160000 0.63116350     0.8121526 0.5728554  1.0956314       0.8417668\n330      3.636364 1.13618180     1.3618170 1.3684763  1.1037128       1.5015144\n331      3.421053 0.71838840     0.7723284 0.8200699  1.1160708       1.5024347\n332      3.000000 1.41317940     1.5949482 1.4749368  1.1239030       1.1547005\n333      3.092593 1.04803190     1.1538863 1.1036508  1.1498066       1.2017051\n334      3.030303 1.29796560     1.4367224 1.3644976  1.1516090       1.1315048\n335      2.769231 1.34008700     1.3768926 1.3647344  1.1528949       1.4232502\n336      4.000000 1.30558240     1.4832397 1.3483997  1.2210279       0.8944272\n337      3.184211 1.00277390     1.0135446 1.0917505  1.2239198       1.3326516\n338      3.444444 1.15778930     1.3645163 1.1608700  1.2382784       1.0416176\n339      3.200000 0.92646280     1.1005049 0.9189366  0.3162278       1.5491933\n340      4.600000 0.24152295     0.3162278 0.4216370  0.3162278       0.6992059\n341      4.647059 0.41322106     0.5144958 0.3233808  0.5745131       0.6063391\n342      4.200000 0.33747430     0.4216370 0.3162278  0.6666667       0.6324555\n343      3.875000 1.54650140     1.5776213 1.7288403  0.6749486       1.3562027\n344      3.437500 1.44769930     1.5370426 1.4127397  0.7187953       1.0935416\n345      3.187500 0.57373047     0.6191392 0.5773503  0.8164966       0.9810708\n346      4.250000 1.64654520     1.7126977 1.6465452  0.8232726       0.8864053\n347      3.000000 1.48644670     1.5886502 1.4242793  0.9411239       1.4638501\n348      3.034483 0.96042377     1.2614012 0.9001916  0.9813532       1.0850529\n349      2.625000 0.68630000     0.7172800 0.8297000  1.0072200       1.0959400\n350      3.363636 1.30905940     1.4893562 1.2210279  1.0357255       0.6741999\n351      3.636364 0.80750000     0.9625004 0.9714540  1.0568269       1.3988245\n352      2.966102 1.37152520     1.4525821 1.4169220  1.0669052       1.3641481\n353      3.428571 0.80622580     1.0757057 0.7464200  1.0712698       1.0281745\n354      2.825000 1.14243270     1.2315400 1.2074394  1.0734965       1.0594508\n355      2.000000 1.43745890     1.5050420 1.4034589  1.1381804       1.4142136\n356      2.727273 1.67738970     1.7002674 1.7529196  1.1677484       1.4206273\n357      2.823529 1.14777600     1.2363048 1.2610817  1.2119016       1.1407225\n358      2.906977 0.97585590     1.0081791 1.0534049  1.2176242       1.1508579\n359      2.200000 1.02198060     1.1005049 1.1972190  1.2292726       1.1352924\n360      2.562500 1.18241190     1.3593477 1.1726039  1.2740441       1.3149778\n361      2.555556 1.32958880     1.4935760 1.2787993  1.3397283       1.4232502\n362      3.411765 1.54842470     1.5537972 1.6917165  1.4309504       1.0036697\n363      2.533333 1.28822500     1.3345233 1.3557637  1.4474937       1.1254629\n364      2.700000 1.20761480     1.4757296 1.0593499  1.5634719       0.9486833\n365      2.545455 1.37510340     1.5782614 1.3003496  1.6180797       1.6348478\n366      2.272727 0.91701096     0.9244163 1.0357255  1.6292776       1.4206273\n\n\n\nselected_data &lt;- Rateprof[, c(\"quality\", \"helpfulness\", \"clarity\", \"easiness\", \"raterInterest\")]\npairs(selected_data)\n\n\n\n\nAccording to the scatter plot matrix of average professor evaluations for the themes of quality, clarity, helpfulness, easiness, and rater interest, the variables quality, clarity, and helpfulness appear to have substantial positive connections. Helpfulness, clarity, and quality appear to have a significantly less positive link with variable ease. Rater interest does not appear to be related to any of the other factors. As a result, we may conclude that Quality, helpfulness, and clarity have the most obvious linear links with one another, although Easiness and raterInterest do not appear to have linear ties with the other factors."
  },
  {
    "objectID": "posts/HW3_SaisrinivasAmbatipudi.html#question-5",
    "href": "posts/HW3_SaisrinivasAmbatipudi.html#question-5",
    "title": "603Homework_Saisrinivas_Ambatipudi",
    "section": "Question 5:",
    "text": "Question 5:\nFor the student.survey data file in the smss package, conduct regression analyses relating (by convention, y denotes the outcome variable, x denotes the explanatory variable) (i) y = political ideology and x = religiosity, (ii) y = high school GPA and x = hours of TV watching. (You can use ?student.survey in the R console, after loading the package, to see what each variable means.) (a) Graphically portray how the explanatory variable relates to the outcome variable in each of the two cases (b) Summarize and interpret results of inferential analyses."
  },
  {
    "objectID": "posts/HW3_SaisrinivasAmbatipudi.html#answer-4",
    "href": "posts/HW3_SaisrinivasAmbatipudi.html#answer-4",
    "title": "603Homework_Saisrinivas_Ambatipudi",
    "section": "Answer:",
    "text": "Answer:\n\ndata(student.survey)\nstudent.survey\n\n   subj ge ag  hi  co   dh    dr   tv sp ne ah    ve pa                    pi\n1     1  m 32 2.2 3.5    0  5.00  3.0  5  0  0 FALSE  r          conservative\n2     2  f 23 2.1 3.5 1200  0.30 15.0  7  5  6 FALSE  d               liberal\n3     3  f 27 3.3 3.0 1300  1.50  0.0  4  3  0 FALSE  d               liberal\n4     4  f 35 3.5 3.2 1500  8.00  5.0  5  6  3 FALSE  i              moderate\n5     5  m 23 3.1 3.5 1600 10.00  6.0  6  3  0 FALSE  i          very liberal\n6     6  m 39 3.5 3.5  350  3.00  4.0  5  7  0 FALSE  d               liberal\n7     7  m 24 3.6 3.7    0  0.20  5.0 12  4  2 FALSE  i               liberal\n8     8  f 31 3.0 3.0 5000  1.50  5.0  3  3  1 FALSE  i               liberal\n9     9  m 34 3.0 3.0 5000  2.00  7.0  5  3  0 FALSE  i          very liberal\n10   10  m 28 4.0 3.1  900  2.00  1.0  1  2  1 FALSE  i      slightly liberal\n11   11  m 23 2.3 2.6  253  1.50 10.0 15  1  1 FALSE  r slightly conservative\n12   12  f 27 3.5 3.6  190  3.00 14.0  3  7  0 FALSE  d               liberal\n13   13  m 36 3.3 3.5  245  1.50  6.0 15 12  5 FALSE  d          very liberal\n14   14  m 28 3.2 3.2  500  6.00  3.0 10  1  2 FALSE  i              moderate\n15   15  f 28 3.0 3.5 3500  1.00  4.0  3  1  0 FALSE  d          very liberal\n16   16  f 25 3.8 3.3  210 10.00  7.0  6  1  0 FALSE  i               liberal\n17   17  f 41 4.0 3.0 1000 15.00  6.0  7  3 10 FALSE  i      slightly liberal\n18   18  m 50 3.8 3.8    0  3.00  5.0  9  6 10 FALSE  d               liberal\n19   19  m 71 4.0 3.5 5000  3.00  6.0 12  2  2 FALSE  i               liberal\n20   20  f 28 3.0 3.8  120  1.00 25.0  0  0  2 FALSE  d          very liberal\n21   21  f 26 3.7 3.7 8000  8.00  4.0  4  4  1 FALSE  i              moderate\n22   22  f 27 4.0 3.7    2  2.50  4.0  2  7  0 FALSE  i               liberal\n23   23  m 31 2.7 3.5 1700  5.00  7.0  7  2  0 FALSE  r     very conservative\n24   24  f 23 3.7 3.7    2  2.00  7.0  4  2  0 FALSE  i              moderate\n25   25  m 23 3.2 3.8  450  4.00  0.0  7  7  3 FALSE  i          very liberal\n26   26  f 44 3.0 3.0    0  2.00  2.0  3  2  3 FALSE  i      slightly liberal\n27   27  m 26 3.7 3.0 1000  3.00  8.0  2  7  0 FALSE  d               liberal\n28   28  f 31 3.7 3.8  850 10.00 10.0  3  7  0 FALSE  r slightly conservative\n29   29  m 24 3.3 3.1  420  2.00 10.0  6  5  0 FALSE  d              moderate\n30   30  f 26 3.3 3.3 1200  0.75 10.0  0  3  0 FALSE  r               liberal\n31   31  m 26 3.3 3.5 1000  1.50  0.0  3  3  3 FALSE  d               liberal\n32   32  f 32 3.5 3.9  150 12.00 10.0  2  0  0 FALSE  d               liberal\n33   33  m 26 3.4 3.4 2000  1.50  2.0  7 14  0 FALSE  d               liberal\n34   34  f 22 3.2 2.8  316  2.00 10.0  3  5  2 FALSE  i               liberal\n35   35  f 24 3.5 3.9  900  1.75  8.0  0  0  1 FALSE  d          very liberal\n36   36  m 24 3.6 3.3  250  2.00  4.0  6  3  1 FALSE  r slightly conservative\n37   37  m 23 3.8 3.7  180  0.50 10.0  5  7  0 FALSE  i               liberal\n38   38  m 33 3.4 3.4 6000  1.50  8.0  5  6  2 FALSE  i               liberal\n39   39  m 23 2.8 3.2  950  2.00 37.0 10  5  0 FALSE  r slightly conservative\n40   40  m 31 3.8 3.5 1100  0.75  0.5  3  5  2 FALSE  r          conservative\n41   41  m 26 3.4 3.4 1300  1.20  0.0  8  2  0 FALSE  i               liberal\n42   42  m 28 2.0 3.0  360  0.25 10.0  8  3  0 FALSE  d      slightly liberal\n43   43  f 24 3.8 3.9 1800  2.00  2.0  5  4  1 FALSE  r          conservative\n44   44  m 23 3.0 3.6  900 15.00 12.0  0  5  0 FALSE  r slightly conservative\n45   45  f 25 3.0 4.0 5000  5.00  1.5  0  4  0 FALSE  i              moderate\n46   46  f 24 3.0 3.5  300  1.00 10.0  5  5  0 FALSE  d               liberal\n47   47  f 27 3.0 3.8 2000 20.00 28.0  7 14  2 FALSE  r      slightly liberal\n48   48  m 24 3.3 3.8  630  1.30  2.0  3  5  0 FALSE  r     very conservative\n49   49  f 26 3.8 4.0 1200  1.00  0.0  4  3  1 FALSE  d               liberal\n50   50  f 27 3.0 4.0  580  2.00  5.0 15  1  2 FALSE  d          very liberal\n51   51  m 32 3.0 3.0 2000  5.00  5.0  5  2  1 FALSE  r slightly conservative\n52   52  f 41 4.0 4.0    0  8.00  8.0  4  2  2 FALSE  r              moderate\n53   53  f 29 3.0 3.9  300  3.70  2.0  5  1 11 FALSE  d               liberal\n54   54  f 50 3.5 3.8    6  6.00  7.0  3  7  0 FALSE  d               liberal\n55   55  f 22 3.4 3.7   80  7.00 10.0  1  2  2 FALSE  i               liberal\n56   56  f 23 3.6 3.2  375  1.50  5.0 10  5  0 FALSE  r          conservative\n57   57  m 26 3.5 3.6 2000  0.30 16.0  8  3  0 FALSE  d              moderate\n58   58  m 30 3.0 3.0    1  1.10  1.0  4  3  0 FALSE  i      slightly liberal\n59   59  f 23 3.0 3.0  112  0.50 15.0  3  3  0 FALSE  i              moderate\n60   60  f 22 3.4 3.0  650  4.00  8.0 16  7  1 FALSE  i              moderate\n             re    ab    aa    ld\n1    most weeks FALSE FALSE FALSE\n2  occasionally FALSE FALSE    NA\n3    most weeks FALSE FALSE    NA\n4  occasionally FALSE FALSE FALSE\n5         never FALSE FALSE FALSE\n6  occasionally FALSE FALSE    NA\n7  occasionally FALSE FALSE FALSE\n8  occasionally FALSE FALSE FALSE\n9  occasionally FALSE FALSE    NA\n10        never FALSE FALSE FALSE\n11 occasionally FALSE FALSE FALSE\n12 occasionally FALSE FALSE    NA\n13 occasionally FALSE FALSE FALSE\n14 occasionally FALSE FALSE FALSE\n15        never FALSE FALSE FALSE\n16   every week FALSE FALSE FALSE\n17   every week FALSE    NA FALSE\n18        never FALSE FALSE FALSE\n19        never FALSE FALSE FALSE\n20 occasionally FALSE FALSE FALSE\n21 occasionally FALSE FALSE FALSE\n22 occasionally FALSE FALSE FALSE\n23   every week FALSE FALSE FALSE\n24        never FALSE FALSE FALSE\n25        never FALSE FALSE FALSE\n26   most weeks FALSE FALSE FALSE\n27 occasionally FALSE FALSE    NA\n28   most weeks FALSE FALSE FALSE\n29 occasionally FALSE FALSE    NA\n30 occasionally FALSE FALSE    NA\n31 occasionally FALSE FALSE FALSE\n32 occasionally FALSE FALSE FALSE\n33        never FALSE FALSE FALSE\n34 occasionally FALSE FALSE    NA\n35 occasionally FALSE FALSE    NA\n36   every week FALSE FALSE FALSE\n37        never FALSE FALSE    NA\n38        never FALSE FALSE FALSE\n39   most weeks FALSE FALSE FALSE\n40   most weeks FALSE FALSE    NA\n41 occasionally FALSE FALSE FALSE\n42        never FALSE FALSE    NA\n43   every week FALSE FALSE FALSE\n44        never FALSE FALSE FALSE\n45 occasionally FALSE FALSE FALSE\n46        never FALSE FALSE FALSE\n47 occasionally FALSE FALSE FALSE\n48   every week FALSE FALSE FALSE\n49        never FALSE FALSE FALSE\n50 occasionally FALSE FALSE FALSE\n51   every week FALSE FALSE FALSE\n52 occasionally FALSE FALSE FALSE\n53 occasionally FALSE FALSE FALSE\n54 occasionally FALSE FALSE    NA\n55        never FALSE FALSE    NA\n56   every week FALSE FALSE FALSE\n57 occasionally FALSE FALSE    NA\n58   every week FALSE FALSE FALSE\n59   most weeks FALSE FALSE FALSE\n60 occasionally FALSE FALSE FALSE"
  },
  {
    "objectID": "posts/HW3_SaisrinivasAmbatipudi.html#i",
    "href": "posts/HW3_SaisrinivasAmbatipudi.html#i",
    "title": "603Homework_Saisrinivas_Ambatipudi",
    "section": "(i)",
    "text": "(i)\n\n## plot(student.survey$re, student.survey$pi, main=\"Political Ideology vs Religiosity\", xlab=\"Religiosity\", ylab=\"Political Ideology\")\n\nstudent.survey %&gt;%\n  select(c(pi, re)) %&gt;%\n  ggplot() + \n  geom_bar(aes(x = re, fill = as.factor(pi)), position = \"dodge\") +\n  xlab(\"Religiosity\") +\n  ylab(\"Political Ideology\") +\n  scale_fill_manual(values = c(\"#FF0000\", \"#FF8000\", \"#FFFF00\", \"#80FF00\", \"#00FF00\", \"#00FF80\", \"#00FFFF\", \"#0080FF\", \"#0000FF\", \"#8000FF\", \"#FF00FF\", \"#FF0080\")) +\n  labs(fill = \"Political Ideology\")\n\n\n\n\nReligiosity and political ideology seem to have a positive relationship."
  },
  {
    "objectID": "posts/HW3_SaisrinivasAmbatipudi.html#ii",
    "href": "posts/HW3_SaisrinivasAmbatipudi.html#ii",
    "title": "603Homework_Saisrinivas_Ambatipudi",
    "section": "(ii)",
    "text": "(ii)\n\nstudent.survey %&gt;%\n  select(tv, hi) %&gt;%\n  ggplot(aes(x = tv, y = hi)) +\n  geom_point(color = \"steelblue\", alpha = 0.6) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\", linetype = \"solid\", size = 1) +\n  labs(title = \"High School GPA vs Hours of TV Watching\",\n       x = \"Hours of TV Watching\",\n       y = \"High School GPA\") +\n  theme_minimal() +\n  theme(panel.grid.major = element_line(color = \"gray\", linetype = \"dotted\"))\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nHigh school GPA and TV-watching appear to have a negative relationship."
  },
  {
    "objectID": "posts/HW3_SaisrinivasAmbatipudi.html#a-2",
    "href": "posts/HW3_SaisrinivasAmbatipudi.html#a-2",
    "title": "603Homework_Saisrinivas_Ambatipudi",
    "section": "(a)",
    "text": "(a)\n\nmodel1 &lt;- lm(as.numeric(pi) ~ as.numeric(re), data=student.survey)\nsummary(model1)\n\n\nCall:\nlm(formula = as.numeric(pi) ~ as.numeric(re), data = student.survey)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.81243 -0.87160  0.09882  1.12840  3.09882 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      0.9308     0.4252   2.189   0.0327 *  \nas.numeric(re)   0.9704     0.1792   5.416 1.22e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.345 on 58 degrees of freedom\nMultiple R-squared:  0.3359,    Adjusted R-squared:  0.3244 \nF-statistic: 29.34 on 1 and 58 DF,  p-value: 1.221e-06\n\n\nThere is a statistically significant link between religion and political ideology at a significance level of 0.01. The association is modest and positive, implying that as weekly church attendance grows, political ideology shifts toward the right."
  },
  {
    "objectID": "posts/HW3_SaisrinivasAmbatipudi.html#b-2",
    "href": "posts/HW3_SaisrinivasAmbatipudi.html#b-2",
    "title": "603Homework_Saisrinivas_Ambatipudi",
    "section": "(b)",
    "text": "(b)\n\nmodel2 &lt;- lm(as.numeric(hi) ~ as.numeric(tv), data=student.survey)\nsummary(model2)\n\n\nCall:\nlm(formula = as.numeric(hi) ~ as.numeric(tv), data = student.survey)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.2583 -0.2456  0.0417  0.3368  0.7051 \n\nCoefficients:\n                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)     3.441353   0.085345  40.323   &lt;2e-16 ***\nas.numeric(tv) -0.018305   0.008658  -2.114   0.0388 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4467 on 58 degrees of freedom\nMultiple R-squared:  0.07156,   Adjusted R-squared:  0.05555 \nF-statistic: 4.471 on 1 and 58 DF,  p-value: 0.03879\n\n\nThere is a negative relationship between weekly TV viewing hours and high school GPA, with a slope of -0.018, indicating that as weekly TV viewing hours increase, a student’s GPA tends to fall. At a significance threshold of 0.05, there is a statistically significant association between weekly television viewing and GPA. The R-squared value, however, is close to zero, indicating that the regression model does not make a strong forecast for the observed variables. This is not surprising given that there does not appear to be a linear trend in the data when looking at the scatter plot with hours of television viewed and GPA."
  },
  {
    "objectID": "posts/HW1_OllieMurphy.html",
    "href": "posts/HW1_OllieMurphy.html",
    "title": "Homework 1",
    "section": "",
    "text": "Question 1: Use the LungCapData to answer the following questions.\nRequire packages and import data\n\n\nCode\nlibrary(here)\n\n\nhere() starts at C:/Users/Ollie/OneDrive - University of Massachusetts/Spring_2023/Quant_Analysis/603_Spring_2023\n\n\nCode\nlibrary(readxl)\n\n\nWarning: package 'readxl' was built under R version 4.1.3\n\n\nCode\nlibrary(dplyr)\n\n\nWarning: package 'dplyr' was built under R version 4.1.3\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nCode\ndf &lt;- read_excel(\"_data/LungCapData.xls\")\n\n\n\na) What does the distributuion of LungCap look like?\n\n\nCode\nhist(df$LungCap, main = \"Histogram of Lung Capacity\", xlab = \"Lung Capacity\")\n\n\n\n\n\nBased on the histogram, lung capacity appears to be distributed normally with a mean around 7 to 8, a minimum of 0, and a maximum of 15\n\n\nb) Compare the probability distribution of the LungCap with responst to Males and Females.\n\n\nCode\nboxplot(LungCap ~ Gender, data = df)\n\n\n\n\n\nThe distributions are of a similar size, but male capacity is higher in mean as well as quartiles and min/max.\n\n\nc) Compare the mean lung capacities for smokers and non-smokers. Does it make sense?\n\n\nCode\ndf %&gt;%\n  group_by(Smoke) %&gt;%\n  summarise(LungCap = (mean(LungCap,na.rm = TRUE)))\n\n\n# A tibble: 2 x 2\n  Smoke LungCap\n  &lt;chr&gt;   &lt;dbl&gt;\n1 no       7.77\n2 yes      8.65\n\n\nThe mean lung capacities likely don’t make sense, as you might expect for smoking to diminish lung capacity. However, it could be that people with better lung capacity are better at smoking and therefore do it more, or the inclusion of children, who are less likely to smoke and also have smaller lung capacities.\n\n\nd) Examine the relationship between Smoking and Lung Capacity within age groups: “Less than or equal to 13”, “14 to 15”, “16 to 17”, and “greater than or equal to 18”.\n\n\nCode\nsmoke_df = df%&gt;%\n  mutate(\n   AgeRange = case_when(\n    Age &lt; 14 ~ \"13 and Under\",\n    Age == 14 ~ \"14 to 15\",\n    Age == 15 ~ \"14 to 15\",\n    Age == 16 ~ \"16 to 17\",\n    Age == 17 ~ \"16 to 17\",\n    Age &gt; 17 ~ \"18 and Over\"\n  )) %&gt;%\n  filter(Smoke == \"yes\")%&gt;%\n  group_by(AgeRange)%&gt;%\n    summarize(mean(LungCap, na.rm = TRUE))\n\nnosmoke_df  = df%&gt;%\n  mutate(\n   AgeRange = case_when(\n    Age &lt; 14 ~ \"13 and Under\",\n    Age == 14 ~ \"14 to 15\",\n    Age == 15 ~ \"14 to 15\",\n    Age == 16 ~ \"16 to 17\",\n    Age == 17 ~ \"16 to 17\",\n    Age &gt; 17 ~ \"18 and Over\"\n  )) %&gt;%\n  filter(Smoke == \"no\")%&gt;%\n  group_by(AgeRange)%&gt;%\n    summarize(mean(LungCap, na.rm = TRUE))\n\nsmokeByAge = left_join(smoke_df, nosmoke_df, by = (\"AgeRange\"))\nnames(smokeByAge) = c(\"AgeRange\", \"Smoker\", \"NonSmoker\")\n\nsmokeByAge\n\n\n# A tibble: 4 x 3\n  AgeRange     Smoker NonSmoker\n  &lt;chr&gt;         &lt;dbl&gt;     &lt;dbl&gt;\n1 13 and Under   7.20      6.36\n2 14 to 15       8.39      9.14\n3 16 to 17       9.38     10.5 \n4 18 and Over   10.5      11.1 \n\n\n\n\ne) Compare the lung capacities for smokers and non-smokers within each age group. Is your answer different from the one in part c? What could possibly be going on here?\nWhile the youngest age group follows the same trend observed in part c (smokers with higher lung capacity than non-smokers), the inverse is true for all older groups. I suspect that, as I mentioned above, the data is skewed by virture of the fact that older groups are more likely to smoke as well as have a larger lung capacity. When compared within their age range (therefore at similar states of development), the results reflect that smoking likely diminishes lung capacity.\n\n\n\nQuestion 2: Let X = number of prior convictions for prisoners at a state prison at which there are 810 prisoners\n\n\nCode\nconvictionsfreq = data.frame(convictions = c(0,1,2,3,4), frequency = c(128,434,160,64,24))\nconvictionsfreq\n\n\n  convictions frequency\n1           0       128\n2           1       434\n3           2       160\n4           3        64\n5           4        24\n\n\n\na) What is the probability that a randomly selected inmate has exactly 2 prior convictions?\n\n\nCode\n160/(128+434+160+64+24)\n\n\n[1] 0.1975309\n\n\n\n\nb) What is the probability that a randomly selected inmate has fewer than 2 prior convictions?\n\n\nCode\n(128+434)/(128+434+160+64+24)\n\n\n[1] 0.6938272\n\n\n\n\nc) What is the probability that a randomly selected inmate has 2 or fewer prior convictions?\n\n\nCode\n(128+434+160)/(128+434+160+64+24)\n\n\n[1] 0.891358\n\n\n\n\nd) What is the probability that a randomly selected inmate has more than 2 prior convictions?\n\n\nCode\n(64+24)/(128+434+160+64+24)\n\n\n[1] 0.108642\n\n\n\n\ne) What is the expected value1 for the number of prior convictions?\n\n\nCode\n(0*(128/(128+434+160+64+24)))+(1*(434/(128+434+160+64+24)))+(2*(160/(128+434+160+64+24)))+(3*(64/(128+434+160+64+24)))+(4*(24/(128+434+160+64+24)))\n\n\n[1] 1.28642\n\n\n\n\nf) Calculate the variance and the standard deviation for the Prior Convictions.\n\n\nCode\nvar(convictionsfreq$convictions)\n\n\n[1] 2.5\n\n\nCode\nsd(convictionsfreq$convictions)\n\n\n[1] 1.581139"
  },
  {
    "objectID": "posts/dacss603_final_LauraCollazo.html",
    "href": "posts/dacss603_final_LauraCollazo.html",
    "title": "Check-in 2: DACSS 603 Final Project",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(psych)\nlibrary(lattice)\nlibrary(FSA)\nlibrary(kableExtra)\n\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/dacss603_final_LauraCollazo.html#dataset",
    "href": "posts/dacss603_final_LauraCollazo.html#dataset",
    "title": "Check-in 2: DACSS 603 Final Project",
    "section": "Dataset",
    "text": "Dataset\nThe data for this study was collected during the last month of the Fall 2021 semester at a large public university in the mid-west of the United States (Jeng et al., 2023). Students were recruited through voluntary response sampling in an online introduction to statistics course and received extra credit for participating in the study. The instructor of this course was not a member of the research team. A total of 240 students completed the survey and responses from 17 students were removed due to either missing demographics, the same rating for every example, or identical response made to more than 50% of open-ended questions. In total, responses from 223 students are included.\nThe full dataset can be found here.\n\nVariables\nTo answer the research question for the present study, the following variables have been selected. Details on these variables are included in following sections.\n\nrace\ngender\nyear\nbel_1\nbel_2\nbel_2_r\nbel_3\nbel_4\nbel_4_r\nbel_5\nbel_5_r\nbel_6\n\nUsing the above variables, additional variables were created to aid in analysis. The variable bel_sum was created to sum an overall belonging score for each participant. In addition, the variable urm was created to indicate if a student belongs to an underrepresented minority group or not.\n\nRace/Ethnicity\nThe response options for race include “Asian or Asian American” (1), “Black or African American” (2), “Hispanic or Latino” (3), “White” (4), and “Other” (5). Students who reported their race as “Other” (n = 10) have been removed from this analysis as it cannot be determined if they belong to an underrepresented minority group or not.\n\n\nGender\nThe responses for gender include “Man” (2) and “Woman or non-binary” (1). The researchers who collected this data explain that their were so few students who identified as non-binary that the sample was too small for a separate analysis (Jeng et al., 2023). What they did do was run all analysis twice, once with non-binary students excluded and once with this group combine with respondents who identified as women. They found their findings to be the same in both instances and therefore choose to combine these two groups. For the purpose of this study, this variable will be coded as “Male” and “Female.”\n\n\nYear\nThe variable year includes four response options: 1, 2, 3 or 4. These correspond to being a Freshman, Sophomore, Junior, or Senior, and values have been coded as such.\n\n\nBelonging\nA total of 6 Likert questions were asked to measure social belonging which were adapted from Goodenow’s (1993) Psychological Sense of School Membership (PSSM) scale. Five response options (“Not at all true”, “Slightly true”, “Moderately true”, “Mostly true”, and “Completely true”) were provided to students to answer the below questions. Questions 2, 4, and 5 were reversed scored which is why 2 variables exist for each of these questions.\n\nI feel like a real part of this class (bel_1)\nSometimes I feel as if I don’t belong in this class(bel_2/bel_2_r)\nI am included in lots of activities in this class(bel_3)\nI feel very different from most other students in this class (bel_4/ bel_4_r)\nI wish I were in a different class (bel_5/bel_5_r)\nI feel proud of belonging to this class (bel_6)\n\n\n\nExplore Data\n\nPrepare Data\n\n\nCode\n# read in data\n\nbelonging_data &lt;- read_csv(\"_data/belonging_survey_2022-07-08.csv\", show_col_types = FALSE)\n\n# tidy data\n\ndata &lt;- belonging_data %&gt;%\n  \n# filter to include only 1 set of student belonging responses\n  \n  filter(example_num == 1) %&gt;% \n  \n# filter to include remove the racial category \"Other\"\n  \n  filter(race != 5) %&gt;% \n  \n# select needed columns\n  \n  select(race, gender, year, bel_1, bel_2, bel_2_r, bel_3, bel_4, bel_4_r, bel_5, bel_5_r, bel_6) %&gt;%\n\n# create variable bel_sum which sums all belonging responses\n  \n  mutate(bel_sum = rowSums(across(c(bel_1, bel_2_r, bel_3, bel_4_r, bel_5_r, bel_6)))) %&gt;% \n\n# create variable is_urm which turns race into a binary variable\n\n  mutate(urm = case_when(race == 1 ~ \"not underrepresented\",\n                         race == 4 ~ \"not underrepresented\",\n                         race == 2 ~ \"underrepresented\",\n                         race == 3 ~ \"underrepresented\",))\n\n### Create new variables with Likert scores as an ordered factor\n\ndata$f_bel_1 = factor(data$bel_1,\n                       ordered = TRUE,\n                       levels = c(\"1\", \"2\", \"3\", \"4\", \"5\")\n                       )\n\ndata$f_bel_2 = factor(data$bel_2,\n                       ordered = TRUE,\n                       levels = c(\"1\", \"2\", \"3\", \"4\", \"5\")\n                       )\n\ndata$f_bel_3 = factor(data$bel_3,\n                       ordered = TRUE,\n                       levels = c(\"1\", \"2\", \"3\", \"4\", \"5\")\n                       )\n\ndata$f_bel_4 = factor(data$bel_4,\n                       ordered = TRUE,\n                       levels = c(\"1\", \"2\", \"3\", \"4\", \"5\")\n                       )\n\ndata$f_bel_5 = factor(data$bel_5,\n                       ordered = TRUE,\n                       levels = c(\"1\", \"2\", \"3\", \"4\", \"5\")\n                       )\n\ndata$f_bel_6 = factor(data$bel_6,\n                       ordered = TRUE,\n                       levels = c(\"1\", \"2\", \"3\", \"4\", \"5\")\n                       )\n\n# recode data\n  \ndata &lt;- data %&gt;% \n  mutate(gender = recode(gender,\n                            `1` = \"Female\",\n                            `2` = \"Male\")) %&gt;%\n  mutate(race = recode(race,\n                            `1` = \"Asian\",\n                            `2` = \"Black\",\n                            `3` = \"Hispanic\",\n                            `4` = \"White\")) %&gt;%\n  mutate(across(bel_1:bel_6, \n                ~ recode(.x, `1` = \"Not at all true\",\n                             `2` = \"Slightly true\",\n                             `3` = \"Moderately true\",\n                             `4` = \"Mostly true\",\n                             `5` = \"Completely true\"))) %&gt;% \n  mutate(year = recode(year,\n                        `1` = \"Freshman\",\n                        `2` = \"Sophmore\",\n                        `3` = \"Junior\",\n                        `4` = \"Senior\")) \n\n\n\n\nExamine Data\n\n\nCode\n# examine data\n\ndescribe_data &lt;- describe(x=data) %&gt;% \n  select(c(vars, n, mean, sd, median, min, max, range))\n\nkable(describe_data) %&gt;% \n  kable_styling(\"striped\")\n\n\n\n\n\n\nvars\nn\nmean\nsd\nmedian\nmin\nmax\nrange\n\n\n\n\nrace*\n1\n213\n2.816901\n1.2846319\n3\n1\n4\n3\n\n\ngender*\n2\n213\n1.244131\n0.4305830\n1\n1\n2\n1\n\n\nyear*\n3\n213\n2.220657\n1.3399330\n2\n1\n4\n3\n\n\nbel_1*\n4\n213\n2.488263\n1.2155065\n2\n1\n5\n4\n\n\nbel_2*\n5\n213\n3.835681\n0.8334086\n4\n1\n5\n4\n\n\nbel_2_r*\n6\n213\n1.652582\n1.1580137\n1\n1\n5\n4\n\n\nbel_3*\n7\n213\n3.122066\n1.3612326\n3\n1\n5\n4\n\n\nbel_4*\n8\n213\n3.741784\n1.0568691\n4\n1\n5\n4\n\n\nbel_4_r*\n9\n213\n1.967136\n1.2029248\n1\n1\n5\n4\n\n\nbel_5*\n10\n213\n3.981221\n0.7394697\n4\n1\n5\n4\n\n\nbel_5_r*\n11\n213\n1.469484\n0.9292207\n1\n1\n5\n4\n\n\nbel_6*\n12\n213\n2.225352\n1.1958716\n2\n1\n5\n4\n\n\nbel_sum\n13\n213\n23.610329\n4.4935264\n25\n6\n30\n24\n\n\nurm*\n14\n213\n1.253521\n0.4360514\n1\n1\n2\n1\n\n\nf_bel_1*\n15\n213\n3.680751\n1.0952106\n4\n1\n5\n4\n\n\nf_bel_2*\n16\n213\n1.568075\n1.0100040\n1\n1\n5\n4\n\n\nf_bel_3*\n17\n213\n2.920188\n1.2879031\n3\n1\n5\n4\n\n\nf_bel_4*\n18\n213\n1.868545\n1.0602582\n1\n1\n5\n4\n\n\nf_bel_5*\n19\n213\n1.370892\n0.8290126\n1\n1\n5\n4\n\n\nf_bel_6*\n20\n213\n3.816901\n1.1773394\n4\n1\n5\n4\n\n\n\n\n\n\n\nCode\nstr(data)\n\n\ntibble [213 × 20] (S3: tbl_df/tbl/data.frame)\n $ race   : chr [1:213] \"White\" \"White\" \"Asian\" \"White\" ...\n $ gender : chr [1:213] \"Female\" \"Male\" \"Female\" \"Female\" ...\n $ year   : chr [1:213] \"Freshman\" \"Freshman\" \"Senior\" \"Freshman\" ...\n $ bel_1  : chr [1:213] \"Mostly true\" \"Slightly true\" \"Completely true\" \"Mostly true\" ...\n $ bel_2  : chr [1:213] \"Moderately true\" \"Slightly true\" \"Not at all true\" \"Not at all true\" ...\n $ bel_2_r: chr [1:213] \"Moderately true\" \"Mostly true\" \"Completely true\" \"Completely true\" ...\n $ bel_3  : chr [1:213] \"Mostly true\" \"Mostly true\" \"Mostly true\" \"Mostly true\" ...\n $ bel_4  : chr [1:213] \"Slightly true\" \"Moderately true\" \"Mostly true\" \"Slightly true\" ...\n $ bel_4_r: chr [1:213] \"Mostly true\" \"Moderately true\" \"Slightly true\" \"Mostly true\" ...\n $ bel_5  : chr [1:213] \"Slightly true\" \"Slightly true\" \"Not at all true\" \"Not at all true\" ...\n $ bel_5_r: chr [1:213] \"Mostly true\" \"Mostly true\" \"Completely true\" \"Completely true\" ...\n $ bel_6  : chr [1:213] \"Mostly true\" \"Moderately true\" \"Completely true\" \"Completely true\" ...\n $ bel_sum: num [1:213] 23 20 26 27 28 17 23 25 29 25 ...\n $ urm    : chr [1:213] \"not underrepresented\" \"not underrepresented\" \"not underrepresented\" \"not underrepresented\" ...\n $ f_bel_1: Ord.factor w/ 5 levels \"1\"&lt;\"2\"&lt;\"3\"&lt;\"4\"&lt;..: 4 2 5 4 4 4 3 4 4 3 ...\n $ f_bel_2: Ord.factor w/ 5 levels \"1\"&lt;\"2\"&lt;\"3\"&lt;\"4\"&lt;..: 3 2 1 1 1 4 1 1 1 1 ...\n $ f_bel_3: Ord.factor w/ 5 levels \"1\"&lt;\"2\"&lt;\"3\"&lt;\"4\"&lt;..: 4 4 4 4 4 3 2 1 5 3 ...\n $ f_bel_4: Ord.factor w/ 5 levels \"1\"&lt;\"2\"&lt;\"3\"&lt;\"4\"&lt;..: 2 3 4 2 1 3 1 1 1 1 ...\n $ f_bel_5: Ord.factor w/ 5 levels \"1\"&lt;\"2\"&lt;\"3\"&lt;\"4\"&lt;..: 2 2 1 1 1 2 1 1 1 1 ...\n $ f_bel_6: Ord.factor w/ 5 levels \"1\"&lt;\"2\"&lt;\"3\"&lt;\"4\"&lt;..: 4 3 5 5 5 1 3 5 5 4 ...\n\n\n\n\nExamine Variables\n\n\nCode\n# create xtabs and bar plot to visualize variables\n\n# race\n\nxt_race &lt;- xtabs(~race, data = data)\n\nkable(xt_race) %&gt;% \n  kable_styling(\"striped\")\n\n\n\n\n\nrace\nFreq\n\n\n\n\nAsian\n59\n\n\nBlack\n21\n\n\nHispanic\n33\n\n\nWhite\n100\n\n\n\n\n\n\n\nCode\nbarplot(xt_race, \n        xlab = \"Race\",\n        ylab = \"Frequency\")\n\n\n\n\n\nCode\n# urm\n\nxt_urm &lt;- xtabs(~urm, data = data)\n\nkable(xt_urm) %&gt;% \n  kable_styling(\"striped\")\n\n\n\n\n\nurm\nFreq\n\n\n\n\nnot underrepresented\n159\n\n\nunderrepresented\n54\n\n\n\n\n\n\n\nCode\nbarplot(xt_urm, \n        xlab = \"URM\",\n        ylab = \"Frequency\")\n\n\n\n\n\nCode\n# gender\n\nxt_gender &lt;- xtabs(~gender, data = data)\n\nkable(xt_gender) %&gt;% \n  kable_styling(\"striped\")\n\n\n\n\n\ngender\nFreq\n\n\n\n\nFemale\n161\n\n\nMale\n52\n\n\n\n\n\n\n\nCode\nbarplot(xt_gender, \n        xlab = \"Gender\",\n        ylab = \"Frequency\")\n\n\n\n\n\nCode\n# bel_sum\n\nxt_sum &lt;- xtabs(~bel_sum, data = data)\n\nkable(xt_sum) %&gt;% \n  kable_styling(\"striped\")\n\n\n\n\n\nbel_sum\nFreq\n\n\n\n\n6\n1\n\n\n8\n1\n\n\n10\n1\n\n\n11\n2\n\n\n12\n1\n\n\n13\n2\n\n\n14\n2\n\n\n15\n3\n\n\n16\n1\n\n\n17\n7\n\n\n18\n9\n\n\n19\n6\n\n\n20\n7\n\n\n21\n10\n\n\n22\n14\n\n\n23\n19\n\n\n24\n16\n\n\n25\n31\n\n\n26\n21\n\n\n27\n25\n\n\n28\n11\n\n\n29\n12\n\n\n30\n11\n\n\n\n\n\n\n\nCode\nbarplot(xt_sum, \n        xlab = \"Belonging\",\n        ylab = \"Frequency\")\n\n\n\n\n\nCode\n# f_bel_1\n\nxt_1 &lt;- xtabs(~f_bel_1, data = data)\n\nkable(xt_1) %&gt;% \n  kable_styling(\"striped\")\n\n\n\n\n\nf_bel_1\nFreq\n\n\n\n\n1\n9\n\n\n2\n22\n\n\n3\n52\n\n\n4\n75\n\n\n5\n55\n\n\n\n\n\n\n\nCode\nbarplot(xt_1, \n        xlab = \"I feel like a real part of this class\",\n        ylab = \"Frequency\")\n\n\n\n\n\nCode\n# f_bel_2\n\nxt_2 &lt;- xtabs(~f_bel_2, data = data)\n\nkable(xt_2) %&gt;% \n  kable_styling(\"striped\")\n\n\n\n\n\nf_bel_2\nFreq\n\n\n\n\n1\n149\n\n\n2\n28\n\n\n3\n19\n\n\n4\n13\n\n\n5\n4\n\n\n\n\n\n\n\nCode\nbarplot(xt_2, \n        xlab = \"Sometimes I feel as if I don’t belong in this class\",\n        ylab = \"Frequency\")\n\n\n\n\n\nCode\n# f_bel_3\n\nxt_3 &lt;- xtabs(~f_bel_3, data = data)\n\nkable(xt_3) %&gt;% \n  kable_styling(\"striped\")\n\n\n\n\n\nf_bel_3\nFreq\n\n\n\n\n1\n36\n\n\n2\n49\n\n\n3\n52\n\n\n4\n48\n\n\n5\n28\n\n\n\n\n\n\n\nCode\nbarplot(xt_3, \n        xlab = \"I am included in lots of activities in this class\",\n        ylab = \"Frequency\")\n\n\n\n\n\nCode\n# f_bel_4\n\nxt_4 &lt;- xtabs(~f_bel_4, data = data)\n\nkable(xt_4) %&gt;% \n  kable_styling(\"striped\")\n\n\n\n\n\nf_bel_4\nFreq\n\n\n\n\n1\n108\n\n\n2\n48\n\n\n3\n38\n\n\n4\n15\n\n\n5\n4\n\n\n\n\n\n\n\nCode\nbarplot(xt_4, \n        xlab = \"I feel very different from most other students in this class\",\n        ylab = \"Frequency\")\n\n\n\n\n\nCode\n# f_bel_5\n\nxt_5 &lt;- xtabs(~f_bel_5, data = data)\n\nkable(xt_5) %&gt;% \n  kable_styling(\"striped\")\n\n\n\n\n\nf_bel_5\nFreq\n\n\n\n\n1\n164\n\n\n2\n32\n\n\n3\n9\n\n\n4\n3\n\n\n5\n5\n\n\n\n\n\n\n\nCode\nbarplot(xt_5, \n        xlab = \"I wish I were in a different class\",\n        ylab = \"Frequency\")\n\n\n\n\n\nCode\n# f_bel_6\n\nxt_6 &lt;- xtabs(~f_bel_6, data = data)\n\nkable(xt_6) %&gt;% \n  kable_styling(\"striped\")\n\n\n\n\n\nf_bel_6\nFreq\n\n\n\n\n1\n13\n\n\n2\n14\n\n\n3\n50\n\n\n4\n58\n\n\n5\n78\n\n\n\n\n\n\n\nCode\nbarplot(xt_6, \n        xlab = \"I feel proud of belonging to this class\",\n        ylab = \"Frequency\")"
  },
  {
    "objectID": "posts/dacss603_final_LauraCollazo.html#anova",
    "href": "posts/dacss603_final_LauraCollazo.html#anova",
    "title": "Check-in 2: DACSS 603 Final Project",
    "section": "ANOVA",
    "text": "ANOVA\nThe first hypothesis test conducted is an ANOVA. This test has been selected to compare the means of URM and non-URM students.\n\n\nCode\nfit_sum &lt;- aov(bel_sum ~ urm, data = data)\n\nsummary(fit_sum)\n\n\n             Df Sum Sq Mean Sq F value Pr(&gt;F)  \nurm           1     72   72.23   3.621 0.0584 .\nResiduals   211   4208   19.95                 \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe results of this ANOVA reveal that Pr(&gt;F) is 0.0584. This is just slightly greater than p &lt; .05, therefore the effect size should also be examined before deciding if the null hypothesis should or shouldn’t be rejected. Eta squared has been selected to measure the effect size.\n\n\nCode\neffectsize::eta_squared(fit_sum)\n\n\nFor one-way between subjects designs, partial eta squared is equivalent\n  to eta squared. Returning eta squared.\n\n\n# Effect Size for ANOVA\n\nParameter | Eta2 |       95% CI\n-------------------------------\nurm       | 0.02 | [0.00, 1.00]\n\n- One-sided CIs: upper bound fixed at [1.00].\n\n\nThe effect size for the ANOVA between bel_sum and urm is 0.02 which is small. This indicates the null hypothesis should not be rejected and means the difference in mean of belonging between students in this study who are and are not URM is not statistically significant."
  },
  {
    "objectID": "posts/dacss603_final_LauraCollazo.html#chi-squared",
    "href": "posts/dacss603_final_LauraCollazo.html#chi-squared",
    "title": "Check-in 2: DACSS 603 Final Project",
    "section": "Chi-squared",
    "text": "Chi-squared\nThe next round of hypothesis testing examines the explanatory outcome variable urm against each of the six individual belonging questions that make up the belonging scale. As these are all categorical variables, a chi-squared test will be used.\nThe first chi-squared test examines bel_1, which includes responses to the statement “I feel like a real part of this class.” It results in a p-value of 0.6553 which is not statistically significant.\n\n\nCode\nchi_bel_plot &lt;- function(mydata, myxvar, mytitle) {\n  ggplot({{mydata}}, aes(x=factor ({{myxvar}}, levels = c(\"Not at all true\", \"Slightly true\", \"Moderately true\", \"Mostly true\", \"Completely true\")),y=Freq, fill=urm)) +\n  geom_bar(stat=\"identity\",position=\"dodge\") +\n  labs(x = mytitle)\n}\n\n# chisq and bel_1\n\nchisq.test(data$bel_1, data$urm)\n\n\n\n    Pearson's Chi-squared test\n\ndata:  data$bel_1 and data$urm\nX-squared = 2.4404, df = 4, p-value = 0.6553\n\n\nCode\nbel_1_urm &lt;- data.frame(with(data, table(bel_1,urm)))\n\nchi_bel_plot(bel_1_urm, bel_1, \"I feel like a real part of this class\")\n\n\n\n\n\nThe second chi-squared test examines bel_2, which includes responses to the statement “Sometimes I feel as if I don’t belong in this class.” It results in a p-value of 0.01644 which is statistically significant.\n\n\nCode\n# chisq and bel_2\n\nchisq.test(data$bel_2, data$urm)\n\n\n\n    Pearson's Chi-squared test\n\ndata:  data$bel_2 and data$urm\nX-squared = 12.098, df = 4, p-value = 0.01664\n\n\nCode\nbel_2_urm &lt;- data.frame(with(data, table(bel_2,urm)))\n\nchi_bel_plot(bel_2_urm, bel_2, \"Sometimes I feel as if I don’t belong in this class\")\n\n\n\n\n\nThis chi-squared test examines bel_3, which includes responses to the statement “I am included in lots of activities in this class.” It results in a p-value of 0.9815 which is not statistically significant.\n\n\nCode\n# chisq and bel_3 (SIGNIFICANT P)\n\nchisq.test(data$bel_3, data$urm)\n\n\n\n    Pearson's Chi-squared test\n\ndata:  data$bel_3 and data$urm\nX-squared = 0.41141, df = 4, p-value = 0.9815\n\n\nCode\nbel_3_urm &lt;- data.frame(with(data, table(bel_3,urm)))\n\nchi_bel_plot(bel_3_urm, bel_3, \"I am included in lots of activities in this class\")\n\n\n\n\n\nThis next chi-squared test examines bel_4, which includes responses to the statement “I feel very different from most other students in this class.” It results in a p-value of 0.1081 which is not statistically significant.\n\n\nCode\n# chisq and bel_4\n\nchisq.test(data$bel_4, data$urm)\n\n\n\n    Pearson's Chi-squared test\n\ndata:  data$bel_4 and data$urm\nX-squared = 7.5825, df = 4, p-value = 0.1081\n\n\nCode\nbel_4_urm &lt;- data.frame(with(data, table(bel_4,urm)))\n\nchi_bel_plot(bel_4_urm, bel_4, \"I feel very different from most other students in this class\")\n\n\n\n\n\nThe following chi-squared test examines bel_5, which includes responses to the statement “I wish I were in a different class.” It results in a p-value of 0.02648 which is statistically significant.\n\n\nCode\n# chisq and bel_5 (SIGNIFICANT P)\n\nchisq.test(data$bel_5, data$urm)\n\n\n\n    Pearson's Chi-squared test\n\ndata:  data$bel_5 and data$urm\nX-squared = 11.007, df = 4, p-value = 0.02648\n\n\nCode\nbel_5_urm &lt;- data.frame(with(data, table(bel_5,urm)))\n\nchi_bel_plot(bel_5_urm, bel_5, \"I wish I were in a different class\")\n\n\n\n\n\nThis final chi-squared test examines bel_6, which includes responses to the statement “I feel proud of belonging to this class.” It results in a p-value of 0.2512 which is not statistically significant.\n\n\nCode\n# chisq and bel_6\n\nchisq.test(data$bel_6, data$urm)\n\n\n\n    Pearson's Chi-squared test\n\ndata:  data$bel_6 and data$urm\nX-squared = 5.372, df = 4, p-value = 0.2512\n\n\nCode\nbel_6_urm &lt;- data.frame(with(data, table(bel_6,urm)))\n\nchi_bel_plot(bel_6_urm, bel_6, \"I feel proud of belonging to this class\")\n\n\n\n\n\nTo recap the results of these six chi-squared tests, bel_2, “Sometimes I feel as if I don’t belong in this class”, and bel_5, “I wish I were in a different class”, were both statistically significant. This indicates a relationship between each of these two specific measure of belonging and urm."
  },
  {
    "objectID": "posts/dacss603_final_LauraCollazo.html#model-comparision",
    "href": "posts/dacss603_final_LauraCollazo.html#model-comparision",
    "title": "Check-in 2: DACSS 603 Final Project",
    "section": "Model comparision",
    "text": "Model comparision\nIn this section regression models will be compared to determine which has the best fit. In addition to the variables used in hypothesis testing, gender and year will be include as control variables and urm * gender as an interaction term. This interaction term has been included as previous research indicates the intersection of gender and racial identities can reveal significant differences (Rainey et al., 2018).\nThe first regression model examines bel_sum and urm. Note that the p-value, 0.05841, is identical to what is seen in the ANOVA results above. The adjusted R-squared, 0.01221, is very close to 0 which indicates the response variable, bel_sum, is not explained by the predictor variable, urm. Another way to think of this is that belonging to an URM group only explains 1.22% of the variation in a student’s sense of belonging.\nFrom this model it can be concluded that being part of a URM group leads to a -1.3386 decrease in belonging compared to those who are not-URM.\n\n\nCode\n# bel_sum & urm\n\nsummary(lm(bel_sum ~ urm, data = data))\n\n\n\nCall:\nlm(formula = bel_sum ~ urm, data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-16.611  -1.950   1.050   3.050   7.389 \n\nCoefficients:\n                    Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)          23.9497     0.3542  67.621   &lt;2e-16 ***\nurmunderrepresented  -1.3386     0.7034  -1.903   0.0584 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.466 on 211 degrees of freedom\nMultiple R-squared:  0.01687,   Adjusted R-squared:  0.01221 \nF-statistic: 3.621 on 1 and 211 DF,  p-value: 0.05841\n\n\nCode\n# Fit the linear model\nmodel &lt;- lm(bel_sum ~ urm, data = data)\n\n\nThe second regression model examines bel_sum, urm, and gender. The p-value, 0.0114, is significant, however, the adjusted R-square is low. This indicates the relationship between these variables is not strong enough to explain the variation between the response and predictor variables.\nFrom this model it can be concluded that gender is statistically significant. Identifying as male leads to a -1.6454 decrease in belonging compared to females.\n\n\nCode\n# bel_sum, urm & gender\n\nsummary(lm(bel_sum ~ urm + gender, data = data))\n\n\n\nCall:\nlm(formula = bel_sum ~ urm + gender, data = data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.3923  -2.3429   0.6571   2.9623   8.6077 \n\nCoefficients:\n                    Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)          24.3429     0.3889  62.593   &lt;2e-16 ***\nurmunderrepresented  -1.3052     0.6963  -1.875   0.0622 .  \ngenderMale           -1.6454     0.7051  -2.333   0.0206 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.42 on 210 degrees of freedom\nMultiple R-squared:  0.04172,   Adjusted R-squared:  0.03259 \nF-statistic: 4.571 on 2 and 210 DF,  p-value: 0.0114\n\n\nThe third regression model replaces gender with year. The overall p-value, 0.003374, in this model is more significant than the last, however the adjusted R-squared is still low. This model explains just 5.47% of the variation between the response and predictor variables.\nFrom this model it can be concluded that being a Junior or Senior is statistically significant. Being a Senior leads to the greatest decrease, -3.2020, in belonging compared to students in other years of their studies. Being a Junior shows a -2.6332 decrease in belonging compared to students in other groups.\n\n\nCode\n# bel_sum, urm & year\n\nsummary(lm(bel_sum ~ urm + year, data = data))\n\n\n\nCall:\nlm(formula = bel_sum ~ urm + year, data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-14.390  -1.870   0.293   2.495   9.610 \n\nCoefficients:\n                    Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)          24.7070     0.4527  54.579  &lt; 2e-16 ***\nurmunderrepresented  -1.1149     0.6915  -1.612  0.10843    \nyearJunior           -2.6332     0.9602  -2.742  0.00663 ** \nyearSenior           -3.2020     1.2096  -2.647  0.00874 ** \nyearSophmore         -0.8367     0.6829  -1.225  0.22193    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.369 on 208 degrees of freedom\nMultiple R-squared:  0.07259,   Adjusted R-squared:  0.05476 \nF-statistic:  4.07 on 4 and 208 DF,  p-value: 0.003374\n\n\nThis next regression model includes bel_sum, urm, gender and year. It has the most significant p-value, 0.001652, of any model yet. The adjusted R-squared value is also higher, however it only explains 6.63% percent of the variation which is still rather low.\nThis model also shows that being a Junior or Senior is statistically significant. Being Male is now just slightly above p &lt; .05.\n\n\nCode\n# bel_sum, urm, gender & year\n\nsummary(lm(bel_sum ~ urm + gender + year, data = data))\n\n\n\nCall:\nlm(formula = bel_sum ~ urm + gender + year, data = data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-13.9744  -2.1816   0.8184   3.0256   9.1568 \n\nCoefficients:\n                    Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)          24.9744     0.4711  53.010   &lt;2e-16 ***\nurmunderrepresented  -1.1034     0.6872  -1.606   0.1099    \ngenderMale           -1.3365     0.6998  -1.910   0.0576 .  \nyearJunior           -2.3901     0.9626  -2.483   0.0138 *  \nyearSenior           -3.0278     1.2054  -2.512   0.0128 *  \nyearSophmore         -0.7928     0.6790  -1.168   0.2443    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.341 on 207 degrees of freedom\nMultiple R-squared:  0.08865,   Adjusted R-squared:  0.06663 \nF-statistic: 4.027 on 5 and 207 DF,  p-value: 0.001652\n\n\nThe final regression model using bel_sum includes the interaction between urm and gender as the explanatory variable. It has a significant p-value, 0.008266, but the adjusted R-squared is again low explaining just 4.1% of the variation between variables.\n\n\nCode\n# bel_sum, urm * gender\n\nsummary(lm(bel_sum ~ urm * gender, data = data))\n\n\n\nCall:\nlm(formula = bel_sum ~ urm * gender, data = data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-13.9286  -2.1736   0.8264   2.8264  10.0714 \n\nCoefficients:\n                               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                     24.1736     0.4000  60.428   &lt;2e-16 ***\nurmunderrepresented             -0.6236     0.8026  -0.777   0.4381    \ngenderMale                      -0.9367     0.8183  -1.145   0.2536    \nurmunderrepresented:genderMale  -2.6847     1.5927  -1.686   0.0934 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.4 on 209 degrees of freedom\nMultiple R-squared:  0.05457,   Adjusted R-squared:  0.041 \nF-statistic: 4.021 on 3 and 209 DF,  p-value: 0.008266"
  },
  {
    "objectID": "posts/dacss603_final_LauraCollazo.html#residuals-vs-fitted",
    "href": "posts/dacss603_final_LauraCollazo.html#residuals-vs-fitted",
    "title": "Check-in 2: DACSS 603 Final Project",
    "section": "Residuals vs Fitted",
    "text": "Residuals vs Fitted\nIn the Residuals vs Fitted plot both the linearity and constant variance assumptions are violated. These violations are evident as the line is not linear and the points are not equally horizontal around the line at 0. There are also residuals which stand out from the rest indicating outliers.\n\n\nCode\nplot(lm(bel_sum ~ urm + gender + year, data = data), which = 1)"
  },
  {
    "objectID": "posts/dacss603_final_LauraCollazo.html#normal-q-q",
    "href": "posts/dacss603_final_LauraCollazo.html#normal-q-q",
    "title": "Check-in 2: DACSS 603 Final Project",
    "section": "Normal Q-Q",
    "text": "Normal Q-Q\nIn the QQ plot, the residuals fall on the line towards the center but curve towards the ends. This indicates the data isn’t part of a normal distribution, therefore the normality assumption has been violated (Ford, 2015).\n\n\nCode\nplot(lm(bel_sum ~ urm + gender + year, data = data), which = 2)"
  },
  {
    "objectID": "posts/dacss603_final_LauraCollazo.html#scale-location",
    "href": "posts/dacss603_final_LauraCollazo.html#scale-location",
    "title": "Check-in 2: DACSS 603 Final Project",
    "section": "Scale-Location",
    "text": "Scale-Location\nIn the Scale-Location plot the red line is not horizontal, rather it is decreasing. The variance of the points is also not equal which indicates heteroskedasticity. This is a violation of the constant variance assumption.\n\n\nCode\nplot(lm(bel_sum ~ urm + gender + year, data = data), which = 3)"
  },
  {
    "objectID": "posts/dacss603_final_LauraCollazo.html#cooks-distance",
    "href": "posts/dacss603_final_LauraCollazo.html#cooks-distance",
    "title": "Check-in 2: DACSS 603 Final Project",
    "section": "Cook’s distance",
    "text": "Cook’s distance\nThe Cook’s Distance plot shows a violation of the influential observation assumption. This is because there are points with a Cook’s distance larger than 4/n, which in this study is 4/213 = 0.019.\n\n\nCode\nplot(lm(bel_sum ~ urm + gender + year, data = data), which = 4)"
  },
  {
    "objectID": "posts/Jerin_Jacob_Final_Project.html",
    "href": "posts/Jerin_Jacob_Final_Project.html",
    "title": "Final Project 603",
    "section": "",
    "text": "Now a days, movies are a well marketed entertainment product. Just like any other products in the market, movies are also having an allocated a marketing budget and promotional activities are done in scale. This often result in the opening weekend’s gross ticketing volume to rise. But are the pre-release promotional activities helping the movie to collect more or is it just creating a hype initially? Or does the movie’s gross collection is not at all dependant on pre release promotions? This dataset has 200 highest grossing movies of 2022. It has both the opening week’s gross as well as the total gross collection of the movies, along with other variables. Assuming that opening week’s collection is depending on the pre-release promotion, by looking on the relationship between opening week’s gross and total gross, I am trying to see how the pre-release activities help the producers earn more in boxoffice.\nResearch Question: Is the total collection of a movie depending on the pre-release promotional activities?\nNull hypothesis: The total gross collection of a movie is not dependant on the opening week’s collection\nAlternative hypotheses: Total gross collection is positively dependant on the opening week’s collection"
  },
  {
    "objectID": "posts/Jerin_Jacob_Final_Project.html#reading-the-data",
    "href": "posts/Jerin_Jacob_Final_Project.html#reading-the-data",
    "title": "Final Project 603",
    "section": "Reading the data",
    "text": "Reading the data\n\n\nCode\nlibrary(readxl)\nlibrary(tidyverse)\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.0     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.1     ✔ tibble    3.2.0\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the \u001b]8;;http://conflicted.r-lib.org/\u0007conflicted package\u001b]8;;\u0007 to force all conflicts to become errors\n\n\n\n\nCode\ndf &lt;- read.csv(\"_data/project_data.csv\")\n#df\n\n\nThere are 10 variables with 200 rows.\n\nCOLUMN DESCRIPTION\n\n‘Rank’: rank of the movie\n‘Release’: release date of the movie\n‘Gross’: domestic gross of the movie\n‘max_th’: maximum number of theaters the movie was released in\n‘Opening’: gross on opening weekend\n‘perc_tot_gr’: domestic percentage of the total gross\n‘open_th’: number of theaters the movie opened in\n‘Open’: opening date\n‘Close’: closing date\n‘Distributor’: name of the distributor\n‘int_gross’: international gross\n‘world_gross’: worldwide gross"
  },
  {
    "objectID": "posts/Jerin_Jacob_Final_Project.html#summary-of-each-variables",
    "href": "posts/Jerin_Jacob_Final_Project.html#summary-of-each-variables",
    "title": "Final Project 603",
    "section": "Summary of each variables",
    "text": "Summary of each variables\n\n\nCode\nsummary(df)\n\n\n      Rank          Release              Gross               max_th      \n Min.   :  1.00   Length:200         Min.   :   304287   Min.   :   5.0  \n 1st Qu.: 50.75   Class :character   1st Qu.:  1028500   1st Qu.: 457.8  \n Median :100.50   Mode  :character   Median :  3995788   Median :1083.5  \n Mean   :100.50                      Mean   : 36977151   Mean   :1760.2  \n 3rd Qu.:150.25                      3rd Qu.: 21273142   3rd Qu.:3189.0  \n Max.   :200.00                      Max.   :718732821   Max.   :4751.0  \n                                                                         \n    Opening           perc_tot_gr       open_th           Open          \n Min.   :     3265   Min.   : 0.10   Min.   :   1.0   Length:200        \n 1st Qu.:   243919   1st Qu.:20.50   1st Qu.: 160.5   Class :character  \n Median :  1066404   Median :35.45   Median : 916.5   Mode  :character  \n Mean   : 11649361   Mean   :32.05   Mean   :1588.8                     \n 3rd Qu.:  8137392   3rd Qu.:43.25   3rd Qu.:3156.2                     \n Max.   :187420998   Max.   :77.80   Max.   :4735.0                     \n                                                                        \n    Close           Distributor          int_gross          world_gross       \n Length:200         Length:200         Min.   :4.201e+03   Min.   :5.100e+03  \n Class :character   Class :character   1st Qu.:7.076e+05   1st Qu.:2.099e+06  \n Mode  :character   Mode  :character   Median :6.311e+06   Median :1.287e+07  \n                                       Mean   :5.177e+07   Mean   :8.765e+07  \n                                       3rd Qu.:3.040e+07   3rd Qu.:6.049e+07  \n                                       Max.   :1.539e+09   Max.   :2.176e+09  \n                                       NA's   :3                              \n\n\nUsing the glimpse() function, let’s have a look at how our data would look like!\n\n\nCode\nglimpse(df)\n\n\nRows: 200\nColumns: 12\n$ Rank        &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17,…\n$ Release     &lt;chr&gt; \"Top Gun: Maverick\", \"Avatar: The Way of Water\", \"Black Pa…\n$ Gross       &lt;int&gt; 718732821, 636955746, 453474324, 411331607, 376851080, 369…\n$ max_th      &lt;int&gt; 4751, 4340, 4396, 4534, 4697, 4427, 4417, 4375, 4258, 4402…\n$ Opening     &lt;int&gt; 126707459, 134100226, 181339761, 187420998, 145075625, 107…\n$ perc_tot_gr &lt;dbl&gt; 17.6, 21.1, 40.0, 45.6, 38.5, 28.9, 36.3, 42.0, 37.8, 39.8…\n$ open_th     &lt;int&gt; 4735, 4202, 4396, 4534, 4676, 4391, 4417, 4375, 4234, 4402…\n$ Open        &lt;chr&gt; \"2022-05-27\", \"2022-12-16\", \"2022-11-11\", \"2022-05-06\", \"2…\n$ Close       &lt;chr&gt; \"2022-12-16\", \"\", \"\", \"\", \"2022-09-23\", \"\", \"\", \"2022-10-1…\n$ Distributor &lt;chr&gt; \"Paramount Pictures\", \"20th Century Studios\", \"Walt Disney…\n$ int_gross   &lt;int&gt; 770000000, 1539273359, 389276658, 544444197, 625127000, 56…\n$ world_gross &lt;dbl&gt; 1488732821, 2176229105, 842750982, 955775804, 1001978080, …"
  },
  {
    "objectID": "posts/Jerin_Jacob_Final_Project.html#references",
    "href": "posts/Jerin_Jacob_Final_Project.html#references",
    "title": "Final Project 603",
    "section": "References:",
    "text": "References:\n\nNasir, Suphan & Öcal, Figen. (2016). Film Marketing: The Impact of Publicity Activities on Demand Generation. 10.4018/978-1-5225-0143-5.ch019.\nElizabeth Cooper-Martin (1991) ,“Consumers and Movies: Some Findings on Experiential Products”, in NA - Advances in Consumer Research Volume 18, eds. Rebecca H. Holman and Michael R. Solomon, Provo, UT : Association for Consumer Research, Pages: 372-378."
  },
  {
    "objectID": "posts/FinalProject.html",
    "href": "posts/FinalProject.html",
    "title": "Final Project Check-in 1",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(dplyr)\nlibrary(ggplot2)\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/FinalProject.html#overview",
    "href": "posts/FinalProject.html#overview",
    "title": "Final Project Check-in 1",
    "section": "Overview",
    "text": "Overview\nThe disastrous effects that a highly contagious disease can have on the world have been strongly illustrated by the COVID-19 pandemic. Millions of people have passed away as a consequence of the pandemic and also impacted the lives of billions of people around the world. Current state of affairs has brought to light the necessity for research on factor and tactics to effectively combat pandemics in the future.\nIt’s critical to comprehend the variables affecting COVID-19 mortality as the pandemic spreads further. The goal of this study is to look at the correlations between a nation’s COVID-19 mortality rate and its population density, median age, GDP per-capita, prevalence of diabetes, hospital beds per 1,000 people, and human development index.\nIn this project I’m aiming to research To what extent do these socioeconomic factors contribute to the variation in COVID-19 mortality rate across the world and derive the relationship of COVID-19 mortality rate with population density,median age, GDP per capita, diabetes prevalence, hospital beds per thousand people and human development index.\n#DataSet\nThe data set contains time series data of around 193 countries around the world. There are around 84,000 records of the countries over the period of time.\nDatasource: https://www.kaggle.com/datasets/fedesoriano/coronavirus-covid19-vaccinations-data\n\n\nCode\ndf &lt;- read_excel(\"_data/COVID_Data.xlsx\")\ndf_selected &lt;- df[,c(\"iso_code\",\"continent\",\"location\",\"date\",\"total_cases_per_million\",\"population_density\",\"median_age\",\n\"gdp_per_capita\",\"diabetes_prevalence\",\"hospital_beds_per_thousand\",\"human_development_index\")]\ndataset_dim &lt;- (dim(df_selected))\ndataset_dim\n\n\n[1] 84772    11\n\n\nCode\ncountries_count &lt;- (length(unique(df_selected$location)))\ncountries_count\n\n\n[1] 193\n\n\nCode\ncountries_list &lt;- (unique(df_selected$location))\n\nhead(df_selected)\n\n\n# A tibble: 6 × 11\n  iso_code conti…¹ locat…² date  total…³ popul…⁴ media…⁵ gdp_p…⁶ diabe…⁷ hospi…⁸\n  &lt;chr&gt;    &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1 AFG      Asia    Afghan… 2020…   0.873    54.4    18.6   1804.    9.59     0.5\n2 AFG      Asia    Afghan… 2020…   1.05     54.4    18.6   1804.    9.59     0.5\n3 AFG      Asia    Afghan… 2020…   1.10     54.4    18.6   1804.    9.59     0.5\n4 AFG      Asia    Afghan… 2020…   1.95     54.4    18.6   1804.    9.59     0.5\n5 AFG      Asia    Afghan… 2020…   2.06     54.4    18.6   1804.    9.59     0.5\n6 AFG      Asia    Afghan… 2020…   2.34     54.4    18.6   1804.    9.59     0.5\n# … with 1 more variable: human_development_index &lt;dbl&gt;, and abbreviated\n#   variable names ¹​continent, ²​location, ³​total_cases_per_million,\n#   ⁴​population_density, ⁵​median_age, ⁶​gdp_per_capita, ⁷​diabetes_prevalence,\n#   ⁸​hospital_beds_per_thousand\n\n\nCode\nsummary(df_selected)\n\n\n   iso_code          continent           location             date          \n Length:84772       Length:84772       Length:84772       Length:84772      \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n                                                                            \n total_cases_per_million population_density   median_age    gdp_per_capita    \n Min.   :     0.02       Min.   :    1.98   Min.   :15.10   Min.   :   661.2  \n 1st Qu.:   454.56       1st Qu.:   36.25   1st Qu.:22.20   1st Qu.:  4466.5  \n Median :  2579.10       Median :   82.60   Median :30.60   Median : 13367.6  \n Mean   : 14350.40       Mean   :  361.01   Mean   :30.76   Mean   : 19633.0  \n 3rd Qu.: 16110.12       3rd Qu.:  205.86   3rd Qu.:39.60   3rd Qu.: 27936.9  \n Max.   :179667.38       Max.   :19347.50   Max.   :48.20   Max.   :116935.6  \n NA's   :1               NA's   :4819       NA's   :5783    NA's   :6696      \n diabetes_prevalence hospital_beds_per_thousand human_development_index\n Min.   : 0.990      Min.   : 0.100             Min.   :0.394          \n 1st Qu.: 5.290      1st Qu.: 1.300             1st Qu.:0.602          \n Median : 7.110      Median : 2.400             Median :0.756          \n Mean   : 7.651      Mean   : 3.047             Mean   :0.731          \n 3rd Qu.: 9.740      3rd Qu.: 4.200             3rd Qu.:0.852          \n Max.   :22.020      Max.   :13.800             Max.   :0.957          \n NA's   :4872        NA's   :12687              NA's   :5802           \n\n\nMethodology:\nMultiple linear regression models will be used to carry out the analysis. The socioeconomic determinants will be the independent variables, where as the COVID-19 mortality rate will be the dependent variable.\nExpected Results:\nThe findings of this investigation will aid in understanding the variables affecting the COVID-19 mortality rate. Population density, median age, diabetes prevalence, and hospital beds per thousand people are anticipated to have a positive correlation with the COVID-19 mortality rate, whereas GDP per capita and the human development index are anticipated to have a negative correlation. Planning public health policies and actions to lessen the effects of COVID-19 will benefit from the findings.\nConclusion:\nThe goal of this study is to understand the relationship between socioeconomic factors and the COVID-19 mortality rate. The results will be helpful in planning public health policy and initiatives and will shed light on the factors that affect the COVID-19 death rate."
  },
  {
    "objectID": "posts/603_Final_Project_Check_In_2_Akhilesh_Kumar.html",
    "href": "posts/603_Final_Project_Check_In_2_Akhilesh_Kumar.html",
    "title": "603_Final_Project_Check_In_2_Akhilesh_KUmar",
    "section": "",
    "text": "The blood transfusion dataset contain 748 samples with 5 input features: Input Features: • Recency (number of months since the last donation) • Frequency (total number of donations) • Monetary (total blood donated in c.c.) • Time (number of months since the first donation) • Age (age of the donor)\nSource: https://www.kaggle.com/datasets/ninalabiba/blood-transfusion-dataset\nThis summary function presents a statistical description of a dataset related to blood donation, with five variables: Recency, Frequency, Monetary, Time, and whether the individual donated blood in March 2007. Here’s a breakdown of each variable:\nRecency (months): This variable represents the number of months since the last blood donation. The minimum value is 0 months, indicating that some individuals donated blood very recently. The mean is 9.507 months, suggesting that, on average, people donated blood around 9.5 months ago. The maximum value is 74 months, which means the longest gap between donations is 74 months.\nFrequency (times): This variable shows the total number of times an individual has donated blood. The minimum value is 1, meaning that at least one person has only donated blood once. The mean is 5.515 times, indicating that people, on average, have donated blood about 5.5 times. The maximum value is 50 times, showing that some individuals have donated blood quite frequently.\nMonetary (c.c. blood): This variable represents the total volume of blood donated by an individual, measured in cubic centimeters (c.c.). The minimum value is 250 c.c., which corresponds to the minimum single donation volume. The mean is 1379 c.c., suggesting that, on average, individuals have donated around 1.379 liters of blood. The maximum value is 12,500 c.c., indicating that the highest total volume donated by a person is 12.5 liters.\nTime (months): This variable measures the length of time an individual has been donating blood. The minimum value is 2 months, suggesting that some individuals are relatively new to blood donation. The mean is 34.28 months, indicating that, on average, people have been donating blood for about 34.3 months. The maximum value is 98 months, showing that some individuals have been donating blood for a long time.\nWhether he/she donated blood in March 2007: This is a binary variable that indicates if an individual donated blood in March 2007. The mean is 0.238, which means that about 23.8% of the individuals in the dataset donated blood in that specific month.\nThe summary function provides an overview of the dataset’s key statistics, such as minimum, 1st quartile, median, mean, 3rd quartile, and maximum values for each variable. This information helps to understand the distribution, central tendency, and spread of the data.\n\nBD &lt;- read_csv(\"C:/603_Spring_2023_new/posts/_data/transfusion.csv\")\n\nRows: 748 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (5): Recency (months), Frequency (times), Monetary (c.c. blood), Time (m...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndata(BD)\n\nWarning in data(BD): data set 'BD' not found\n\nBD\n\n# A tibble: 748 × 5\n   `Recency (months)` `Frequency (times)` Monetary (c.c. blood…¹ Time …² wheth…³\n                &lt;dbl&gt;               &lt;dbl&gt;                  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n 1                  2                  50                  12500      98       1\n 2                  0                  13                   3250      28       1\n 3                  1                  16                   4000      35       1\n 4                  2                  20                   5000      45       1\n 5                  1                  24                   6000      77       0\n 6                  4                   4                   1000       4       0\n 7                  2                   7                   1750      14       1\n 8                  1                  12                   3000      35       0\n 9                  2                   9                   2250      22       1\n10                  5                  46                  11500      98       1\n# … with 738 more rows, and abbreviated variable names\n#   ¹​`Monetary (c.c. blood)`, ²​`Time (months)`,\n#   ³​`whether he/she donated blood in March 2007`\n\n\n\nsummary(BD)\n\n Recency (months) Frequency (times) Monetary (c.c. blood) Time (months)  \n Min.   : 0.000   Min.   : 1.000    Min.   :  250         Min.   : 2.00  \n 1st Qu.: 2.750   1st Qu.: 2.000    1st Qu.:  500         1st Qu.:16.00  \n Median : 7.000   Median : 4.000    Median : 1000         Median :28.00  \n Mean   : 9.507   Mean   : 5.515    Mean   : 1379         Mean   :34.28  \n 3rd Qu.:14.000   3rd Qu.: 7.000    3rd Qu.: 1750         3rd Qu.:50.00  \n Max.   :74.000   Max.   :50.000    Max.   :12500         Max.   :98.00  \n whether he/she donated blood in March 2007\n Min.   :0.000                             \n 1st Qu.:0.000                             \n Median :0.000                             \n Mean   :0.238                             \n 3rd Qu.:0.000                             \n Max.   :1.000"
  },
  {
    "objectID": "posts/603_Final_Project_Check_In_2_Akhilesh_Kumar.html#data-description",
    "href": "posts/603_Final_Project_Check_In_2_Akhilesh_Kumar.html#data-description",
    "title": "603_Final_Project_Check_In_2_Akhilesh_KUmar",
    "section": "",
    "text": "The blood transfusion dataset contain 748 samples with 5 input features: Input Features: • Recency (number of months since the last donation) • Frequency (total number of donations) • Monetary (total blood donated in c.c.) • Time (number of months since the first donation) • Age (age of the donor)\nSource: https://www.kaggle.com/datasets/ninalabiba/blood-transfusion-dataset\nThis summary function presents a statistical description of a dataset related to blood donation, with five variables: Recency, Frequency, Monetary, Time, and whether the individual donated blood in March 2007. Here’s a breakdown of each variable:\nRecency (months): This variable represents the number of months since the last blood donation. The minimum value is 0 months, indicating that some individuals donated blood very recently. The mean is 9.507 months, suggesting that, on average, people donated blood around 9.5 months ago. The maximum value is 74 months, which means the longest gap between donations is 74 months.\nFrequency (times): This variable shows the total number of times an individual has donated blood. The minimum value is 1, meaning that at least one person has only donated blood once. The mean is 5.515 times, indicating that people, on average, have donated blood about 5.5 times. The maximum value is 50 times, showing that some individuals have donated blood quite frequently.\nMonetary (c.c. blood): This variable represents the total volume of blood donated by an individual, measured in cubic centimeters (c.c.). The minimum value is 250 c.c., which corresponds to the minimum single donation volume. The mean is 1379 c.c., suggesting that, on average, individuals have donated around 1.379 liters of blood. The maximum value is 12,500 c.c., indicating that the highest total volume donated by a person is 12.5 liters.\nTime (months): This variable measures the length of time an individual has been donating blood. The minimum value is 2 months, suggesting that some individuals are relatively new to blood donation. The mean is 34.28 months, indicating that, on average, people have been donating blood for about 34.3 months. The maximum value is 98 months, showing that some individuals have been donating blood for a long time.\nWhether he/she donated blood in March 2007: This is a binary variable that indicates if an individual donated blood in March 2007. The mean is 0.238, which means that about 23.8% of the individuals in the dataset donated blood in that specific month.\nThe summary function provides an overview of the dataset’s key statistics, such as minimum, 1st quartile, median, mean, 3rd quartile, and maximum values for each variable. This information helps to understand the distribution, central tendency, and spread of the data.\n\nBD &lt;- read_csv(\"C:/603_Spring_2023_new/posts/_data/transfusion.csv\")\n\nRows: 748 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (5): Recency (months), Frequency (times), Monetary (c.c. blood), Time (m...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndata(BD)\n\nWarning in data(BD): data set 'BD' not found\n\nBD\n\n# A tibble: 748 × 5\n   `Recency (months)` `Frequency (times)` Monetary (c.c. blood…¹ Time …² wheth…³\n                &lt;dbl&gt;               &lt;dbl&gt;                  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n 1                  2                  50                  12500      98       1\n 2                  0                  13                   3250      28       1\n 3                  1                  16                   4000      35       1\n 4                  2                  20                   5000      45       1\n 5                  1                  24                   6000      77       0\n 6                  4                   4                   1000       4       0\n 7                  2                   7                   1750      14       1\n 8                  1                  12                   3000      35       0\n 9                  2                   9                   2250      22       1\n10                  5                  46                  11500      98       1\n# … with 738 more rows, and abbreviated variable names\n#   ¹​`Monetary (c.c. blood)`, ²​`Time (months)`,\n#   ³​`whether he/she donated blood in March 2007`\n\n\n\nsummary(BD)\n\n Recency (months) Frequency (times) Monetary (c.c. blood) Time (months)  \n Min.   : 0.000   Min.   : 1.000    Min.   :  250         Min.   : 2.00  \n 1st Qu.: 2.750   1st Qu.: 2.000    1st Qu.:  500         1st Qu.:16.00  \n Median : 7.000   Median : 4.000    Median : 1000         Median :28.00  \n Mean   : 9.507   Mean   : 5.515    Mean   : 1379         Mean   :34.28  \n 3rd Qu.:14.000   3rd Qu.: 7.000    3rd Qu.: 1750         3rd Qu.:50.00  \n Max.   :74.000   Max.   :50.000    Max.   :12500         Max.   :98.00  \n whether he/she donated blood in March 2007\n Min.   :0.000                             \n 1st Qu.:0.000                             \n Median :0.000                             \n Mean   :0.238                             \n 3rd Qu.:0.000                             \n Max.   :1.000"
  },
  {
    "objectID": "posts/603_Final_Project_Check_In_2_Akhilesh_Kumar.html#eda",
    "href": "posts/603_Final_Project_Check_In_2_Akhilesh_Kumar.html#eda",
    "title": "603_Final_Project_Check_In_2_Akhilesh_KUmar",
    "section": "EDA:",
    "text": "EDA:\nExploratory Data Analysis would be performed along with graphs, including the variables (Dependent and Independent Variables) in the Blood Transfusion Dataset (Recency (months), Frequency (times), Monetary (c.c. blood), Time (months), whether he/she donated blood in March 2007)"
  },
  {
    "objectID": "posts/603_Final_Project_Check_In_2_Akhilesh_Kumar.html#detailed-research-statement",
    "href": "posts/603_Final_Project_Check_In_2_Akhilesh_Kumar.html#detailed-research-statement",
    "title": "603_Final_Project_Check_In_2_Akhilesh_KUmar",
    "section": "Detailed Research Statement:",
    "text": "Detailed Research Statement:\n\nHypothesis 1: There is a significant difference in the past frequency of blood donations between donors who have donated in March 2007 and those who have not.\n\nConfounding variables recency of blood donations, Monetary Value of blood donations, Time (months) of blood donations\n\nHypothesis 2: There is a significant difference in the Recency of blood donations between donors who have donated in March 2007 and those who have not.\n\nConfounding variables: Past frequency of blood donations, Monetary Value of blood donations, Time (months) of blood donations\n\nHypothesis 3: There is a significant difference in the Monetary Value of blood donations between donors who have donated in March 2007 and those who have not.\n\nConfounding variables: Past frequency of blood donations, Recency of blood donations, Time (months) of blood donations\n\nHypothesis 4: There is a significant difference in the Time (months) of blood donations between donors who have donated in March 2007 and those who have not.\n\nConfounding variables: Past frequency of blood donations, Recency of blood donations, Monetary Value of blood donations\n\nHypothesis 5: Donors who have donated blood more frequently in the past (i.e. a higher average number of donations per month) are more likely to donate again in March 2007.\nHypothesis 6: Donors who have donated higher average Monetary (c.c. blood) (per month) are more likely to donate again in March 2007.\nHypothesis 7: Donors who have donated blood more recently (Recency (months)) and with higher frequency (Frequency (times)) are more likely to donate again in March 2007.\nHypothesis 8: Donors who have donated blood more recently (Recency (months)) and with higher blood donation in past (Monetary (c.c. blood)) are more likely to donate again in March 2007.\nHypothesis 9: Donors who have donated blood more recently (Recency (months)) and with Time (months) are more likely to donate again in March 2007."
  },
  {
    "objectID": "posts/603_Final_Project_Check_In_2_Akhilesh_Kumar.html#demographic-data",
    "href": "posts/603_Final_Project_Check_In_2_Akhilesh_Kumar.html#demographic-data",
    "title": "603_Final_Project_Check_In_2_Akhilesh_KUmar",
    "section": "Demographic Data:",
    "text": "Demographic Data:\nWe have considered removing demographic data from the scope, as we couldn’t find any relevant data resources on internet. So confounding variables are limited to the variable available in the blood transfusion dataset."
  },
  {
    "objectID": "posts/603_Final_Project_Check_In_2_Akhilesh_Kumar.html#graphs",
    "href": "posts/603_Final_Project_Check_In_2_Akhilesh_Kumar.html#graphs",
    "title": "603_Final_Project_Check_In_2_Akhilesh_KUmar",
    "section": "Graphs:",
    "text": "Graphs:\nWherever necessary bar graph, scatter plot, correlation, regression, logistic regression graphs would be included for the hypothesis mentioned above."
  },
  {
    "objectID": "posts/603_Final_Project_Check_In_2_Akhilesh_Kumar.html#building-a-donor-retention-strategy",
    "href": "posts/603_Final_Project_Check_In_2_Akhilesh_Kumar.html#building-a-donor-retention-strategy",
    "title": "603_Final_Project_Check_In_2_Akhilesh_KUmar",
    "section": "Building a Donor Retention Strategy:",
    "text": "Building a Donor Retention Strategy:\nUse the insights gained to develop a donor retention strategy for the blood donation center. Identify factors that are associated with donor churn (i.e., donors who stop donating blood), and develop a plan to mitigate these factors. This can help healthcare and blood donation organizations to develop strategies to retain donors over the long term."
  },
  {
    "objectID": "posts/603_Final_Project_Check_In_2_Akhilesh_Kumar.html#expected-contribution",
    "href": "posts/603_Final_Project_Check_In_2_Akhilesh_Kumar.html#expected-contribution",
    "title": "603_Final_Project_Check_In_2_Akhilesh_KUmar",
    "section": "Expected Contribution:",
    "text": "Expected Contribution:\nSai Srinivas: Exploratory Analysis, Research Question 1-4\nAkhilesh Kumar: and Research Question 6-9, Donor Retention Strategy"
  },
  {
    "objectID": "posts/603_Final_Project_Check_In_2_Akhilesh_Kumar.html#references",
    "href": "posts/603_Final_Project_Check_In_2_Akhilesh_Kumar.html#references",
    "title": "603_Final_Project_Check_In_2_Akhilesh_KUmar",
    "section": "References:",
    "text": "References:\nKaggle: https://www.kaggle.com/datasets/ninalabiba/blood-transfusion-dataset\nOriginal Owner and Donor: Prof. I-Cheng Yeh, Department of Information Management, Chung-Hua University, Hsin Chu, Taiwan 30067, R.O.C., e-mail:icyeh ‘@’ chu.edu.tw, TEL:886-3-5186511, Date Donated: October 3, 2008"
  },
  {
    "objectID": "posts/FinalPart1_MiguelCuriel.html",
    "href": "posts/FinalPart1_MiguelCuriel.html",
    "title": "Final Project Check In 1",
    "section": "",
    "text": "The technology industry - aka tech - has been one of the highest employers in the twenty-first century. Not only that, but it has been praised for having some of the happiest workers1 2.\nBut, what has made this industry so appealing? A case study on Google published in the International Journal of Corporate Social Responsibility in 20173 points to several elements that make high-tech unique, such as having a distinct culture proposition, aligning individual behaviors to company-wide goals, having managers be coaches rather than bosses, and being able to interact with people from other cultures.\nEvidently, Google is one-in-a-million high-tech company, but there are certainly commonalities shared with smaller new tech (startup) companies. Culture Amp, a company focused on surveying employees in startups, elaborated an analysis based on their results from 2015-2020 surveys4 and mention that elements such as an open and honest two-way communication, workplace flexibility, and fair division of workload, are what make new tech companies valued.\nHowever, the previous data omits the downsides of such cultures. Besides the multiple blog posts and news articles one can find talking about burnout5, the darker side of tech also includes (but is not limited to) ageism6, gender inequality7 8, and even migration issues9 10.\nAdditionally, a survey lead by Blind in 202111 found that, out of 2400 workers in tech, 64% said their mental health is worse after the pandemic. What’s more, layoffs by the thousands, plummeting stock prices, and generalized revaluation of an entire industry’s value, are just few words that can describe what has happened to tech in 2022 and 2023. An industry that once employed over 5 million people in the US alone12 and had nearly 700 billion dollars in funding worldwide13 has now laid off over 300 thousand people14 and nearly halved in funding.\nIt is reasonable to hypothesize that the aforementioned events will take a toll on the workers of this industry, but what has been the actual mental health state of the actors involved? More specifically, the proposed research question is:\n\nWhat is the trend in mental health issues among workers in the technology industry from 2017 to 2021, as measured by survey data, and what factors may contribute to these changes?"
  },
  {
    "objectID": "posts/FinalPart1_MiguelCuriel.html#footnotes",
    "href": "posts/FinalPart1_MiguelCuriel.html#footnotes",
    "title": "Final Project Check In 1",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFox, M. (2016, November 11). Why Are Tech Workers So Satisfied With Their Jobs? Retrieved March 17, 2023, from https://www.forbes.com/sites/meimeifox/2016/11/11/why-are-tech-workers-so-satisfied-with-their-jobs/?sh=4eac1918a059↩︎\nWronski, L., & Cohen, J. (2019, November 4). This is the industry sector that has some of the happiest workers in America. Retrieved March 17, 2023, from https://www.cnbc.com/2019/11/04/this-is-the-industry-that-has-some-of-the-happiness-workers-in-america.html↩︎\nKim, K. T. (2017). GOOGLE: A reflection of culture, leader, and management. International Journal of Corporate Social Responsibility, 2(10). https://doi.org/10.1186/s40991-017-0021-0↩︎\nMcPherson, J. (n.d.). Tech company cultures are not all the same. Culture Amp. Retrieved March 17, 2023, from https://www.cultureamp.com/blog/tech-company-culture↩︎\nGoncharov, A. (2023, March 13). How I burnt out in FAANG, but my job was not the problem. Blog.Goncharov.ai. Retrieved March 17, 2023, from https://blog.goncharov.ai/how-i-burnt-out-in-faang-but-my-job-was-not-the-problem↩︎\nRosales, A., & Jakob, S. (2021). Perceptions of age in contemporary tech. Sciendo, 42(1), 79-91. https://doi.org/10.2478/nor-2021-0021↩︎\nMickey, E. L. (2021). The Organization of Networking and Gender Inequality in the New Economy: Evidence from the Tech Industry. Work & Occupations, 49(4), 383-420. https://doi.org/10.1177/07308884221102134↩︎\nHardey, M. (2020). The Culture of Women in Tech : An Unsuitable Job for a Woman (1st ed.). Emerald Publishing.↩︎\nBanerjee, P., & Rincón, L. (2019). Trouble in Tech Paradise. Journal of Water Resources Planning & Management, 145(4), 24-29. https://doi.org/10.1177/1536504219854714↩︎\nMatloff, N. (2013). Immigration and the tech industry: As a labour shortage remedy, for innovation, or for cost savings? Migration Letters, 10(2), 210-227. ISSN: 1741-8984 Online ISSN: 1741-8992↩︎\nBlind (2021, January 29). Deteriorating Mental Health In The Workplace. Retrieved March 17, 2023, from https://www.teamblind.com/blog/index.php/2021/01/29/deteriorating-mental-health-in-the-workplace/↩︎\nThe United States Bureau of Labor and Statistics via CompTIA (2023, March 3). Cyberstates 2021: The Definitive Guide to the Tech Industry and Workforce. Retrieved March 17, 2023, from https://www.comptia.org/content/tech-jobs-report↩︎\nCrunchbase News. (2023, January 5). Global VC Funding on a Slide since Q4 2022. Retrieved March 17, 2023, from https://news.crunchbase.com/venture/global-vc-funding-slide-q4-2022↩︎\nLayoffs.fyi. (n.d.). Layoffs.fyi - Tracking all tech startup layoffs since COVID-19. https://layoffs.fyi↩︎\nOpen Sourcing Mental Health (n.d.). About OSMH. Retrieved March 18, 2023, from https://osmhhelp.org/about/about-osmi.html↩︎"
  },
  {
    "objectID": "posts/Tyler_Tewksbury_Final1.html",
    "href": "posts/Tyler_Tewksbury_Final1.html",
    "title": "Final Project Part 1",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/Tyler_Tewksbury_Final1.html#background-and-research-question",
    "href": "posts/Tyler_Tewksbury_Final1.html#background-and-research-question",
    "title": "Final Project Part 1",
    "section": "Background and Research Question",
    "text": "Background and Research Question\nIn 2020, after the release of the Netflix series The Queen’s Gambit, interest in chess was at an all-time high. This led many fans of the show to use popular websites such as Chess.com and Lichess.org to begin learning the game. These websites use a rating system that mimics that of official in-person chess leagues, increasing your rating number as you win and decreasing as you lose. This can be used to measure one’s skill in chess, and determining if they can enter certain competitions.\nWhen playing chess online, as you face someone completely random that the website matches you against, there is no guarantee that you will play against someone with an identical rating. Thus, there will typically be a difference between the two players’ rating. Obviously the player with the higher rating would be more likely to win, right? That is where this study comes in. By quantifying the effect of rating difference on win chance, players may be able to understand more about the match they are currently in. Knowing how likely they are to win, how likely their opponent is to win, and this could lead to further interesting research about making the most fair chess matches possible. As there are not any academic studies on the topic, there is no proven indicator that a slight discrepancy in rating has is an indicator to a player’s win chance. This poses the research question:\nHow strong of a predictor is the difference between players chess rating in determining the victor?"
  },
  {
    "objectID": "posts/Tyler_Tewksbury_Final1.html#dataset",
    "href": "posts/Tyler_Tewksbury_Final1.html#dataset",
    "title": "Final Project Part 1",
    "section": "Dataset",
    "text": "Dataset\nThe dataset being used is sourced from Kaggle: https://www.kaggle.com/datasets/datasnaek/chess\nGathered in 2016, the dataset contains information from over 20,000 matches on Lichess.org via the Lichess API. Information on the players’, their opening moves, the results of the match, and more are all columns within the dataset.\n\n\nCode\n#reading in the dataset\nchess &lt;- read.csv(\"_data/chess_games.csv\")"
  },
  {
    "objectID": "posts/Tyler_Tewksbury_Final1.html#descriptive-statistics",
    "href": "posts/Tyler_Tewksbury_Final1.html#descriptive-statistics",
    "title": "Final Project Part 1",
    "section": "Descriptive Statistics",
    "text": "Descriptive Statistics\n\n\nCode\nstr(chess)\n\n\n'data.frame':   20058 obs. of  16 variables:\n $ id            : chr  \"TZJHLljE\" \"l1NXvwaE\" \"mIICvQHh\" \"kWKvrqYL\" ...\n $ rated         : chr  \"FALSE\" \"TRUE\" \"TRUE\" \"TRUE\" ...\n $ created_at    : num  1.5e+12 1.5e+12 1.5e+12 1.5e+12 1.5e+12 ...\n $ last_move_at  : num  1.5e+12 1.5e+12 1.5e+12 1.5e+12 1.5e+12 ...\n $ turns         : int  13 16 61 61 95 5 33 9 66 119 ...\n $ victory_status: chr  \"outoftime\" \"resign\" \"mate\" \"mate\" ...\n $ winner        : chr  \"white\" \"black\" \"white\" \"white\" ...\n $ increment_code: chr  \"15+2\" \"5+10\" \"5+10\" \"20+0\" ...\n $ white_id      : chr  \"bourgris\" \"a-00\" \"ischia\" \"daniamurashov\" ...\n $ white_rating  : int  1500 1322 1496 1439 1523 1250 1520 1413 1439 1381 ...\n $ black_id      : chr  \"a-00\" \"skinnerua\" \"a-00\" \"adivanov2009\" ...\n $ black_rating  : int  1191 1261 1500 1454 1469 1002 1423 2108 1392 1209 ...\n $ moves         : chr  \"d4 d5 c4 c6 cxd5 e6 dxe6 fxe6 Nf3 Bb4+ Nc3 Ba5 Bf4\" \"d4 Nc6 e4 e5 f4 f6 dxe5 fxe5 fxe5 Nxe5 Qd4 Nc6 Qe5+ Nxe5 c4 Bb4+\" \"e4 e5 d3 d6 Be3 c6 Be2 b5 Nd2 a5 a4 c5 axb5 Nc6 bxc6 Ra6 Nc4 a4 c3 a3 Nxa3 Rxa3 Rxa3 c4 dxc4 d5 cxd5 Qxd5 exd5 \"| __truncated__ \"d4 d5 Nf3 Bf5 Nc3 Nf6 Bf4 Ng4 e3 Nc6 Be2 Qd7 O-O O-O-O Nb5 Nb4 Rc1 Nxa2 Ra1 Nb4 Nxa7+ Kb8 Nb5 Bxc2 Bxc7+ Kc8 Qd\"| __truncated__ ...\n $ opening_eco   : chr  \"D10\" \"B00\" \"C20\" \"D02\" ...\n $ opening_name  : chr  \"Slav Defense: Exchange Variation\" \"Nimzowitsch Defense: Kennedy Variation\" \"King's Pawn Game: Leonardis Variation\" \"Queen's Pawn Game: Zukertort Variation\" ...\n $ opening_ply   : int  5 4 3 3 5 4 10 5 6 4 ...\n\n\nThe dataset contains 20058 observations across 16 variables.\n\n\nCode\nsummary(chess)\n\n\n      id               rated             created_at         last_move_at      \n Length:20058       Length:20058       Min.   :1.377e+12   Min.   :1.377e+12  \n Class :character   Class :character   1st Qu.:1.478e+12   1st Qu.:1.478e+12  \n Mode  :character   Mode  :character   Median :1.496e+12   Median :1.496e+12  \n                                       Mean   :1.484e+12   Mean   :1.484e+12  \n                                       3rd Qu.:1.503e+12   3rd Qu.:1.503e+12  \n                                       Max.   :1.504e+12   Max.   :1.504e+12  \n     turns        victory_status        winner          increment_code    \n Min.   :  1.00   Length:20058       Length:20058       Length:20058      \n 1st Qu.: 37.00   Class :character   Class :character   Class :character  \n Median : 55.00   Mode  :character   Mode  :character   Mode  :character  \n Mean   : 60.47                                                           \n 3rd Qu.: 79.00                                                           \n Max.   :349.00                                                           \n   white_id          white_rating    black_id          black_rating \n Length:20058       Min.   : 784   Length:20058       Min.   : 789  \n Class :character   1st Qu.:1398   Class :character   1st Qu.:1391  \n Mode  :character   Median :1567   Mode  :character   Median :1562  \n                    Mean   :1597                      Mean   :1589  \n                    3rd Qu.:1793                      3rd Qu.:1784  \n                    Max.   :2700                      Max.   :2723  \n    moves           opening_eco        opening_name        opening_ply    \n Length:20058       Length:20058       Length:20058       Min.   : 1.000  \n Class :character   Class :character   Class :character   1st Qu.: 3.000  \n Mode  :character   Mode  :character   Mode  :character   Median : 4.000  \n                                                          Mean   : 4.817  \n                                                          3rd Qu.: 6.000  \n                                                          Max.   :28.000  \n\n\nLooking at the summary, it is clear what variables will be used and if any new columns will be added. The following will prove relevance to the research question:\n\n`rated``\n`victory_status``\nwinner\nwhite_id\nwhite_rating\nblack_id\nblack_rating\n\nA new column containing the difference between the rating will be added in the next iteration for analysis. Having this added column will make the functions necessary for analysis easier, as calculating the difference will not need to be repeated for each observation.\n\nwhite_rating and black_rating\n\n\nCode\nrange(chess$white_rating)\n\n\n[1]  784 2700\n\n\nCode\nrange(chess$black_rating)\n\n\n[1]  789 2723\n\n\nThe ranges of the two sides are nearly identical, and are quite large nearing 2000. This could be both good and bad for the study, as the large range could prove significant, but it may be necessary to break the models into smaller ranges. This could also be interesting, perhaps seeing if the rating differences at a lower level matter more than that of a higher level, or vice versa."
  },
  {
    "objectID": "posts/Tyler_Tewksbury_Final1.html#proposed-models",
    "href": "posts/Tyler_Tewksbury_Final1.html#proposed-models",
    "title": "Final Project Part 1",
    "section": "Proposed Models",
    "text": "Proposed Models\nThe obvious model for this question will be a a linear probability regression, as the victory status is a binary variable. Proposed models initially are:\nLinear probability including unranked Linear probability excluding unranked\nThere will be more models, potentially differentiating between the different ranges as stated earlier. More possibilities include looking at exclusively drawn game data, analyzing favored openings depending on rank, or possibly finding other predictors if the rank difference is not significant."
  },
  {
    "objectID": "posts/HW2_EmmaNarkewicz.html",
    "href": "posts/HW2_EmmaNarkewicz.html",
    "title": "Homework 2",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.3.6      ✔ purrr   0.3.4 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.0      ✔ stringr 1.4.1 \n✔ readr   2.1.2      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()"
  },
  {
    "objectID": "posts/HW2_EmmaNarkewicz.html#a",
    "href": "posts/HW2_EmmaNarkewicz.html#a",
    "title": "Homework 2",
    "section": "A",
    "text": "A\nTest whether the mean income of female employees differs from $500 per week. Include assumptions, hypotheses, test statistic, and P-value. Interpret the result.\nAssumptions of a 1-sample t-test:\n\npopulation is normally distributed\nobservations in our sample are generated independently of one another\n\nInformation we are provided is that:\n\nPopulation mean (μ) = 500\nSample mean (y bar) = 410\nSample sd = 90 s\nSample n = 9\n\nNull Hypothesis: (H0) female mean income μ = 500 Alternative Hypothesis: (Ha) female mean income μ ≠ 500\nCalculate the t statistic using the equation:\nt = (y bar - μ ) / (sd estimate / sqrt(N))\n\n\nCode\n## Calculating tscore\n\nt =  (410 - 500) / (90/sqrt(9))\nt\n\n\n[1] -3\n\n\nThe t-score is -3\nTo solve for the p-value from the t-score I will use the pt() function, with a q = the t-statistic of -3, df = 8 (9-1).\nIt it is important to note that an assumption of the pt() function is you are looking for the probability of the lower tail, not both tails. Because the t score is negative looking at the lower tail is appropriate, but to get the probability for 2-tailed test we need to multiply the lower tail probability by 2.\n\n\nCode\n#Calculating p-value\n\np_lower_tail = pt(q = t, df =8, lower.tail = TRUE)\np_lower_tail\n\n\n[1] 0.008535841\n\n\nThis gives us the probability that μ is less than 500, but that is only 1 tail, so we multiply this problity by 2 to get the p value for our 2-sided t-test\n\n\nCode\n#Two-side t\np_twotail =  p_lower_tail * 2\np_twotail\n\n\n[1] 0.01707168\n\n\nWith a 2-tail p value of 0.017, we can confidently reject the null hypothesis that female salary μ = 500 as p&lt; 0.05. This suggests that the true mean salary of female workers is not equal to $500."
  },
  {
    "objectID": "posts/HW2_EmmaNarkewicz.html#b",
    "href": "posts/HW2_EmmaNarkewicz.html#b",
    "title": "Homework 2",
    "section": "B",
    "text": "B\nReport the P-value for Ha: μ &lt; 500. Interpret.\n\n\nCode\n#P-value Ha: μ &lt; 500\n\np_lower_tail = pt(q = t, df =8, lower.tail = TRUE)\n\np_lower_tail\n\n\n[1] 0.008535841\n\n\nWith a small p-value of 0.00854 we reject the null hypothesis that μ = 500, suggesting that the true mean salary of female workers is less than $500."
  },
  {
    "objectID": "posts/HW2_EmmaNarkewicz.html#c",
    "href": "posts/HW2_EmmaNarkewicz.html#c",
    "title": "Homework 2",
    "section": "C",
    "text": "C\nReport and interpret the P-value for Ha: μ &gt; 500.\n\n\nCode\n#P-value Ha: μ &gt; 500\n\np_upper_tail = pt(q = t, df =8, lower.tail = FALSE)\n\np_upper_tail\n\n\n[1] 0.9914642\n\n\nWith a large p-value of 0.99 we fail to reject the null hypothesis that μ = 500, suggesting that the true means salary of female workers is not greater than $500."
  },
  {
    "objectID": "posts/HW2_EmmaNarkewicz.html#a-1",
    "href": "posts/HW2_EmmaNarkewicz.html#a-1",
    "title": "Homework 2",
    "section": "A",
    "text": "A\nShow that Show that t = 1.95 and P-value = 0.051 for Jones. Show that t = 1.97 and P-value = 0.049 for Smith.\nBecause we are provided with standard error not standard deviation, I used the equation:\nt = (ȳ - μ) / se\n\n\nCode\n#Jones T score\n\nt_Jones = (519.5 - 500) / 10.0\nt_Jones\n\n\n[1] 1.95\n\n\nFor Jones the t score is indeed 1.95\nTo calculate the p-value, I will use pt(), q = 1.95, df = (1000-1) = 999). Because Jone’s t-score is positive, we look at the upper tail.\n\n\nCode\n#Calculate 1-tail p-value Jones\np_upper_tail_Jones = pt(q = t_Jones, df = 999, lower.tail = FALSE)\np_upper_tail_Jones\n\n\n[1] 0.02572777\n\n\nThe p value of the upper tail for Jones is 0.02572777, but because the alternative hypothesis is 2-tailed, it needs to be multiplied by 2\n\n\nCode\n#Calculating p_value Jones\np_Jones = p_upper_tail_Jones * 2\np_Jones\n\n\n[1] 0.05145555\n\n\nThis shows the p-value of Jones is indeed 0.51\n\n\nCode\n# Smith T score \nt_Smith = (519.7 - 500) / 10.0\nt_Smith\n\n\n[1] 1.97\n\n\nThe t-score for Smith is indeed 1.97. The p value is calculated the same way as for Jones, using pt() & the upper tail:\n\n\nCode\n#Calculate 1-tail p-value Smith\np_upper_tail_Smith = pt(q = t_Smith, df = 999, lower.tail = FALSE)\np_upper_tail_Smith\n\n\n[1] 0.02455713\n\n\n\n\nCode\n#Smith p=value\np_Smith = p_upper_tail_Smith * 2\np_Smith\n\n\n[1] 0.04911426\n\n\nSmith’s p-value does = 0.049"
  },
  {
    "objectID": "posts/HW2_EmmaNarkewicz.html#b-1",
    "href": "posts/HW2_EmmaNarkewicz.html#b-1",
    "title": "Homework 2",
    "section": "B",
    "text": "B\nUsing an alpha level = 0.05 for both\n\nJones’ p-value = 0.51 means a non-statistically significant result, where we fail to reject the null hypothesis that μ = 500\n\nSmith’s p=value = 0.49 means a statistically significant results, where we reject the null hypothesis that μ = 500"
  },
  {
    "objectID": "posts/HW2_EmmaNarkewicz.html#c-1",
    "href": "posts/HW2_EmmaNarkewicz.html#c-1",
    "title": "Homework 2",
    "section": "C",
    "text": "C\nThis example showcases how reporting the actual p-value is important, as in this case the p-values are very close between Jones & Smith, which are both right on the edge of significance (0.050 +/- 0.01). A p-value of 0.049 & a p-value of 0.0000000049 are both less than 0.05 & would both lead us to reject the null hypothesis, but the second p-value is highly significant while the first p-value is only barely significant at an alpha level of 0.05. Reporting the p-value itself gives readers & other researchers information to much better understand your statistical findings than just reporting if the p value is &gt; or &lt; 0.05 or if you do or do not reject H0."
  },
  {
    "objectID": "posts/FPchck_young.html",
    "href": "posts/FPchck_young.html",
    "title": "Final Project Check In",
    "section": "",
    "text": "Spring is the season when baseball starts. In Korea, baseball is also the most popular league that attracts the largest number of spectators among all professional sports, and I have been a big fan since I was young. As you know, baseball is a sport with countless numbers, and this data analysis is actually done a lot. I can’t think of an interesting way to use and learn various statistical techniques in the subject of quantity analysis than using the sport of baseball. Therefore, I chose baseball as the my final project topic."
  },
  {
    "objectID": "posts/FPchck_young.html#key-concepts-and-operational-definitions",
    "href": "posts/FPchck_young.html#key-concepts-and-operational-definitions",
    "title": "Final Project Check In",
    "section": "Key Concepts and Operational Definitions",
    "text": "Key Concepts and Operational Definitions\nA baseball team’s score is simply available. However, it is necessary to first determine how to define the manager’s intervention. There are various roles that managers can play in baseball’s offense. They include determining the batting order, using pinch-hitters, bunting, etc. Although it has not been confirmed yet, the director’s intervention in this project will be determined by the number of attempts to steal and bunt in consideration of the ease of obtaining data. In summary, the independent variable is the baseball manager’s intervention in the game(offense), and the dependent variable is the team’s score."
  },
  {
    "objectID": "posts/FPchck_young.html#how-to-test-hypothesis",
    "href": "posts/FPchck_young.html#how-to-test-hypothesis",
    "title": "Final Project Check In",
    "section": "How to test hypothesis",
    "text": "How to test hypothesis\nThere are many ways to do this. First of all, it is possible to simply compare the average score between teams with a lot of manager intervention and teams with less manager intervention. However, this simple comparison may not take into account the team’s differences in offensive power. In other words, if a team with strong offensive power (i.e., a team with the ability to score more points without the manager’s intervention) had more coach intervention, the conclusion could be distorted. Therefore, considering this, a method of obtaining the expected score level of each team using a regression model and comparing how much the actual score was can be used."
  },
  {
    "objectID": "posts/Final_Project_check_in_3_Diana_Rinker.html#final-project-check-in-2-diana-rinker.",
    "href": "posts/Final_Project_check_in_3_Diana_Rinker.html#final-project-check-in-2-diana-rinker.",
    "title": "Final Project check-in 2",
    "section": "Final Project check-in 2, Diana Rinker.",
    "text": "Final Project check-in 2, Diana Rinker."
  },
  {
    "objectID": "posts/Final_Project_check_in_3_Diana_Rinker.html#popularity",
    "href": "posts/Final_Project_check_in_3_Diana_Rinker.html#popularity",
    "title": "Final Project check-in 2",
    "section": "1.1.Popularity",
    "text": "1.1.Popularity\n\nPage views\nThis is the most general metric, representing how many views the post received. Views do not distinguish repeated views by the same person.\n\n\nCode\n# str(merged)\nggplot(data=merged, mapping=aes(x=`Page views`))+\n  geom_histogram()\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\nCode\nmerged$post.month &lt;-as.numeric(merged$post.month)\nmerged$year_month &lt;- paste0(merged$post.year, \"-\", sprintf(\"%02d\", merged$post.month))\n\nggplot( data=merged, mapping=aes(y=`Page views`, x=year_month))+\n          geom_boxplot()+\n  labs(title=\"Number of post wiews per month\", x=\"Month\", y=\"Number of vews\")+ \n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\nCode\n# ggplot(data=merged, mapping=aes(x=log(`Page views`)))+\n#   geom_histogram()\n\n\nFrom this graph we can see that the numbers of views is increased over time. To get a better understanding of it, lets review other metrics.\n\n\nPage visits and unique users\nVisit is an instance of a user engaging with bdc website. The user can visit a few pages of the site and re-visit them, creaing few views in once visit.Once th user left the website, the visit is over.\nThere also can be many visits by the same viewer. To account for repeated users there is “Uniques” variable. It tells us how many unique users saw the post. The relationship will always be that\nUniques &lt; Visits &lt; Views\nLets plot them together:\n\n\nCode\n# str(merged)\nggplot(merged, aes(x =post.date )) +\n     geom_bar(aes(y = `Page views`, fill = \"Page views\"), stat = \"identity\", position = \"dodge\", width = 0.6) +\n     geom_bar(aes(y = Visits, fill = \"Visits\"), stat = \"identity\", position = \"dodge\", width = 0.6) +\n     geom_bar(aes(y = Uniques, fill = \"Unique users\"), stat = \"identity\", position = \"dodge\", width = 0.6)    +\n     labs(title = \"Views, visits and unique users per month\", x = \"Post.date\", y = \"Value\") + \n     theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\nThis graph also shows increase in all three metrics in 2022-2023. To see how these metrics crrespond to each other, I will calculate “visits ratio” and “unique.ratio”.\n\n\nPage unique viewers.\nBecuse “Uniques” variable represents number of unique people who came to the page and viewed it at least once, his metric represents popularity of the post. It’s distribution shows us that not all posts are equally popular:\n\n\nCode\n# colnames(merged)\nggplot(data=merged, mapping=aes(x=Uniques))+\n  geom_histogram()\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nWe can see a long tail on the right, showing that there is a number of posts who are way more popular. If we look at the distribution of this variable over time, we will see significant increase of average popularity after December 2021.\n\n\nCode\nggplot( data=merged, mapping=aes(y=Uniques, x=merged$year_month))+\n          geom_boxplot()+\n  labs(title=\"Number of unique viewers per month\", x=\"Month\", y=\"Number of unique viewers\")+ \n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\nWarning: Use of `merged$year_month` is discouraged.\nℹ Use `year_month` instead."
  },
  {
    "objectID": "posts/Final_Project_check_in_3_Diana_Rinker.html#engagement-metrics",
    "href": "posts/Final_Project_check_in_3_Diana_Rinker.html#engagement-metrics",
    "title": "Final Project check-in 2",
    "section": "1.2. Engagement metrics:",
    "text": "1.2. Engagement metrics:\n\nVisit ratio and unique ratio:\n“visits ratio” is the number of visits per total views. This metric tells us how often the page being re-viewed. The lower the ratio means that the page was viewed more times during single visit. Since repeated view can be considered a higher engagement, the lower visit ratio indicates higher engagement.\n“unique ratio” - number of unique users per total views. It is telling us how often the same person re-visited the post. Lower ratio indicates repeated users, therefore higher engagement.\nBoth ratios are always in the range of 0 - 1.\n\n\nCode\n  merged &lt;- merged %&gt;%\n     mutate(uniques.ratio = Uniques / `Page views`)%&gt;%\n      mutate(visits.ratio = Visits / `Page views`)\n# str(merged)\n\n     ggplot(merged, aes( y = uniques.ratio, x =post.date, )) +\n          geom_point(color =\"green\" )    +\n          labs(title = \"Unique.ratio per post \", x = \"Post.date\", y = \"Value\") + \n          theme(axis.text.x = element_text(angle = 45, hjust = 1))  \n\n\n\n\n\nCode\n      ggplot(merged, aes( y = visits.ratio, x =post.date, )) +\n          geom_point(color =\"blue\" )    +\n          labs(title = \"Visits.ratio per post \", x = \"Post.date\", y = \"Value\") + \n          theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\nFrom the two graphs above we can see that the two variables are distributed very similarly.\nIf we plot them against each other and visualize their correlation, we can see that they highly correlated especially in higher values.\n\n\nCode\n      plot(merged$uniques.ratio, merged$visits.ratio)\n\n\n\n\n\nCode\ncorrelation &lt;- cor(merged$uniques.ratio, merged$visits.ratio)\ncorrelation\n\n\n[1] 0.915789\n\n\nDensity of the distribution also showing that most posts are visited once and by unique users, i.e. post is read once by each user.\n\n\nCode\n# DENSITY DISTRIBUTION HERE \n\n\n\n\nExit rate\nThis variable is measuring how many people visited the page and then left the website after the first view. This metric is the best measure of engagement for all users, as it represents the first step after being exposed to the post - either quitting the site or remaining on the site.\nHere can see the distribution of this variable :\n\n\nCode\n# str(merged)\nmerged$`Exit rate` &lt;- as.numeric(sub(\"%\", \"\", merged$`Exit rate`)) / 100\n\nggplot(data=merged, mapping=aes(x=`Exit rate`))+\n  geom_histogram()\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nWe can also see consistent increase of this value ovr time, similar to the trend seen before:\n\n\nCode\nggplot(merged, mapping = aes(x=year_month , y=`Exit rate`, fill=year_month ))+\n  geom_boxplot() +\n  labs(title = \"distribution of `Exit rate` per post \", y = \"Exit rate\" , x=\"Month\")+ \n     theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\nNumber of reactions\nThis represents total amount of likes and dislikes of comments per post.To like/dislike the comment, user doesn’t have to log in.\n\n\nCode\n# merged$year_month \n# colnames(merged)\n\nggplot(data=merged, mapping=aes(x=post.total.likes))+\n  geom_histogram()+\n  labs(title=\" Number of all reactions per post\")\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nSince we already saw that there is large variability in popularity of posts, which can natually impact absolute amount of reactions. To account for number of views, we calculate reactions.ratio:\n\n\nCode\n  merged &lt;- merged %&gt;%\n     mutate(reactions.ratio = post.total.likes / `Page views`)\n  \n# str(merged)\nggplot(merged, mapping = aes(x=year_month , y=reactions.ratio, fill=year_month ))+\n  geom_boxplot() +\n  labs(title = \"distribution of reactions \", y = \"Percentage of reactions \" )+\n  scale_y_continuous(breaks = seq (from=0, to= 10000, by= 100)) + \n     theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\nThe absolute number of reactions and the ratio appear to decrease over time.\n\n\nNumber of comments: engagement metric for subset of readers. Absolute and per views.\nTo comment on the requires logging in from a user, which is an indicator of greater engagement of an individual user. Therefore this variable represents engagement of a subset of users - more loyal readers who have created an account.\n\n\nCode\n# merged$year_month \nggplot(data=merged, mapping=aes(x=n.comments))+\n  geom_histogram()\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nAnd change in the distribution over time:\n\n\nCode\nggplot(merged, mapping = aes(x=year_month , y=n.comments, fill=year_month ))+\n  geom_boxplot() +\n  labs(title = \"distribution of comments per post \", y = \"Number of comments\" )+\n  scale_y_continuous(breaks = seq (from=0, to= 10000, by= 100)) + \n     theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\nThe trend in amount of comments over time is different. While previously reviewed variables seemed to increase over time, this variable decreases.\nWe can calculate the ratio of comments per If we compare absolute values fo comments and ratio of comments per page views, we see the decrease in both cases, while the ratio’s decrease is more dramatic:\n\n\nCode\n  merged &lt;- merged %&gt;%\n     mutate(comments.ratio = n.comments/ `Page views`)\n\nggplot(merged, mapping = aes(x=year_month , y=comments.ratio, fill=year_month ))+\n  geom_boxplot() +\n  labs(title = \"distribution of comments ratio (comments/page views) \", y = \"Number of comments\" )+\n  scale_y_continuous(breaks = seq (from=0, to= 10000, by= 100)) + \n     theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\nWE can see that with an increase of views (mostly driven by post popularity), comments ratio decreeases. So, newly added viewers are not adding to the engaged subset of readers. \nThe distribution of comments per post over time:\n\n\nCode\n# str(merged)\nggplot(merged, mapping = aes(x=year_month , y=post.total.likes, fill=year_month ))+\n  geom_boxplot() +\n  labs(title = \"distribution of reactions over time  \", y = \"Percentage of reactions \" )+\n  scale_y_continuous(breaks = seq (from=0, to= 10000, by= 100)) + \n     theme(axis.text.x = element_text(angle = 45, hjust = 1))"
  },
  {
    "objectID": "posts/Final_Project_check_in_3_Diana_Rinker.html#impact-of-popularity-on-engagement.",
    "href": "posts/Final_Project_check_in_3_Diana_Rinker.html#impact-of-popularity-on-engagement.",
    "title": "Final Project check-in 2",
    "section": "1.3 Impact of popularity on engagement.",
    "text": "1.3 Impact of popularity on engagement.\nSince we noticed a pattern in distribution of describbed variables, lets find out how they are interconnected, and how popularity of the post impact users’ engagement:\n\n\nCode\npairs(subset (merged, select=c(Uniques, `Exit rate`, uniques.ratio, visits.ratio, reactions.ratio )))\n\n\n\n\n\nThis graph showing that popularity is strongly correlated with all engagement metrics, such as Exit rate, uniques ratio, visit ratio and reaction’s ratio, where an increase in popularity causes decrease of engagement across all 4 metrics for engagement.\nWe can also see that the relationship between Popularity and engagement rate are curvi-linear. If we log the values, we will see linear relationship:\n\n\nCode\npairs(subset (merged, select=c(log(Uniques), log(`Exit rate`), uniques.ratio, visits.ratio, reactions.ratio )))\n\n\nError in `x[r, vars, drop = drop]`:\n! Can't subset columns with `vars`.\n✖ Can't convert from `j` &lt;double&gt; to &lt;integer&gt; due to loss of precision.\n\n\nCode\n# WHT DOES THE GRAPH SHOWS POST.DATE and POST.DATE1  INSTEAD OF \"UNIQUES\" and  \"Exit rate\"? \n\n\nAs we saw in the distributions of variables above, all engagement metrics are strongly correlate with each other the popularity metric.\n\n1.3.1. Modeling engagement ~ popularity:\nLets review how it is impacting exit rate : I will use “Exit rate” s my main engagement variable, as it reporesents the first engagement chouce that all site users make: to stay on the site or leave. Using log() of popularity is shoing a better fitted model:\n\n\nCode\nggplot(merged, mapping=aes(x=log(Uniques), y=`Exit rate` ))+\n  geom_point()\n\n\n\n\n\nCode\nsummary(lm(`Exit rate`~ + log(Uniques) , data = merged))\n\n\n\nCall:\nlm(formula = `Exit rate` ~ +log(Uniques), data = merged)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.259486 -0.016818  0.001917  0.019850  0.073240 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -0.332581   0.027678  -12.02   &lt;2e-16 ***\nlog(Uniques)  0.113870   0.002855   39.89   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.03073 on 533 degrees of freedom\nMultiple R-squared:  0.749, Adjusted R-squared:  0.7486 \nF-statistic:  1591 on 1 and 533 DF,  p-value: &lt; 2.2e-16\n\n\nWe can see significance of popularity and high R^2 of the model, and can conclude that popularity overall decreases engagement. To explore this connection further, lets mreview Sources of users that are coming to the website.\n\n\n1.3.2. How popularity impacts engagement: Referral sources.\nThe website analytics provides information on where the viewers are coming from to the blog page. for example, if people clicked on the blog link posted on FaceBook, that would be referral from social media. If people clicked on the blog link within BDC website, that would be “BDC referral visit”.\nThere are 5 sources of referrals, each corresponding with a variable in the data set. Variable’s value is a number of visits from this referral source.\n    \"Search + amp referral visits\"\n    \"Direct (non-email) referral visits\"\n    \"Other website referral visits\"\n    \"Social referral visits\"\n    \"BDC referral visits\"\n    \"Visits when post was on LL HP\" \n\n\nCode\n#renaming variables for convenience: \nmerged&lt;-merged%&gt;%\n  rename(google =\"Search + amp referral visits\",\n         direct =\"Direct (non-email) referral visits\",\n         other.web = \"Other website referral visits\",\n          social= \"Social referral visits\",\n          bdc= \"BDC referral visits\",\n          ll= \"Visits when post was on LL HP\" )\n\nggplot(merged, mapping=aes(x=post.date))+\n  geom_point(aes(y=google), color=\"red\")+\n  geom_point(aes(y=direct), color=\"green\")+\n  geom_point(aes(y=other.web), color=\"yellow\")+\n  geom_point(aes(y=social), color=\"purple\")+\n  geom_point(aes(y=bdc), color=\"blue\")+\n  geom_point(aes(y=ll), color=\"pink\")  +\n  labs(title = \"referral sources per post \", y = \"Number of referrals\" , x=\"Post\")+ \n     theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\nThis graph clearly demonstrates increase of “Search + amp referral visits” after December 2021, while other referral sources maintain the same level over time. This increase matches changes observed in popularity ( Page Views), and engagement metrics that are highly correlated with popularity. It also doubles views of the posts in many cases and might significanly influence our model and conclusions.\n\n\nEarly comments’impact\nWe saw that increased popularity is associated with the sources of the viewers and might come from the search engines. We want to find out if early engagement in the post ( comments within first 10 hours of the post time) trigger its appearance in google news listing and therefore cause sharp increase in popularity.\nWe are interested to find out, what characteristics of the post (prior to it being picked up by google), correlate with its appearance on google news.\nNumber of comments within 1st 10 hours from the post.\n\n\nCode\n#  calculating number of comments within first 10 hours \n\n# date and time of the post \npost.date  &lt;-comments.data %&gt;%\n  group_by(post_id)%&gt;%\n  arrange(written_at)%&gt;%\n  summarize(post.dt = first(written_at), \n            n.coms =n())%&gt;%\n  filter (n.coms&gt;100)%&gt;%\n  select(post_id, post.dt, n.coms)%&gt;%\narrange(desc(post_id))\n# head(post.date)\n\n# Calculating age of the post \ncomments.data &lt;- merge( comments.data , post.date , by = \"post_id\", all = TRUE)\ncomments.data &lt;-comments.data %&gt;%\n  mutate (age = difftime(  written_at,post.dt, units = \"hours\"), \n          early = ifelse(age&lt;10, 1, 0))\n\nearly.coms  &lt;-comments.data%&gt;%\n  group_by(post_id)%&gt;%\n  arrange(written_at)%&gt;%\n  summarize(early.sum = sum(early), \n            n.coms =n())%&gt;%\n  filter (n.coms&gt;100)%&gt;%\n  select(post_id, early.sum)%&gt;%\narrange(desc(post_id))\n\n# Adding number of early comments to merged dataset:\nmerged &lt;- merge( merged, early.coms , by = \"post_id\", all = TRUE)\n\nhead(merged )\n\n\n   post_id  post.date\n1 27067757 2023-03-01\n2 27067763 2023-02-28\n3 27067769 2023-02-27\n4 27067775 2023-02-24\n5 27067781 2023-02-23\n6 27067787 2023-02-22\n                                                                             Letter\n1                          love letters | blog | We&'ve gained weight – as a couple\n2           love letters | blog | She was a friendly ghost, but I was still ghosted\n3 love letters | blog | I waited for her to initiate sex – and it ended my marriage\n4                         love letters | blog | My husband doesn&'t share his money\n5                              love letters | blog | My wife won&'t go back to work\n6                                love letters | blog | He doesn&'t have time for me\n  Page views google direct Visits Uniques other.web social   bdc   ll Exits\n1      24890  10407  10180  22766   20765       349     56  7088 2785 18547\n2      15575   1210  10297  13597   11677       214     85  7461 2832 10127\n3      28594   7027  15609  26115   23914       909     87 12957 3016 22464\n4      72718  43419  19921  67007   62122       311    134 14106 4216 60915\n5      67537  43173  16863  62888   59272       310    152 12344 2739 56929\n6      28778  14466   9698  26269   23865       331     67  6332 3003 21515\n  Exit rate dup n.comments post.year post.month post.likes post.dislikes\n1      0.81   0        208      2023          3       1001           167\n2      0.74   0        264      2023          2       1908           118\n3      0.86   0        241      2023          2       1195           167\n4      0.91   0        321      2023          2       1921           341\n5      0.91   0        294      2023          2       1445           549\n6      0.82   0        182      2023          2       1337            85\n  post.total.likes blocked.sum pct.positive year_month uniques.ratio\n1             1168           1     85.70205    2023-03     0.8342708\n2             2026           1     94.17572    2023-02     0.7497271\n3             1362           1     87.73862    2023-02     0.8363293\n4             2262           3     84.92485    2023-02     0.8542864\n5             1994           0     72.46740    2023-02     0.8776226\n6             1422           0     94.02250    2023-02     0.8292793\n  visits.ratio reactions.ratio comments.ratio early.sum\n1    0.9146645      0.04692648    0.008356770       192\n2    0.8730016      0.13008026    0.016950241       247\n3    0.9133035      0.04763237    0.008428342       214\n4    0.9214637      0.03110647    0.004414313       261\n5    0.9311637      0.02952456    0.004353169       240\n6    0.9128153      0.04941275    0.006324275       142\n\n\nNow, as I calculated number of early comments for all posts, I can see if that number correlated with Google referrals.\n\n\nCode\nmerged.2&lt;-merged%&gt;%\n  filter(google&gt;10000)\ncorrelation.2 &lt;- cor(merged.2$early.sum, merged.2$google)\ncorrelation.2\n\n\n[1] 0.2736053\n\n\nCode\nplot(merged.2$early.sum, merged.2$google)\n\n\n\n\n\n\n\nCode\nsummary(lm(google ~early.sum, data=merged.2))\n\n\n\nCall:\nlm(formula = google ~ early.sum, data = merged.2)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-15035  -6141  -2482   3559  54900 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)  5908.84    5265.21   1.122  0.26406   \nearly.sum      94.56      30.73   3.077  0.00261 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 10600 on 117 degrees of freedom\nMultiple R-squared:  0.07486,   Adjusted R-squared:  0.06695 \nF-statistic: 9.467 on 1 and 117 DF,  p-value: 0.002605\n\n\nCode\nsummary(lm(google ~early.sum, data=merged))\n\n\n\nCall:\nlm(formula = google ~ early.sum, data = merged)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n -7036  -5548  -4652   2036  68716 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 8921.388   1789.530   4.985 8.38e-07 ***\nearly.sum    -12.627      9.921  -1.273    0.204    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 9806 on 533 degrees of freedom\nMultiple R-squared:  0.00303,   Adjusted R-squared:  0.001159 \nF-statistic:  1.62 on 1 and 533 DF,  p-value: 0.2037\n\n\nWe can see significant connection between early comments and google referrals, only on the data set of google referrals above 10000 visits. Also, R^2 is low in this model, which suggests that there are other factors that are not considered in this model. I will explore how days of the week and time of the post contribute to this relationship.\n\n\nCode\n# calculating day of the week for the post: \ncolnames(comments.data)\n\n\n [1] \"post_id\"           \"content\"           \"message_id\"       \n [4] \"user_id\"           \"user_name\"         \"display_name\"     \n [7] \"image_url\"         \"email\"             \"email_verified\"   \n[10] \"created_at\"        \"private_profile\"   \"approved\"         \n[13] \"written_at\"        \"parent\"            \"absolute_likes\"   \n[16] \"absolute_dislikes\" \"comment.year\"      \"com.year\"         \n[19] \"post.dt\"           \"n.coms\"            \"age\"              \n[22] \"early\"            \n\n\nCode\n# Calculating age of the post \nlibrary(lubridate)\n\n\n\nAttaching package: 'lubridate'\n\n\nThe following objects are masked from 'package:base':\n\n    date, intersect, setdiff, union\n\n\nCode\ncomments.data &lt;-comments.data %&gt;%\n  mutate (weekday = wday(post.dt, label = TRUE), \n          weekend = ifelse(weekday ==\"Fri\" | weekday == \"Sat\", 1, 0))\n\n# class(week.days$weekday)\n# levels(week.days$weekday) \n# week.days$weekend\n\nweekdays &lt;-comments.data %&gt;%\n  group_by(post_id)%&gt;%\n  arrange(written_at)%&gt;%\n  summarize(post.weekday = first(weekday), \n            post.weekend =first(weekend),\n            n.coms =n())%&gt;%\n  filter (n.coms&gt;100)%&gt;%\n  select(post_id, post.weekday,post.weekend )%&gt;%\narrange(desc(post_id))\n\n# Adding number of early comments to merged dataset:\nmerged &lt;- merge( merged, weekdays , by = \"post_id\", all = TRUE)\ncolnames(merged)\n\n\n [1] \"post_id\"          \"post.date\"        \"Letter\"           \"Page views\"      \n [5] \"google\"           \"direct\"           \"Visits\"           \"Uniques\"         \n [9] \"other.web\"        \"social\"           \"bdc\"              \"ll\"              \n[13] \"Exits\"            \"Exit rate\"        \"dup\"              \"n.comments\"      \n[17] \"post.year\"        \"post.month\"       \"post.likes\"       \"post.dislikes\"   \n[21] \"post.total.likes\" \"blocked.sum\"      \"pct.positive\"     \"year_month\"      \n[25] \"uniques.ratio\"    \"visits.ratio\"     \"reactions.ratio\"  \"comments.ratio\"  \n[29] \"early.sum\"        \"post.weekday\"     \"post.weekend\"    \n\n\nCode\n# levels(merged$post.weekday)\n\n\n\n\nPost weekdays to the model\n\n\nCode\nmerged.2&lt;-merged %&gt;%\n  filter(google&gt;10000)\n\n plot(merged$post.weekday, merged$google)\n\n\n\n\n\nCode\nsummary(lm(log(google) ~early.sum +post.weekday , data=merged.2))\n\n\n\nCall:\nlm(formula = log(google) ~ early.sum + post.weekday, data = merged.2)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.85556 -0.25080 -0.05083  0.23281  1.25348 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)     9.30191    0.20350  45.710  &lt; 2e-16 ***\nearly.sum       0.00352    0.00119   2.958  0.00377 ** \npost.weekday.L  0.01571    0.08732   0.180  0.85751    \npost.weekday.Q -0.02154    0.08544  -0.252  0.80145    \npost.weekday.C -0.02274    0.08280  -0.275  0.78409    \npost.weekday^4  0.15962    0.08043   1.985  0.04962 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4058 on 113 degrees of freedom\nMultiple R-squared:  0.102, Adjusted R-squared:  0.06229 \nF-statistic: 2.568 on 5 and 113 DF,  p-value: 0.03066\n\n\nCode\n# table(merged.weekdays$post.weekday)\n\nggplot (merged, mapping =aes(x=early.sum, y=google, color =post.weekday))+\n  geom_point( ) +\n  geom_smooth(method=\"lm\")+\n  labs(title = \"All google referrals and early comments\", y = \"Google referrals\" , x=\"Early comments\")+ \n     theme(axis.text.x = element_text(angle = 45, hjust = 1))+\n  facet_wrap(~post.weekday)\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nCode\nggplot (merged.2, mapping =aes(x=early.sum, y=google, color =post.weekday))+\n  geom_point( ) +\n  geom_smooth(method=\"lm\")+\n  labs(title = \"Google referrals over 10K and early comments\", y = \"Google referrals\" , x=\"Early comments\")+ \n     theme(axis.text.x = element_text(angle = 45, hjust = 1))+\n  facet_wrap(~post.weekday)\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nEarly comments indeed influence Google referrals. However, this influence is different on different days of the week and appears more pronounced on Thursdays and Fridays. This overall supports our suggestion that early engagement of loyal users through comments impacts post popularity. further investigation of factors would be helpful, with the goal to model fit (R^2).\nOur hypothesis that large amount of early comments triggers google referrals has some preliminary support with the few restrictiions to be considere: - the connection only appears on the posts with google referrals above 10000. - This connection is atronger for only thursdays and Fridays and appears very weak on other days of the week. - low coefficient and high deviation of data from predicated valaues (low R^2) suggest that there are other , more influential, factors that were not considered."
  },
  {
    "objectID": "posts/Final_Project_check_in_3_Diana_Rinker.html#conclusion",
    "href": "posts/Final_Project_check_in_3_Diana_Rinker.html#conclusion",
    "title": "Final Project check-in 2",
    "section": "1.4.Conclusion",
    "text": "1.4.Conclusion\nAs we reviewed a variety of engagement metric, I found: 1. That they are all strongly correlated to each other and influenced by post popularity: Engagement overall decreases as popularity increases.In our further analysis I will need to include “Uniques” variable in the model to control for popularity\n\nAlso as a result of exploration of engagement metrics, we can distinguish two types of users: superficial visitors and loyal readers. The main difference is that loyal readers have created accounts and therefore can comment. It is possible that htese two groups of readers respond differently to popularity increase, as well as other Independent variables. Therefore, the difference in types of readers must be considered when addressing our main hypothesi: whether authors engagement and other factors impacts reader’s engagement.\nEarly engagement of loyal users through comments might contribute to dramatic changes in post popularity.\n\nNow, lets consider independent variables of our main hypothesis. # 2. Independent variables (IV):\n\n\nCode\nlibrary(stargazer)\n\n\n\nPlease cite as: \n\n\n Hlavac, Marek (2022). stargazer: Well-Formatted Regression and Summary Statistics Tables.\n\n\n R package version 5.2.3. https://CRAN.R-project.org/package=stargazer \n\n\nCode\nstargazer()\n\n\nError in if (substr(inside[i], 1, nchar(\"list(\")) == \"list(\") {: missing value where TRUE/FALSE needed\n\n\n\nAuthors comments\nTo identify, how much the author of the blog is engaged in the post, I will create an additional variable derived from a user_name field.\n\n\nCode\n# str(merged)\ncomments.data$user_name&lt;-  ifelse (is.na(comments.data$user_name), 0, comments.data$user_name)\ncomments.data$author&lt;-  ifelse (comments.data$user_name==\"MeredithGoldstein\", 1, 0)\n\ncomments.grouped &lt;-comments.data %&gt;%\n  group_by(post_id)%&gt;%\n  summarize(n.comments=n(),\n            author.sum = sum(author))\n\n# dim(comments.grouped )\n# colnames(comments.grouped )\n# class(comments.grouped$author.sum)\n\n# Comments data contains rows that dont actually reporesent posts, and were crearted by web support team for troubleshooting. I need to remove these rows. They typically have very low number of comments\ncomments.grouped &lt;-comments.grouped %&gt;%\nfilter(n.comments &gt;100)  # removing invalid posts created by the  website management team.\n\ncomments.grouped &lt;-comments.grouped %&gt;%\n  select(post_id, author.sum)\n dim(comments.grouped)\n\n\n[1] 535   2\n\n\nCode\n#adding author.sum to main data set: \nmerged &lt;- merge( merged , comments.grouped, by = \"post_id\", all = TRUE)\n\n\nggplot(data=merged, mapping = aes(x=year_month , y=author.sum, fill=year_month ))+\n  geom_boxplot() +\n  labs(title = \"distribution of Author's comments by months\", y = \"Author's comments\" , x=\"Month\")+ \n     theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\nThis graph shows, that majority of posts have no author’s comments. However, if we look at the box plots’ upper ranges, we can suggest a trend of decrease in author’s comments with time.\n\n\nMood of the post.\nThis is a numerical variable, calculated as percentage of “thumbs up” from all likes (both “thumbs up” and “thumbs down”).\n\n\nCode\nggplot(merged, mapping = aes(x=year_month , y=pct.positive, fill=year_month ))+\n  geom_boxplot() +\n  labs(title = \"distribution of Mood per post \", y = \"Mood\" , x=\"Month\")+ \n     theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\nBlocked comments per post.\nNow I will visualize amount of blocked comments per post:\n\n\nCode\nggplot(merged, mapping = aes(x=year_month , y=blocked.sum, fill=year_month ))+\n  geom_boxplot() +\n  labs(title = \"Number Blocked comments  per post \", y = \"Number of blocked comments per post\" , x=\"Month\")+ \n     theme(axis.text.x = element_text(angle = 45, hjust = 1))"
  },
  {
    "objectID": "posts/Final_Project_check_in_3_Diana_Rinker.html#basic-model.",
    "href": "posts/Final_Project_check_in_3_Diana_Rinker.html#basic-model.",
    "title": "Final Project check-in 2",
    "section": "3.1 Basic model.",
    "text": "3.1 Basic model.\nI will start with creating a simple model of engagement ~ popularity and author’s engagement:\n\n\nCode\n# colnames(merged)\nsummary(lm(`Exit rate` ~ log(Uniques)+ author.sum, data = merged))\n\n\n\nCall:\nlm(formula = `Exit rate` ~ log(Uniques) + author.sum, data = merged)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.257486 -0.016558  0.001669  0.019958  0.072030 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -0.326263   0.027749 -11.758   &lt;2e-16 ***\nlog(Uniques)  0.113326   0.002857  39.663   &lt;2e-16 ***\nauthor.sum   -0.003389   0.001602  -2.116   0.0348 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.03063 on 532 degrees of freedom\nMultiple R-squared:  0.7511,    Adjusted R-squared:  0.7502 \nF-statistic: 802.9 on 2 and 532 DF,  p-value: &lt; 2.2e-16\n\n\nCode\nsummary(lm(`Exit rate` ~ log(Uniques)+ author.sum, data = merged.2021))\n\n\n\nCall:\nlm(formula = `Exit rate` ~ log(Uniques) + author.sum, data = merged.2021)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.257995 -0.018855  0.000548  0.027265  0.069625 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -0.402793   0.096359  -4.180 4.18e-05 ***\nlog(Uniques)  0.121731   0.010266  11.857  &lt; 2e-16 ***\nauthor.sum   -0.002533   0.002568  -0.986    0.325    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.0372 on 224 degrees of freedom\nMultiple R-squared:  0.3856,    Adjusted R-squared:  0.3801 \nF-statistic:  70.3 on 2 and 224 DF,  p-value: &lt; 2.2e-16\n\n\nCode\nsummary(lm(`Exit rate` ~ log(Uniques)+ author.sum, data = merged.2022))\n\n\n\nCall:\nlm(formula = `Exit rate` ~ log(Uniques) + author.sum, data = merged.2022)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.083434 -0.014968  0.002043  0.017982  0.053266 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -0.354200   0.029528 -11.995  &lt; 2e-16 ***\nlog(Uniques)  0.115989   0.002974  38.996  &lt; 2e-16 ***\nauthor.sum   -0.005402   0.001997  -2.705  0.00722 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.02455 on 305 degrees of freedom\nMultiple R-squared:  0.8369,    Adjusted R-squared:  0.8359 \nF-statistic: 782.7 on 2 and 305 DF,  p-value: &lt; 2.2e-16\n\n\nTo see how engagement of loyal readers readers impacted , I will use another measure for engagement as DV, which represents only registered users: number of comments.\n\n\nCode\nsummary(lm(n.comments ~ log(Uniques) + author.sum , data = merged))\n\n\n\nCall:\nlm(formula = n.comments ~ log(Uniques) + author.sum, data = merged)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-98.670 -35.171  -8.842  26.596 181.918 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   238.521     45.076   5.292 1.78e-07 ***\nlog(Uniques)   -4.667      4.641  -1.006   0.3151    \nauthor.sum      4.872      2.602   1.872   0.0617 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 49.75 on 532 degrees of freedom\nMultiple R-squared:  0.009115,  Adjusted R-squared:  0.00539 \nF-statistic: 2.447 on 2 and 532 DF,  p-value: 0.08752\n\n\nCode\nsummary(lm(n.comments ~ log(Uniques) + author.sum , data = merged.2021))\n\n\n\nCall:\nlm(formula = n.comments ~ log(Uniques) + author.sum, data = merged.2021)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-103.136  -39.312   -9.017   29.162  167.046 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)   143.700    145.619   0.987    0.325\nlog(Uniques)    6.956     15.515   0.448    0.654\nauthor.sum      4.701      3.881   1.211    0.227\n\nResidual standard error: 56.21 on 224 degrees of freedom\nMultiple R-squared:  0.007832,  Adjusted R-squared:  -0.001026 \nF-statistic: 0.8841 on 2 and 224 DF,  p-value: 0.4145\n\n\nCode\nsummary(lm(n.comments ~ log(Uniques) + author.sum , data = merged.2022))\n\n\n\nCall:\nlm(formula = n.comments ~ log(Uniques) + author.sum, data = merged.2022)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-79.852 -29.930  -6.484  23.930 136.369 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    17.613     48.397   0.364 0.716156    \nlog(Uniques)   16.647      4.875   3.415 0.000725 ***\nauthor.sum      2.245      3.273   0.686 0.493388    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 40.23 on 305 degrees of freedom\nMultiple R-squared:  0.03718,   Adjusted R-squared:  0.03087 \nF-statistic: 5.889 on 2 and 305 DF,  p-value: 0.003095\n\n\nThis shows that loyal readers’ engagement is decreasing with popularity only in 20022, and somewhat increasing with author’s comments.\n\nanalize change in x1 and x2 to y , considering log()"
  },
  {
    "objectID": "posts/Final_Project_check_in_3_Diana_Rinker.html#adding-otherconfounding-variables.",
    "href": "posts/Final_Project_check_in_3_Diana_Rinker.html#adding-otherconfounding-variables.",
    "title": "Final Project check-in 2",
    "section": "3.2 Adding other/confounding variables.",
    "text": "3.2 Adding other/confounding variables.\nNow I will add other variables to see if there is any impact on the results of the model with engagement of both groups of readers. For number of comments, I would have to select either early.sum or n.comments, due to high correlation between them\n\n\nCode\n(cor(merged$early.sum, merged$n.comments))\n\n\n[1] 0.9507822\n\n\nI am including n.comments in the model, to see if engagement of loyal readers through comments impact overall engagement:\n\n\nCode\n# colnames(merged)\nsummary(lm(`Exit rate` ~ log(Uniques)+ author.sum + pct.positive +blocked.sum+ post.weekday +n.comments, data = merged))\n\n\n\nCall:\nlm(formula = `Exit rate` ~ log(Uniques) + author.sum + pct.positive + \n    blocked.sum + post.weekday + n.comments, data = merged)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.249664 -0.014653  0.001237  0.018954  0.061566 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    -3.658e-01  3.738e-02  -9.787  &lt; 2e-16 ***\nlog(Uniques)    1.135e-01  2.803e-03  40.492  &lt; 2e-16 ***\nauthor.sum     -4.158e-03  1.551e-03  -2.681  0.00756 ** \npct.positive    1.488e-04  2.369e-04   0.628  0.53032    \nblocked.sum    -3.634e-04  2.099e-04  -1.732  0.08391 .  \npost.weekday.L -1.959e-03  3.001e-03  -0.653  0.51425    \npost.weekday.Q  1.332e-02  2.879e-03   4.627 4.69e-06 ***\npost.weekday.C  1.109e-04  2.825e-03   0.039  0.96869    \npost.weekday^4  3.235e-03  2.797e-03   1.157  0.24793    \nn.comments      1.384e-04  2.642e-05   5.237 2.36e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.02933 on 525 degrees of freedom\nMultiple R-squared:  0.7748,    Adjusted R-squared:  0.771 \nF-statistic: 200.7 on 9 and 525 DF,  p-value: &lt; 2.2e-16\n\n\nNumber of comments shows to be significant, negatively impacting overall engagement. Considering our earlier finding, that early comments increase popularity and therefore decrease overall engagement, this is expected. Tuesday as a level of “weekday” variable also shows to significantly decrease engagement.\nAuthors engagement and blocked comments seem to increase engagement.\n\n\nCode\nsummary(lm(n.comments ~ log(Uniques) + author.sum + pct.positive +blocked.sum +post.weekday, data = merged))\n\n\n\nCall:\nlm(formula = n.comments ~ log(Uniques) + author.sum + pct.positive + \n    blocked.sum + post.weekday, data = merged)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-84.53 -33.89  -9.63  25.39 183.31 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    106.8646    61.5191   1.737  0.08296 .  \nlog(Uniques)    -1.8812     4.6253  -0.407  0.68438    \nauthor.sum       3.9147     2.5540   1.533  0.12593    \npct.positive     1.1648     0.3878   3.004  0.00279 ** \nblocked.sum      0.8864     0.3443   2.575  0.01030 *  \npost.weekday.L  21.7036     4.8624   4.464 9.87e-06 ***\npost.weekday.Q   6.0475     4.7449   1.275  0.20304    \npost.weekday.C   1.0266     4.6622   0.220  0.82581    \npost.weekday^4  -0.1307     4.6164  -0.028  0.97743    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 48.41 on 526 degrees of freedom\nMultiple R-squared:  0.07252,   Adjusted R-squared:  0.05841 \nF-statistic: 5.141 on 8 and 526 DF,  p-value: 3.455e-06\n\n\nWhile general viewers show no impact by mood of the post , we see significant positive correlation of loyals’ engagement with mood of the post and higher significance for blocked comments, both positive, i.e increasing engagement. Monday also appears having significantly more engagement than other days among loyal readers. We see no impact of author’s comments on loyal readers engagement.\nNumber of comments, representing engagement is somewhat impacted by three independent variables, but this model has low R^2 , which suggests that there are other factors impacting loyal reader’s engagement that are not considered in this model."
  },
  {
    "objectID": "posts/Final_Project_check_in_3_Diana_Rinker.html#using-different-datasets-for-comparing",
    "href": "posts/Final_Project_check_in_3_Diana_Rinker.html#using-different-datasets-for-comparing",
    "title": "Final Project check-in 2",
    "section": "3.3 Using different datasets for comparing:",
    "text": "3.3 Using different datasets for comparing:\n\nExit rate for 2021 and 2022+:\n\n\nCode\n# colnames(merged)\nsummary(lm(`Exit rate` ~ log(Uniques)+ author.sum + pct.positive +blocked.sum+ post.weekday +n.comments, data = merged.2021))\n\n\n\nCall:\nlm(formula = `Exit rate` ~ log(Uniques) + author.sum + pct.positive + \n    blocked.sum + post.weekday + n.comments, data = merged.2021)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.244230 -0.015806  0.003251  0.019441  0.065831 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    -5.812e-01  1.116e-01  -5.210 4.39e-07 ***\nlog(Uniques)    1.235e-01  9.831e-03  12.561  &lt; 2e-16 ***\nauthor.sum     -4.424e-03  2.381e-03  -1.858   0.0646 .  \npct.positive    1.259e-03  5.191e-04   2.425   0.0161 *  \nblocked.sum    -2.138e-04  4.056e-04  -0.527   0.5987    \npost.weekday.L -1.749e-03  5.286e-03  -0.331   0.7410    \npost.weekday.Q  1.295e-02  5.258e-03   2.462   0.0146 *  \npost.weekday.C -2.901e-03  5.081e-03  -0.571   0.5687    \npost.weekday^4  3.453e-03  5.054e-03   0.683   0.4952    \nn.comments      2.463e-04  4.188e-05   5.883 1.51e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.03414 on 217 degrees of freedom\nMultiple R-squared:  0.4985,    Adjusted R-squared:  0.4777 \nF-statistic: 23.97 on 9 and 217 DF,  p-value: &lt; 2.2e-16\n\n\nCode\nsummary(lm(`Exit rate` ~ log(Uniques)+ author.sum + pct.positive +blocked.sum+ post.weekday +n.comments, data = merged.2022))\n\n\n\nCall:\nlm(formula = `Exit rate` ~ log(Uniques) + author.sum + pct.positive + \n    blocked.sum + post.weekday + n.comments, data = merged.2022)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.07654 -0.01420  0.00208  0.01628  0.05515 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    -3.325e-01  3.635e-02  -9.148  &lt; 2e-16 ***\nlog(Uniques)    1.155e-01  3.003e-03  38.459  &lt; 2e-16 ***\nauthor.sum     -4.620e-03  1.985e-03  -2.328   0.0206 *  \npct.positive   -1.434e-04  2.359e-04  -0.608   0.5438    \nblocked.sum    -2.807e-04  2.190e-04  -1.282   0.2009    \npost.weekday.L -1.786e-03  3.296e-03  -0.542   0.5884    \npost.weekday.Q  1.289e-02  3.095e-03   4.163 4.12e-05 ***\npost.weekday.C  1.952e-03  3.068e-03   0.636   0.5250    \npost.weekday^4  3.930e-03  3.003e-03   1.309   0.1917    \nn.comments     -1.771e-05  3.548e-05  -0.499   0.6181    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.02393 on 298 degrees of freedom\nMultiple R-squared:  0.8486,    Adjusted R-squared:  0.8441 \nF-statistic: 185.6 on 9 and 298 DF,  p-value: &lt; 2.2e-16\n\n\nExit rate (engagement) still correlates with popularity of the site or bot 2021 and 2021,confirming the same finding from un-splitted data set earlier. However, author’s comments appear only be relevant to all user’s engagement in 2022, and not in 2021. Mood of the post appears significant for user’s engagement only in 2021, and blocked comments only appear significant in 2022.\n\n\nN.comments for 2021 and 2022+:\n\n\nCode\nsummary(lm(n.comments ~ log(Uniques)+ author.sum + pct.positive +blocked.sum+ post.weekday, data = merged.2021))\n\n\n\nCall:\nlm(formula = n.comments ~ log(Uniques) + author.sum + pct.positive + \n    blocked.sum + post.weekday, data = merged.2021)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-95.02 -40.56 -10.70  32.14 150.18 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)    173.7642   180.0468   0.965  0.33556   \nlog(Uniques)     4.5559    15.8974   0.287  0.77470   \nauthor.sum       3.8482     3.8424   1.002  0.31769   \npct.positive    -0.1075     0.8396  -0.128  0.89820   \nblocked.sum      0.5916     0.6548   0.904  0.36724   \npost.weekday.L  26.3003     8.3618   3.145  0.00189 **\npost.weekday.Q  10.5329     8.4749   1.243  0.21527   \npost.weekday.C  -2.0932     8.2170  -0.255  0.79917   \npost.weekday^4  -4.3466     8.1689  -0.532  0.59521   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 55.22 on 218 degrees of freedom\nMultiple R-squared:  0.06813,   Adjusted R-squared:  0.03393 \nF-statistic: 1.992 on 8 and 218 DF,  p-value: 0.0486\n\n\nCode\nsummary(lm(n.comments ~ log(Uniques)+ author.sum + pct.positive +blocked.sum+ post.weekday, data = merged.2022))\n\n\n\nCall:\nlm(formula = n.comments ~ log(Uniques) + author.sum + pct.positive + \n    blocked.sum + post.weekday, data = merged.2022)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-75.125 -28.163  -7.343  22.445 162.266 \n\nCoefficients:\n                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    -87.88716   59.02199  -1.489 0.137526    \nlog(Uniques)    18.02978    4.78219   3.770 0.000197 ***\nauthor.sum       1.53044    3.23409   0.473 0.636401    \npct.positive     1.02697    0.37997   2.703 0.007270 ** \nblocked.sum      0.95168    0.35262   2.699 0.007354 ** \npost.weekday.L  17.85304    5.27276   3.386 0.000804 ***\npost.weekday.Q  -0.06442    5.04498  -0.013 0.989820    \npost.weekday.C   4.05318    4.99432   0.812 0.417691    \npost.weekday^4   0.34109    4.89468   0.070 0.944490    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 39 on 299 degrees of freedom\nMultiple R-squared:  0.1132,    Adjusted R-squared:  0.08943 \nF-statistic: 4.769 on 8 and 299 DF,  p-value: 1.573e-05\n\n\nNone of the variables show significance for loyal user’s engagement(comments) in 2021, except Monday. However, loyal users are significantly influenced by popularity, mood of the post and blocked comments and Monday in 2022."
  },
  {
    "objectID": "posts/Final_Project_check_in_3_Diana_Rinker.html#diagnostic-of-models",
    "href": "posts/Final_Project_check_in_3_Diana_Rinker.html#diagnostic-of-models",
    "title": "Final Project check-in 2",
    "section": "3.4 Diagnostic of models:",
    "text": "3.4 Diagnostic of models:\nDV: Exit rate. Diagnostics for the whole dataset:\n\n\nCode\n# colnames(merged)\nmodel.ex.r&lt;- lm(`Exit rate` ~ log(Uniques)+ author.sum + pct.positive +blocked.sum +n.comments, data = merged)\npar(mfrow = c(2,3))\nplot(model.ex.r, which = 1:6)\n\n\n\n\n\nDV: Exit rate. Diagnostics for split data(2021 and 2022+).\n\n\nCode\nex.r.2021 &lt;- lm(`Exit rate` ~ log(Uniques)+ author.sum + pct.positive +blocked.sum, data = merged.2021)\nex.r.2022 &lt;-lm(`Exit rate` ~ log(Uniques)+ author.sum + pct.positive +blocked.sum, data = merged.2022)\npar(mfrow = c(2,3))\nplot(ex.r.2021, which = 1:6)\n\n\n\n\n\nCode\npar(mfrow = c(2,3))\nplot(ex.r.2022, which = 1:6)\n\n\n\n\n\nDV: number of comments. Diagnostics for the whole dataset:\n\n\nCode\nmodel.n.com&lt;- lm(n.comments ~ log(Uniques) + author.sum + pct.positive +blocked.sum, data = merged)\npar(mfrow = c(2,3))\nplot(model.n.com, which = 1:6)\n\n\n\n\n\nDV: number of comments. Diagnostics for split data(2021 and 2022+).\n\n\nCode\nn.com.2021 &lt;- lm(n.comments ~ log(Uniques) + author.sum + pct.positive +blocked.sum, data = merged.2021)\nn.com.2022 &lt;-lm(n.comments ~ log(Uniques) + author.sum + pct.positive +blocked.sum, data = merged.2022)\npar(mfrow = c(2,3))\nplot(n.com.2021, which = 1:6)\n\n\n\n\n\nCode\npar(mfrow = c(2,3))\nplot(n.com.2022, which = 1:6)"
  },
  {
    "objectID": "posts/abigailbalint_hw4.html",
    "href": "posts/abigailbalint_hw4.html",
    "title": "Homework 4",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(readxl)\nlibrary(alr4)\nlibrary(smss)\nknitr::opts_chunk$set(echo = TRUE)\n\n\n\nQuestion 1\n\nEquation: ŷ = −10,536 + 53.8x1 + 2.84x2 Subbing the size of home and lot size out for the variables in the equation: ŷ = −10,536 + 53.8(1240) + 2.84(18000) solving for y:\n\n\n\nCode\na = -10536 + (53.8*1240) + (2.84*18000)\nprint(a)\n\n\n[1] 107296\n\n\nCode\na2=145000-a\nprint(a2)\n\n\n[1] 37704\n\n\nThe predicted selling price is 107296. To find the residual, I am subtracting what it sold for from the predicted price. This means our prediction was low and the house sold for 37704 more than was predicted.\n\nIt is predicted to increase 53.8 dollars per square foot as this is the coefficient in the equation for home size square footage.\nI think because the coefficient for home size is so much bigger than lot size, if we divide the coefficients by each other that would give the amount of home square feet we would need to have the same impact of one square foot in lot size, so 18.94 sq feet.\n\n\n\nCode\nc=53.8/2.84\nprint(c)\n\n\n[1] 18.94366\n\n\n\n\nQuestion 2\n\n\nCode\nsummary(salary)\n\n\n     degree      rank        sex          year            ysdeg      \n Masters:34   Asst :18   Male  :38   Min.   : 0.000   Min.   : 1.00  \n PhD    :18   Assoc:14   Female:14   1st Qu.: 3.000   1st Qu.: 6.75  \n              Prof :20               Median : 7.000   Median :15.50  \n                                     Mean   : 7.481   Mean   :16.12  \n                                     3rd Qu.:11.000   3rd Qu.:23.25  \n                                     Max.   :25.000   Max.   :35.00  \n     salary     \n Min.   :15000  \n 1st Qu.:18247  \n Median :23719  \n Mean   :23798  \n 3rd Qu.:27258  \n Max.   :38045  \n\n\n\nThe p value is .07 so we fail to reject the null hypothesis and are not able to say there is a significant difference in male vs female salaries.\n\n\n\nCode\nmale &lt;- salary %&gt;%\n  filter(sex == \"Male\")\nfemale &lt;- salary %&gt;%\n  filter(sex == \"Female\")\nt.test(male$salary, female$salary, var.equal = TRUE)\n\n\n\n    Two Sample t-test\n\ndata:  male$salary and female$salary\nt = 1.8474, df = 50, p-value = 0.0706\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -291.257 6970.550\nsample estimates:\nmean of x mean of y \n 24696.79  21357.14 \n\n\n\nFitting model and summary below:\n\n\n\nCode\nmodel1 &lt;- lm(salary ~ degree + rank + sex + year + ysdeg, data = salary)\nsummary(model1)\n\n\n\nCall:\nlm(formula = salary ~ degree + rank + sex + year + ysdeg, data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4045.2 -1094.7  -361.5   813.2  9193.1 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 15746.05     800.18  19.678  &lt; 2e-16 ***\ndegreePhD    1388.61    1018.75   1.363    0.180    \nrankAssoc    5292.36    1145.40   4.621 3.22e-05 ***\nrankProf    11118.76    1351.77   8.225 1.62e-10 ***\nsexFemale    1166.37     925.57   1.260    0.214    \nyear          476.31      94.91   5.018 8.65e-06 ***\nysdeg        -124.57      77.49  -1.608    0.115    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2398 on 45 degrees of freedom\nMultiple R-squared:  0.855, Adjusted R-squared:  0.8357 \nF-statistic: 44.24 on 6 and 45 DF,  p-value: &lt; 2.2e-16\n\n\nc: degree: Degree has a positive coefficient but not a significant p-value, indicating as degree increases salary does as well but not at a statistically significant level. rank: Rank has a very high positive coefficient as well as being very statistically significant, indicating a strong relationship between this variable and salary. sex: Similarly to degree, sex has a positive coefficient but not a significant p-value, indicating a positive slope here but not a statistically significant level. year: This variable is statistically significant with a positive slope indicating more years in a current rank may increase salary. ysdeg: This variable is not significant and actually has a negative slope/coefficient.\nd: When I relevel rank I can see that it flips rank to have a negative slope/coefficient now instead of a positive one.\n\n\nCode\nlevels(salary$rank)\n\n\n[1] \"Asst\"  \"Assoc\" \"Prof\" \n\n\nCode\nrankrl &lt;- relevel(salary$rank, ref=\"Prof\")\nmodel2 &lt;- lm(salary ~ degree + rankrl + sex + year + ysdeg, data = salary)\nsummary(model2)\n\n\n\nCall:\nlm(formula = salary ~ degree + rankrl + sex + year + ysdeg, data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4045.2 -1094.7  -361.5   813.2  9193.1 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  26864.81    1375.29  19.534  &lt; 2e-16 ***\ndegreePhD     1388.61    1018.75   1.363    0.180    \nrankrlAsst  -11118.76    1351.77  -8.225 1.62e-10 ***\nrankrlAssoc  -5826.40    1012.93  -5.752 7.28e-07 ***\nsexFemale     1166.37     925.57   1.260    0.214    \nyear           476.31      94.91   5.018 8.65e-06 ***\nysdeg         -124.57      77.49  -1.608    0.115    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2398 on 45 degrees of freedom\nMultiple R-squared:  0.855, Adjusted R-squared:  0.8357 \nF-statistic: 44.24 on 6 and 45 DF,  p-value: &lt; 2.2e-16\n\n\ne.When I take rank out of the model, year and ysdeg now have a p-value that is statistically significant (a drastic change).\n\n\nCode\nmodel3 &lt;- lm(salary ~ degree + sex + year + ysdeg, data = salary)\nsummary(model3)\n\n\n\nCall:\nlm(formula = salary ~ degree + sex + year + ysdeg, data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8146.9 -2186.9  -491.5  2279.1 11186.6 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 17183.57    1147.94  14.969  &lt; 2e-16 ***\ndegreePhD   -3299.35    1302.52  -2.533 0.014704 *  \nsexFemale   -1286.54    1313.09  -0.980 0.332209    \nyear          351.97     142.48   2.470 0.017185 *  \nysdeg         339.40      80.62   4.210 0.000114 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3744 on 47 degrees of freedom\nMultiple R-squared:  0.6312,    Adjusted R-squared:  0.5998 \nF-statistic: 20.11 on 4 and 47 DF,  p-value: 1.048e-09\n\n\n\nTo avoid multicollinearity, I don’t think we should use year and ysdeg both within the model since we now know there is an interaction due to the new dean 15 years ago that could cause these two variables to be related.\n\nBelow I am making a new variable that is if there is more than 15 years since highest degree they are assigned yes, otherwise no. I can see when fitting a model against salary this is very statistically significant, indicating the dean may have had an impact.\n\n\nCode\nsalary2 &lt;- salary  %&gt;%\n  mutate(ysdeg15 = ifelse(ysdeg &gt; 15, \"yes\", \"no\"))\nmodel4 &lt;- lm(salary ~ ysdeg15, data = salary2)\nsummary(model4)    \n\n\n\nCall:\nlm(formula = salary ~ ysdeg15, data = salary2)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n -8294  -3486  -1772   3829  10576 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  20125.9      913.4  22.033  &lt; 2e-16 ***\nysdeg15yes    7343.5     1291.8   5.685 6.73e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4658 on 50 degrees of freedom\nMultiple R-squared:  0.3926,    Adjusted R-squared:  0.3804 \nF-statistic: 32.32 on 1 and 50 DF,  p-value: 6.734e-07\n\n\n\n\nQuestion 3\n\n\nCode\nlibrary(smss)\ndata(\"house.selling.price\")\nhouse &lt;- house.selling.price\nsummary(house)\n\n\n      case            Taxes           Beds       Baths           New      \n Min.   :  1.00   Min.   :  20   Min.   :2   Min.   :1.00   Min.   :0.00  \n 1st Qu.: 25.75   1st Qu.:1178   1st Qu.:3   1st Qu.:2.00   1st Qu.:0.00  \n Median : 50.50   Median :1614   Median :3   Median :2.00   Median :0.00  \n Mean   : 50.50   Mean   :1908   Mean   :3   Mean   :1.96   Mean   :0.11  \n 3rd Qu.: 75.25   3rd Qu.:2238   3rd Qu.:3   3rd Qu.:2.00   3rd Qu.:0.00  \n Max.   :100.00   Max.   :6627   Max.   :5   Max.   :4.00   Max.   :1.00  \n     Price             Size     \n Min.   : 21000   Min.   : 580  \n 1st Qu.: 93225   1st Qu.:1215  \n Median :132600   Median :1474  \n Mean   :155331   Mean   :1629  \n 3rd Qu.:169625   3rd Qu.:1865  \n Max.   :587000   Max.   :4050  \n\n\n\nBased on this model, both whether it is new or not and the size are statistically significant variables in predicting price, although size is much more significant with a much smaller p value, the positive coefficient for whether the home is new or not is much higher.\n\n\n\nCode\nmodel5 &lt;- lm(Price ~ New + Size, data = house)\nsummary(model5)\n\n\n\nCall:\nlm(formula = Price ~ New + Size, data = house)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-205102  -34374   -5778   18929  163866 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -40230.867  14696.140  -2.738  0.00737 ** \nNew          57736.283  18653.041   3.095  0.00257 ** \nSize           116.132      8.795  13.204  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 53880 on 97 degrees of freedom\nMultiple R-squared:  0.7226,    Adjusted R-squared:  0.7169 \nF-statistic: 126.3 on 2 and 97 DF,  p-value: &lt; 2.2e-16\n\n\n\nPrediction equation for new: y=-40230.867 + 57736.283new + 116.132size\n3000 square feet for new vs not:\n\n\n\nCode\nnew &lt;- -40230.867 + 57736.283*1 + 116.132*3000\nprint (new)\n\n\n[1] 365901.4\n\n\nCode\nnotnew &lt;- -40230.867 + 57736.283*0 + 116.132*3000\nprint(notnew)\n\n\n[1] 308165.1\n\n\nSelling price of new: 365901.4 Selling price of not new: 308165.1\n\nFitting model with interaction:\n\n\n\nCode\nmodel6 &lt;- lm(Price ~ New*Size, data = house)\nsummary(model6)\n\n\n\nCall:\nlm(formula = Price ~ New * Size, data = house)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-175748  -28979   -6260   14693  192519 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -22227.808  15521.110  -1.432  0.15536    \nNew         -78527.502  51007.642  -1.540  0.12697    \nSize           104.438      9.424  11.082  &lt; 2e-16 ***\nNew:Size        61.916     21.686   2.855  0.00527 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 52000 on 96 degrees of freedom\nMultiple R-squared:  0.7443,    Adjusted R-squared:  0.7363 \nF-statistic: 93.15 on 3 and 96 DF,  p-value: &lt; 2.2e-16\n\n\n\nPlot for new:\n\n\n\nCode\nhousenew &lt;- house %&gt;%\n  filter(New==1)\nggplot(data = housenew, aes(x = Size, y = Price)) +\n  geom_point() +\n  labs(x = \"Home size\", y = \"Selling price\") +\n  geom_smooth(method = \"lm\", se = FALSE)\n\n\n`geom_smooth()` using formula 'y ~ x'\n\n\n\n\n\nPlot for not new:\n\n\nCode\nhousenotnew &lt;- house %&gt;%\n  filter(New==0)\nggplot(data = housenotnew, aes(x = Size, y = Price)) +\n  geom_point() +\n  labs(x = \"Home size\", y = \"Selling price\") +\n  geom_smooth(method = \"lm\", se = FALSE)\n\n\n`geom_smooth()` using formula 'y ~ x'\n\n\n\n\n\n\nSame answer as C I think.\n\n300 square feet for new vs not was: Selling price of new: 365901.4 Selling price of not new: 308165.1\n\n\nNow doing this for 1500 sf I got the same result. Is thi s because the new vs not new coefficient holds so much weight here?\n\n\nCode\nnew2 &lt;- -40230.867 + 57736.283*1 + 116.132*1500\nprint (new)\n\n\n[1] 365901.4\n\n\nCode\nnotnew2 &lt;- -40230.867 + 57736.283*0 + 116.132*1500\nprint(notnew)\n\n\n[1] 308165.1\n\n\n\nI would use the model without interaction here because since the new vs not is so strong I feel like it overpowers the size variable and it makes more sense to look at them separately."
  },
  {
    "objectID": "posts/Kristin_Abijaoude_HW3.html",
    "href": "posts/Kristin_Abijaoude_HW3.html",
    "title": "Hw 3 by Kristin Abijaoude",
    "section": "",
    "text": "Code\n# load packages\npackages &lt;- c(\"readr\", \"ggplot2\", \"caret\", \"summarytools\", \"smss\", \"alr4\", \"tidyverse\", \"dplyr\", \"stats\", \"pwr\")\nlapply(packages, require, character.only = TRUE)\n\n\nLoading required package: readr\n\n\nLoading required package: ggplot2\n\n\nLoading required package: caret\n\n\nLoading required package: lattice\n\n\nLoading required package: summarytools\n\n\nLoading required package: smss\n\n\nLoading required package: alr4\n\n\nLoading required package: car\n\n\nLoading required package: carData\n\n\nLoading required package: effects\n\n\nUse the command\n    lattice::trellis.par.set(effectsTheme())\n  to customize lattice options for effects plots.\nSee ?effectsTheme for details.\n\n\nLoading required package: tidyverse\n\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ tibble  3.1.8     ✔ dplyr   1.1.0\n✔ tidyr   1.2.1     ✔ stringr 1.5.0\n✔ purrr   1.0.0     ✔ forcats 0.5.2\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n✖ purrr::lift()   masks caret::lift()\n✖ dplyr::recode() masks car::recode()\n✖ purrr::some()   masks car::some()\n✖ tibble::view()  masks summarytools::view()\nLoading required package: pwr\n\n\n[[1]]\n[1] TRUE\n\n[[2]]\n[1] TRUE\n\n[[3]]\n[1] TRUE\n\n[[4]]\n[1] TRUE\n\n[[5]]\n[1] TRUE\n\n[[6]]\n[1] TRUE\n\n[[7]]\n[1] TRUE\n\n[[8]]\n[1] TRUE\n\n[[9]]\n[1] TRUE\n\n[[10]]\n[1] TRUE\n\n\n\n\nCode\ndata(UN11)\nsummary(UN11)\n\n\n        region      group       fertility         ppgdp         \n Africa    :53   oecd  : 31   Min.   :1.134   Min.   :   114.8  \n Asia      :50   other :115   1st Qu.:1.754   1st Qu.:  1283.0  \n Europe    :39   africa: 53   Median :2.262   Median :  4684.5  \n Latin Amer:20                Mean   :2.761   Mean   : 13012.0  \n Caribbean :17                3rd Qu.:3.545   3rd Qu.: 15520.5  \n Oceania   :17                Max.   :6.925   Max.   :105095.4  \n (Other)   : 3                                                  \n    lifeExpF        pctUrban     \n Min.   :48.11   Min.   : 11.00  \n 1st Qu.:65.66   1st Qu.: 39.00  \n Median :75.89   Median : 59.00  \n Mean   :72.29   Mean   : 57.93  \n 3rd Qu.:79.58   3rd Qu.: 75.00  \n Max.   :87.12   Max.   :100.00  \n                                 \n\n\nCode\nhead(UN11)\n\n\n                region  group fertility   ppgdp lifeExpF pctUrban\nAfghanistan       Asia  other     5.968   499.0    49.49       23\nAlbania         Europe  other     1.525  3677.2    80.40       53\nAlgeria         Africa africa     2.142  4473.0    75.00       67\nAngola          Africa africa     5.135  4321.9    53.17       59\nAnguilla     Caribbean  other     2.000 13750.1    81.10      100\nArgentina   Latin Amer  other     2.172  9162.1    79.89       93\n\n\n\n\nfertility: Predictor\nppgdp: Response\n\n\n\n\n\nCode\nplot(x = UN11$fertility, y = UN11$ppgdp,\n     xlab = \"Fertility\",\n     ylab = \"GDP\")\n\n\n\n\n\nBased on this graph, a straight-line mean function does not seem to be plausible for a summary of this graph, due to the fact that the points are clustered in one area.\n\n\n\n\n\nCode\nplot(x = log(UN11$fertility), y = log(UN11$ppgdp),\n     xlab = \"Fertility\",\n     ylab = \"GDP\")\n\n\n\n\n\nOn the other hand, it’s plausible to make a straight line mean function from the graph above. It paints a clearer picture of a trend. In other words, one can see from the graph that there is a correlation between fertility rates and GDP- richer countries tend to have fewer children per woman."
  },
  {
    "objectID": "posts/Kristin_Abijaoude_HW3.html#a",
    "href": "posts/Kristin_Abijaoude_HW3.html#a",
    "title": "Hw 3 by Kristin Abijaoude",
    "section": "",
    "text": "fertility: Predictor\nppgdp: Response"
  },
  {
    "objectID": "posts/Kristin_Abijaoude_HW3.html#b",
    "href": "posts/Kristin_Abijaoude_HW3.html#b",
    "title": "Hw 3 by Kristin Abijaoude",
    "section": "",
    "text": "Code\nplot(x = UN11$fertility, y = UN11$ppgdp,\n     xlab = \"Fertility\",\n     ylab = \"GDP\")\n\n\n\n\n\nBased on this graph, a straight-line mean function does not seem to be plausible for a summary of this graph, due to the fact that the points are clustered in one area."
  },
  {
    "objectID": "posts/Kristin_Abijaoude_HW3.html#c",
    "href": "posts/Kristin_Abijaoude_HW3.html#c",
    "title": "Hw 3 by Kristin Abijaoude",
    "section": "",
    "text": "Code\nplot(x = log(UN11$fertility), y = log(UN11$ppgdp),\n     xlab = \"Fertility\",\n     ylab = \"GDP\")\n\n\n\n\n\nOn the other hand, it’s plausible to make a straight line mean function from the graph above. It paints a clearer picture of a trend. In other words, one can see from the graph that there is a correlation between fertility rates and GDP- richer countries tend to have fewer children per woman."
  },
  {
    "objectID": "posts/Kristin_Abijaoude_HW3.html#political-ideology-vs-religiosity",
    "href": "posts/Kristin_Abijaoude_HW3.html#political-ideology-vs-religiosity",
    "title": "Hw 3 by Kristin Abijaoude",
    "section": "Political Ideology vs Religiosity",
    "text": "Political Ideology vs Religiosity\n\n\nCode\nstudent.survey&lt;- student.survey%&gt;% \n       mutate(p_i = case_when(pi == \"very liberal\" ~ 1,\n                              pi == \"liberal\" ~ 2,\n                              pi == \"slightly liberal\" ~ 3,\n                              pi == \"moderate\" ~ 4,\n                              pi == \"slightly conservative\" ~ 5,\n                              pi == \"conservative\" ~ 6,\n                              pi == \"very conservative\" ~ 7,\n                              TRUE ~ 0)) \nstudent.survey&lt;- student.survey%&gt;% \n       mutate(r_e = case_when(re == \"never\" ~ 1,\n                              re == \"occasionally\" ~ 2,\n                              re == \"most weeks\" ~ 3,\n                              re == \"every week\" ~ 4,\n                              TRUE ~ 0))\n\nreligion &lt;- lm(p_i ~ r_e, data = student.survey)\nreligion\n\n\n\nCall:\nlm(formula = p_i ~ r_e, data = student.survey)\n\nCoefficients:\n(Intercept)          r_e  \n     0.9308       0.9704  \n\n\nCode\nplot(religion)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInterpretion\nThe intercept coefficient is around 0.93, meaning that one’s political ideology starts around the “very liberal” area, and the religiosity coefficient starts around 0.97, meaning that one basically never attends religious services. In other word, this model starts of with a very liberal person who never attends church. The more religious a person is (i.e. the more often they attend religious services at places such as church or synagogue etc.), the more ideologically conservative they tend to be."
  },
  {
    "objectID": "posts/Kristin_Abijaoude_HW3.html#high-school-gpa-vs-hours-of-watching-tv",
    "href": "posts/Kristin_Abijaoude_HW3.html#high-school-gpa-vs-hours-of-watching-tv",
    "title": "Hw 3 by Kristin Abijaoude",
    "section": "High School GPA vs Hours of Watching TV",
    "text": "High School GPA vs Hours of Watching TV\n\n\nCode\nTV &lt;- lm(hi ~ tv, data = student.survey)\nTV\n\n\n\nCall:\nlm(formula = hi ~ tv, data = student.survey)\n\nCoefficients:\n(Intercept)           tv  \n    3.44135     -0.01831  \n\n\nCode\nplot(TV)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInterpretation\nWhat we have here is the Intercept of around 3.44, which means that this is the student’s expected high school GPA when they watch 0 hours of TV. The negative coefficient of -0.01831 means that the more hours one watches TV, the lower their GPA by this amount."
  },
  {
    "objectID": "posts/FinalPart2_MiguelCuriel.html",
    "href": "posts/FinalPart2_MiguelCuriel.html",
    "title": "Final Project Check In 2",
    "section": "",
    "text": "Code\n# load neccesary packages\nlibrary(tidyverse) # used for elementary data wrangling and visualization\nlibrary(naniar) # used for missing values visualization\nlibrary(summarytools) # used for table summarizing descriptive statistics\nlibrary(corrplot) # used for correlation plots\nlibrary(ggridges) # used for joint plots\nlibrary(boot) # used for the PRESS statistic\nlibrary(knitr) # used for table with statistics"
  },
  {
    "objectID": "posts/FinalPart2_MiguelCuriel.html#testing-h1",
    "href": "posts/FinalPart2_MiguelCuriel.html#testing-h1",
    "title": "Final Project Check In 2",
    "section": "Testing H1",
    "text": "Testing H1\nTo answer the first research question - whether there has been an increase in mental health disorders throughout the years - we can run a Chi-squared of independence. This will allow us to test whether there is an association between two variables (year and mental disorders) in the population.\nThe test is conducted by comparing the observed frequencies of the data to the expected frequencies and it assumes that data is independent and that the expected frequencies are not too small (as a rule of thumb, no cell of the contingency table should have less than 5 observations, and in the present analysis the smallest cell has 7).\n\n\nCode\n# create a contingency table of the data\ncont_tbl &lt;- table(df$has_mental_disorder, df$year)\n\n# print contingency table\n# cont_tbl\n# cat(\"Contingency Table:\\n\")\n# table(df$has_mental_disorder, df$year)\nprint_table &lt;- function(var1, var2, annotation) {\n  cat(paste0(annotation, \"\\n\"))\n  print(table(df$has_mental_disorder, df$year))\n}\n\nprint_table(var1, var2, \"Contingency Table\")\n\n\nContingency Table\n     \n      2017 2018 2019 2020 2021\n  No   111   64   42   18    7\n  Yes  213  140   96   20   28\n\n\nCode\n# perform a chi-squared test of independence\nchisq.test(cont_tbl)\n\n\n\n    Pearson's Chi-squared test\n\ndata:  cont_tbl\nX-squared = 7.1175, df = 4, p-value = 0.1298\n\n\nThe results of the Chi-squared test suggest that the association between the two variables - year and mental disorders - could plausibly be due to chance alone. In other words, there is not enough evidence to suggest that mental disorders have increased over the years among mental health workers."
  },
  {
    "objectID": "posts/FinalPart2_MiguelCuriel.html#testing-h2",
    "href": "posts/FinalPart2_MiguelCuriel.html#testing-h2",
    "title": "Final Project Check In 2",
    "section": "Testing H2",
    "text": "Testing H2\nThe second hypothesis does not pertain to mental health over the years; rather, it seeks to inquire whether the importance that employers place on mental health is associated to mental disorders. Since we are dealing with a binary categorical variable (mental disorders = yes/no) and a numerical discrete variable (valued placed by employers = 1-10), a logistic regression model can help test this hypothesis.\nA logistic regression uses the logistic function to estimate the probability of the binary response variable taking on a certain value given the values of the predictor variables. Additionally, we first experiment the model’s performance with a single predictor variable, and we will then include control variables to compare the models.\n\n\nCode\n# transform no/yes to 0/1\ndf$has_mental_disorder &lt;- ifelse(df$has_mental_disorder == \"Yes\", 1, 0)\n\n# simple log model\nlog_simple &lt;- glm(has_mental_disorder ~ employer_mh_importance\n                  ,data = df\n                  ,family = binomial)\n\n# print model summary\nsummary(log_simple)\n\n\n\nCall:\nglm(formula = has_mental_disorder ~ employer_mh_importance, family = binomial, \n    data = df)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-1.5875  -1.4616   0.8735   0.9028   0.9631  \n\nCoefficients:\n                       Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)             0.92657    0.18661   4.965 6.86e-07 ***\nemployer_mh_importance -0.03991    0.03240  -1.232    0.218    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 934.65  on 738  degrees of freedom\nResidual deviance: 933.12  on 737  degrees of freedom\nAIC: 937.12\n\nNumber of Fisher Scoring iterations: 4\n\n\n\n\nCode\n# create a data frame with the predictor variable and binary outcome\ndata_df &lt;- data.frame(predictor_variable = df$employer_mh_importance\n                      , binary_outcome = df$has_mental_disorder)\n\n# create the plot with the logistic regression line and data points\nggplot(data_df, aes(x = predictor_variable, y = binary_outcome)) +\n  geom_point() +\n  stat_smooth(method = \"glm\", method.args = list(family = \"binomial\"), se = TRUE) +\n  xlab(\"Level of importance that employers place on mental health\") +\n  ylab(\"Presence of mental disorder\")\n\n\n\n\n\nFrom the simple logistic model, we find that the relation is not statistically significant. This is initially proven by the associated p-value (.21) and is further corroborated by the plot (we expect an S-shaped curve, but get a slightly negative straight line).\nWhile this initially suggests there is not a strong relationship between variables, it could also be that the model is not properly constructed. In the next section, I will explore an enhanced model that includes interaction effects."
  },
  {
    "objectID": "posts/FinalPart2_MiguelCuriel.html#footnotes",
    "href": "posts/FinalPart2_MiguelCuriel.html#footnotes",
    "title": "Final Project Check In 2",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFox, M. (2016, November 11). Why Are Tech Workers So Satisfied With Their Jobs? Retrieved March 17, 2023, from https://www.forbes.com/sites/meimeifox/2016/11/11/why-are-tech-workers-so-satisfied-with-their-jobs/?sh=4eac1918a059↩︎\nWronski, L., & Cohen, J. (2019, November 4). This is the industry sector that has some of the happiest workers in America. Retrieved March 17, 2023, from https://www.cnbc.com/2019/11/04/this-is-the-industry-that-has-some-of-the-happiness-workers-in-america.html↩︎\nKim, K. T. (2017). GOOGLE: A reflection of culture, leader, and management. International Journal of Corporate Social Responsibility, 2(10). https://doi.org/10.1186/s40991-017-0021-0↩︎\nMcPherson, J. (n.d.). Tech company cultures are not all the same. Culture Amp. Retrieved March 17, 2023, from https://www.cultureamp.com/blog/tech-company-culture↩︎\nGoncharov, A. (2023, March 13). How I burnt out in FAANG, but my job was not the problem. Blog.Goncharov.ai. Retrieved March 17, 2023, from https://blog.goncharov.ai/how-i-burnt-out-in-faang-but-my-job-was-not-the-problem↩︎\nRosales, A., & Jakob, S. (2021). Perceptions of age in contemporary tech. Sciendo, 42(1), 79-91. https://doi.org/10.2478/nor-2021-0021↩︎\nMickey, E. L. (2021). The Organization of Networking and Gender Inequality in the New Economy: Evidence from the Tech Industry. Work & Occupations, 49(4), 383-420. https://doi.org/10.1177/07308884221102134↩︎\nHardey, M. (2020). The Culture of Women in Tech : An Unsuitable Job for a Woman (1st ed.). Emerald Publishing.↩︎\nBanerjee, P., & Rincón, L. (2019). Trouble in Tech Paradise. Journal of Water Resources Planning & Management, 145(4), 24-29. https://doi.org/10.1177/1536504219854714↩︎\nMatloff, N. (2013). Immigration and the tech industry: As a labour shortage remedy, for innovation, or for cost savings? Migration Letters, 10(2), 210-227. ISSN: 1741-8984 Online ISSN: 1741-8992↩︎\nBlind (2021, January 29). Deteriorating Mental Health In The Workplace. Retrieved March 17, 2023, from https://www.teamblind.com/blog/index.php/2021/01/29/deteriorating-mental-health-in-the-workplace/↩︎\nThe United States Bureau of Labor and Statistics via CompTIA (2023, March 3). Cyberstates 2021: The Definitive Guide to the Tech Industry and Workforce. Retrieved March 17, 2023, from https://www.comptia.org/content/tech-jobs-report↩︎\nCrunchbase News. (2023, January 5). Global VC Funding on a Slide since Q4 2022. Retrieved March 17, 2023, from https://news.crunchbase.com/venture/global-vc-funding-slide-q4-2022↩︎\nLayoffs.fyi. (n.d.). Layoffs.fyi - Tracking all tech startup layoffs since COVID-19. https://layoffs.fyi↩︎\nOpen Sourcing Mental Health (n.d.). About OSMH. Retrieved March 18, 2023, from https://osmhhelp.org/about/about-osmi.html↩︎"
  },
  {
    "objectID": "posts/HW1_Tyler_Tewksbury.html",
    "href": "posts/HW1_Tyler_Tewksbury.html",
    "title": "Homework - 1",
    "section": "",
    "text": "First, let’s read in the data from the Excel file:\n\n\nCode\nlibrary(readxl)\n\n\nWarning: package 'readxl' was built under R version 4.2.3\n\n\nCode\nlibrary(tidyverse)\n\n\nWarning: package 'tidyverse' was built under R version 4.2.3\n\n\nWarning: package 'ggplot2' was built under R version 4.2.3\n\n\nWarning: package 'tibble' was built under R version 4.2.3\n\n\nWarning: package 'tidyr' was built under R version 4.2.3\n\n\nWarning: package 'readr' was built under R version 4.2.3\n\n\nWarning: package 'purrr' was built under R version 4.2.3\n\n\nWarning: package 'stringr' was built under R version 4.2.3\n\n\nWarning: package 'forcats' was built under R version 4.2.3\n\n\nWarning: package 'lubridate' was built under R version 4.2.3\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.0     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.1     ✔ tibble    3.2.0\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nCode\ndf &lt;- read_excel(\"_data/LungCapData.xls\")\n\n\nThe distribution of LungCap looks as follows:\n\n\nCode\nhist(df$LungCap)\n\n\n\n\n\nThe histogram suggests that the distribution is close to a normal distribution. Most of the observations are close to the mean. Very few observations are close to the margins (0 and 15).\n\n\n\n\n\nCode\nboxplot(LungCap ~ Gender, df )\n\n\n\n\n\nThe probability distribution with respect to Males and Females is very similar. The min, max, and median are all slightly higher for males.\n\n\n\n\n\nCode\nmean(subset(df$LungCap, df$Smoke == \"no\"))\n\n\n[1] 7.770188\n\n\nCode\nmean(subset(df$LungCap, df$Smoke == \"yes\"))\n\n\n[1] 8.645455\n\n\nLung capacity for smokers is higher in this dataset, which does not seem to make sense.\n##d\n\n\nCode\ndf &lt;- df %&gt;% \n  mutate(\n    age_group = dplyr::case_when(\n      Age &lt;= 13            ~ \"&lt;=13\",\n      Age == 14 | Age == 15 ~ \"14-15\",\n      Age == 16 | Age == 17 ~ \"16-17\",\n      Age &gt;= 18             ~ \"&gt;=18\"\n    )\n)\n\ndf2 &lt;- df %&gt;%\n  group_by(age_group, Smoke) %&gt;%\n  summarise_at(vars(LungCap),  list(AvgLungCap = mean))\n \ndf2\n\n\n# A tibble: 8 × 3\n# Groups:   age_group [4]\n  age_group Smoke AvgLungCap\n  &lt;chr&gt;     &lt;chr&gt;      &lt;dbl&gt;\n1 14-15     no          9.14\n2 14-15     yes         8.39\n3 16-17     no         10.5 \n4 16-17     yes         9.38\n5 &lt;=13      no          6.36\n6 &lt;=13      yes         7.20\n7 &gt;=18      no         11.1 \n8 &gt;=18      yes        10.5 \n\n\nThe relationship between age and lung capacity implies that lung capacity increases as one gets older.\n##e\nFor smokers specifically, their lung capacity is higher for all age groups except &gt;=18. This differs from part C, where all smokers had higher lung capacity. There are a few possible explanations for this.\n\n\nCode\ndf %&gt;% group_by(Smoke, age_group) %&gt;% summarise(count = n())\n\n\n`summarise()` has grouped output by 'Smoke'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 8 × 3\n# Groups:   Smoke [2]\n  Smoke age_group count\n  &lt;chr&gt; &lt;chr&gt;     &lt;int&gt;\n1 no    14-15       105\n2 no    16-17        77\n3 no    &lt;=13        401\n4 no    &gt;=18         65\n5 yes   14-15        15\n6 yes   16-17        20\n7 yes   &lt;=13         27\n8 yes   &gt;=18         15\n\n\nThere are far more people under 13 in this dataset than those above 18, a majority of whom do not smoke. Above 18 as well there are more nonsmokers than smokers. The large count of those under 13 are likely skewing the analysis from part C."
  },
  {
    "objectID": "posts/HW1_Tyler_Tewksbury.html#a",
    "href": "posts/HW1_Tyler_Tewksbury.html#a",
    "title": "Homework - 1",
    "section": "",
    "text": "First, let’s read in the data from the Excel file:\n\n\nCode\nlibrary(readxl)\n\n\nWarning: package 'readxl' was built under R version 4.2.3\n\n\nCode\nlibrary(tidyverse)\n\n\nWarning: package 'tidyverse' was built under R version 4.2.3\n\n\nWarning: package 'ggplot2' was built under R version 4.2.3\n\n\nWarning: package 'tibble' was built under R version 4.2.3\n\n\nWarning: package 'tidyr' was built under R version 4.2.3\n\n\nWarning: package 'readr' was built under R version 4.2.3\n\n\nWarning: package 'purrr' was built under R version 4.2.3\n\n\nWarning: package 'stringr' was built under R version 4.2.3\n\n\nWarning: package 'forcats' was built under R version 4.2.3\n\n\nWarning: package 'lubridate' was built under R version 4.2.3\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.0     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.1     ✔ tibble    3.2.0\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nCode\ndf &lt;- read_excel(\"_data/LungCapData.xls\")\n\n\nThe distribution of LungCap looks as follows:\n\n\nCode\nhist(df$LungCap)\n\n\n\n\n\nThe histogram suggests that the distribution is close to a normal distribution. Most of the observations are close to the mean. Very few observations are close to the margins (0 and 15)."
  },
  {
    "objectID": "posts/HW1_Tyler_Tewksbury.html#b",
    "href": "posts/HW1_Tyler_Tewksbury.html#b",
    "title": "Homework - 1",
    "section": "",
    "text": "Code\nboxplot(LungCap ~ Gender, df )\n\n\n\n\n\nThe probability distribution with respect to Males and Females is very similar. The min, max, and median are all slightly higher for males."
  },
  {
    "objectID": "posts/HW1_Tyler_Tewksbury.html#c",
    "href": "posts/HW1_Tyler_Tewksbury.html#c",
    "title": "Homework - 1",
    "section": "",
    "text": "Code\nmean(subset(df$LungCap, df$Smoke == \"no\"))\n\n\n[1] 7.770188\n\n\nCode\nmean(subset(df$LungCap, df$Smoke == \"yes\"))\n\n\n[1] 8.645455\n\n\nLung capacity for smokers is higher in this dataset, which does not seem to make sense.\n##d\n\n\nCode\ndf &lt;- df %&gt;% \n  mutate(\n    age_group = dplyr::case_when(\n      Age &lt;= 13            ~ \"&lt;=13\",\n      Age == 14 | Age == 15 ~ \"14-15\",\n      Age == 16 | Age == 17 ~ \"16-17\",\n      Age &gt;= 18             ~ \"&gt;=18\"\n    )\n)\n\ndf2 &lt;- df %&gt;%\n  group_by(age_group, Smoke) %&gt;%\n  summarise_at(vars(LungCap),  list(AvgLungCap = mean))\n \ndf2\n\n\n# A tibble: 8 × 3\n# Groups:   age_group [4]\n  age_group Smoke AvgLungCap\n  &lt;chr&gt;     &lt;chr&gt;      &lt;dbl&gt;\n1 14-15     no          9.14\n2 14-15     yes         8.39\n3 16-17     no         10.5 \n4 16-17     yes         9.38\n5 &lt;=13      no          6.36\n6 &lt;=13      yes         7.20\n7 &gt;=18      no         11.1 \n8 &gt;=18      yes        10.5 \n\n\nThe relationship between age and lung capacity implies that lung capacity increases as one gets older.\n##e\nFor smokers specifically, their lung capacity is higher for all age groups except &gt;=18. This differs from part C, where all smokers had higher lung capacity. There are a few possible explanations for this.\n\n\nCode\ndf %&gt;% group_by(Smoke, age_group) %&gt;% summarise(count = n())\n\n\n`summarise()` has grouped output by 'Smoke'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 8 × 3\n# Groups:   Smoke [2]\n  Smoke age_group count\n  &lt;chr&gt; &lt;chr&gt;     &lt;int&gt;\n1 no    14-15       105\n2 no    16-17        77\n3 no    &lt;=13        401\n4 no    &gt;=18         65\n5 yes   14-15        15\n6 yes   16-17        20\n7 yes   &lt;=13         27\n8 yes   &gt;=18         15\n\n\nThere are far more people under 13 in this dataset than those above 18, a majority of whom do not smoke. Above 18 as well there are more nonsmokers than smokers. The large count of those under 13 are likely skewing the analysis from part C."
  },
  {
    "objectID": "posts/Homework3_AlexaPotter.html",
    "href": "posts/Homework3_AlexaPotter.html",
    "title": "Homework 3",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\n\n-- Attaching packages --------------------------------------- tidyverse 1.3.2 --\nv ggplot2 3.4.0      v purrr   0.3.5 \nv tibble  3.1.8      v dplyr   1.0.10\nv tidyr   1.2.1      v stringr 1.5.0 \nv readr   2.1.3      v forcats 0.5.2 \n-- Conflicts ------------------------------------------ tidyverse_conflicts() --\nx dplyr::filter() masks stats::filter()\nx dplyr::lag()    masks stats::lag()\n\n\nCode\nlibrary(car)\n\n\nLoading required package: carData\n\nAttaching package: 'car'\n\nThe following object is masked from 'package:dplyr':\n\n    recode\n\nThe following object is masked from 'package:purrr':\n\n    some\n\n\nCode\nlibrary(smss)\ndata(\"student.survey\", package = \"smss\")\ndf &lt;- student.survey\nlibrary(alr4)\n\n\nLoading required package: effects\nlattice theme set by effectsTheme()\nSee ?effectsTheme for details.\n\n\nCode\ndata(\"UN11\", package = \"alr4\")\ndf &lt;- UN11\ndata(\"Rateprof\", package = \"alr4\")\ndf &lt;- Rateprof\ndata(\"water\", package = \"alr4\")\ndf &lt;- water"
  },
  {
    "objectID": "posts/Homework3_AlexaPotter.html#a",
    "href": "posts/Homework3_AlexaPotter.html#a",
    "title": "Homework 3",
    "section": "A",
    "text": "A\nIdentify the predictor and the response.\nPredictor: x = ppgdp Response: y = fertility"
  },
  {
    "objectID": "posts/Homework3_AlexaPotter.html#b",
    "href": "posts/Homework3_AlexaPotter.html#b",
    "title": "Homework 3",
    "section": "B",
    "text": "B\nDraw the scatterplot of fertility on the vertical axis versus ppgdp on the horizontal axis and summarize the information in this graph. Does a straight-line mean function seem to be plausible for a summary of this graph?\n\n\nCode\nggplot(UN11, aes(y = fertility, x = ppgdp)) +\n  geom_point()+\n  stat_smooth(method = \"lm\", se = FALSE)\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nThis distribution has more of a curve to it rather than a straight line. The fitted line does not seem accurately to reflect the distribution."
  },
  {
    "objectID": "posts/Homework3_AlexaPotter.html#c",
    "href": "posts/Homework3_AlexaPotter.html#c",
    "title": "Homework 3",
    "section": "C",
    "text": "C\nDraw the scatterplot of log(fertility) versus log(ppgdp) using natural logarithms. Does the simple linear regression model seem plausible for a summary of this graph? If you use a different base of logarithms, the shape of the graph won’t change, but the values on the axes will change.\n\n\nCode\nggplot(UN11, aes(y = log(fertility), x = log(ppgdp))) +\n  geom_point()+\n  stat_smooth(method = \"lm\", se = FALSE)\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nYes the simpler linear regression does seem more plausible to fit and describe this distribution."
  },
  {
    "objectID": "posts/Homework3_AlexaPotter.html#a-1",
    "href": "posts/Homework3_AlexaPotter.html#a-1",
    "title": "Homework 3",
    "section": "A",
    "text": "A\nHow, if at all, does the slope of the prediction equation change?\nThe slope would not change in this conversion but the y intercept would."
  },
  {
    "objectID": "posts/Homework3_AlexaPotter.html#b-1",
    "href": "posts/Homework3_AlexaPotter.html#b-1",
    "title": "Homework 3",
    "section": "B",
    "text": "B\nHow, if at all, does the correlation change?\nCorrelation would not change as its standardized, its value does not depend on the unit of measurement."
  },
  {
    "objectID": "posts/Homework3_AlexaPotter.html#a-2",
    "href": "posts/Homework3_AlexaPotter.html#a-2",
    "title": "Homework 3",
    "section": "A",
    "text": "A\nGraphically portray how the explanatory variable relates to the outcome variable in each of the two cases\n\ni\ny = political ideology and x = religiosity\n\n\nCode\n#head(student.survey)\n#unique(student.survey$pi)\n#unique(student.survey$re)\n\n#class(student.survey$pi) &lt;- \"character\"\n#class(student.survey$re) &lt;- \"character\"\n\nstudent.survey %&gt;%recode(pi, c(\"very conservative\" = \"7\", \n                            \"conservative\"= \"6\", \n                            \"slightly conservative\" = \"5\", \n                            \"moderate\" = \"4\", \n                            \"slightly liberal\" = \"3\", \n                            \"liberal\"= \"2\", \n                            \"very liberal\"= \"1\",\n                            .default = \"0\"))\n\n\nWarning in if (as.factor) {: the condition has length &gt; 1 and only the first\nelement will be used\n\n\nError in if (as.factor) {: argument is not interpretable as logical\n\n\nCode\nstudent.survey %&gt;%recode(re, c(\"every week\" = \"4\", \n                            \"most weeks\" = \"3\", \n                            \"occasionally \"= \"2\", \n                           \"never\"= \"1\",\n                           .default = \"0\"))\n\n\nError in is.factor(x): object 're' not found\n\n\nCode\nclass(student.survey$pi) &lt;- \"numeric\"\nclass(student.survey$re) &lt;- \"numeric\"\n\n\n\n\nCode\nggplot(data = student.survey, aes(x=re, y=pi))+\n  geom_point()+\n  geom_smooth(method = \"lm\", se = FALSE)+\n  scale_x_continuous(labels= c(\"every week\", \n                            \"most weeks\", \n                            \"occasionally\", \n                            \"never\"),\n                     breaks = c(4,3,2,1))+\n  scale_y_continuous(labels = c(\"very conservative\", \n                            \"conservative\", \n                            \"slightly conservative\", \n                            \"moderate\", \n                            \"slightly liberal\", \n                            \"liberal\", \n                            \"very liberal\"),\n                     breaks = c(7,6,5,4,3,2,1)) +\n  labs(x = \"Religiosity\", y = \"Political Ideology\")\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\nCode\nsummary(lm(pi~re, data=student.survey))\n\n\n\nCall:\nlm(formula = pi ~ re, data = student.survey)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.81243 -0.87160  0.09882  1.12840  3.09882 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   0.9308     0.4252   2.189   0.0327 *  \nre            0.9704     0.1792   5.416 1.22e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.345 on 58 degrees of freedom\nMultiple R-squared:  0.3359,    Adjusted R-squared:  0.3244 \nF-statistic: 29.34 on 1 and 58 DF,  p-value: 1.221e-06\n\n\n\n\nii\ny = high school GPA and x = hours of TV watching\n\n\nCode\nggplot(data = student.survey, aes(x=tv, y=hi))+\n  geom_point()+\n  geom_smooth(method = \"lm\", se = FALSE)+\n  labs(x = \"Hours of TV Watching\", y = \"High School GPA\")\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\nCode\nsummary(lm(hi~tv, data=student.survey))\n\n\n\nCall:\nlm(formula = hi ~ tv, data = student.survey)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.2583 -0.2456  0.0417  0.3368  0.7051 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  3.441353   0.085345  40.323   &lt;2e-16 ***\ntv          -0.018305   0.008658  -2.114   0.0388 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4467 on 58 degrees of freedom\nMultiple R-squared:  0.07156,   Adjusted R-squared:  0.05555 \nF-statistic: 4.471 on 1 and 58 DF,  p-value: 0.03879"
  },
  {
    "objectID": "posts/Homework3_AlexaPotter.html#b-2",
    "href": "posts/Homework3_AlexaPotter.html#b-2",
    "title": "Homework 3",
    "section": "B",
    "text": "B\nSummarize and interpret results of inferential analyses.\nThe results in both regressions show they are statistically significant, however, the first graph displays the results as a linear regression better than the second example. A different form of regression would be better to capture the relationship rather than a linear regression."
  },
  {
    "objectID": "posts/Final_Project_Check-In-Akhilesh.html",
    "href": "posts/Final_Project_Check-In-Akhilesh.html",
    "title": "603_Project_Check_In_Akhilesh",
    "section": "",
    "text": "The blood transfusion dataset contain 748 samples with 5 input features: Input Features: • Recency (number of months since the last donation) • Frequency (total number of donations) • Monetary (total blood donated in c.c.) • Time (number of months since the first donation) • Age (age of the donor)\nSource: https://www.kaggle.com/datasets/ninalabiba/blood-transfusion-dataset\n\n\nCode\nBD &lt;- read_csv(\"_data/transfusion.csv\")\n\n\nRows: 748 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (5): Recency (months), Frequency (times), Monetary (c.c. blood), Time (m...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nCode\nBD\n\n\n# A tibble: 748 × 5\n   `Recency (months)` `Frequency (times)` Monetary (c.c. blood…¹ Time …² wheth…³\n                &lt;dbl&gt;               &lt;dbl&gt;                  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n 1                  2                  50                  12500      98       1\n 2                  0                  13                   3250      28       1\n 3                  1                  16                   4000      35       1\n 4                  2                  20                   5000      45       1\n 5                  1                  24                   6000      77       0\n 6                  4                   4                   1000       4       0\n 7                  2                   7                   1750      14       1\n 8                  1                  12                   3000      35       0\n 9                  2                   9                   2250      22       1\n10                  5                  46                  11500      98       1\n# … with 738 more rows, and abbreviated variable names\n#   ¹​`Monetary (c.c. blood)`, ²​`Time (months)`,\n#   ³​`whether he/she donated blood in March 2007`"
  },
  {
    "objectID": "posts/Final_Project_Check-In-Akhilesh.html#data-description",
    "href": "posts/Final_Project_Check-In-Akhilesh.html#data-description",
    "title": "603_Project_Check_In_Akhilesh",
    "section": "",
    "text": "The blood transfusion dataset contain 748 samples with 5 input features: Input Features: • Recency (number of months since the last donation) • Frequency (total number of donations) • Monetary (total blood donated in c.c.) • Time (number of months since the first donation) • Age (age of the donor)\nSource: https://www.kaggle.com/datasets/ninalabiba/blood-transfusion-dataset\n\n\nCode\nBD &lt;- read_csv(\"_data/transfusion.csv\")\n\n\nRows: 748 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (5): Recency (months), Frequency (times), Monetary (c.c. blood), Time (m...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nCode\nBD\n\n\n# A tibble: 748 × 5\n   `Recency (months)` `Frequency (times)` Monetary (c.c. blood…¹ Time …² wheth…³\n                &lt;dbl&gt;               &lt;dbl&gt;                  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n 1                  2                  50                  12500      98       1\n 2                  0                  13                   3250      28       1\n 3                  1                  16                   4000      35       1\n 4                  2                  20                   5000      45       1\n 5                  1                  24                   6000      77       0\n 6                  4                   4                   1000       4       0\n 7                  2                   7                   1750      14       1\n 8                  1                  12                   3000      35       0\n 9                  2                   9                   2250      22       1\n10                  5                  46                  11500      98       1\n# … with 738 more rows, and abbreviated variable names\n#   ¹​`Monetary (c.c. blood)`, ²​`Time (months)`,\n#   ³​`whether he/she donated blood in March 2007`"
  },
  {
    "objectID": "posts/Final_Project_Check-In-Akhilesh.html#research-questions",
    "href": "posts/Final_Project_Check-In-Akhilesh.html#research-questions",
    "title": "603_Project_Check_In_Akhilesh",
    "section": "Research Questions:",
    "text": "Research Questions:\nBlood Donation Prediction Frequency:\nThe aim of this study is to develop linear regression, logistic regression machine learning model, to accurately predict whether a blood donor is likely to donate in the future. The results of this study could be useful in developing targeted strategies for donor recruitment and retention, ultimately improving the availability and accessibility of blood donations.\nIdentify Factors that Affect Blood Donation:\nHow do donation behaviors vary across different regions, and what factors may contribute to these variations? Using hypothesis testing, this research project aims to compare the donation patterns of donors from different regions based on demographic and donation-related variables, such as age, gender, donation frequency, and time since last donation. The findings can provide insights into the regional variations in blood donation behaviors and inform targeted strategies to address these differences, potentially leading to increased donation rates and more efficient allocation of resources for blood donation organizations. The study will visualize the results using plots and charts to identify any significant patterns or trends that could help healthcare and blood donation organizations to develop effective strategies to increase blood donation rates.\nSegmentation of Blood Donors:\nUse clustering techniques to segment blood donors based on their demographic and donation history characteristics. Explore methods such as K-means clustering or hierarchical clustering to identify groups of donors with similar characteristics. Examine the differences between these groups and explore their donation patterns over time.\nBuilding a Donor Retention Strategy:\nUse the insights gained from the previous projects to develop a donor retention strategy for the blood donation center. Identify factors that are associated with donor churn (i.e., donors who stop donating blood), and develop a plan to mitigate these factors. This can help healthcare and blood donation organizations to develop strategies to retain donors over the long term."
  },
  {
    "objectID": "posts/Final_Project_Check-In-Akhilesh.html#summary-of-the-data",
    "href": "posts/Final_Project_Check-In-Akhilesh.html#summary-of-the-data",
    "title": "603_Project_Check_In_Akhilesh",
    "section": "Summary of the data",
    "text": "Summary of the data\n\n\nCode\nsummary(BD)\n\n\n Recency (months) Frequency (times) Monetary (c.c. blood) Time (months)  \n Min.   : 0.000   Min.   : 1.000    Min.   :  250         Min.   : 2.00  \n 1st Qu.: 2.750   1st Qu.: 2.000    1st Qu.:  500         1st Qu.:16.00  \n Median : 7.000   Median : 4.000    Median : 1000         Median :28.00  \n Mean   : 9.507   Mean   : 5.515    Mean   : 1379         Mean   :34.28  \n 3rd Qu.:14.000   3rd Qu.: 7.000    3rd Qu.: 1750         3rd Qu.:50.00  \n Max.   :74.000   Max.   :50.000    Max.   :12500         Max.   :98.00  \n whether he/she donated blood in March 2007\n Min.   :0.000                             \n 1st Qu.:0.000                             \n Median :0.000                             \n Mean   :0.238                             \n 3rd Qu.:0.000                             \n Max.   :1.000"
  },
  {
    "objectID": "posts/Final_Project_Check-In-Akhilesh.html#descrpition-of-the-variables",
    "href": "posts/Final_Project_Check-In-Akhilesh.html#descrpition-of-the-variables",
    "title": "603_Project_Check_In_Akhilesh",
    "section": "Descrpition of the Variables:",
    "text": "Descrpition of the Variables:\nThis summary function presents a statistical description of a dataset related to blood donation, with five variables: Recency, Frequency, Monetary, Time, and whether the individual donated blood in March 2007. Here’s a breakdown of each variable:\nRecency (months): This variable represents the number of months since the last blood donation. The minimum value is 0 months, indicating that some individuals donated blood very recently. The mean is 9.507 months, suggesting that, on average, people donated blood around 9.5 months ago. The maximum value is 74 months, which means the longest gap between donations is 74 months.\nFrequency (times): This variable shows the total number of times an individual has donated blood. The minimum value is 1, meaning that at least one person has only donated blood once. The mean is 5.515 times, indicating that people, on average, have donated blood about 5.5 times. The maximum value is 50 times, showing that some individuals have donated blood quite frequently.\nMonetary (c.c. blood): This variable represents the total volume of blood donated by an individual, measured in cubic centimeters (c.c.). The minimum value is 250 c.c., which corresponds to the minimum single donation volume. The mean is 1379 c.c., suggesting that, on average, individuals have donated around 1.379 liters of blood. The maximum value is 12,500 c.c., indicating that the highest total volume donated by a person is 12.5 liters.\nTime (months): This variable measures the length of time an individual has been donating blood. The minimum value is 2 months, suggesting that some individuals are relatively new to blood donation. The mean is 34.28 months, indicating that, on average, people have been donating blood for about 34.3 months. The maximum value is 98 months, showing that some individuals have been donating blood for a long time.\nWhether he/she donated blood in March 2007: This is a binary variable that indicates if an individual donated blood in March 2007. The mean is 0.238, which means that about 23.8% of the individuals in the dataset donated blood in that specific month.\nThe summary function provides an overview of the dataset’s key statistics, such as minimum, 1st quartile, median, mean, 3rd quartile, and maximum values for each variable. This information helps to understand the distribution, central tendency, and spread of the data."
  },
  {
    "objectID": "posts/Final_Project_Check-In-Akhilesh.html#expected-contribution",
    "href": "posts/Final_Project_Check-In-Akhilesh.html#expected-contribution",
    "title": "603_Project_Check_In_Akhilesh",
    "section": "Expected Contribution:",
    "text": "Expected Contribution:\n\nAkhilesh Kumar: Will work on the Segmentation of Blood Donors and Building a Donor Retention Strategy\nSai Srinivas: Will work on the Blood Donation Prediction Frequency and Identify Factors that Affect Blood Donation"
  },
  {
    "objectID": "posts/Final_Project_Check-In-Akhilesh.html#references",
    "href": "posts/Final_Project_Check-In-Akhilesh.html#references",
    "title": "603_Project_Check_In_Akhilesh",
    "section": "References:",
    "text": "References:\nKaggle: https://www.kaggle.com/datasets/ninalabiba/blood-transfusion-dataset\nOriginal Owner and Donor: Prof. I-Cheng Yeh, Department of Information Management, Chung-Hua University, Hsin Chu, Taiwan 30067, R.O.C., e-mail:icyeh ‘@’ chu.edu.tw, TEL:886-3-5186511, Date Donated: October 3, 2008"
  },
  {
    "objectID": "posts/HW1_MiguelCuriel.html",
    "href": "posts/HW1_MiguelCuriel.html",
    "title": "Homework 1",
    "section": "",
    "text": "First, let’s read in the data from the Excel file:\n\n\nCode\nlibrary(readxl)\ndf &lt;- read_excel(\"_data/LungCapData.xls\")\n\n\nThe distribution of LungCap looks as follows:\n\n\nCode\nhist(df$LungCap)\n\n\n\n\n\nThe histogram suggests that the distribution is close to a normal distribution. Most of the observations are close to the mean. Very few observations are close to the margins (0 and 15).\n\n\n\nCompare the probability distribution of the LungCap with respect to Males and Females?\n\n\nCode\nsuppressPackageStartupMessages(library(dplyr))\nboxplot(LungCap~Gender, data=df)\n\n\n\n\n\nThe boxplot suggests that males tend to have a slightly higher lung capacity than females. While the mean of both genders is close to each other (~8), males’ interquartile range is slightly higher (approximately from 6.5 to 10) when compared to the females’ (approximately from 6 to 9).\n\n\n\nCompare the mean lung capacities for smokers and non-smokers. Does it make sense?\n\n\nCode\ndf %&gt;%\n  group_by(Smoke) %&gt;%\n  summarise(mean = mean(LungCap), n = n())\n\n\n# A tibble: 2 × 3\n  Smoke  mean     n\n  &lt;chr&gt; &lt;dbl&gt; &lt;int&gt;\n1 no     7.77   648\n2 yes    8.65    77\n\n\nSmokers have a mean lung capacity of 8.65, versus non-smokers who have a lung capacity of 7.77. These results do not make sense, as we would expect non-smokers to have greater lung capacity.\n\n\n\nExamine the relationship between Smoking and Lung Capacity within age groups: “less than or equal to 13”, “14 to 15”, “16 to 17”, and “greater than or equal to 18”.\n\n\nCode\nage_groups &lt;- df %&gt;% \n  mutate(\n    # Create categories\n    AgeGroup = case_when(\n      Age &lt;= 13            ~ \"0 to 13\",\n      Age == 14 | Age == 15 ~ \"14 to 15\",\n      Age == 16 | Age == 17 ~ \"16 to 17\",\n      Age &gt;= 18             ~ \"18 and above\"\n    ),\n    # Convert to factor\n    AgeGroup = factor(\n      AgeGroup,\n      level = c(\"0 to 13\", \"14 to 15\",\"16 to 17\", \"18 and above\")\n    )\n  )\n\nboxplot(LungCap~AgeGroup, data=age_groups)\n\n\n\n\n\nThe boxplot suggests that the greater the age, the greater the lung capacity. This is especially evident when comparing the lowest age group (0-13) versus the rest of them, but between the two oldest age groups (16-17 vs 18+), this is the least evident. This makes sense as the greatest physical growth usually happens earlier in life - from childhood to puberty - but then this slows down in the late teens and it almost entirely stops during young adulthood.\n\n\n\nCompare the lung capacities for smokers and non-smokers within each age group. Is your answer different from the one in part c. What could possibly be going on here?\n\n\nCode\nlibrary(ggplot2)\n\nage_groups %&gt;% ggplot(aes(x=AgeGroup, y=LungCap, fill=Smoke)) + geom_boxplot()\n\n\n\n\n\nThe answer is different than what I found in 1.C. I previously found that the lung capacity mean for non-smokers was less than the smoking counterpart; however, this new box plot suggests that most non-smoking age groups actually have better lung capacity than smokers. The only age group where this condition isn’t met is in the youngest ages (0-13) so it is likely that this group that is skewing the overall smoker versus non-smoker analysis."
  },
  {
    "objectID": "posts/HW1_MiguelCuriel.html#a",
    "href": "posts/HW1_MiguelCuriel.html#a",
    "title": "Homework 1",
    "section": "",
    "text": "First, let’s read in the data from the Excel file:\n\n\nCode\nlibrary(readxl)\ndf &lt;- read_excel(\"_data/LungCapData.xls\")\n\n\nThe distribution of LungCap looks as follows:\n\n\nCode\nhist(df$LungCap)\n\n\n\n\n\nThe histogram suggests that the distribution is close to a normal distribution. Most of the observations are close to the mean. Very few observations are close to the margins (0 and 15)."
  },
  {
    "objectID": "posts/HW1_MiguelCuriel.html#b",
    "href": "posts/HW1_MiguelCuriel.html#b",
    "title": "Homework 1",
    "section": "",
    "text": "Compare the probability distribution of the LungCap with respect to Males and Females?\n\n\nCode\nsuppressPackageStartupMessages(library(dplyr))\nboxplot(LungCap~Gender, data=df)\n\n\n\n\n\nThe boxplot suggests that males tend to have a slightly higher lung capacity than females. While the mean of both genders is close to each other (~8), males’ interquartile range is slightly higher (approximately from 6.5 to 10) when compared to the females’ (approximately from 6 to 9)."
  },
  {
    "objectID": "posts/HW1_MiguelCuriel.html#c",
    "href": "posts/HW1_MiguelCuriel.html#c",
    "title": "Homework 1",
    "section": "",
    "text": "Compare the mean lung capacities for smokers and non-smokers. Does it make sense?\n\n\nCode\ndf %&gt;%\n  group_by(Smoke) %&gt;%\n  summarise(mean = mean(LungCap), n = n())\n\n\n# A tibble: 2 × 3\n  Smoke  mean     n\n  &lt;chr&gt; &lt;dbl&gt; &lt;int&gt;\n1 no     7.77   648\n2 yes    8.65    77\n\n\nSmokers have a mean lung capacity of 8.65, versus non-smokers who have a lung capacity of 7.77. These results do not make sense, as we would expect non-smokers to have greater lung capacity."
  },
  {
    "objectID": "posts/HW1_MiguelCuriel.html#d",
    "href": "posts/HW1_MiguelCuriel.html#d",
    "title": "Homework 1",
    "section": "",
    "text": "Examine the relationship between Smoking and Lung Capacity within age groups: “less than or equal to 13”, “14 to 15”, “16 to 17”, and “greater than or equal to 18”.\n\n\nCode\nage_groups &lt;- df %&gt;% \n  mutate(\n    # Create categories\n    AgeGroup = case_when(\n      Age &lt;= 13            ~ \"0 to 13\",\n      Age == 14 | Age == 15 ~ \"14 to 15\",\n      Age == 16 | Age == 17 ~ \"16 to 17\",\n      Age &gt;= 18             ~ \"18 and above\"\n    ),\n    # Convert to factor\n    AgeGroup = factor(\n      AgeGroup,\n      level = c(\"0 to 13\", \"14 to 15\",\"16 to 17\", \"18 and above\")\n    )\n  )\n\nboxplot(LungCap~AgeGroup, data=age_groups)\n\n\n\n\n\nThe boxplot suggests that the greater the age, the greater the lung capacity. This is especially evident when comparing the lowest age group (0-13) versus the rest of them, but between the two oldest age groups (16-17 vs 18+), this is the least evident. This makes sense as the greatest physical growth usually happens earlier in life - from childhood to puberty - but then this slows down in the late teens and it almost entirely stops during young adulthood."
  },
  {
    "objectID": "posts/HW1_MiguelCuriel.html#e",
    "href": "posts/HW1_MiguelCuriel.html#e",
    "title": "Homework 1",
    "section": "",
    "text": "Compare the lung capacities for smokers and non-smokers within each age group. Is your answer different from the one in part c. What could possibly be going on here?\n\n\nCode\nlibrary(ggplot2)\n\nage_groups %&gt;% ggplot(aes(x=AgeGroup, y=LungCap, fill=Smoke)) + geom_boxplot()\n\n\n\n\n\nThe answer is different than what I found in 1.C. I previously found that the lung capacity mean for non-smokers was less than the smoking counterpart; however, this new box plot suggests that most non-smoking age groups actually have better lung capacity than smokers. The only age group where this condition isn’t met is in the youngest ages (0-13) so it is likely that this group that is skewing the overall smoker versus non-smoker analysis."
  },
  {
    "objectID": "posts/HW1_MiguelCuriel.html#a-1",
    "href": "posts/HW1_MiguelCuriel.html#a-1",
    "title": "Homework 1",
    "section": "a",
    "text": "a\nWhat is the probability that a randomly selected inmate has exactly 2 prior convictions?\n160/810 = .1975 = 19.75%"
  },
  {
    "objectID": "posts/HW1_MiguelCuriel.html#b-1",
    "href": "posts/HW1_MiguelCuriel.html#b-1",
    "title": "Homework 1",
    "section": "b",
    "text": "b\nWhat is the probability that a randomly selected inmate has fewer than 2 prior convictions?\n(128+434)/810 = 562/810 = .6938 = 69.38%"
  },
  {
    "objectID": "posts/HW1_MiguelCuriel.html#c-1",
    "href": "posts/HW1_MiguelCuriel.html#c-1",
    "title": "Homework 1",
    "section": "c",
    "text": "c\nWhat is the probability that a randomly selected inmate has 2 or fewer prior convictions?\n(128+434+160)/810 = 722/810 = .8914 = 89.14%"
  },
  {
    "objectID": "posts/HW1_MiguelCuriel.html#d-1",
    "href": "posts/HW1_MiguelCuriel.html#d-1",
    "title": "Homework 1",
    "section": "d",
    "text": "d\nWhat is the probability that a randomly selected inmate has more than 2 prior convictions?\n(64+24)/810 = 88/810 = .1086 = 10.86%"
  },
  {
    "objectID": "posts/HW1_MiguelCuriel.html#e-1",
    "href": "posts/HW1_MiguelCuriel.html#e-1",
    "title": "Homework 1",
    "section": "e",
    "text": "e\nWhat is the expected value for the number of prior convictions? (The expected value of a discrete random variable X, symbolized as E(X), is often referred to as the long-term average or mean)\n\n\nCode\n(0*128/810) + (1*434/810) + (2*160/810) + (3*64/810) + (4*24/810) \n\n\n[1] 1.28642"
  },
  {
    "objectID": "posts/HW1_MiguelCuriel.html#f",
    "href": "posts/HW1_MiguelCuriel.html#f",
    "title": "Homework 1",
    "section": "f",
    "text": "f\nCalculate the variance and the standard deviation for the Prior Convictions.\nBelow is the variance:\n\n\nCode\n((0-1.28642)^2 * 0) + ((1-1.28642)^2 * .5358) + ((2-1.28642)^2 * .395) + ((3-1.28642)^2 * .237) + ((4-1.28642)^2 * .1185)\n\n\n[1] 1.813581\n\n\nAnd below is the standard deviation:\n\n\nCode\nsqrt(((0-1.28642)^2 * 0) + ((1-1.28642)^2 * .5358) + ((2-1.28642)^2 * .395) + ((3-1.28642)^2 * .237) + ((4-1.28642)^2 * .1185))\n\n\n[1] 1.346693"
  },
  {
    "objectID": "posts/HW3_LTucksmith.html",
    "href": "posts/HW3_LTucksmith.html",
    "title": "HW3",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(alr4)\nlibrary(smss)\nknitr::opts_chunk$set(echo = TRUE)\n\n\n\nUnited Nations (Data file: UN11in alr4) The data in the file UN11 contains several variables, including ppgdp, the gross national product per person in U.S. dollars, and fertility, the birth rate per 1000 females, both from the year 2009. The data are for 199 localities, mostly UN member countries, but also other areas such as Hong Kong that are not independent countries. The data were collected from the United Nations (2011). We will study the dependence of fertility on ppgdp.\n\n\nIdentify the predictor and the response.\n\nPredictor: ppgpd Response: Fertility\n\nDraw the scatter plot of fertility on the vertical axis versus ppgdp on the horizontal axis and summarize the information in this graph. Does a straight-line mean function seem to be plausible for a summary of this graph?\n\nNo, a straight-line mean function doesn’t seem plausible for this graph as ppgdp isn’t evenly distributed across the x-asix, as seen in the scatter plot. As fewer data are found further along the x-axis, a straight-line mean function wouldn’t accurately summarize the data.\n\n\nCode\nun &lt;- data.frame(UN11)\nhead(un)\n\n\n                region  group fertility   ppgdp lifeExpF pctUrban\nAfghanistan       Asia  other     5.968   499.0    49.49       23\nAlbania         Europe  other     1.525  3677.2    80.40       53\nAlgeria         Africa africa     2.142  4473.0    75.00       67\nAngola          Africa africa     5.135  4321.9    53.17       59\nAnguilla     Caribbean  other     2.000 13750.1    81.10      100\nArgentina   Latin Amer  other     2.172  9162.1    79.89       93\n\n\nCode\nggplot(un, aes(x=ppgdp, y=fertility)) + geom_point()\n\n\n\n\n\n\nDraw the scatterplot of log(fertility) versus log(ppgdp) using natural logarithms. Does the simple linear regression model seem plausible for a summary of this graph? If you use a different base of logarithms, the shape of the graph won’t change, but the values on the axes will change.\n\nYes, a simple linear regression model seems plausible for this graph as the data is structured in linear fashion, showing that as fertility goes up, ppgpd goes down.\n\n\nCode\nun &lt;- data.frame(UN11)\nhead(un)\n\n\n                region  group fertility   ppgdp lifeExpF pctUrban\nAfghanistan       Asia  other     5.968   499.0    49.49       23\nAlbania         Europe  other     1.525  3677.2    80.40       53\nAlgeria         Africa africa     2.142  4473.0    75.00       67\nAngola          Africa africa     5.135  4321.9    53.17       59\nAnguilla     Caribbean  other     2.000 13750.1    81.10      100\nArgentina   Latin Amer  other     2.172  9162.1    79.89       93\n\n\nCode\nggplot(un, aes(x=log(ppgdp), y=log(fertility))) + geom_point()\n\n\n\n\n\n\nAnnual income, in dollars, is an explanatory variable in a regression analysis. For a British version of the report on the analysis, all responses are converted to British pounds sterling (1 pound equals about 1.33 dollars, as of 2016).\n\n\nHow, if at all, does the slope of the prediction equation change?\n\nThe slope of the prediction equation would change as the currency conversion would change the range of income on the axis, which in turn the shape of the data and the regression line.\n\nHow, if at all, does the correlation change?\n\nThe correlation of the prediction equation wouldn’t change as the currency conversion is a linear change, not affecting the magnitude of a correlation between the variables.\n\nWater runoff in the Sierras (Data file: water in alr4) Can Southern California’s water supply in future years be predicted from past data? One factor affecting water availability is stream runoff. If runoff could be predicted, engineers, planners, and policy makers could do their jobs more efficiently. The data file contains 43 years’ worth of precipitation measurements taken at six sites in the Sierra Nevada mountains (labeled APMAM, APSAB, APSLAKE, OPBPC, OPRC, and OPSLAKE) and stream runoff volume at a site near Bishop, California, labeled BSAAM. Draw the scatterplot matrix for these data and summarize the information available from these plots. (Hint: Use the pairs() function.)\n\nFrom the below scatter plot matrix, the measurements for the sites don’t change drastically as the years age, but many do take a similar decline in the more recent years, while the ones who don’t decline in recent years have instead slightly inclined. Particularly, the APMAM, APSAB, APSLAKE, all trend upwards from ~1980 on, while OPBPC, OPRC, OPSLAKE, and BSAAM all trend downward in the same time period. We can also see from the matrix that the sites measurement comparisons within these two groups are clustered together and aligned seemingly linear, while sites comparisons across the two groups do not have a visable relationship, linear or otherwise.\n\n\nCode\nwater &lt;- data.frame(water)\nhead(water)\n\n\n  Year APMAM APSAB APSLAKE OPBPC  OPRC OPSLAKE  BSAAM\n1 1948  9.13  3.58    3.91  4.10  7.43    6.47  54235\n2 1949  5.28  4.82    5.20  7.55 11.11   10.26  67567\n3 1950  4.20  3.77    3.67  9.52 12.20   11.35  66161\n4 1951  4.60  4.46    3.93 11.14 15.15   11.13  68094\n5 1952  7.15  4.99    4.88 16.34 20.05   22.81 107080\n6 1953  9.70  5.65    4.91  8.88  8.15    7.41  67594\n\n\nCode\npairs(water)\n\n\n\n\n\n\nProfessor ratings (Data file: Rateprof in alr4) In the website and online forum RateMyProfessors.com, students rate and comment on their instructors. Launched in 1999, the site includes millions of ratings on thousands of instructors. The data file includes the summaries of the ratings of 364 instructors at a large campus in the Midwest (Bleske-Rechek and Fritsch, 2011). Each instructor included in the data had at least 10 ratings over a several year period. Students provided ratings of 1–5 on quality, helpfulness, clarity, easiness of instructor’s courses, and raterInterest in the subject matter covered in the instructor’s courses. The data file provides the averages of these five ratings. Create a scatterplot matrix of these five variables. Provide a brief description of the relationships between the five ratings.\n\nOf the five variables, easiness and raterInterst appear to be the least correlated with the other three variables.We can see this as in the scatterplot matrix, the easiness and raterInterest plots are the least linear. On the other hand, the quality, helfulness, and clarity ratings appear to be strongly correlated as their scatterplots form a clearly linear shape.\n\n\nCode\nrate &lt;- data.frame(Rateprof)\nhead(rate)\n\n\n  gender numYears numRaters numCourses pepper discipline              dept\n1   male        7        11          5     no        Hum           English\n2   male        6        11          5     no        Hum Religious Studies\n3   male       10        43          2     no        Hum               Art\n4   male       11        24          5     no        Hum           English\n5   male       11        19          7     no        Hum           Spanish\n6   male       10        15          9     no        Hum           Spanish\n   quality helpfulness  clarity easiness raterInterest sdQuality sdHelpfulness\n1 4.636364    4.636364 4.636364 4.818182      3.545455 0.5518564     0.6741999\n2 4.318182    4.545455 4.090909 4.363636      4.000000 0.9020179     0.9341987\n3 4.790698    4.720930 4.860465 4.604651      3.432432 0.4529343     0.6663898\n4 4.250000    4.458333 4.041667 2.791667      3.181818 0.9325048     0.9315329\n5 4.684211    4.684211 4.684211 4.473684      4.214286 0.6500112     0.8200699\n6 4.233333    4.266667 4.200000 4.533333      3.916667 0.8632717     1.0327956\n  sdClarity sdEasiness sdRaterInterest\n1 0.5045250  0.4045199       1.1281521\n2 0.9438798  0.5045250       1.0744356\n3 0.4129681  0.5407021       1.2369438\n4 0.9990938  0.5882300       1.3322506\n5 0.5823927  0.6117753       0.9749613\n6 0.7745967  0.6399405       0.6685579\n\n\nCode\nratings &lt;- select(rate, c('quality','helpfulness','clarity', 'easiness', 'raterInterest'))\npairs(ratings)\n\n\n\n\n\n\nFor the student.survey data file in the smss package, conduct regression analyses relating (by convention, y denotes the outcome variable, x denotes the explanatory variable)\n\n\ny = political ideology and x = religiosity,\ny = high school GPA and x = hours of TV watching. (You can use ?student.survey in the R console, after loading the package, to see what each variable means.)\n\n\nGraphically portray how the explanatory variable relates to the outcome variable in each of the two cases\nSummarize and interpret results of inferential analyses.\n\nIn the first grap, we are looking at a relationship between a categorical predictor and response variable. I used geom_count so that overlapping data points wouldn’t affect the appearance of the relationship. From the graph, we can see that liberal and very liberal repsondents attend church the least frequently. Those who attended religious services most frequently have a range of political beliefs, a higher concentration of politically like-minded folks attending church more often isn’t seen in this dataset.\nLooking at the other graph, I used geom_smooth as this graph looks at the relationship between two continuous variables. From the graph, there is not a strong linear pattern, indicating that the relationship between these two variables isn’t linear. However, we can see that the most tv was watched by respondents with a GPA under 3.0, and respondents with a GPA above 3.0 watched the least tv.\n\n\nCode\nggplot(student.survey, aes(re, pi)) + geom_count()\n\n\nError in ggplot(student.survey, aes(re, pi)): object 'student.survey' not found\n\n\nCode\nggplot(student.survey, aes(hi, tv)) + geom_smooth()\n\n\nError in ggplot(student.survey, aes(hi, tv)): object 'student.survey' not found"
  },
  {
    "objectID": "posts/HW1_EmmaNarkewicz.html",
    "href": "posts/HW1_EmmaNarkewicz.html",
    "title": "Homework 1",
    "section": "",
    "text": "First, let’s read in the data from the Excel file:\n\n\nCode\nlibrary(readxl)\ndf &lt;- read_excel(\"_data/LungCapData.xls\")\n\n\nThe distribution of LungCap looks as follows:\n\n\nCode\nhist(df$LungCap)\n\n\n\n\n\nThe histogram suggests that the distribution is close to a normal distribution. Most of the observations are close to the mean. Very few observations are close to the margins (0 and 15).\n##b\nTo compare the distribution of the Lung Capacity with respect to Male & Females, I used ggplot to create a side by side boxplot of Lung Capacity for female and males in the sample, with female lung capacity in red & male lung capacity in blue.\n\n\nCode\n#load libraries\nlibrary(dplyr)\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nCode\nlibrary(ggplot2)\n\n\nggplot(data = df, aes(x= Gender, y = LungCap, fill = Gender)) +\n  geom_boxplot() +  scale_y_continuous(breaks = c(0,2,4,6,8,10,12,14,16)) +\n  labs(title = \"Box Plot - Lung Capacity by Gender\", x = \"Gender\", y = \"Lung Capacity (L)\") \n\n\n\n\n\nThe box-plot above shows that males in the sample had on average a higher lung capacity than female in the sample, with a higher minimum, Q1, median, Q3, & maximum lung capacity. 50% of the lung capacity of females in the sample was between 5.75 L - 9.25 L with a median lung capacity of ~7.75 L. In contrast, 50% of the lung capacity of males in the sample was between 6.5L - 10.5L with a median lung capacity of ~8.25L.\nFrom the size of the boxes & whiskers, it seems that a good amount of the male & female lung capacity is clustered around the median, but to better visualize the distribution, not just the difference between male females, I created a histogram below facet-wrapped by gender.\n\n\nCode\n##Histogram\nggplot(df, aes(x= LungCap, fill = Gender)) + geom_histogram() + facet_wrap(vars(`Gender`)) + labs(title = \"Histogram - Lung Capacity by Gender\", x = \"Lung Capacity (L)\", y = \"Frequency\") \n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nThe histogram also reflects a higher lung capacity for men in the sample than women as seen on the x-axis. It also shows that the lung capacity is somewhat normally distributed overall with most of the observations near the mean, though there are several peaks & dips in the histogram where a fewer or more individuals have a specific lung capacity than would be seen in a normal distribution. Additionally, the histogram for female lung capacity has more data concentrated around the mean, making it taller & pointier than the male lung capacity distribution. The male lung capacity histogram on the right has two modes (seen as peaks) at 7L & 11L on the left & right of the mean, unlike in a true normal distribution.\n##c\nTo compare the mean lung capacity of smokers to non-smokers in the sample, I used the group_by(), select(), & summarize() functions to get a table of the mean lung capacity in L for smokers vs. non-smokers in the sample.\n\n\nCode\n#Mean smokers vs. non-smokers.\n\ndf %&gt;%\n  group_by(`Smoke`) %&gt;%\n  select(`Smoke`, `LungCap`) %&gt;%\n  summarize_all(mean, na.rm = TRUE)\n\n\n# A tibble: 2 × 2\n  Smoke LungCap\n  &lt;chr&gt;   &lt;dbl&gt;\n1 no       7.77\n2 yes      8.65\n\n\nThe mean lung capacity of smokers (Smoke = yes) was 8.65 L compared to a mean lung capacity of 7.77 L for non-smokers (Smoke = no).\nI was not expecting the mean Lung Capacity for smokers in this sample to be higher r than the mean Lung Capacity for non-smokers in the sample. This is because smoking harms lungs & thus is expected to decrease lung capacity.\nHowever, I anticipate that other factors such as age & gender might contribute to this counter-intuitive finding. For example, we saw in the previous question that males in the sample have a higher lung capacity then females. So if more smokers are male, the difference in lung capacity between smokers & non-smokers might be explained by gender more than smoking status.\n##d\nTo examine the relationship between Smoking and Lung Capacity within age groups: “less than or equal to 13”, “14 to 15”, “16 to 17”, and “greater than or equal to 18”, I used the case_when() to recreate a new Age_Range variable corresponding to these age groups.\n\n\nCode\n##Using Case_When to create age_range variable\ndf_by_age &lt;- df %&gt;%\n  mutate(Age_Range = case_when(\n    Age &lt;= 13 ~ \"13 or younger\",\n    Age &gt;= 14 & Age &lt; 16 ~ \"14 to 15\",\n    Age &gt;= 15 & Age &lt; 18 ~ \"16 to 17\",\n    Age &gt;= 16 ~ \"18 or older\")\n)\n\n\nI first wanted to see the relative age of the sample. As seen in the table below, over half the sample is 13 or younger (484), with the least common age group in thh sample being 18 or older (80).\n\n\nCode\n#Counts by age\ntable(select(df_by_age, Age_Range))\n\n\nAge_Range\n13 or younger      14 to 15      16 to 17   18 or older \n          428           120            97            80 \n\n\nThis got me curious, as I don’t know many 13-year-olds or people younger than 13 who smoke. The below table shows for each age range how many people smoke vs not smoke. While 27 individuals who were 13 or younger in the sample smoked, this is dwarfed by the 401 13-year-old or younger individuals who don’t smoke. While the number of 13 or younger individuals who smoke is higher than the number of 18 or older individuals who smoke (27 vs. 15), relative to the entire age-group a much higher proportion of the 18 or older age group smoked (15/80) than in the 13 or younger group (27/428).\n\n\nCode\n#Counts yes or no smoking by age\ntable(select(df_by_age, Age_Range, Smoke))\n\n\n               Smoke\nAge_Range        no yes\n  13 or younger 401  27\n  14 to 15      105  15\n  16 to 17       77  20\n  18 or older    65  15\n\n\nAfter examining the relationship between age & smoking, I then wanted to examine the relationship between age & lung capacity before factoring in smoking status. I would expect that as you go through puberty and become and adult (18+) your lung capacity with increase from before puberty (13 and younger). The table below using group_by(), select(), & summarize_all() follows this expected trend, with mean lung capacity increasing for each age group from 6.41 L for 13 or younger to 10.96 L for 18+ individuals in the sample. The mean lung capacity of 18+ is only slightly higher than the mean lung capacity at 16-17 years-old.\n\n\nCode\n#Mean lung capacity by age group\ndf_by_age %&gt;%\n  group_by(`Age_Range`) %&gt;%\n  select(`Age_Range`, `LungCap`) %&gt;%\n  summarise_all(mean, na.rm = TRUE)\n\n\n# A tibble: 4 × 2\n  Age_Range     LungCap\n  &lt;chr&gt;           &lt;dbl&gt;\n1 13 or younger    6.41\n2 14 to 15         9.05\n3 16 to 17        10.2 \n4 18 or older     11.0 \n\n\nFinally, I once again used the group_by(), select(), and summarise_all() function, grouping by age_range & smoking status & calculating the mean lung capacity for smokers & non smokers in each age range, which I will interpret in 1e.\n\n\nCode\n##Mean lung cap by age_range smoker vs. non-smoker\ndf_by_age %&gt;%\n  group_by(`Smoke`, `Age_Range`) %&gt;%\n  select(`Age_Range`, `LungCap`, `Smoke`) %&gt;%\n  summarise_all(mean, na.rm = TRUE)\n\n\n# A tibble: 8 × 3\n# Groups:   Smoke [2]\n  Smoke Age_Range     LungCap\n  &lt;chr&gt; &lt;chr&gt;           &lt;dbl&gt;\n1 no    13 or younger    6.36\n2 no    14 to 15         9.14\n3 no    16 to 17        10.5 \n4 no    18 or older     11.1 \n5 yes   13 or younger    7.20\n6 yes   14 to 15         8.39\n7 yes   16 to 17         9.38\n8 yes   18 or older     10.5 \n\n\n##e In contrast to 1c, where smokers had a higher mean lung capacity than non-smokers (8.64 L vs 7.77 L), for every age group except for 13 or younger non-smokers had a higher lung capacity than non-smokers.\n\nFor the 13 or younger: non-smoker lung capacity = 6.36L, smoker lung capacity = 7.20 L\nFor 14-15: non-smoker lung capacity = 9.14 L, smoker lung capacity = 8.39 L\nFor 16-17: non-smoker lung capacity = 10.47 L, smoker lung capacity = 9.38 L\nFor 18+: non-smoker lung capacity = 11.07L, smoker lung capacity = 10.51 L\n\nConsidering that in part d it was found that average lung capacity increases with age & that a smaller proportion of younger individuals in the sample smoked than the older students, one possible explanation for the finding in 1c is that the higher mean lung capacity for smokers overall in the sample could be due to a higher age of smokers in the sample, not due to smoking.\nIt is worth noting also that the sample smoking & nonsmoking mean lung capacity are closer to the mean lung capacity of the 13 & younger & 14-15 year old age groups, reflecting the larger number of younger folks in the sample compare.\nFor fun, I calculated the mean age for smokers & non smokers in the sample, and smokers indeed were older with a mean age of 14.8, compared to a mean non-smoker age of 12.03.\n\n\nCode\n#Mean age smoker vs. non-smoker\ndf %&gt;%\n  group_by(`Smoke`) %&gt;%\n  select(`Smoke`, `Age`) %&gt;%\n  summarise_all(mean, na.rm = TRUE)\n\n\n# A tibble: 2 × 2\n  Smoke   Age\n  &lt;chr&gt; &lt;dbl&gt;\n1 no     12.0\n2 yes    14.8\n\n\n#Question 2\n##a\nTo answer these questions I used R more of a calculator, plugging in the frequencies from the HW1 Prior Convictions frequency table to probability equations we learned in class.\nThe probability that a randomly selected inmate has exactly 2 prior convictions is the P(X=2) which is 0.198.\n\n\nCode\n#Calculating P(X=2)\nprob_2_pc = 160/810\nprob_2_pc\n\n\n[1] 0.1975309\n\n\n##b\nThe probability that a randomly selected inmate has fewer than 2 prior convictions is the P(X &lt; 2) which is equal to the P(X=0 or 1) which is equal to P(X=0) + P(X=1) which equals a probability of 0.694.\n\n\nCode\n#Calculating P(X&lt;2)\nprob_0_pc = 128/810\nprob_1_pc = 434/810\nprob_less_2_pc = prob_0_pc + prob_1_pc\nprob_less_2_pc\n\n\n[1] 0.6938272\n\n\n##c\nThe probability that a randomly selected inmate has 2 or fewer prior convictions is the P(X ≤ 2) which is equal to the P(X=0 or 1 or 2) which is equal to P(X=0) + P(X=1) + P(X=2) which equals a probability of 0.891.\n\n\nCode\n#Calculating P(X≤2)\n\nprob_0_pc = 128/810\nprob_1_pc = 434/810\nprob_2_pc = 160/810\nprob_equal_or_less_2pc = prob_0_pc + prob_1_pc + prob_2_pc\nprob_equal_or_less_2pc\n\n\n[1] 0.891358\n\n\n##d The probability that a randomly selected inmate has greater than 2 prior convictions is the P(X &gt;2) which is equal to the P(X=3 or 4) which is equal to P(X=3) + P(X=4), which gives a probability of 0.109.\nBecause probabilities should be cumulative, the probability of having greater than 2 prior convictions & the probability of having 2 or less prior convictions should add up to 1. Therefor, 1 - P(X≤2) should equal P(X &gt;2) which we confirm it does.\n\n\nCode\n#Calculating P(x&gt;2) the first way\nprob_3_pc = 64/810\nprob_3_pc\n\n\n[1] 0.07901235\n\n\nCode\nprob_4_pc = 24/810\nprob_4_pc\n\n\n[1] 0.02962963\n\n\nCode\nprob_greater_2_pc = prob_3_pc + prob_4_pc\nprob_greater_2_pc\n\n\n[1] 0.108642\n\n\nCode\n#Calculating P(x&gt;2) the second way\nsame = 1 - prob_equal_or_less_2pc\nsame\n\n\n[1] 0.108642\n\n\n##e\nThe expected value for the number of prior convictions is the long term mean of the sample which can be calculated with the equation: 𝐸(𝑋)=𝜇=∑𝑥𝑃(𝑥)\nThe expected value for number of prior convictions is 1.286\nThis is not an integer, but means that in the long-term sample mean will be close to 1, which aligns with the frequency table, where 1 is the most frequent number of prior convictions.\n\n\nCode\n#Expected value calc\nexpected_value = (0 * prob_0_pc) + (1 * (prob_1_pc)) + (2 * (prob_2_pc)) + (3* (prob_3_pc)) + (4 * (prob_4_pc))\nexpected_value\n\n\n[1] 1.28642\n\n\nCode\n##Check that mean same\n\n((0*128)+(1*434) + (2*160) + (3*64) + (4*24))/810\n\n\n[1] 1.28642\n\n\n##f\nTo calculate the variance of the sample I used the equation Var(X) = Σx^2p − μ^2, where the mean is the expected value calculated in e.\nThis gives a variance of 0.856 years squared and a standard deviation of 0.925 years.\n\n\nCode\n#Calculating variance\nVariance = (((0^2) * prob_0_pc) + ((1^2)* prob_1_pc) + ((2^2) * prob_2_pc) + ((3^2) * prob_3_pc) + ((4^2) * prob_4_pc)) - (expected_value^2)\nVariance\n\n\n[1] 0.8562353\n\n\nCode\n#Calculating sd from variance\nsd = sqrt(Variance)\nsd\n\n\n[1] 0.9253298"
  },
  {
    "objectID": "posts/HW1_EmmaNarkewicz.html#a",
    "href": "posts/HW1_EmmaNarkewicz.html#a",
    "title": "Homework 1",
    "section": "",
    "text": "First, let’s read in the data from the Excel file:\n\n\nCode\nlibrary(readxl)\ndf &lt;- read_excel(\"_data/LungCapData.xls\")\n\n\nThe distribution of LungCap looks as follows:\n\n\nCode\nhist(df$LungCap)\n\n\n\n\n\nThe histogram suggests that the distribution is close to a normal distribution. Most of the observations are close to the mean. Very few observations are close to the margins (0 and 15).\n##b\nTo compare the distribution of the Lung Capacity with respect to Male & Females, I used ggplot to create a side by side boxplot of Lung Capacity for female and males in the sample, with female lung capacity in red & male lung capacity in blue.\n\n\nCode\n#load libraries\nlibrary(dplyr)\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nCode\nlibrary(ggplot2)\n\n\nggplot(data = df, aes(x= Gender, y = LungCap, fill = Gender)) +\n  geom_boxplot() +  scale_y_continuous(breaks = c(0,2,4,6,8,10,12,14,16)) +\n  labs(title = \"Box Plot - Lung Capacity by Gender\", x = \"Gender\", y = \"Lung Capacity (L)\") \n\n\n\n\n\nThe box-plot above shows that males in the sample had on average a higher lung capacity than female in the sample, with a higher minimum, Q1, median, Q3, & maximum lung capacity. 50% of the lung capacity of females in the sample was between 5.75 L - 9.25 L with a median lung capacity of ~7.75 L. In contrast, 50% of the lung capacity of males in the sample was between 6.5L - 10.5L with a median lung capacity of ~8.25L.\nFrom the size of the boxes & whiskers, it seems that a good amount of the male & female lung capacity is clustered around the median, but to better visualize the distribution, not just the difference between male females, I created a histogram below facet-wrapped by gender.\n\n\nCode\n##Histogram\nggplot(df, aes(x= LungCap, fill = Gender)) + geom_histogram() + facet_wrap(vars(`Gender`)) + labs(title = \"Histogram - Lung Capacity by Gender\", x = \"Lung Capacity (L)\", y = \"Frequency\") \n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nThe histogram also reflects a higher lung capacity for men in the sample than women as seen on the x-axis. It also shows that the lung capacity is somewhat normally distributed overall with most of the observations near the mean, though there are several peaks & dips in the histogram where a fewer or more individuals have a specific lung capacity than would be seen in a normal distribution. Additionally, the histogram for female lung capacity has more data concentrated around the mean, making it taller & pointier than the male lung capacity distribution. The male lung capacity histogram on the right has two modes (seen as peaks) at 7L & 11L on the left & right of the mean, unlike in a true normal distribution.\n##c\nTo compare the mean lung capacity of smokers to non-smokers in the sample, I used the group_by(), select(), & summarize() functions to get a table of the mean lung capacity in L for smokers vs. non-smokers in the sample.\n\n\nCode\n#Mean smokers vs. non-smokers.\n\ndf %&gt;%\n  group_by(`Smoke`) %&gt;%\n  select(`Smoke`, `LungCap`) %&gt;%\n  summarize_all(mean, na.rm = TRUE)\n\n\n# A tibble: 2 × 2\n  Smoke LungCap\n  &lt;chr&gt;   &lt;dbl&gt;\n1 no       7.77\n2 yes      8.65\n\n\nThe mean lung capacity of smokers (Smoke = yes) was 8.65 L compared to a mean lung capacity of 7.77 L for non-smokers (Smoke = no).\nI was not expecting the mean Lung Capacity for smokers in this sample to be higher r than the mean Lung Capacity for non-smokers in the sample. This is because smoking harms lungs & thus is expected to decrease lung capacity.\nHowever, I anticipate that other factors such as age & gender might contribute to this counter-intuitive finding. For example, we saw in the previous question that males in the sample have a higher lung capacity then females. So if more smokers are male, the difference in lung capacity between smokers & non-smokers might be explained by gender more than smoking status.\n##d\nTo examine the relationship between Smoking and Lung Capacity within age groups: “less than or equal to 13”, “14 to 15”, “16 to 17”, and “greater than or equal to 18”, I used the case_when() to recreate a new Age_Range variable corresponding to these age groups.\n\n\nCode\n##Using Case_When to create age_range variable\ndf_by_age &lt;- df %&gt;%\n  mutate(Age_Range = case_when(\n    Age &lt;= 13 ~ \"13 or younger\",\n    Age &gt;= 14 & Age &lt; 16 ~ \"14 to 15\",\n    Age &gt;= 15 & Age &lt; 18 ~ \"16 to 17\",\n    Age &gt;= 16 ~ \"18 or older\")\n)\n\n\nI first wanted to see the relative age of the sample. As seen in the table below, over half the sample is 13 or younger (484), with the least common age group in thh sample being 18 or older (80).\n\n\nCode\n#Counts by age\ntable(select(df_by_age, Age_Range))\n\n\nAge_Range\n13 or younger      14 to 15      16 to 17   18 or older \n          428           120            97            80 \n\n\nThis got me curious, as I don’t know many 13-year-olds or people younger than 13 who smoke. The below table shows for each age range how many people smoke vs not smoke. While 27 individuals who were 13 or younger in the sample smoked, this is dwarfed by the 401 13-year-old or younger individuals who don’t smoke. While the number of 13 or younger individuals who smoke is higher than the number of 18 or older individuals who smoke (27 vs. 15), relative to the entire age-group a much higher proportion of the 18 or older age group smoked (15/80) than in the 13 or younger group (27/428).\n\n\nCode\n#Counts yes or no smoking by age\ntable(select(df_by_age, Age_Range, Smoke))\n\n\n               Smoke\nAge_Range        no yes\n  13 or younger 401  27\n  14 to 15      105  15\n  16 to 17       77  20\n  18 or older    65  15\n\n\nAfter examining the relationship between age & smoking, I then wanted to examine the relationship between age & lung capacity before factoring in smoking status. I would expect that as you go through puberty and become and adult (18+) your lung capacity with increase from before puberty (13 and younger). The table below using group_by(), select(), & summarize_all() follows this expected trend, with mean lung capacity increasing for each age group from 6.41 L for 13 or younger to 10.96 L for 18+ individuals in the sample. The mean lung capacity of 18+ is only slightly higher than the mean lung capacity at 16-17 years-old.\n\n\nCode\n#Mean lung capacity by age group\ndf_by_age %&gt;%\n  group_by(`Age_Range`) %&gt;%\n  select(`Age_Range`, `LungCap`) %&gt;%\n  summarise_all(mean, na.rm = TRUE)\n\n\n# A tibble: 4 × 2\n  Age_Range     LungCap\n  &lt;chr&gt;           &lt;dbl&gt;\n1 13 or younger    6.41\n2 14 to 15         9.05\n3 16 to 17        10.2 \n4 18 or older     11.0 \n\n\nFinally, I once again used the group_by(), select(), and summarise_all() function, grouping by age_range & smoking status & calculating the mean lung capacity for smokers & non smokers in each age range, which I will interpret in 1e.\n\n\nCode\n##Mean lung cap by age_range smoker vs. non-smoker\ndf_by_age %&gt;%\n  group_by(`Smoke`, `Age_Range`) %&gt;%\n  select(`Age_Range`, `LungCap`, `Smoke`) %&gt;%\n  summarise_all(mean, na.rm = TRUE)\n\n\n# A tibble: 8 × 3\n# Groups:   Smoke [2]\n  Smoke Age_Range     LungCap\n  &lt;chr&gt; &lt;chr&gt;           &lt;dbl&gt;\n1 no    13 or younger    6.36\n2 no    14 to 15         9.14\n3 no    16 to 17        10.5 \n4 no    18 or older     11.1 \n5 yes   13 or younger    7.20\n6 yes   14 to 15         8.39\n7 yes   16 to 17         9.38\n8 yes   18 or older     10.5 \n\n\n##e In contrast to 1c, where smokers had a higher mean lung capacity than non-smokers (8.64 L vs 7.77 L), for every age group except for 13 or younger non-smokers had a higher lung capacity than non-smokers.\n\nFor the 13 or younger: non-smoker lung capacity = 6.36L, smoker lung capacity = 7.20 L\nFor 14-15: non-smoker lung capacity = 9.14 L, smoker lung capacity = 8.39 L\nFor 16-17: non-smoker lung capacity = 10.47 L, smoker lung capacity = 9.38 L\nFor 18+: non-smoker lung capacity = 11.07L, smoker lung capacity = 10.51 L\n\nConsidering that in part d it was found that average lung capacity increases with age & that a smaller proportion of younger individuals in the sample smoked than the older students, one possible explanation for the finding in 1c is that the higher mean lung capacity for smokers overall in the sample could be due to a higher age of smokers in the sample, not due to smoking.\nIt is worth noting also that the sample smoking & nonsmoking mean lung capacity are closer to the mean lung capacity of the 13 & younger & 14-15 year old age groups, reflecting the larger number of younger folks in the sample compare.\nFor fun, I calculated the mean age for smokers & non smokers in the sample, and smokers indeed were older with a mean age of 14.8, compared to a mean non-smoker age of 12.03.\n\n\nCode\n#Mean age smoker vs. non-smoker\ndf %&gt;%\n  group_by(`Smoke`) %&gt;%\n  select(`Smoke`, `Age`) %&gt;%\n  summarise_all(mean, na.rm = TRUE)\n\n\n# A tibble: 2 × 2\n  Smoke   Age\n  &lt;chr&gt; &lt;dbl&gt;\n1 no     12.0\n2 yes    14.8\n\n\n#Question 2\n##a\nTo answer these questions I used R more of a calculator, plugging in the frequencies from the HW1 Prior Convictions frequency table to probability equations we learned in class.\nThe probability that a randomly selected inmate has exactly 2 prior convictions is the P(X=2) which is 0.198.\n\n\nCode\n#Calculating P(X=2)\nprob_2_pc = 160/810\nprob_2_pc\n\n\n[1] 0.1975309\n\n\n##b\nThe probability that a randomly selected inmate has fewer than 2 prior convictions is the P(X &lt; 2) which is equal to the P(X=0 or 1) which is equal to P(X=0) + P(X=1) which equals a probability of 0.694.\n\n\nCode\n#Calculating P(X&lt;2)\nprob_0_pc = 128/810\nprob_1_pc = 434/810\nprob_less_2_pc = prob_0_pc + prob_1_pc\nprob_less_2_pc\n\n\n[1] 0.6938272\n\n\n##c\nThe probability that a randomly selected inmate has 2 or fewer prior convictions is the P(X ≤ 2) which is equal to the P(X=0 or 1 or 2) which is equal to P(X=0) + P(X=1) + P(X=2) which equals a probability of 0.891.\n\n\nCode\n#Calculating P(X≤2)\n\nprob_0_pc = 128/810\nprob_1_pc = 434/810\nprob_2_pc = 160/810\nprob_equal_or_less_2pc = prob_0_pc + prob_1_pc + prob_2_pc\nprob_equal_or_less_2pc\n\n\n[1] 0.891358\n\n\n##d The probability that a randomly selected inmate has greater than 2 prior convictions is the P(X &gt;2) which is equal to the P(X=3 or 4) which is equal to P(X=3) + P(X=4), which gives a probability of 0.109.\nBecause probabilities should be cumulative, the probability of having greater than 2 prior convictions & the probability of having 2 or less prior convictions should add up to 1. Therefor, 1 - P(X≤2) should equal P(X &gt;2) which we confirm it does.\n\n\nCode\n#Calculating P(x&gt;2) the first way\nprob_3_pc = 64/810\nprob_3_pc\n\n\n[1] 0.07901235\n\n\nCode\nprob_4_pc = 24/810\nprob_4_pc\n\n\n[1] 0.02962963\n\n\nCode\nprob_greater_2_pc = prob_3_pc + prob_4_pc\nprob_greater_2_pc\n\n\n[1] 0.108642\n\n\nCode\n#Calculating P(x&gt;2) the second way\nsame = 1 - prob_equal_or_less_2pc\nsame\n\n\n[1] 0.108642\n\n\n##e\nThe expected value for the number of prior convictions is the long term mean of the sample which can be calculated with the equation: 𝐸(𝑋)=𝜇=∑𝑥𝑃(𝑥)\nThe expected value for number of prior convictions is 1.286\nThis is not an integer, but means that in the long-term sample mean will be close to 1, which aligns with the frequency table, where 1 is the most frequent number of prior convictions.\n\n\nCode\n#Expected value calc\nexpected_value = (0 * prob_0_pc) + (1 * (prob_1_pc)) + (2 * (prob_2_pc)) + (3* (prob_3_pc)) + (4 * (prob_4_pc))\nexpected_value\n\n\n[1] 1.28642\n\n\nCode\n##Check that mean same\n\n((0*128)+(1*434) + (2*160) + (3*64) + (4*24))/810\n\n\n[1] 1.28642\n\n\n##f\nTo calculate the variance of the sample I used the equation Var(X) = Σx^2p − μ^2, where the mean is the expected value calculated in e.\nThis gives a variance of 0.856 years squared and a standard deviation of 0.925 years.\n\n\nCode\n#Calculating variance\nVariance = (((0^2) * prob_0_pc) + ((1^2)* prob_1_pc) + ((2^2) * prob_2_pc) + ((3^2) * prob_3_pc) + ((4^2) * prob_4_pc)) - (expected_value^2)\nVariance\n\n\n[1] 0.8562353\n\n\nCode\n#Calculating sd from variance\nsd = sqrt(Variance)\nsd\n\n\n[1] 0.9253298"
  },
  {
    "objectID": "posts/XiaoyanHu_Finalproject1.html",
    "href": "posts/XiaoyanHu_Finalproject1.html",
    "title": "Final Project Checkin-1",
    "section": "",
    "text": "Code\nlibrary(tidyr)\nlibrary(dplyr)\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nCode\nlibrary(readxl)\nlibrary(ggplot2)"
  },
  {
    "objectID": "posts/XiaoyanHu_Finalproject1.html#introduction-and-background",
    "href": "posts/XiaoyanHu_Finalproject1.html#introduction-and-background",
    "title": "Final Project Checkin-1",
    "section": "Introduction and background",
    "text": "Introduction and background\nThe Chinese government implemented the one-child policy in 1979, which resulted in the increasing proportion of one-child families and the “four-two-one” family structure consisting of four grandparents, two parents, and one child. Despite being blessed with relatively more family and social resources, only children may face physical and socio-psychological problems during development, including an elevated risk for overweight and obesity and negative psychosocial consequences. Previous studies have shown that only children had a higher likelihood of overweight or obesity, compared with children who had one or more siblings. Over obesity, mental healthy is also interesting to explore that how it is related to overweight/obesity, as well as sib-size, in young adolescents affects mental health.。"
  },
  {
    "objectID": "posts/XiaoyanHu_Finalproject1.html#research-questions",
    "href": "posts/XiaoyanHu_Finalproject1.html#research-questions",
    "title": "Final Project Checkin-1",
    "section": "research questions",
    "text": "research questions\n\nDoes obesity positively related to mental health?\nwhat are factors that affects mental healthy?\ndoes sibling or obeisty directily related to mental health?"
  },
  {
    "objectID": "posts/XiaoyanHu_Finalproject1.html#key-predictors",
    "href": "posts/XiaoyanHu_Finalproject1.html#key-predictors",
    "title": "Final Project Checkin-1",
    "section": "key predictors",
    "text": "key predictors\n\nmental health\nsibling number\nobisity rate\ngender"
  },
  {
    "objectID": "posts/XiaoyanHu_Finalproject1.html#hypothesis",
    "href": "posts/XiaoyanHu_Finalproject1.html#hypothesis",
    "title": "Final Project Checkin-1",
    "section": "hypothesis",
    "text": "hypothesis\n\nHigher obesity rate increase the risk of depression\nhigher family income increase the rate of obesity\nMore sibling reduce the risk of both depression and anxiety."
  },
  {
    "objectID": "posts/XiaoyanHu_Finalproject1.html#data-description",
    "href": "posts/XiaoyanHu_Finalproject1.html#data-description",
    "title": "Final Project Checkin-1",
    "section": "data description",
    "text": "data description\n\n\nCode\ndata&lt;-read_excel(\"/Users/cassie199/Desktop/23spring/603_Spring_2023-1/posts/_data/mentalhealth_data.xlsx\")\nhead(data)\n\n\n# A tibble: 6 × 29\n  T0depres…¹ T0anx…² T1dep…³ T1anx…⁴ Height Weight    WC    HC   SBP   DBP   FBG\n       &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1         31      35      41      35   153.   34.6    58  67      98    60   4.4\n2         35      24      35      25   172.   46.1    63  78     110    70   3.9\n3         31      34      37      26   146.   38.9    72  77.7   102    62   4.6\n4         27      31      42      35   162.   46.8    62  80     116    80   4.5\n5         31      26      49      33   154.   36.4    56  72      90    60   4.2\n6         30      28      47      32   164.   40.6    55  73     102    70   3.7\n# … with 18 more variables: TC &lt;dbl&gt;, TG &lt;dbl&gt;, `HDL-C` &lt;dbl&gt;, `LDL-C` &lt;dbl&gt;,\n#   BMI &lt;dbl&gt;, WHR &lt;dbl&gt;, WtHR &lt;dbl&gt;, `Family location` &lt;dbl&gt;,\n#   `Number of siblings` &lt;dbl&gt;,\n#   `How much time do you spend with your father in elementary school?` &lt;dbl&gt;,\n#   `How much time do you spend with your mother in elementary school?` &lt;dbl&gt;,\n#   `Father’s education level` &lt;dbl&gt;, `Mother’s education level` &lt;dbl&gt;,\n#   `Family financial situation` &lt;dbl&gt;, `Sleeping hours` &lt;dbl&gt;, …\n\n\nCode\nglimpse(data)\n\n\nRows: 1,348\nColumns: 29\n$ T0depression                                                        &lt;dbl&gt; 31…\n$ T0anxiety                                                           &lt;dbl&gt; 35…\n$ T1depression                                                        &lt;dbl&gt; 41…\n$ T1anxiety                                                           &lt;dbl&gt; 35…\n$ Height                                                              &lt;dbl&gt; 15…\n$ Weight                                                              &lt;dbl&gt; 34…\n$ WC                                                                  &lt;dbl&gt; 58…\n$ HC                                                                  &lt;dbl&gt; 67…\n$ SBP                                                                 &lt;dbl&gt; 98…\n$ DBP                                                                 &lt;dbl&gt; 60…\n$ FBG                                                                 &lt;dbl&gt; 4.…\n$ TC                                                                  &lt;dbl&gt; 3.…\n$ TG                                                                  &lt;dbl&gt; 0.…\n$ `HDL-C`                                                             &lt;dbl&gt; 0.…\n$ `LDL-C`                                                             &lt;dbl&gt; 2.…\n$ BMI                                                                 &lt;dbl&gt; 14…\n$ WHR                                                                 &lt;dbl&gt; 0.…\n$ WtHR                                                                &lt;dbl&gt; 0.…\n$ `Family location`                                                   &lt;dbl&gt; 2,…\n$ `Number of siblings`                                                &lt;dbl&gt; 2,…\n$ `How much time do you spend with your father in elementary school?` &lt;dbl&gt; 5,…\n$ `How much time do you spend with your mother in elementary school?` &lt;dbl&gt; 5,…\n$ `Father’s education level`                                          &lt;dbl&gt; 4,…\n$ `Mother’s education level`                                          &lt;dbl&gt; 3,…\n$ `Family financial situation`                                        &lt;dbl&gt; 3,…\n$ `Sleeping hours`                                                    &lt;dbl&gt; 3,…\n$ `Skipping breakfast`                                                &lt;dbl&gt; 1,…\n$ Vigorous                                                            &lt;dbl&gt; 1,…\n$ Moderate                                                            &lt;dbl&gt; 2,…\n\n\nCode\nsum(is.na(data))\n\n\n[1] 728\n\n\nCode\nplot(data$T0depression~data$BMI)\n\n\n\n\n\nThis dataset including 1348 variables and 29 columns. there are 728 NA in this data set. all variables was presented as numberic data. descriptive data was also presented as degrees such as education level, family financial situation and depression rate. By pre-plotting depression rate vs BMI, we can see that some ouliers may need to deal with and there is no siginifcant disrtibution on graph. More data processing is needed in future process."
  },
  {
    "objectID": "posts/HW4_JustineShakespeare.html",
    "href": "posts/HW4_JustineShakespeare.html",
    "title": "Homework 4",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(alr4)\nlibrary(smss)\nlibrary(stargazer)"
  },
  {
    "objectID": "posts/HW4_JustineShakespeare.html#a",
    "href": "posts/HW4_JustineShakespeare.html#a",
    "title": "Homework 4",
    "section": "A",
    "text": "A\nA particular home of 1240 square feet on a lot of 18,000 square feet sold for $145,000. Find the predicted selling price and the residual, and interpret.\nx1 = 1240 x2 = 18000 y = 145000\n\n\nCode\nx1 = 1240\nx2 = 18000\n\n-10536 + (53.8*x1) + (2.84*x2)\n\n\n[1] 107296\n\n\nThe predicted selling point is $107,296 for this house. Given that the actual selling value is $145,000, this model underpredicts the value of the house.\nThe residual for this data point is the difference between y_hat and y. In this case, that is $145,000 - $107,296, which is $37,704. This house sold for $37,704 more than the predicted value.\n\n\nCode\n145000 - 107296\n\n\n[1] 37704"
  },
  {
    "objectID": "posts/HW4_JustineShakespeare.html#b",
    "href": "posts/HW4_JustineShakespeare.html#b",
    "title": "Homework 4",
    "section": "B",
    "text": "B\nFor fixed lot size, how much is the house selling price predicted to increase for each square-foot increase in home size? Why?\nThe house selling price is predicted to increase $53.80 for each square-foot increase, controlling for lot size. 53.80 is the slope or coefficient of the house size.\nŷ = −10,536 + 53.8x1 + 2.84x2"
  },
  {
    "objectID": "posts/HW4_JustineShakespeare.html#c",
    "href": "posts/HW4_JustineShakespeare.html#c",
    "title": "Homework 4",
    "section": "C",
    "text": "C\nAccording to this prediction equation, for fixed home size, how much would lot size need to increase to have the same impact as a one-square-foot increase in home size?\nA one square foot increase in the size of the home would lead to a $53.80 increase in the sales prices, and a one square foot increase in the size of the lot would lead to a $2.84 increase in the sales price. The find how much the lot size would need to increase to have the same impact as a one square foot increase in home size we can this equation 53.8=x*2.84 and solve for x.\n\n\nCode\n53.8/2.84\n\n\n[1] 18.94366\n\n\nThe lot would need to increase by over 18.94 square feet to increase the sales price as much as one square foot increase in home size."
  },
  {
    "objectID": "posts/HW4_JustineShakespeare.html#a-1",
    "href": "posts/HW4_JustineShakespeare.html#a-1",
    "title": "Homework 4",
    "section": "A",
    "text": "A\nTest the hypothesis that the mean salary for men and women is the same, without regard to any other variable but sex. Explain your findings.\nTo test that hypothesis that the mean salary for men and women is the same, we’ll run a two sample t-test.\n\n\nCode\nt.test(formula = salary ~ sex, data = salary)\n\n\n\n    Welch Two Sample t-test\n\ndata:  salary by sex\nt = 1.7744, df = 21.591, p-value = 0.09009\nalternative hypothesis: true difference in means between group Male and group Female is not equal to 0\n95 percent confidence interval:\n -567.8539 7247.1471\nsample estimates:\n  mean in group Male mean in group Female \n            24696.79             21357.14 \n\n\nThe p-value of this test is larger than 0.05, so if we choose that for our alpha value then we should retain the null hypothesis that the mean salary for men and women is the same. The 95% confidence interval also spans from -567.8539 to 7,247.1471, which includes 0 and indicates that the means are not significantly different from one another."
  },
  {
    "objectID": "posts/HW4_JustineShakespeare.html#b-1",
    "href": "posts/HW4_JustineShakespeare.html#b-1",
    "title": "Homework 4",
    "section": "B",
    "text": "B\nRun a multiple linear regression with salary as the outcome variable and everything else as predictors, including sex. Assuming no interactions between sex and the other predictors, obtain a 95% confidence interval for the difference in salary between males and females.\n\n\nCode\nQbLM &lt;- (lm(formula = salary ~ degree + rank + sex + year + ysdeg, data = salary))\n\nstargazer(QbLM, data = salary, type = 'text')\n\n\n\n===============================================\n                        Dependent variable:    \n                    ---------------------------\n                              salary           \n-----------------------------------------------\ndegreePhD                    1,388.613         \n                            (1,018.747)        \n                                               \nrankAssoc                  5,292.361***        \n                            (1,145.398)        \n                                               \nrankProf                   11,118.760***       \n                            (1,351.772)        \n                                               \nsexFemale                    1,166.373         \n                             (925.569)         \n                                               \nyear                        476.309***         \n                             (94.914)          \n                                               \nysdeg                        -124.574          \n                             (77.486)          \n                                               \nConstant                   15,746.050***       \n                             (800.178)         \n                                               \n-----------------------------------------------\nObservations                    52             \nR2                             0.855           \nAdjusted R2                    0.836           \nResidual Std. Error     2,398.418 (df = 45)    \nF Statistic           44.239*** (df = 6; 45)   \n===============================================\nNote:               *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01\n\n===============================================\nStatistic N     Mean    St. Dev.   Min    Max  \n-----------------------------------------------\nyear      52   7.481      5.508     0      25  \nysdeg     52   16.115    10.222     1      35  \nsalary    52 23,797.650 5,917.289 15,000 38,045\n-----------------------------------------------\n\n\nAfter running the multiple linear regression and using the stargazer() function we see in the results that the coefficient for the sex variable is 1166.37 for sex Female with a standard error of 925.569. We can also see from these results that there are 52 observations, so n = 52. With these figures we can calculate the 95% confidence interval.\n\n\nCode\n# CI = (X bar) ± (t × s/sqrt(n)) = CI = (X bar) ± (t × se)\n\nt_score &lt;- qt(.025, df = 45)\n\n1166.373 + (t_score * 925.569)\n\n\n[1] -697.8187\n\n\nCode\n1166.373 - (t_score * 925.569)\n\n\n[1] 3030.565\n\n\nOr we can use the confint() function in R:\n\n\nCode\n  confint(QbLM) \n\n\n                 2.5 %      97.5 %\n(Intercept) 14134.4059 17357.68946\ndegreePhD    -663.2482  3440.47485\nrankAssoc    2985.4107  7599.31080\nrankProf     8396.1546 13841.37340\nsexFemale    -697.8183  3030.56452\nyear          285.1433   667.47476\nysdeg        -280.6397    31.49105"
  },
  {
    "objectID": "posts/HW4_JustineShakespeare.html#c-1",
    "href": "posts/HW4_JustineShakespeare.html#c-1",
    "title": "Homework 4",
    "section": "C",
    "text": "C\nInterpret your finding for each predictor variable; discuss (a) statistical significance, (b) interpretation of the coefficient / slope in relation to the outcome variable and other variables\nIn this regression analysis the sex, ysdeg, and degree variables are not statistically significant. Because these variables are not significant I will not interpret the coefficients.\nThe rank and year variables were found to be statistically significant at the level of 0.01 (1%). The coefficients indicate that with the “Assoc” rank, a person will make $5,292.361 more than a person with the “Asst” rank (since the “Assoc” rank was not included in the regression output we know it was the baseline). With the rank “Prof” a person will make $11,118.760 more than a person with “Asst” rank.\nWith each unit increase of the year variable a person will make $476.309 more in salary."
  },
  {
    "objectID": "posts/HW4_JustineShakespeare.html#d",
    "href": "posts/HW4_JustineShakespeare.html#d",
    "title": "Homework 4",
    "section": "D",
    "text": "D\nChange the baseline category for the rank variable. Interpret the coefficients related to rank again.\n\n\nCode\nsalary$rank &lt;- relevel(salary$rank, ref = \"Prof\")\n\nsummary(lm(formula = salary ~ degree + rank + sex + year + ysdeg, data = salary))\n\n\n\nCall:\nlm(formula = salary ~ degree + rank + sex + year + ysdeg, data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4045.2 -1094.7  -361.5   813.2  9193.1 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  26864.81    1375.29  19.534  &lt; 2e-16 ***\ndegreePhD     1388.61    1018.75   1.363    0.180    \nrankAsst    -11118.76    1351.77  -8.225 1.62e-10 ***\nrankAssoc    -5826.40    1012.93  -5.752 7.28e-07 ***\nsexFemale     1166.37     925.57   1.260    0.214    \nyear           476.31      94.91   5.018 8.65e-06 ***\nysdeg         -124.57      77.49  -1.608    0.115    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2398 on 45 degrees of freedom\nMultiple R-squared:  0.855, Adjusted R-squared:  0.8357 \nF-statistic: 44.24 on 6 and 45 DF,  p-value: &lt; 2.2e-16\n\n\nI used the relevel() function to change the baseline value for the rank variable to “Prof”. Now the results tell us that people with a “Asst” rank make $11,118.76 less than people with a “Prof” rank, and people with an “Assoc” rank make $5,826.40 than those with a “Prof” rank. Both of these findings are statistically significant."
  },
  {
    "objectID": "posts/HW4_JustineShakespeare.html#e",
    "href": "posts/HW4_JustineShakespeare.html#e",
    "title": "Homework 4",
    "section": "E",
    "text": "E\n… Exclude the variable rank, refit, and summarize how your findings changed, if they did.\n\n\nCode\nQbLM &lt;- (lm(formula = salary ~ degree + rank + sex + year + ysdeg, data = salary))\n\nQbLM_noRank &lt;- (lm(formula = salary ~ degree + sex + year + ysdeg, data = salary))\n\nstargazer(QbLM, QbLM_noRank, data = salary, type = 'text')\n\n\n\n=================================================================\n                                 Dependent variable:             \n                    ---------------------------------------------\n                                       salary                    \n                             (1)                    (2)          \n-----------------------------------------------------------------\ndegreePhD                 1,388.613             -3,299.349**     \n                         (1,018.747)            (1,302.520)      \n                                                                 \nrankAsst                -11,118.760***                           \n                         (1,351.772)                             \n                                                                 \nrankAssoc               -5,826.403***                            \n                         (1,012.933)                             \n                                                                 \nsexFemale                 1,166.373              -1,286.544      \n                          (925.569)             (1,313.089)      \n                                                                 \nyear                      476.309***             351.969**       \n                           (94.914)              (142.481)       \n                                                                 \nysdeg                      -124.574              339.399***      \n                           (77.486)               (80.621)       \n                                                                 \nConstant                26,864.810***          17,183.570***     \n                         (1,375.288)            (1,147.942)      \n                                                                 \n-----------------------------------------------------------------\nObservations                  52                     52          \nR2                          0.855                  0.631         \nAdjusted R2                 0.836                  0.600         \nResidual Std. Error  2,398.418 (df = 45)    3,743.502 (df = 47)  \nF Statistic         44.239*** (df = 6; 45) 20.107*** (df = 4; 47)\n=================================================================\nNote:                                 *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01\n\n===============================================\nStatistic N     Mean    St. Dev.   Min    Max  \n-----------------------------------------------\nyear      52   7.481      5.508     0      25  \nysdeg     52   16.115    10.222     1      35  \nsalary    52 23,797.650 5,917.289 15,000 38,045\n-----------------------------------------------\n\n\nRemoving the rank variable from the analysis changes the significance of some variables. With the rank variable included in the model the variables degree and ysdeg were not significant and the variable year was significant at the 0.01 (1%) level. With the rank variable removed both the degree and ysdeg variables are now statistically significant, with degree at the 0.05 (5%) level and ysdeg at the 0.01 (1%) level. The year variable also dropped one significance level, from 0.01 (1%) to 0.05 (5%).\nThe sign also flipped for the variables ysdeg, sexFemale, and degreePhD."
  },
  {
    "objectID": "posts/HW4_JustineShakespeare.html#f",
    "href": "posts/HW4_JustineShakespeare.html#f",
    "title": "Homework 4",
    "section": "F",
    "text": "F\nEveryone in this dataset was hired the year they earned their highest degree. It is also known that a new Dean was appointed 15 years ago, and everyone in the dataset who earned their highest degree 15 years ago or less than that has been hired by the new Dean. Some people have argued that the new Dean has been making offers that are a lot more generous to newly hired faculty than the previous one and that this might explain some of the variation in Salary.\nCreate a new variable that would allow you to test this hypothesis and run another multiple regression model to test this. Select variables carefully to make sure there is no multicollinearity. Explain why multicollinearity would be a concern in this case and how you avoided it. Do you find support for the hypothesis that the people hired by the new Dean are making higher than those that were not?\n\n\nCode\nQ2F &lt;- salary %&gt;% \n  mutate(\"new_dean\" = case_when(\n    ysdeg &lt;=15 ~ 1, \n    ysdeg &gt;15 ~ 0))\n\nsummary(lm(salary ~ new_dean + degree + rank + sex + year, data = Q2F))\n\n\n\nCall:\nlm(formula = salary ~ new_dean + degree + rank + sex + year, \n    data = Q2F)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3403.3 -1387.0  -167.0   528.2  9233.8 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  24425.32    1107.52  22.054  &lt; 2e-16 ***\nnew_dean      2163.46    1072.04   2.018   0.0496 *  \ndegreePhD      818.93     797.48   1.027   0.3100    \nrankAsst    -11096.95    1191.00  -9.317 4.54e-12 ***\nrankAssoc    -6124.28    1028.58  -5.954 3.65e-07 ***\nsexFemale      907.14     840.54   1.079   0.2862    \nyear           434.85      78.89   5.512 1.65e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2362 on 45 degrees of freedom\nMultiple R-squared:  0.8594,    Adjusted R-squared:  0.8407 \nF-statistic: 45.86 on 6 and 45 DF,  p-value: &lt; 2.2e-16\n\n\nSince the new variable, new_dean, was built from the ysdeg variable, to avoid multicollinearity I did not include the ysdeg variable in the regression model (since the new_dean variable was included). We can confirm that these two variables would likely cause multicollinearity by checking that their correlation is high.\n\n\nCode\ncor.test(Q2F$new_dean, Q2F$ysdeg)\n\n\n\n    Pearson's product-moment correlation\n\ndata:  Q2F$new_dean and Q2F$ysdeg\nt = -11.101, df = 50, p-value = 4.263e-15\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.9074548 -0.7411040\nsample estimates:\n       cor \n-0.8434239 \n\n\nIt appears that these two variables have a high negative correlation.\nThe output of the regression with the new_dean variable shows that if a person was hired by the new dean they would earn $2,163.46 more in salary than if they were hired by the old dean. This finding is significant at the 0.05 (5%) level."
  },
  {
    "objectID": "posts/HW4_JustineShakespeare.html#a-2",
    "href": "posts/HW4_JustineShakespeare.html#a-2",
    "title": "Homework 4",
    "section": "A",
    "text": "A\nUsing the house.selling.price data, run and report regression results modeling y = selling price (in dollars) in terms of size of home (in square feet) and whether the home is new (1 = yes; 0 = no). In particular, for each variable; discuss statistical significance and interpret the meaning of the coefficient.\n\n\nCode\ndata(house.selling.price)\nglimpse(house.selling.price)\n\n\nRows: 100\nColumns: 7\n$ case  &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 1…\n$ Taxes &lt;int&gt; 3104, 1173, 3076, 1608, 1454, 2997, 4054, 3002, 6627, 320, 630, …\n$ Beds  &lt;int&gt; 4, 2, 4, 3, 3, 3, 3, 3, 5, 3, 3, 3, 3, 3, 3, 2, 3, 3, 2, 3, 2, 4…\n$ Baths &lt;int&gt; 2, 1, 2, 2, 3, 2, 2, 2, 4, 2, 2, 2, 2, 2, 1, 1, 2, 1, 2, 2, 1, 3…\n$ New   &lt;int&gt; 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1…\n$ Price &lt;int&gt; 279900, 146500, 237700, 200000, 159900, 499900, 265500, 289900, …\n$ Size  &lt;int&gt; 2048, 912, 1654, 2068, 1477, 3153, 1355, 2075, 3990, 1160, 1220,…\n\n\n\n\nCode\nhouseData &lt;- (lm(Price ~ Size + New, data = house.selling.price))\nsummary(houseData)\n\n\n\nCall:\nlm(formula = Price ~ Size + New, data = house.selling.price)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-205102  -34374   -5778   18929  163866 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -40230.867  14696.140  -2.738  0.00737 ** \nSize           116.132      8.795  13.204  &lt; 2e-16 ***\nNew          57736.283  18653.041   3.095  0.00257 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 53880 on 97 degrees of freedom\nMultiple R-squared:  0.7226,    Adjusted R-squared:  0.7169 \nF-statistic: 126.3 on 2 and 97 DF,  p-value: &lt; 2.2e-16\n\n\nThe regression output indicates that both the Size and the New variable are significant (Size at the .1% level and New at the 1% level.) The coefficients tell use that with an increase in size, the price will increase by $116.132, and that the price for new houses will be $57,736.283 more than old houses."
  },
  {
    "objectID": "posts/HW4_JustineShakespeare.html#b-2",
    "href": "posts/HW4_JustineShakespeare.html#b-2",
    "title": "Homework 4",
    "section": "B",
    "text": "B\nReport and interpret the prediction equation, and form separate equations relating selling price to size for new and for not new homes.\nE(price) = -40,230.867 + 116.132 * Size + 57736.283 * New\nNew home: E(price) = -40,230.867 + 116.132 * Size + 57736.283 * 1\nWe use 1 for x1 with a new home and get the following equation: E(price) = 17,505.42 + 116.132 * Size\nOld home: E(price) = -40,230.867 + 116.132 * Size + 57736.283 * 0\nWe use 0 for x1 with an old home and get the following equation:\nE(price) = -40,230.867 + 116.132 * Size"
  },
  {
    "objectID": "posts/HW4_JustineShakespeare.html#c-2",
    "href": "posts/HW4_JustineShakespeare.html#c-2",
    "title": "Homework 4",
    "section": "C",
    "text": "C\nFind the predicted selling price for a home of 3000 square feet that is (i) new, (ii) not new.\n\n\nCode\n# Equation: -40,230.867 + 116.132 * size + 57736.283 * new\n\n# new home\nsize &lt;- 3000\nnew &lt;- 1\n-40230.867 + 116.132 * size + 57736.283 * new\n\n\n[1] 365901.4\n\n\nCode\n# old home\nsize &lt;- 3000\nnew &lt;- 0\n-40230.867 + 116.132 * size + 57736.283 * new\n\n\n[1] 308165.1\n\n\nThe predicted price of a new 3,000 square foot home is $365,901.40 and the predicted price of an old 3,000 square foot home is $308,165.10."
  },
  {
    "objectID": "posts/HW4_JustineShakespeare.html#d-1",
    "href": "posts/HW4_JustineShakespeare.html#d-1",
    "title": "Homework 4",
    "section": "D",
    "text": "D\nFit another model, this time with an interaction term allowing interaction between size and new, and report the regression results\n\n\nCode\nsummary(lm(Price ~ Size + New + Size*New, data = house.selling.price))\n\n\n\nCall:\nlm(formula = Price ~ Size + New + Size * New, data = house.selling.price)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-175748  -28979   -6260   14693  192519 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -22227.808  15521.110  -1.432  0.15536    \nSize           104.438      9.424  11.082  &lt; 2e-16 ***\nNew         -78527.502  51007.642  -1.540  0.12697    \nSize:New        61.916     21.686   2.855  0.00527 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 52000 on 96 degrees of freedom\nMultiple R-squared:  0.7443,    Adjusted R-squared:  0.7363 \nF-statistic: 93.15 on 3 and 96 DF,  p-value: &lt; 2.2e-16\n\n\nThe regression results indicate that the Size variable is still significant at the 0.001 (.1%) level, but the New variable is no longer significant. The interaction term Size*New is significant at the 0.01 (1%) level."
  },
  {
    "objectID": "posts/HW4_JustineShakespeare.html#e-1",
    "href": "posts/HW4_JustineShakespeare.html#e-1",
    "title": "Homework 4",
    "section": "E",
    "text": "E\nReport the lines relating the predicted selling price to the size for homes that are (i) new, (ii) not new.\nI assume by “report the lines” we mean interpret the coefficients here…\nequation: E(price) = -40230.867 + 104.438 * size - 78527.502 * new + 61.916(size*new)\nNew house: We replace “new” with 1. -22227.808 + 104.438 * size - 78527.502 * 1 + 61.916(size*1)\n-100755.3 + 166.354 * size\nOld house: We replace “new” with 0 -22227.808 + 104.438 * size - 78527.502 * 0 + 61.916(size*0)\n-22227.808 + 104.438 * size"
  },
  {
    "objectID": "posts/HW4_JustineShakespeare.html#f-1",
    "href": "posts/HW4_JustineShakespeare.html#f-1",
    "title": "Homework 4",
    "section": "F",
    "text": "F\nFind the predicted selling price for a home of 3000 square feet that is (i) new, (ii) not new.\n\n\nCode\n# new house\nsize &lt;- 3000\nnew &lt;- 1\n-22227.808 + 104.438 * size - 78527.502 * new + 61.916*(size*new)\n\n\n[1] 398306.7\n\n\nCode\n# old house\nnew &lt;- 0\n-22227.808 + 104.438 * size - 78527.502 * new + 61.916*(size*new)\n\n\n[1] 291086.2\n\n\n\nThe predicted price for a new house that is 3,000 square feet is $398,306.70.\nThe predicted price for an old house of 3,000 square feet is $291,086.20."
  },
  {
    "objectID": "posts/HW4_JustineShakespeare.html#g",
    "href": "posts/HW4_JustineShakespeare.html#g",
    "title": "Homework 4",
    "section": "G",
    "text": "G\nFind the predicted selling price for a home of 1500 square feet that is (i) new, (ii) not new. Comparing to (F), explain how the difference in predicted selling prices changes as the size of home increases.\n\n\nCode\n# new house\nsize &lt;- 1500\nnew &lt;- 1\n-22227.808 + 104.438 * size - 78527.502 * new + 61.916*(size*new)\n\n\n[1] 148775.7\n\n\nCode\n# old house\nnew &lt;- 0\n-22227.808 + 104.438 * size - 78527.502 * new + 61.916*(size*new)\n\n\n[1] 134429.2\n\n\n\nThe predicted price for a new house that is 1,500 square feet is $148,775.70.\nThe predicted price for an old house of 1,500 square feet is $134,429.20.\n\n\n\nCode\n# F\n398306.7 - 291086.2\n\n\n[1] 107220.5\n\n\nCode\n# G\n148775.7 - 134429.2\n\n\n[1] 14346.5\n\n\nThe price difference between old and new houses is greater for larger houses ($107,220.50) than for smaller houses ($14,346.50). This is captured in the interaction term."
  },
  {
    "objectID": "posts/HW4_JustineShakespeare.html#h",
    "href": "posts/HW4_JustineShakespeare.html#h",
    "title": "Homework 4",
    "section": "H",
    "text": "H\nDo you think the model with interaction or the one without it represents the relationship of size and new to the outcome price? What makes you prefer one model over another?\n\n\nCode\nHSPmodel &lt;- (lm(Price ~ Size + New, data = house.selling.price))\n\nHSPmodel_ia &lt;- (lm(Price ~ Size + New + Size*New, data = house.selling.price))\n\nstargazer(HSPmodel, HSPmodel_ia, type = 'text')\n\n\n\n==================================================================\n                                 Dependent variable:              \n                    ----------------------------------------------\n                                        Price                     \n                              (1)                    (2)          \n------------------------------------------------------------------\nSize                      116.132***              104.438***      \n                            (8.795)                (9.424)        \n                                                                  \nNew                      57,736.280***           -78,527.500      \n                         (18,653.040)            (51,007.640)     \n                                                                  \nSize:New                                          61.916***       \n                                                   (21.686)       \n                                                                  \nConstant                -40,230.870***           -22,227.810      \n                         (14,696.140)            (15,521.110)     \n                                                                  \n------------------------------------------------------------------\nObservations                  100                    100          \nR2                           0.723                  0.744         \nAdjusted R2                  0.717                  0.736         \nResidual Std. Error  53,880.950 (df = 97)    51,998.110 (df = 96) \nF Statistic         126.335*** (df = 2; 97) 93.151*** (df = 3; 96)\n==================================================================\nNote:                                  *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01\n\n\nI prefer the model with the interaction term because the interaction term is significant and both the R^2 and adjusted R^2 figures are higher than for the model without the interaction term. The residual standard errors are also smaller for the model with the interaction term."
  },
  {
    "objectID": "posts/AlexisGamez_HW3.html",
    "href": "posts/AlexisGamez_HW3.html",
    "title": "Blog Post #3",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(alr4)\nlibrary(smss)\nlibrary(ggplot2)\nlibrary(stargazer)\n\nknitr::opts_chunk$set(echo = TRUE)\n\n\n\nQuestion 1\nUnited Nations (Data file: UN11in alr4) The data in the file UN11 contains several variables, including ppgdp, the gross national product per person in U.S. dollars, and fertility, the birth rate per 1000 females, both from the year 2009. The data are for 199 localities, mostly UN member countries, but also other areas such as Hong Kong that are not independent countries. The data were collected from the United Nations (2011). We will study the dependence of fertility on ppgdp.\n\n\nCode\n# Loading in data\ndata(UN11)\n\n\n(a) Identify the predictor and the response.\nIn this scenario, the predictor variable is ppgdp (GDP per capita) and the response variable is fertility (birthrate per 1000 women).\n(b) Draw the scatterplot of fertility on the vertical axis versus ppgdp on the horizontal axis and summarize the information in this graph. Does a straight-line mean function seem to be plausible for a summary of this graph?\n\n\nCode\nggplot(data = UN11, aes(x = ppgdp, y = fertility)) +\n  geom_point()\n\n\n\n\n\nFrom the plot, it’s easy to tell that a straight-line mean function isn’t plausible to summarize this graph. This graph portrays an L-shaped structure which doesn’t fit a linear function.\n(c) Draw the scatterplot of log(fertility) versus log(ppgdp) using natural logarithms. Does the simple linear regression model seem plausible for a summary of this graph? If you use a different base of logarithms, the shape of the graph won’t change, but the values on the axes will change.\n\n\nCode\nggplot(data = UN11, aes(x = log(ppgdp), y = log(fertility))) +\n  geom_point()\n\n\n\n\n\nIn this case, using natural logarithms, a linear regression model is plausible! We can see tracings of linearity and even distribution using a little imagination to visualize a negatively-sloped, straight line receding through the center of the point distribution.\n\n\nQuestion 2\nAnnual income, in dollars, is an explanatory variable in a regression analysis. For a British version of the report on the analysis, all responses are converted to British pounds sterling (1 pound equals about 1.33 dollars, as of 2016).\n(a) How, if at all, does the slope of the prediction equation change?\nThe slope of the prediction equation would change in the same way that dollars would be converted to pounds, i.e. divide by 1.33.\n(b) How, if at all, does the correlation change?\nCorrelation would not change at all since it isn’t affected by unit change.\n\n\nQuestion 3\nWater runoff in the Sierras (Data file: water in alr4) Can Southern California’s water supply in future years be predicted from past data? One factor affecting water availability is stream runoff. If runoff could be predicted, engineers, planners, and policy makers could do their jobs more efficiently. The data file contains 43 years’ worth of precipitation measurements taken at six sites in the Sierra Nevada mountains (labeled APMAM, APSAB, APSLAKE, OPBPC, OPRC, and OPSLAKE) and stream runoff volume at a site near Bishop, California, labeled BSAAM. Draw the scatterplot matrix for these data and summarize the information available from these plots. (Hint: Use the pairs() function.)\n\n\nCode\ndata(water)\n\npairs(water)\n\n\n\n\n\nFrom the visualization above, I do not believe that you could accurately predict Southern California’s water supply in future years using past data. There doesn’t appear to be any correlation or pattern in any variable interaction with the Year variable. Furthermore, it seems as thought there is a bit of variable dependence among the “O” and “A” variable groups. Both these reasons lead me to believe that a model derived from this data wouldn’t be accurate.\n\n\nQuestion 4\nProfessor ratings (Data file: Rateprof in alr4) In the website and online forum RateMyProfessors.com, students rate and comment on their instructors. Launched in 1999, the site includes millions of ratings on thousands of instructors. The data file includes the summaries of the ratings of 364 instructors at a large campus in the Midwest (Bleske-Rechek and Fritsch, 2011). Each instructor included in the data had at least 10 ratings over a several year period. Students provided ratings of 1–5 on quality, helpfulness, clarity, easiness of instructor’s courses, and raterInterest in the subject matter covered in the instructor’s courses. The data file provides the averages of these five ratings. Create a scatterplot matrix of these five variables. Provide a brief description of the relationships between the five ratings.\n\n\nCode\ndata(Rateprof)\n\npairs(Rateprof[,c('quality', 'clarity', 'helpfulness',\n                  'easiness', 'raterInterest')])\n\n\n\n\n\nThis visualization presents a different set of circumstances than that from the previous graph. It’s easy to note that there is a very strong pair-wise relationship within this model between the quality, clarity and helpfulness variables. Easiness also seems heavily correlated, but not as strong as the previously mentioned variables. raterInterest also seems somewhat correlated, but not as strongly as any of the previous variables.\n\n\nQuestion 5\nFor the student.survey data file in the smss package, conduct regression analyses relating (by convention, y denotes the outcome variable, x denotes the explanatory variable) (i) y = political ideology and x = religiosity, (ii) y = high school GPA and x = hours of TV watching.\n(You can use ?student.survey in the R console, after loading the package, to see what each variable means.)\n(a) Graphically portray how the explanatory variable relates to the outcome variable in each of the two cases\nThe graph below is a visualization of religiosity and political ideology. In it we can see the more frequently someone practices their religious beliefs, the more likely it that they are conservative. Put conversely, the less religious someone is, the more likely the person will be liberal\n\n\nCode\ndata(student.survey)\n\nggplot(data = student.survey, aes(x = re, fill = pi)) +\n    geom_bar(position = \"fill\")\n\n\n\n\n\nThis last plot shows the relationship between high school GPA and hours watching TV.\n\n\nCode\nggplot(data = student.survey, aes(x = tv, y = hi)) +\n  geom_point() \n\n\n\n\n\n(b) Summarize and interpret results of inferential analyses.\n\n\nCode\nm1 &lt;- lm(as.numeric(pi) ~ as.numeric(re), \n         data = student.survey)\n\nm2 &lt;- lm(hi ~ tv, data = student.survey)\n\nstargazer(m1, m2, type = 'text', \n          dep.var.labels = c('Pol. Ideology', 'HS GPA'),\n          covariate.labels = c('Religiosity', 'Hours of TV')\n          )\n\n\n\n==========================================================\n                                  Dependent variable:     \n                              ----------------------------\n                               Pol. Ideology     HS GPA   \n                                    (1)            (2)    \n----------------------------------------------------------\nReligiosity                       0.970***                \n                                  (0.179)                 \n                                                          \nHours of TV                                     -0.018**  \n                                                 (0.009)  \n                                                          \nConstant                          0.931**       3.441***  \n                                  (0.425)        (0.085)  \n                                                          \n----------------------------------------------------------\nObservations                         60            60     \nR2                                 0.336          0.072   \nAdjusted R2                        0.324          0.056   \nResidual Std. Error (df = 58)      1.345          0.447   \nF Statistic (df = 1; 58)         29.336***       4.471**  \n==========================================================\nNote:                          *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01\n\n\nFrom the stargazer chart, it’s shown that Religiosity is positively correlated with Political Ideology. It’s also shown to be statistically significant at the p&lt;0.01 significance level\nFinally, the chart also shows that TV hours are negatively correlated with HS GPA at a statistically significant level of p&lt;0.05."
  },
  {
    "objectID": "posts/EmmaNarkewicz_FinalProject_CheckIn2.html",
    "href": "posts/EmmaNarkewicz_FinalProject_CheckIn2.html",
    "title": "Final Project Check in 2",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(sjPlot)\nlibrary(sjmisc)\nlibrary(stargazer)\nlibrary(broom)\n\n\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/EmmaNarkewicz_FinalProject_CheckIn2.html#research-question",
    "href": "posts/EmmaNarkewicz_FinalProject_CheckIn2.html#research-question",
    "title": "Final Project Check in 2",
    "section": "Research Question",
    "text": "Research Question\nFor my final project I want to expand on research on the mental health, empathy, and burnout of medical school students using a data set of 886 medical students in Switzerland. The COVID-19 pandemic heightened the mental health challenges of health care workers around the world (Teisman et al., 2021). Numerous studies show that health care workers are prone to compassion fatigue due to working long hours in stressful work environments with continuous exposure to trauma (Jennings, 2009; Rodriguez & Carlotta, 2017; Peters, 2018; Yayha et al., 2021; Carrard et al., 2022; Shin et al., 2022).\nThe Association of American Medical Colleges (AAMC) found that 30% of surveyed medical students and residents met the criteria for depression and 10% reported having suicidal thoughts (Pasturel, 2020). Previous studies conducted on samples of health care workers in Switzerland, Iraq, and South Korea examined the impact of gender on burnout, finding that female medical students had higher rates of empathy and burnout than male coworkers (Carrard et al., 2022; Yahya et al., 2021; Shin et al., 2022). A 2009 multi-site study of medical students in the U.S. found statistically significant differences in depression by gender but not by ethnicity (Goebert et al., 2009). In contrast, the same study found statistically significant differences in suicidal ideation by ethnicity, but not by gender, with Black students reporting the highest rates of suicidal ideation & Caucasian students reporting the lowest rates of suicidal ideation (Goebert et al., 2009).\nResearch Question: Why are some medical students more likely to experience burnout than others?"
  },
  {
    "objectID": "posts/EmmaNarkewicz_FinalProject_CheckIn2.html#hypothesis-testing",
    "href": "posts/EmmaNarkewicz_FinalProject_CheckIn2.html#hypothesis-testing",
    "title": "Final Project Check in 2",
    "section": "Hypothesis Testing",
    "text": "Hypothesis Testing\nI want to explore further how ethnic identity might serve as a protective or risk factor for the burnout of medical students, specifically for international medical students. A 2022 study of medical school students in Croatia found that international medical students experience higher rates of burnout mediated by social and familial loneliness (Gradiski et al., 2022). For my final project I will test whether or not a student’s first language being a national language of Switzerland – where the sample was taken – impacts their burnout. The commonly spoken national languages of Switzerland are French, German, and Italian (Kużelewska, 2016).\nHypothesis: Medical students whose native language is a national language of the country where they are studying will experience lower rates of burnout than medical students with other native languages.\nThe reasoning behind my hypothesis is if a medical student’s native language is one of the national language of Switzerland, they will have benefit from potential protective factors of social, cultural, and familial connections. In contrast, I expect medical students whose native language is not German, French, or Italian to be at higher risk for burnout mediated through increased stress from coping with different culture, language, and physical separation from their family."
  },
  {
    "objectID": "posts/EmmaNarkewicz_FinalProject_CheckIn2.html#descriptive-statistics",
    "href": "posts/EmmaNarkewicz_FinalProject_CheckIn2.html#descriptive-statistics",
    "title": "Final Project Check in 2",
    "section": "Descriptive Statistics",
    "text": "Descriptive Statistics\nThe data set I will be analyzing contains demographic information on 886 medical students in Switzerland. Students answered demographic information about their age, gender, their year in school and well as the results of self-reported empathy, depression, anxiety, and burnout. The data set was downloaded from Kaggle at https://www.kaggle.com/datasets/thedevastator/medical-student-mental-health?select=Codebook+Carrard+et+al.+2022+MedTeach.csv but originally sourced for a 2022 publication in the Medical Teacher Journal by Carrard et al.\nImportant variables I want to explore in my data set as potential risk and protective factors:\n\nNative Language\nAge\nGender\nHaving a romantic partner\nSeeing a psychotherapist\nHours spent studying\nHaving a job\nJefferson Scale Empathy (JSPE) total empathy score\nQuestionnaire of Cognitive and Affective Empathy (QCAE) Cognitive empathy score\nQuestionnaire of Cognitive and Affective Empathy QCAE Affective empathy score\nCenter for Epidemiologic Studies Depression Scale (CES-D) total score\nState & Trait Anxiety (STAI) score\nMaslach Burnout Inventory (MBI) Emotional Exhaustion\nMaslach Burnout Inventory (MBI) Cynicism\nMaslach Burnout Inventory (MBI) Academic Efficacy\n\nEach of the various empathy, mental health, and burnout scales are scored differently, so care needs to be taken in interpreting these findings. For example, a higher score on the emotional exhaustion and cynicism scales of the MBI indicate higher burn out, while a higher score on the MBI personal achievement indicates lower levels of burnout (Maslach et al., 1996).\n\n\nCode\n#Readin Final data set\n\nFinalDataSet &lt;- read_csv(\"_data/med_student_burnout.csv\")\n\n\nRows: 886 Columns: 20\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (20): id, age, year, sex, glang, part, job, stud_h, health, psyt, jspe, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nCode\nFinalDataSet\n\n\n# A tibble: 886 × 20\n      id   age  year   sex glang  part   job stud_h health  psyt  jspe qcae_cog\n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n 1     2    18     1     1   120     1     0     56      3     0    88       62\n 2     4    26     4     1     1     1     0     20      4     0   109       55\n 3     9    21     3     2     1     0     0     36      3     0   106       64\n 4    10    21     2     2     1     0     1     51      5     0   101       52\n 5    13    21     3     1     1     1     0     22      4     0   102       58\n 6    14    26     5     2     1     1     1     10      2     0   102       48\n 7    17    23     5     2     1     1     0     15      3     0   117       58\n 8    21    23     4     1     1     1     1      8      4     0   118       65\n 9    23    23     4     2     1     1     1     20      2     0   118       69\n10    24    22     2     2     1     1     0     20      5     0   108       56\n# … with 876 more rows, and 8 more variables: qcae_aff &lt;dbl&gt;, amsp &lt;dbl&gt;,\n#   erec_mean &lt;dbl&gt;, cesd &lt;dbl&gt;, stai_t &lt;dbl&gt;, mbi_ex &lt;dbl&gt;, mbi_cy &lt;dbl&gt;,\n#   mbi_ea &lt;dbl&gt;\n\n\nPrior to examining the descriptive statistics from the med school data set I recoded qualitative variables stored as numeric values, using the Carrard et al., 2022 code book, replacing 0, 1 with clear demographic information about age, gender, having a partner etc. The explanatory variable NatLang which collapses down into if medical students native language is German, French, or Italian (NatSpeaker) or not (NotNatSpeaker).\nFrom the Carrard et al., 2022 code book:\n\nGender\n\nMale = 1\nFemale = 2\nNon-binary = 3\n\nPartner\n\nSingle = 0\nPartnered = 1\n\nJob\n\nNo = 0\nYes = 1\n\nTherapist in Past 12 months\n\nNo = 0\nYes = 1\n\nHealth Satisfaction\n\nvery dissatisfied = 1\ndissatisfied = 2\nneutral = 3\nsatisfied = 4\nvery satisfied = 5\n\n\n\n\nCode\n#Coding Nat Lang\n\n(table(select(FinalDataSet, glang)))\n\n\nglang\n  1  15  20  37  54  60  63  90  92  95  98 102 104 106 108 114 118 120 121 \n717  31  22   3   1   3   5  45   1   1   1  27   4   6   1   1   2   2  13 \n\n\nCode\n#Recoding categorical variables based on code book\n\nFinalRecoded &lt;- FinalDataSet %&gt;%\n  mutate(NatLang = case_when(\n    glang == 1 | glang == 15 | glang == 90 ~ 1,\n    glang &gt; 1 & glang &lt; 15 | glang &gt; 15  & glang &lt; 90 | glang &gt; 90 ~ 0)\n  ) \n\nFinalRecoded\n\n\n# A tibble: 886 × 21\n      id   age  year   sex glang  part   job stud_h health  psyt  jspe qcae_cog\n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n 1     2    18     1     1   120     1     0     56      3     0    88       62\n 2     4    26     4     1     1     1     0     20      4     0   109       55\n 3     9    21     3     2     1     0     0     36      3     0   106       64\n 4    10    21     2     2     1     0     1     51      5     0   101       52\n 5    13    21     3     1     1     1     0     22      4     0   102       58\n 6    14    26     5     2     1     1     1     10      2     0   102       48\n 7    17    23     5     2     1     1     0     15      3     0   117       58\n 8    21    23     4     1     1     1     1      8      4     0   118       65\n 9    23    23     4     2     1     1     1     20      2     0   118       69\n10    24    22     2     2     1     1     0     20      5     0   108       56\n# … with 876 more rows, and 9 more variables: qcae_aff &lt;dbl&gt;, amsp &lt;dbl&gt;,\n#   erec_mean &lt;dbl&gt;, cesd &lt;dbl&gt;, stai_t &lt;dbl&gt;, mbi_ex &lt;dbl&gt;, mbi_cy &lt;dbl&gt;,\n#   mbi_ea &lt;dbl&gt;, NatLang &lt;dbl&gt;\n\n\n\n\nCode\n#Descriptive statistics for quantitative variables\n\nsummary(FinalRecoded)\n\n\n       id              age             year            sex       \n Min.   :   2.0   Min.   :17.00   Min.   :1.000   Min.   :1.000  \n 1st Qu.: 447.5   1st Qu.:20.00   1st Qu.:1.000   1st Qu.:1.000  \n Median : 876.0   Median :22.00   Median :3.000   Median :2.000  \n Mean   : 889.7   Mean   :22.38   Mean   :3.103   Mean   :1.695  \n 3rd Qu.:1341.8   3rd Qu.:24.00   3rd Qu.:5.000   3rd Qu.:2.000  \n Max.   :1790.0   Max.   :49.00   Max.   :6.000   Max.   :3.000  \n     glang             part             job             stud_h     \n Min.   :  1.00   Min.   :0.0000   Min.   :0.0000   Min.   : 0.00  \n 1st Qu.:  1.00   1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:12.00  \n Median :  1.00   Median :1.0000   Median :0.0000   Median :25.00  \n Mean   : 14.33   Mean   :0.5632   Mean   :0.3488   Mean   :25.29  \n 3rd Qu.:  1.00   3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:36.00  \n Max.   :121.00   Max.   :1.0000   Max.   :1.0000   Max.   :70.00  \n     health           psyt             jspe          qcae_cog    \n Min.   :1.000   Min.   :0.0000   Min.   : 67.0   Min.   :37.00  \n 1st Qu.:3.000   1st Qu.:0.0000   1st Qu.:101.0   1st Qu.:54.00  \n Median :4.000   Median :0.0000   Median :107.0   Median :58.00  \n Mean   :3.778   Mean   :0.2246   Mean   :106.4   Mean   :58.53  \n 3rd Qu.:5.000   3rd Qu.:0.0000   3rd Qu.:113.0   3rd Qu.:63.00  \n Max.   :5.000   Max.   :1.0000   Max.   :125.0   Max.   :76.00  \n    qcae_aff          amsp         erec_mean           cesd      \n Min.   :18.00   Min.   : 6.00   Min.   :0.3571   Min.   : 0.00  \n 1st Qu.:31.00   1st Qu.:20.00   1st Qu.:0.6667   1st Qu.: 9.00  \n Median :35.00   Median :23.00   Median :0.7262   Median :16.00  \n Mean   :34.78   Mean   :23.15   Mean   :0.7201   Mean   :18.05  \n 3rd Qu.:39.00   3rd Qu.:26.75   3rd Qu.:0.7857   3rd Qu.:25.00  \n Max.   :48.00   Max.   :35.00   Max.   :0.9524   Max.   :56.00  \n     stai_t         mbi_ex          mbi_cy          mbi_ea         NatLang     \n Min.   :20.0   Min.   : 5.00   Min.   : 4.00   Min.   :10.00   Min.   :0.000  \n 1st Qu.:34.0   1st Qu.:13.00   1st Qu.: 6.00   1st Qu.:21.00   1st Qu.:1.000  \n Median :43.0   Median :17.00   Median : 9.00   Median :24.00   Median :1.000  \n Mean   :42.9   Mean   :16.88   Mean   :10.08   Mean   :24.21   Mean   :0.895  \n 3rd Qu.:51.0   3rd Qu.:20.00   3rd Qu.:13.00   3rd Qu.:28.00   3rd Qu.:1.000  \n Max.   :77.0   Max.   :30.00   Max.   :24.00   Max.   :36.00   Max.   :1.000  \n\n\nNote that id is not a true numeric variable and therefor the descriptive statistics for it should be disregarded.\nMedical students in the sample studied for an average of 25 hours a week, with a maximum of 70 hours.\nScores on the Jefferson Scale of Physician Empathy (JSPE) range from 20-140 with a higher score indicating higher empathy. The mean JSPE score in the sample was was 106.2 and the median JSPE score was 107.0 indicating relatively high empathy. There was a broad range from as low to 67-125, with the IQR indicating most medical students scored in the low 100s range.\nThere were are scores for all 3 components of MBI burnout: emotional exhaustion, cynicism, and personal achievement.\n\nOn the mbi-ex, medical school students’ scores ranged from 5-30, with a median score of 17 and a mean score of 16.88. According to the MBI score guide, half of medical students in the sample exhibit low-level burnout (scoring 17 or less), and the other half exhibiting moderate burnout in terms of emotional exhaustion.\nOn the mbi-cyn, medical students’ scores ranged from 4-24, with a median of 9 and mean of 10.08. According to the scoring guide, the majority of the sample exhibit moderate burnout (6-11) with some exhibiting high level burnout (12+) in the dimension of cynicism.\nOn the mbi-ea, medical students scores ranged from 10-36, with a median score of 24 and a mean score of 24.01. A score of 33 or less indicates high level of burnout and a score between 24-39 indicates moderate level burnout, with medical students falling in the high and moderate burnout range for personal achievement.\n\n\n\nCode\n## Creating character variables to generate props\n\nFinalProp &lt;- FinalRecoded %&gt;%\nmutate(nat = case_when (\n  NatLang == 0 ~ \"Non Native\",\n  NatLang == 1 ~  \"Native\")\n        )%&gt;%\nmutate(gender = case_when(\n         sex == 1  ~ \"Male\",\n         sex == 2 ~ \"Female\", \n         sex == 3 ~ \"Non-Binary\")\n        ) %&gt;%\n  relocate(`gender`, .before = `age`)%&gt;%\n  select(!contains(\"sex\")) %&gt;%\nmutate(partner = case_when(\n         part == 1 ~ \"partnered\",\n         part == 0  ~ \"single\")\n      )%&gt;%\n  relocate(`partner`, .before = `age`)%&gt;%\n  select(!\"part\") %&gt;%\nmutate(paid_job = case_when(\n         job == 0  ~ \"no_job\",\n         job == 1 ~ \"yes_job\")\n        ) %&gt;%\n  relocate(`paid_job`, .before = `age`)%&gt;%\n  select(!\"job\") %&gt;%\nmutate(health_sat = case_when(\n         health == 1  ~ \"very_dis\",\n         health == 2 ~ \"dis\",\n         health == 3 ~ \"neutral\",\n         health == 4 ~ \"sat\",\n         health  == 5 ~ \"very sat\")\n) %&gt;%\n  relocate(`health_sat`, .before = `age`)%&gt;%\n  select(!\"health\") %&gt;%\nmutate(MHcare = case_when(\n         psyt == 0  ~ \"no_ther\",\n         psyt == 1 ~ \"yes_ther\")\n        ) %&gt;%\nrelocate(`MHcare`, .before = `age`)%&gt;%\n  select(!\"psyt\") \n\nFinalProp\n\n\n# A tibble: 886 × 22\n      id gender partner   paid_job healt…¹ MHcare   age  year glang stud_h  jspe\n   &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;     &lt;chr&gt;    &lt;chr&gt;   &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n 1     2 Male   partnered no_job   neutral no_th…    18     1   120     56    88\n 2     4 Male   partnered no_job   sat     no_th…    26     4     1     20   109\n 3     9 Female single    no_job   neutral no_th…    21     3     1     36   106\n 4    10 Female single    yes_job  very s… no_th…    21     2     1     51   101\n 5    13 Male   partnered no_job   sat     no_th…    21     3     1     22   102\n 6    14 Female partnered yes_job  dis     no_th…    26     5     1     10   102\n 7    17 Female partnered no_job   neutral no_th…    23     5     1     15   117\n 8    21 Male   partnered yes_job  sat     no_th…    23     4     1      8   118\n 9    23 Female partnered yes_job  dis     no_th…    23     4     1     20   118\n10    24 Female partnered no_job   very s… no_th…    22     2     1     20   108\n# … with 876 more rows, 11 more variables: qcae_cog &lt;dbl&gt;, qcae_aff &lt;dbl&gt;,\n#   amsp &lt;dbl&gt;, erec_mean &lt;dbl&gt;, cesd &lt;dbl&gt;, stai_t &lt;dbl&gt;, mbi_ex &lt;dbl&gt;,\n#   mbi_cy &lt;dbl&gt;, mbi_ea &lt;dbl&gt;, NatLang &lt;dbl&gt;, nat &lt;chr&gt;, and abbreviated\n#   variable name ¹​health_sat\n\n\nCode\n#Frequency of categorical & ordinal variables\n prop.table(table(select(FinalProp, nat)))\n\n\nnat\n    Native Non Native \n 0.8950339  0.1049661 \n\n\nCode\n prop.table(table(select(FinalProp, gender)))\n\n\ngender\n     Female        Male  Non-Binary \n0.683972912 0.310383747 0.005643341 \n\n\nCode\n prop.table(table(select(FinalProp, partner)))\n\n\npartner\npartnered    single \n0.5632054 0.4367946 \n\n\nCode\n prop.table(table(select(FinalProp, paid_job)))\n\n\npaid_job\n   no_job   yes_job \n0.6512415 0.3487585 \n\n\nCode\n prop.table(table(select(FinalProp, health_sat)))\n\n\nhealth_sat\n       dis    neutral        sat   very sat   very_dis \n0.09819413 0.15349887 0.45372460 0.25282167 0.04176072 \n\n\nCode\n prop.table(table(select(FinalProp, MHcare)))\n\n\nMHcare\n no_ther yes_ther \n0.775395 0.224605 \n\n\nFrom the proportion tables above it can be seen that majority (90%) of the sample speaks one of the national languages of Switzerland, while only 10% are non native speakers. The sample is also mostly female (68%), with less than 1% identifying as non-binary. Over half (56%) of the medical students reported having partners, but only about a third of medical students had a paid job (34.9%). The most common (45%) report from medical students was that they were satisfied with their health and less than one quarter (22.5%) of medical student reported seeing a therapist in the last 12 months.\nLooking at the gender prop.table, less than 1% of medical students were non-binary. Looking at counts that is only 5 students out of nearly 1000. From an equity standpoint it is great to include diverse gender identities, but for the purposes of analysis & linear regression, there are non non-binary students to meaningfully include in an analysis of gender. Therefor I removed the 5 students who identified as non-binary before conducting analysis. Additionally I recoded the gender dummy variables from 1 & 2 to 0 & 1 for male and female respectiviely.\n\n\nCode\n#Counts genders\n (table(select(FinalProp, gender)))\n\n\ngender\n    Female       Male Non-Binary \n       606        275          5 \n\n\nCode\n#remove non-binary dummy gender & recode \n\nFinalRecoded2 &lt;- FinalRecoded %&gt;%\n  filter(!str_detect(sex, \"3\")) %&gt;%\n  mutate(gender = case_when(\n         sex == 1  ~ 0,\n         sex == 2 ~  1)\n        ) \n  \n#remove non-binary categorical\nFinalProp2 &lt;- FinalProp %&gt;%\n  filter(!str_detect(gender, \"Non-Binary\"))"
  },
  {
    "objectID": "posts/EmmaNarkewicz_FinalProject_CheckIn2.html#analysis",
    "href": "posts/EmmaNarkewicz_FinalProject_CheckIn2.html#analysis",
    "title": "Final Project Check in 2",
    "section": "Analysis",
    "text": "Analysis\nI created several box plots comparing the MBI burnout scores of medical students speaking native language vs. students speaking non-native language. Additionally, I compared the anxiety & depression scored of medical students native and non-native language speaking students\n\n\nCode\n#Emotional Exhaustion Burnout Score\nggplot(data = FinalProp2, aes(x= nat, y = mbi_ex, fill = nat)) + geom_boxplot() +\n  labs(title = \"Box Plot - mbi_ex Emotional Burnout\", x = \"Native Language Spoken\", y = \"mbi_ex\") \n\n\n\n\n\nNative language speaking and non-native language speaking medical students seemed to have nearly identical median scores for Emotional Exhaustion (mbi_ex). The IQR however for non-native languge speaking students does appear to start & end higher (higher Q1 & Q3 MBI-ex scores) than native students, suggesting the potential for higher emotional exhuastion burnout for non-native language speaking students.\n\n\nCode\n#Cynicism Score\nggplot(data = FinalProp, aes(x = nat, y = mbi_cy, fill = nat)) + geom_boxplot() +\n  labs(title = \"Box Plot - mbi_cy Cynicism Burnout\", x = \"Native Language Spoken\", y = \"mbi_cy\") \n\n\n\n\n\nNon-native speaking medical students appeared to score slightly higher on average than non-native speaking medical students on Cynicism as measured by the mbi-cy, with a higher median score. The IQR of mbi-cy scores for non-native language speaking medical students is higher for non-native students with a higher Q1 & Q3. There appears to be one high outlier for native language speakers cynicism scores.\n\n\nCode\n#Academic Efficacy Score\nggplot(data = FinalProp2, aes(x= nat, y = mbi_ea, fill = nat)) + geom_boxplot() +\n  labs(title = \"Box Plot - mbi-ea Academic Efficacy Burnout\", x = \"Native Language Spoken\", y = \"mbi_ea\") \n\n\n\n\n\nLastly, native language speaking and non-native language speaking medical students seemed to have nearly identical median scores for Academic Efficacy burnout (mbi_ea), with one low-outlier of scores for native language speaking medical students. However non-native medical school students have a lower MBI_ea Q1 & Q3 which corresponds with higher burnout on this subscale.\n\n\nCode\n#Depression Score\nggplot(data = FinalProp, aes(x= nat, y = cesd, fill = nat)) + geom_boxplot() +\n  labs(title = \"Box Plot - cesd measure of Depression\", x = \"Native Language Spoken\", y = \"CESD Score\") \n\n\n\n\n\nNon-native national language speakers have a higher median on CESD score than native national language speakers, suggesting higher depression among native students. The Q1 & Q3 of the non-native language speaking medical students CESD is higher than scores for native language speaking students. There are several high outliers for native and non-native language speaking students.\n\n\nCode\n#Anxiety Score\nggplot(data = FinalProp2, aes(x= nat, y = stai_t, fill = nat)) + geom_boxplot() +\n  labs(title = \"Box Plot - stai_t measure of Anxiety\", x = \"Native Language Spoken\", y = \"stai_t Score\") \n\n\n\n\n\nNon-native national languages speaking medical students have a higher median, Q1, & Q3 stai_t anxiety score than native language speaking medical students.\nHigher depression and anxiety score for non-native students are not the hypothesis being tested, but it is plausible that anxiety & depression mediate the impact of speaking a native language on burnout. Anxiety and depression, as measured by stai_t and cesd scores, will be treated as interaction terms in linear regression model, with graphic models used to evaluate this.\n\nGender\nPrevious studies on the impact of gender and burnout found that being female predicted higher burnout. In the linear regression model, I will want to control for the impact of gender on burnout scores by including it as an explanatory variable.\nOn the plots below, I facet wrapped the box & whiskers plot of emotional burnout comparison by native language to visualize difference in the 3 MBI burnout scales when controlled by gender.\n\n\nCode\n#Emotional Exhaustion Burnout Score\nggplot(data = FinalProp2, aes(x= nat, y = mbi_ex, fill = nat)) + geom_boxplot() +\n  labs(title = \"Box Plot Emotional Burnout Controlled by Gender\", x = \"Native Language Spoken\", y = \"MBI_ex Score\") + facet_wrap(vars(`gender`))\n\n\n\n\n\n\n\nCode\n#Emotional Cynicism Burnout Score\nggplot(data = FinalProp2, aes(x= nat, y = mbi_cy, fill = nat)) + geom_boxplot() +\n  labs(title = \"Box Plot Cynicism Burnout Controlled by Gender\", x = \"Native Language Spoken\", y = \"MBI_cy Score\") + facet_wrap(vars(`gender`))\n\n\n\n\n\n\n\nCode\n#Academic Efficacy Burnout Score\nggplot(data = FinalProp2, aes(x= nat, y = mbi_ex, fill = nat)) + geom_boxplot() +\n  labs(title = \"Box Plot Academic Efficacy Burnout Controlled by Gender\", x = \"Native Language Spoken\", y = \"MBI_ea Score\") + facet_wrap(vars(`gender`))\n\n\n\n\n\nControlling for gender there still seems to be a difference between the MBI scores of native and non-native language speaking MBI burnout scores. Looking at the academic efficacy score, for non-native male med students it appears to be higher than native male med students, suggesting lower academic efficacy burnout counter to my hypothesis.\n\n\nApplying feedback from Final Check in 1\nIn response to feedback on Check In 1, I kept categorical data coded as dummy variables 0,1 format for the linear regression. I did keep a “prop”data set record to use to calculate proportions & frequencies of categorical variables.\nIn deciding how to best work with the 3 MBI burnout variables, I reviewed literature on burnout and how prior researchers chose to work with the variables. A 2016 systematic review by Doulougeri et al. of 50 articles on burnout in medical staff as measured by the MBI found that the majority of studies analyzed burn out as sub scales of emotional exhaustion, cynicism (also known as depersonalization), & academic efficacy (also known as professional accomplishment), as opposed to trying to create one total scale.\nThis approach also aligns with the finding that burnout is often conceptualized as a multidimensional construct. Maslach designed the MBI to distinguish between the three sub-scales. Studies on burnout almost universally considered the MBI emotional exhaustion (EE) sub-scale in determining burnout, sometimes alone, sometimes with cynicism, and sometimes with both cynicism & academic efficacy. According to the literature, burn out is complex and multidimensional concepts, and impacts both an individual’s well being and their ability to perform their job as a medical professional.\nBased on all of these factors, I will run three separate models with each of the 3 MBI sub-scales as dependent variables."
  },
  {
    "objectID": "posts/EmmaNarkewicz_FinalProject_CheckIn2.html#hypothesis-testing-1",
    "href": "posts/EmmaNarkewicz_FinalProject_CheckIn2.html#hypothesis-testing-1",
    "title": "Final Project Check in 2",
    "section": "Hypothesis Testing",
    "text": "Hypothesis Testing\nEach MBI burnout sub-scale score will be a response variable for each of the 3 models. These scales are numeric and continuous.\n\nemotional exhaustion\ncynicism\nacademic efficacy\n\nAs mentioned earlier, a higher score on emotional exhaustion & cynicism are considered indicative of burnout, whereas lower scores of academic efficacy are indicative of burnout\nThe explanatory variable I am testing in my hypothesis is whether or not a student speaks a native language of the country they are attending medical school in, coded as a binary dummy variable (0 = non-native, 1 = native)\nAs previous studies have found a relationship between gender and burnout, I will need to have gender as a control variable in the model. I will also examine if I will need to also control for the influence of having a partner, having a job, or seeing a therapist in past 12 months (all also binary dummy variables) on medical student burnout, by adding them to the regression model and seeing their impact.\n\nT test\nTo test my hypothesis that Medical students whose native language is a national language of the country where they are studying will experience lower rates of burnout than medical students with other native languages, I will run a Welch’s Independent T-Test on the data to see if there is a statistically significant difference between the burnout of native and non-native medical students. Specifically, I will run a one-sided test, to see if the mean burnout of non-native students is higher than of native students.\nThe null hypothesis is that there is no difference in the mean burnout of native medical students than non-native medical students\nThe alternative hypothesis is that the mean burnout of native students is less than that non-native medical students. For the emotional exhaustion & cyncicism burnout scores, higher score corresponds to higher burnout, for the academic efficacy, a lower score corresponds with higher burnout.\n\n\nEmotional Exhaustion\nH0: μ = μ0\nHA: μ &lt; μ0\nThe first T-Test compared the emotional exhaustion scores of native and non-native language speaking medical students.\n\n\nCode\nhead(FinalRecoded)\n\n\n# A tibble: 6 × 21\n     id   age  year   sex glang  part   job stud_h health  psyt  jspe qcae_cog\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n1     2    18     1     1   120     1     0     56      3     0    88       62\n2     4    26     4     1     1     1     0     20      4     0   109       55\n3     9    21     3     2     1     0     0     36      3     0   106       64\n4    10    21     2     2     1     0     1     51      5     0   101       52\n5    13    21     3     1     1     1     0     22      4     0   102       58\n6    14    26     5     2     1     1     1     10      2     0   102       48\n# … with 9 more variables: qcae_aff &lt;dbl&gt;, amsp &lt;dbl&gt;, erec_mean &lt;dbl&gt;,\n#   cesd &lt;dbl&gt;, stai_t &lt;dbl&gt;, mbi_ex &lt;dbl&gt;, mbi_cy &lt;dbl&gt;, mbi_ea &lt;dbl&gt;,\n#   NatLang &lt;dbl&gt;\n\n\nCode\n#Ttest\nNative &lt;- subset(FinalRecoded, FinalRecoded$NatLang == 1)\nNonNative &lt;- subset(FinalRecoded, FinalRecoded$NatLang == 0)\n\n\n##One side t-test, MBI ex\n\nttest_ex &lt;- t.test(Native$mbi_ex, NonNative$mbi_ex, var.equal = TRUE, alternative = \"less\")\nttest_ex\n\n\n\n    Two Sample t-test\n\ndata:  Native$mbi_ex and NonNative$mbi_ex\nt = -1.3841, df = 884, p-value = 0.08334\nalternative hypothesis: true difference in means is less than 0\n95 percent confidence interval:\n     -Inf 0.151154\nsample estimates:\nmean of x mean of y \n 16.79445  17.59140 \n\n\nWith a p-value of 0.08 the difference between the mean is statistically significant at the .1 level but not the standard 0.05 level. Therefor I fail to reject the null hypothesis that there is no difference between the native and non-native students MBI_ex scores.\n\n\nCynicism\nH0: μ = μ0\nHA: μ &lt; μ0\n\n\nCode\n#Cynicism\nttest_cy &lt;- t.test(Native$mbi_cy, NonNative$mbi_cy, var.equal = TRUE, alternative = \"less\")\nttest_cy\n\n\n\n    Two Sample t-test\n\ndata:  Native$mbi_cy and NonNative$mbi_cy\nt = -1.2331, df = 884, p-value = 0.1089\nalternative hypothesis: true difference in means is less than 0\n95 percent confidence interval:\n      -Inf 0.2080785\nsample estimates:\nmean of x mean of y \n 10.01387  10.63441 \n\n\nNext the cynicism MBI burnout scores for native and non-native scores were compared in a T-Test. With a p-value of 0.1089, the difference between the means is not statistically significant at the 0.1 or the 0.05 levels. I fail to reject the null hypothesis that there is no difference between the native and non-native medical students MBI_cy scores.\n\n\nAcademic Efficacy\nH0: μ = μ0\nHA: μ &gt; μ0\nLastly, I performed a T-Test on the final MBI sub-score, the academic efficacy between native and non-native language speaking medical students. As a lower MBI_ea score corresponds with higher burnout on this sub-scale the alternative hypothesis of the one-sided T-test is that\n\n\nCode\n#T-test Academic Efficacy \n\nttest_ea &lt;- t.test(Native$mbi_ea, NonNative$mbi_ea, var.equal = TRUE, alternative = \"greater\")\nttest_ea\n\n\n\n    Two Sample t-test\n\ndata:  Native$mbi_ea and NonNative$mbi_ea\nt = -0.29993, df = 884, p-value = 0.6179\nalternative hypothesis: true difference in means is greater than 0\n95 percent confidence interval:\n -0.9891101        Inf\nsample estimates:\nmean of x mean of y \n 24.19168  24.34409 \n\n\nWith a p-value of 0.6179 the mean MBI_ea score of native medical students is not greater than the mean MBI_ea score of native people at all, let alone at the statistically significant level.\n\n\nInterpretation of Hypothesis Testing & Further Tests\nOut of the 3 MBI burnout sub-scales, only the one-sided difference in burnout as measured on the MBI emotional exhaustion scale is statistically significant at the 0.1 level. There is not statistically significant higher burnout on any of the 3 MBI scores at the 0.05 level. I therefor fail to reject the null hypothesis that there is no difference between the mean MBI burnout scores of native and non-native language speaking medical school students.\nThough not my hypothesis, I ran Welch’s T-tests on anxiety (stai_t) and depression (CESD) scores to see if there was a statistically significant difference between native and non-native speaking medical school students. Once again I ran a one-sided t-test, with the assumption that the mean anxiety and depression of non-native language speaking medical students will be higher than that of native language speaking medical students.\nH0: μ = μ0\nHA: μ &lt; μ0\n\n\nCode\n# T-test Anxiety\nttest_stai_t &lt;- t.test(Native$stai_t, NonNative$stai_t, var.equal = TRUE, alternative = \"less\")\nttest_stai_t\n\n\n\n    Two Sample t-test\n\ndata:  Native$stai_t and NonNative$stai_t\nt = -2.5743, df = 884, p-value = 0.005103\nalternative hypothesis: true difference in means is less than 0\n95 percent confidence interval:\n      -Inf -1.214218\nsample estimates:\nmean of x mean of y \n 42.54477  45.91398 \n\n\nWith a p-value of 0.005 there is strong statistical significance to reject the null hypothesis that there is no difference in the mean anxiety of native and non-native language speaking medical students. This suggests that med students who speak the non-native language experience higher anxiety levels than native-language speakers.\n\n\nCode\n# T-test Depression\nttest_cesd &lt;- t.test(Native$cesd, NonNative$cesd, var.equal = TRUE, alternative = \"less\")\nttest_cesd\n\n\n\n    Two Sample t-test\n\ndata:  Native$cesd and NonNative$cesd\nt = -2.7533, df = 884, p-value = 0.00301\nalternative hypothesis: true difference in means is less than 0\n95 percent confidence interval:\n      -Inf -1.387275\nsample estimates:\nmean of x mean of y \n 17.68852  21.13978 \n\n\nWith a p-value of 0.003 there is strong statistical significance to reject the null hypothesis that there is no difference in the mean depression of native and non-native language speaking medical students. This suggests that med students who speak the non-native language experience higher depression levels than native-language speakers.\nThough I failed to reject my null hypothesis of burnout, returning to my research question of” Why are some medical students more likely to experience burnout than others? it seems that native language status, anxiety, depression all interact on one-another to help explain burnout. This will be explored further in the linear regression models."
  },
  {
    "objectID": "posts/EmmaNarkewicz_FinalProject_CheckIn2.html#linear-regressions",
    "href": "posts/EmmaNarkewicz_FinalProject_CheckIn2.html#linear-regressions",
    "title": "Final Project Check in 2",
    "section": "Linear Regressions",
    "text": "Linear Regressions\n\nEmotional Exhaustion Burnout Model (MBI_ex)\nFirst, I created a linear regression model with only native language status as the explanatory variable and mbi_ex emotional exhaustion burnout score as the outcome variable. As seen in the summary of the model below with a p-value of 0.132 and Adjusted R-squared value of 0.001449, variance in native language speaking status alone does explain variance in emotional exhaustion MBI scores. National language is not a statistically significant explanatory variable in the linear regression model. The AIC of this first model is 5422, with the lower the AIC score the better the fit of the model\n\n\nCode\n#Linear regression model with only NatLang as explanatory variable\n\nmodel_ex_1 &lt;- lm(mbi_ex ~ NatLang, data = FinalRecoded2)\nsummary(model_ex_1)\n\n\n\nCall:\nlm(formula = mbi_ex ~ NatLang, data = FinalRecoded2)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-11.7921  -3.7921   0.2079   3.2079  13.2079 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  17.6630     0.5462  32.338   &lt;2e-16 ***\nNatLang      -0.8709     0.5772  -1.509    0.132    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.239 on 879 degrees of freedom\nMultiple R-squared:  0.002584,  Adjusted R-squared:  0.001449 \nF-statistic: 2.277 on 1 and 879 DF,  p-value: 0.1317\n\n\nCode\n## AIC \n\nglance(model_ex_1)$AIC\n\n\n[1] 5422.286\n\n\nThe second linear regression model includes gender as an explanatory variable along with native-language status for the outcome of emotional exhaustion MBI scores. In this regression model gender is statistically significant explanatory variable with a p value of 1.66 x e6, but native-language status is not. The adjusted R-squared value is higher than the first model but still low at 0.02612. The AIC value of this model is 5401 is lower than the first model.\n\n\nCode\n#Linear Regression model with NatLang & gender\nmodel_ex_2 &lt;- lm(mbi_ex ~ NatLang + gender , data = FinalRecoded2)\nsummary(model_ex_2)\n\n\n\nCall:\nlm(formula = mbi_ex ~ NatLang + gender, data = FinalRecoded2)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-12.3723  -3.3723  -0.3723   3.6277  14.4441 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  16.3007     0.6089  26.771  &lt; 2e-16 ***\nNatLang      -0.7449     0.5706  -1.305    0.192    \ngender        1.8164     0.3766   4.823 1.66e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.174 on 878 degrees of freedom\nMultiple R-squared:  0.02833,   Adjusted R-squared:  0.02612 \nF-statistic:  12.8 on 2 and 878 DF,  p-value: 3.317e-06\n\n\nCode\n#AIC\nglance(model_ex_2)$AIC\n\n\n[1] 5401.246\n\n\nIn the third linear regression model was a 3-way interaction of depression (CESD), anxiety (stai_t), & native language as an explanatory variable for MBI emotional exhaustion scores in addition to gender. Adding the 3 way interaction term to the model greatly increased the predictive power of the model, raising the adjusted R-squared value to 0.3885. In this model, Anxiety (stai_t) & Depression * Anxiety (ces*stai_t) are both statistically significant at the 0.05 level, and Depression (cesd) is statistically significant at the 0.001 level. The AIC of 4997 is the lowest of any model so far, suggesting the best fit.\n\n\nCode\n#Linear Regression model with NatLang * Anxiety * Depression & gender explanatory variables\nmodel_ex_3 &lt;- lm(mbi_ex ~ gender + NatLang*cesd*stai_t , data = FinalRecoded2)\nsummary(model_ex_3)\n\n\n\nCall:\nlm(formula = mbi_ex ~ gender + NatLang * cesd * stai_t, data = FinalRecoded2)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-14.0395  -2.7678  -0.1503   2.5525  14.0706 \n\nCoefficients:\n                     Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)          6.629628   2.729549   2.429  0.01535 * \ngender              -0.060270   0.310448  -0.194  0.84611   \nNatLang              0.684336   2.881436   0.237  0.81233   \ncesd                 0.454730   0.140458   3.237  0.00125 **\nstai_t               0.146600   0.068337   2.145  0.03221 * \nNatLang:cesd        -0.125780   0.148435  -0.847  0.39702   \nNatLang:stai_t      -0.010215   0.072736  -0.140  0.88834   \ncesd:stai_t         -0.004831   0.002364  -2.044  0.04128 * \nNatLang:cesd:stai_t  0.002343   0.002559   0.916  0.36011   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.1 on 872 degrees of freedom\nMultiple R-squared:  0.3941,    Adjusted R-squared:  0.3885 \nF-statistic: 70.89 on 8 and 872 DF,  p-value: &lt; 2.2e-16\n\n\nCode\n#AIC\nglance(model_ex_3)$AIC\n\n\n[1] 4997.202\n\n\nTo determine the best model, I added the control variables of hours studied, having a partner, job status, consulting a psychotherapist in past 12 months, & health satisfaction to see the impact on the adjusted R squared value.\n\n\nCode\n##Control variables\n\nhead(FinalRecoded2)\n\n\n# A tibble: 6 × 22\n     id   age  year   sex glang  part   job stud_h health  psyt  jspe qcae_cog\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n1     2    18     1     1   120     1     0     56      3     0    88       62\n2     4    26     4     1     1     1     0     20      4     0   109       55\n3     9    21     3     2     1     0     0     36      3     0   106       64\n4    10    21     2     2     1     0     1     51      5     0   101       52\n5    13    21     3     1     1     1     0     22      4     0   102       58\n6    14    26     5     2     1     1     1     10      2     0   102       48\n# … with 10 more variables: qcae_aff &lt;dbl&gt;, amsp &lt;dbl&gt;, erec_mean &lt;dbl&gt;,\n#   cesd &lt;dbl&gt;, stai_t &lt;dbl&gt;, mbi_ex &lt;dbl&gt;, mbi_cy &lt;dbl&gt;, mbi_ea &lt;dbl&gt;,\n#   NatLang &lt;dbl&gt;, gender &lt;dbl&gt;\n\n\nCode\n##hours studied strengthens model\nsummary(lm(mbi_ex ~ gender + stud_h + NatLang*cesd*stai_t , data = FinalRecoded2))\n\n\n\nCall:\nlm(formula = mbi_ex ~ gender + stud_h + NatLang * cesd * stai_t, \n    data = FinalRecoded2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-13.557  -2.759  -0.194   2.566  14.132 \n\nCoefficients:\n                     Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)          6.041386   2.727628   2.215  0.02703 * \ngender              -0.017473   0.309668  -0.056  0.95502   \nstud_h               0.024347   0.008830   2.757  0.00595 **\nNatLang              0.816214   2.870986   0.284  0.77625   \ncesd                 0.449439   0.139943   3.212  0.00137 **\nstai_t               0.143298   0.068090   2.105  0.03562 * \nNatLang:cesd        -0.125427   0.147877  -0.848  0.39657   \nNatLang:stai_t      -0.008967   0.072463  -0.124  0.90154   \ncesd:stai_t         -0.004716   0.002355  -2.002  0.04559 * \nNatLang:cesd:stai_t  0.002219   0.002550   0.870  0.38433   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.084 on 871 degrees of freedom\nMultiple R-squared:  0.3993,    Adjusted R-squared:  0.3931 \nF-statistic: 64.33 on 9 and 871 DF,  p-value: &lt; 2.2e-16\n\n\nCode\n##partner\nsummary(lm(mbi_ex ~ gender + stud_h + part + NatLang*cesd*stai_t , data = FinalRecoded2))\n\n\n\nCall:\nlm(formula = mbi_ex ~ gender + stud_h + part + NatLang * cesd * \n    stai_t, data = FinalRecoded2)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-13.8288  -2.7626  -0.1144   2.5427  14.2053 \n\nCoefficients:\n                     Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)          5.474759   2.732407   2.004 0.045418 *  \ngender              -0.047923   0.309213  -0.155 0.876870    \nstud_h               0.026136   0.008843   2.955 0.003207 ** \npart                 0.640704   0.281038   2.280 0.022862 *  \nNatLang              0.966911   2.864856   0.338 0.735816    \ncesd                 0.466905   0.139817   3.339 0.000875 ***\nstai_t               0.146331   0.067939   2.154 0.031526 *  \nNatLang:cesd        -0.141738   0.147695  -0.960 0.337491    \nNatLang:stai_t      -0.012644   0.072307  -0.175 0.861224    \ncesd:stai_t         -0.004956   0.002352  -2.107 0.035400 *  \nNatLang:cesd:stai_t  0.002492   0.002546   0.979 0.327957    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.075 on 870 degrees of freedom\nMultiple R-squared:  0.4029,    Adjusted R-squared:  0.396 \nF-statistic:  58.7 on 10 and 870 DF,  p-value: &lt; 2.2e-16\n\n\nCode\n##having job\nsummary(lm(mbi_ex ~ gender + stud_h + part + job + NatLang*cesd*stai_t , data = FinalRecoded2))\n\n\n\nCall:\nlm(formula = mbi_ex ~ gender + stud_h + part + job + NatLang * \n    cesd * stai_t, data = FinalRecoded2)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-13.8833  -2.7470  -0.0854   2.5207  14.1412 \n\nCoefficients:\n                     Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)          5.576368   2.744314   2.032 0.042460 *  \ngender              -0.044168   0.309488  -0.143 0.886549    \nstud_h               0.025410   0.009013   2.819 0.004924 ** \npart                 0.643363   0.281241   2.288 0.022401 *  \njob                 -0.124332   0.295061  -0.421 0.673584    \nNatLang              0.924792   2.867954   0.322 0.747184    \ncesd                 0.464843   0.139968   3.321 0.000934 ***\nstai_t               0.145510   0.068000   2.140 0.032644 *  \nNatLang:cesd        -0.139382   0.147871  -0.943 0.346151    \nNatLang:stai_t      -0.011844   0.072366  -0.164 0.870031    \ncesd:stai_t         -0.004920   0.002355  -2.089 0.036975 *  \nNatLang:cesd:stai_t  0.002448   0.002550   0.960 0.337149    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.076 on 869 degrees of freedom\nMultiple R-squared:  0.403, Adjusted R-squared:  0.3954 \nF-statistic: 53.33 on 11 and 869 DF,  p-value: &lt; 2.2e-16\n\n\nCode\n## seen psychiatrist\nsummary(lm(mbi_ex ~ gender + stud_h + part + psyt + NatLang*cesd*stai_t , data = FinalRecoded2))\n\n\n\nCall:\nlm(formula = mbi_ex ~ gender + stud_h + part + psyt + NatLang * \n    cesd * stai_t, data = FinalRecoded2)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-13.8366  -2.7590  -0.1101   2.5433  14.2074 \n\nCoefficients:\n                     Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)          5.479195   2.736089   2.003 0.045534 *  \ngender              -0.049039   0.310571  -0.158 0.874572    \nstud_h               0.026166   0.008880   2.947 0.003297 ** \npart                 0.640210   0.281454   2.275 0.023170 *  \npsyt                 0.014441   0.349983   0.041 0.967096    \nNatLang              0.963298   2.867838   0.336 0.737030    \ncesd                 0.467043   0.139937   3.338 0.000881 ***\nstai_t               0.146139   0.068138   2.145 0.032250 *  \nNatLang:cesd        -0.141870   0.147815  -0.960 0.337432    \nNatLang:stai_t      -0.012506   0.072426  -0.173 0.862949    \ncesd:stai_t         -0.004957   0.002354  -2.106 0.035478 *  \nNatLang:cesd:stai_t  0.002491   0.002548   0.978 0.328421    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.077 on 869 degrees of freedom\nMultiple R-squared:  0.4029,    Adjusted R-squared:  0.3953 \nF-statistic:  53.3 on 11 and 869 DF,  p-value: &lt; 2.2e-16\n\n\nCode\n## health sat\nsummary(lm(mbi_ex ~ gender + stud_h + part + health + NatLang*cesd*stai_t , data = FinalRecoded2))\n\n\n\nCall:\nlm(formula = mbi_ex ~ gender + stud_h + part + health + NatLang * \n    cesd * stai_t, data = FinalRecoded2)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-14.3761  -2.7426  -0.1272   2.5977  14.6000 \n\nCoefficients:\n                     Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)          7.504517   2.804689   2.676 0.007598 ** \ngender              -0.041008   0.307841  -0.133 0.894058    \nstud_h               0.025756   0.008805   2.925 0.003532 ** \npart                 0.672159   0.279982   2.401 0.016572 *  \nhealth              -0.414872   0.139647  -2.971 0.003051 ** \nNatLang              0.672755   2.853775   0.236 0.813688    \ncesd                 0.463579   0.139197   3.330 0.000904 ***\nstai_t               0.139753   0.067672   2.065 0.039205 *  \nNatLang:cesd        -0.139787   0.147037  -0.951 0.342022    \nNatLang:stai_t      -0.005416   0.072025  -0.075 0.940076    \ncesd:stai_t         -0.005056   0.002342  -2.159 0.031115 *  \nNatLang:cesd:stai_t  0.002378   0.002535   0.938 0.348603    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.056 on 869 degrees of freedom\nMultiple R-squared:  0.4089,    Adjusted R-squared:  0.4014 \nF-statistic: 54.64 on 11 and 869 DF,  p-value: &lt; 2.2e-16\n\n\nVariables that improved model fit were:\n\nHours studied (stud_h)\nHaving a partner (part)\nHealth satisfaction (health)\n\nThe best-fitting linear regression model for MBI_ex as determined by adjusted R-squared includes gender, hours studied, health satisfaction, having a partner, and 3-way interaction between native language, depression, and anxiety as explanatory variables. This has an Adjusted R-Squared value of 0.402, suggesting much of the the variance in MBI emotional exhaustion scores of medical school students is explained by variance in the explanatory variables. The AIC value of 4981 is the lowest of any of the linear regression models, suggesting the best fit.\n\nExplanatory variables significant at the 0.05 level are:\n\nHaving a partner (part)\nAnxiety (stai_t)\n2-way interaction between anxiety & depression (cesd * stai_t )\n\nExplanatory variables significant at the 0.01 level are:\n\nHours studying (stud_h)\nHealth satisfaction (health)\n\nExplanatory variables significant at the 0.01 level are:\n\nDepression (cesd)\n\n\n\n\nCode\n#Model 4\n\nMBI_ex_bestfit &lt;- lm(mbi_ex ~ gender + stud_h + health + part  + NatLang*cesd*stai_t, data = FinalRecoded2)\n\nsummary(MBI_ex_bestfit)\n\n\n\nCall:\nlm(formula = mbi_ex ~ gender + stud_h + health + part + NatLang * \n    cesd * stai_t, data = FinalRecoded2)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-14.3761  -2.7426  -0.1272   2.5977  14.6000 \n\nCoefficients:\n                     Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)          7.504517   2.804689   2.676 0.007598 ** \ngender              -0.041008   0.307841  -0.133 0.894058    \nstud_h               0.025756   0.008805   2.925 0.003532 ** \nhealth              -0.414872   0.139647  -2.971 0.003051 ** \npart                 0.672159   0.279982   2.401 0.016572 *  \nNatLang              0.672755   2.853775   0.236 0.813688    \ncesd                 0.463579   0.139197   3.330 0.000904 ***\nstai_t               0.139753   0.067672   2.065 0.039205 *  \nNatLang:cesd        -0.139787   0.147037  -0.951 0.342022    \nNatLang:stai_t      -0.005416   0.072025  -0.075 0.940076    \ncesd:stai_t         -0.005056   0.002342  -2.159 0.031115 *  \nNatLang:cesd:stai_t  0.002378   0.002535   0.938 0.348603    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.056 on 869 degrees of freedom\nMultiple R-squared:  0.4089,    Adjusted R-squared:  0.4014 \nF-statistic: 54.64 on 11 and 869 DF,  p-value: &lt; 2.2e-16\n\n\nCode\n#AIC \nglance(MBI_ex_bestfit)$AIC\n\n\n[1] 4981.395\n\n\n\n\nCynicism Burnout Model (MBI_cy)\nFor the cynicism burnout sub scale, I once again started by running a linear regression model with only Native Language as the explanatory variable and mbi_cy score as the outcome variable. NatLang was not a statistically significant explanatory variable with a p-value of 0.186. The Adjusted R-Squared of the model was very low at 0.0085, suggesting that variance NatLang alone does not explain the variance in mbi_cy scores for medical students. The AIC of 5183 serves as a baseline for comparison for goodness of fit with future models.\n\n\nCode\n#MBI-cy only nat lang as explanatory variables\nmodel_cy_1 &lt;- lm(mbi_cy ~ NatLang, data = FinalRecoded2)\nsummary(model_cy_1)\n\n\n\nCall:\nlm(formula = mbi_cy ~ NatLang, data = FinalRecoded2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.6630 -3.9962 -0.9962  3.0038 14.0038 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  10.6630     0.4769  22.358   &lt;2e-16 ***\nNatLang      -0.6668     0.5040  -1.323    0.186    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.574 on 879 degrees of freedom\nMultiple R-squared:  0.001988,  Adjusted R-squared:  0.0008526 \nF-statistic: 1.751 on 1 and 879 DF,  p-value: 0.1861\n\n\nCode\n#AIC\nglance(model_cy_1)$AIC\n\n\n[1] 5183.262\n\n\nAs with the previous emotional exhaustion sub-scale, next I added gender as a the second explanatory variable, as the impact of gender on burnout has been studied. Not only were neither NatLang or gender statistically significant (p-values of 0.191 and 0.783 respectively), but the linear regression model with gender & NatLang as explanatory variables had a worse Adjusted R-Squared than the first model with a negative R-squared of -0.0001993. The AIC of 5185 is higher than in the first model, confirming the worse fit of this model.\n\n\nCode\n#MBI-cy  nat lang & gender as explanatory variables\nmodel_cy_2  &lt;- lm(mbi_cy ~ NatLang + gender, data = FinalRecoded2)\nsummary(model_cy_2)\n\n\n\nCall:\nlm(formula = mbi_cy ~ NatLang + gender, data = FinalRecoded2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.6859 -3.9339 -0.9339  2.9745 14.0661 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 10.59434    0.53862  19.669   &lt;2e-16 ***\nNatLang     -0.66049    0.50475  -1.309    0.191    \ngender       0.09161    0.33313   0.275    0.783    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.577 on 878 degrees of freedom\nMultiple R-squared:  0.002074,  Adjusted R-squared:  -0.0001993 \nF-statistic: 0.9123 on 2 and 878 DF,  p-value: 0.402\n\n\nCode\n#AIC\nglance(model_cy_2)$AIC\n\n\n[1] 5185.186\n\n\nFor the third linear regression model I added the 3 way interaction of NatLang x Anxiety (stai-t score) x Depression (cesd) as an explanatory variable to gender in model 2. The Adjusted R-squared value of 0.1775 suggest a much better fiting model than in the prior 2 models. Gender is statistically significant at the 0.01 level (p-value = 0.0125). Depression (cesd, p-value = 0.07813) & Anxiety (stai_t, p-value = 0.08613) are both significant at the 0.1 level. Contrary to model 2, removing gender as an explanatory variable would lower the adjusted R-squared to gender 0.1685. The AIC of this model is 5027, suggesting the best fit so far.\n\n\nCode\n#Model 3 MBI_cy 3 way interaction NatLang x cesd x stai_t + gender\nmodel_cy_3 = lm(mbi_cy ~  + gender + NatLang*cesd*stai_t , data = FinalRecoded2)\nsummary(model_cy_3)\n\n\n\nCall:\nlm(formula = mbi_cy ~ +gender + NatLang * cesd * stai_t, data = FinalRecoded2)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-10.3237  -3.0923  -0.6276   2.4286  15.6652 \n\nCoefficients:\n                     Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)          3.017407   2.763302   1.092  0.27515   \ngender              -1.017625   0.314287  -3.238  0.00125 **\nNatLang              3.154415   2.917068   1.081  0.27983   \ncesd                 0.250788   0.142195   1.764  0.07813 . \nstai_t               0.118863   0.069182   1.718  0.08613 . \nNatLang:cesd        -0.072387   0.150271  -0.482  0.63013   \nNatLang:stai_t      -0.072897   0.073635  -0.990  0.32246   \ncesd:stai_t         -0.002160   0.002393  -0.903  0.36690   \nNatLang:cesd:stai_t  0.001455   0.002590   0.562  0.57435   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.151 on 872 degrees of freedom\nMultiple R-squared:  0.1849,    Adjusted R-squared:  0.1775 \nF-statistic: 24.73 on 8 and 872 DF,  p-value: &lt; 2.2e-16\n\n\nCode\n#AIC\nglance(model_cy_3)$AIC\n\n\n[1] 5018.858\n\n\nCode\n#Test without gender\nsummary(lm(mbi_cy ~  NatLang*cesd*stai_t , data = FinalRecoded2))\n\n\n\nCall:\nlm(formula = mbi_cy ~ NatLang * cesd * stai_t, data = FinalRecoded2)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-10.4698  -3.1266  -0.6844   2.5435  15.2039 \n\nCoefficients:\n                     Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept)          2.689211   2.776402   0.969   0.3330  \nNatLang              3.325117   2.932391   1.134   0.2571  \ncesd                 0.237590   0.142907   1.663   0.0968 .\nstai_t               0.112038   0.069524   1.611   0.1074  \nNatLang:cesd        -0.071845   0.151085  -0.476   0.6345  \nNatLang:stai_t      -0.077049   0.074023  -1.041   0.2982  \ncesd:stai_t         -0.002016   0.002406  -0.838   0.4022  \nNatLang:cesd:stai_t  0.001495   0.002604   0.574   0.5660  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.173 on 873 degrees of freedom\nMultiple R-squared:  0.1751,    Adjusted R-squared:  0.1685 \nF-statistic: 26.48 on 7 and 873 DF,  p-value: &lt; 2.2e-16\n\n\n` To determine the best model, I added the control variables of hours studied, having a partner, job status, consulting a psychotherapist in past 12 months, & health satisfaction to see the impact on the adjusted R squared value.\n\n\nCode\n##Control variables\n\n\n##hours studied strengthens model\nsummary(lm(mbi_cy ~ gender + stud_h + NatLang*cesd*stai_t , data = FinalRecoded2))\n\n\n\nCall:\nlm(formula = mbi_cy ~ gender + stud_h + NatLang * cesd * stai_t, \n    data = FinalRecoded2)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-10.6947  -2.9170  -0.5115   2.5003  15.8265 \n\nCoefficients:\n                     Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)          4.202266   2.725466   1.542  0.12347    \ngender              -1.103828   0.309423  -3.567  0.00038 ***\nstud_h              -0.049040   0.008823  -5.558 3.62e-08 ***\nNatLang              2.888782   2.868711   1.007  0.31422    \ncesd                 0.261446   0.139832   1.870  0.06186 .  \nstai_t               0.125514   0.068036   1.845  0.06540 .  \nNatLang:cesd        -0.073097   0.147759  -0.495  0.62093    \nNatLang:stai_t      -0.075411   0.072406  -1.041  0.29793    \ncesd:stai_t         -0.002394   0.002354  -1.017  0.30943    \nNatLang:cesd:stai_t  0.001705   0.002548   0.669  0.50350    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.081 on 871 degrees of freedom\nMultiple R-squared:  0.2129,    Adjusted R-squared:  0.2047 \nF-statistic: 26.17 on 9 and 871 DF,  p-value: &lt; 2.2e-16\n\n\nCode\n##partner - lowered adjusted r-squared\nsummary(lm(mbi_cy ~ gender + stud_h + part + NatLang*cesd*stai_t , data = FinalRecoded2))\n\n\n\nCall:\nlm(formula = mbi_cy ~ gender + stud_h + part + NatLang * cesd * \n    stai_t, data = FinalRecoded2)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-10.7850  -2.9549  -0.5399   2.5005  15.7597 \n\nCoefficients:\n                     Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)          4.033722   2.737663   1.473 0.141000    \ngender              -1.112886   0.309808  -3.592 0.000346 ***\nstud_h              -0.048508   0.008860  -5.475 5.73e-08 ***\npart                 0.190578   0.281578   0.677 0.498699    \nNatLang              2.933607   2.870368   1.022 0.307050    \ncesd                 0.266642   0.140086   1.903 0.057316 .  \nstai_t               0.126416   0.068070   1.857 0.063629 .  \nNatLang:cesd        -0.077949   0.147979  -0.527 0.598499    \nNatLang:stai_t      -0.076504   0.072446  -1.056 0.291256    \ncesd:stai_t         -0.002465   0.002357  -1.046 0.295846    \nNatLang:cesd:stai_t  0.001786   0.002551   0.700 0.484009    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.082 on 870 degrees of freedom\nMultiple R-squared:  0.2133,    Adjusted R-squared:  0.2042 \nF-statistic: 23.58 on 10 and 870 DF,  p-value: &lt; 2.2e-16\n\n\nCode\n##having job - lowered adjusted r-squared\nsummary(lm(mbi_cy ~ gender + stud_h + job + NatLang*cesd*stai_t , data = FinalRecoded2))\n\n\n\nCall:\nlm(formula = mbi_cy ~ gender + stud_h + job + NatLang * cesd * \n    stai_t, data = FinalRecoded2)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-10.6353  -2.9306  -0.4633   2.5358  15.8792 \n\nCoefficients:\n                     Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)          4.085731   2.737852   1.492 0.135980    \ngender              -1.108179   0.309698  -3.578 0.000365 ***\nstud_h              -0.048219   0.008997  -5.360 1.07e-07 ***\njob                  0.139370   0.295455   0.472 0.637250    \nNatLang              2.936696   2.871789   1.023 0.306780    \ncesd                 0.263839   0.139986   1.885 0.059796 .  \nstai_t               0.126449   0.068095   1.857 0.063658 .  \nNatLang:cesd        -0.075814   0.147938  -0.512 0.608452    \nNatLang:stai_t      -0.076325   0.072464  -1.053 0.292508    \ncesd:stai_t         -0.002435   0.002356  -1.034 0.301638    \nNatLang:cesd:stai_t  0.001755   0.002551   0.688 0.491563    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.083 on 870 degrees of freedom\nMultiple R-squared:  0.2131,    Adjusted R-squared:  0.204 \nF-statistic: 23.56 on 10 and 870 DF,  p-value: &lt; 2.2e-16\n\n\nCode\n## seen psychiatrist - did not improve R-squared\nsummary(lm(mbi_cy ~ gender + stud_h + psyt + NatLang*cesd*stai_t , data = FinalRecoded2))\n\n\n\nCall:\nlm(formula = mbi_cy ~ gender + stud_h + psyt + NatLang * cesd * \n    stai_t, data = FinalRecoded2)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-10.4672  -2.9080  -0.5373   2.4705  15.8724 \n\nCoefficients:\n                     Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)          4.295984   2.727292   1.575  0.11558    \ngender              -1.130528   0.310664  -3.639  0.00029 ***\nstud_h              -0.048290   0.008857  -5.452 6.48e-08 ***\npsyt                 0.338400   0.350042   0.967  0.33394    \nNatLang              2.806847   2.870070   0.978  0.32836    \ncesd                 0.264995   0.139885   1.894  0.05851 .  \nstai_t               0.121060   0.068194   1.775  0.07621 .  \nNatLang:cesd        -0.076487   0.147807  -0.517  0.60495    \nNatLang:stai_t      -0.072240   0.072483  -0.997  0.31921    \ncesd:stai_t         -0.002423   0.002354  -1.029  0.30364    \nNatLang:cesd:stai_t  0.001690   0.002548   0.663  0.50741    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.081 on 870 degrees of freedom\nMultiple R-squared:  0.2137,    Adjusted R-squared:  0.2047 \nF-statistic: 23.65 on 10 and 870 DF,  p-value: &lt; 2.2e-16\n\n\nCode\n## health sat - improved model\nsummary(lm(mbi_cy ~ gender + stud_h + health + NatLang*cesd*stai_t , data = FinalRecoded2))\n\n\n\nCall:\nlm(formula = mbi_cy ~ gender + stud_h + health + NatLang * cesd * \n    stai_t, data = FinalRecoded2)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-10.8164  -2.9790  -0.4869   2.4950  15.9812 \n\nCoefficients:\n                     Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)          5.157591   2.811583   1.834 0.066935 .  \ngender              -1.099923   0.309279  -3.556 0.000396 ***\nstud_h              -0.049257   0.008820  -5.585 3.13e-08 ***\nhealth              -0.192624   0.140328  -1.373 0.170211    \nNatLang              2.748771   2.869069   0.958 0.338293    \ncesd                 0.259503   0.139768   1.857 0.063695 .  \nstai_t               0.122390   0.068039   1.799 0.072395 .  \nNatLang:cesd        -0.071820   0.147687  -0.486 0.626880    \nNatLang:stai_t      -0.071971   0.072412  -0.994 0.320548    \ncesd:stai_t         -0.002435   0.002353  -1.035 0.301001    \nNatLang:cesd:stai_t  0.001646   0.002547   0.646 0.518354    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.079 on 870 degrees of freedom\nMultiple R-squared:  0.2146,    Adjusted R-squared:  0.2055 \nF-statistic: 23.77 on 10 and 870 DF,  p-value: &lt; 2.2e-16\n\n\n\nVariables that increased the adjusted r-squared of the model were:\n\nHours studied (stud_h)\nHealth satisfaction (health)\n\n\nThe best-fitting linear regression model for MBI_cy as determined by adjusted R-squared values includes gender, hours studied, health satisfaction, and the 3-way interaction between native language, depression, and anxiety as explanatory variables. This has an Adjusted R-Squared value of 0.2055, suggesting some of the the variance in MBI cynicism scores of medical school students is explained by variance in the explanatory variables. With the AIC level of 4990, the AIC also confirms this is the best-fitting model.\n\nExplanatory variables significant at the 0.1 level are:\n\nDepress (cesd)\nAnxiety (stai_t)\n\nExplanatory variables significant at the 0.001 level are:\nGender\nHours studying (stud_h)\n\n\n\nCode\n#Model 4\n\nMBI_cy_bestfit &lt;- lm(mbi_cy ~ gender + stud_h + health +  NatLang*cesd*stai_t, data = FinalRecoded2)\nsummary(MBI_cy_bestfit)\n\n\n\nCall:\nlm(formula = mbi_cy ~ gender + stud_h + health + NatLang * cesd * \n    stai_t, data = FinalRecoded2)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-10.8164  -2.9790  -0.4869   2.4950  15.9812 \n\nCoefficients:\n                     Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)          5.157591   2.811583   1.834 0.066935 .  \ngender              -1.099923   0.309279  -3.556 0.000396 ***\nstud_h              -0.049257   0.008820  -5.585 3.13e-08 ***\nhealth              -0.192624   0.140328  -1.373 0.170211    \nNatLang              2.748771   2.869069   0.958 0.338293    \ncesd                 0.259503   0.139768   1.857 0.063695 .  \nstai_t               0.122390   0.068039   1.799 0.072395 .  \nNatLang:cesd        -0.071820   0.147687  -0.486 0.626880    \nNatLang:stai_t      -0.071971   0.072412  -0.994 0.320548    \ncesd:stai_t         -0.002435   0.002353  -1.035 0.301001    \nNatLang:cesd:stai_t  0.001646   0.002547   0.646 0.518354    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.079 on 870 degrees of freedom\nMultiple R-squared:  0.2146,    Adjusted R-squared:  0.2055 \nF-statistic: 23.77 on 10 and 870 DF,  p-value: &lt; 2.2e-16\n\n\nCode\n#AIC\nglance(MBI_cy_bestfit)$AIC\n\n\n[1] 4990.242\n\n\n\n\nAcademic Efficacy Burnout MBI-ea\nAs with the prior 2 MBI subs scales, I first created a linear regression model with NatLang as the only explanatory variable and academic efficacy burnout MBI_ea as the outcome. NatLang is not a statistically significant explanatory variable in the model with a p-value of 0.852. The Adjusted R-squared of -0.0011 shows the model is a very bad for predicting the outcome variable MBI_ea score. The AIC of this model is 5197.\n\n\nCode\n#Academic Efficacy NatLang Alone\nmodel_ea_1 &lt;- lm(mbi_ea ~ NatLang, data = FinalRecoded2)\nsummary(model_ea_1)  \n\n\n\nCall:\nlm(formula = mbi_ea ~ NatLang, data = FinalRecoded2)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-14.2205  -3.2205  -0.2205   3.7795  11.7795 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 24.31522    0.48074  50.579   &lt;2e-16 ***\nNatLang     -0.09469    0.50800  -0.186    0.852    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.611 on 879 degrees of freedom\nMultiple R-squared:  3.952e-05, Adjusted R-squared:  -0.001098 \nF-statistic: 0.03474 on 1 and 879 DF,  p-value: 0.8522\n\n\nCode\n#AIC\nglance(model_ea_1)$AIC\n\n\n[1] 5197.324\n\n\nFor the second model, I added the second explanatory variable of gender, based on previous findings of its impact on burnout. In this model, neither NatLang (p-value = 0.819) or gender (p-value = 0.351) are statistically significant. The fit of the model is still very poor, as indicated by the Adjusted R-squared of -0.00124. Still, very litle of the variance in academic efficacy burnout is explained by the variance in gender and native language status. The AIC is slightly higher at 5198, suggesting a poor fit of the model.\n\n\nCode\n#Linear regression model NatLang & gender\nmodel_ea_2 &lt;- lm(mbi_ea ~ NatLang + gender, data = FinalRecoded2)\nsummary(model_ea_2)\n\n\n\nCall:\nlm(formula = mbi_ea ~ NatLang + gender, data = FinalRecoded2)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-14.4339  -3.1204  -0.1204   3.5661  11.8796 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  24.5503     0.5427  45.238   &lt;2e-16 ***\nNatLang      -0.1164     0.5086  -0.229    0.819    \ngender       -0.3135     0.3356  -0.934    0.351    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.611 on 878 degrees of freedom\nMultiple R-squared:  0.001032,  Adjusted R-squared:  -0.001243 \nF-statistic: 0.4536 on 2 and 878 DF,  p-value: 0.6355\n\n\nCode\n#AIC\nglance(model_ea_2)$AIC\n\n\n[1] 5198.449\n\n\nFor the third linear regression model, I added the 3-way interaction of NatLang x Depression (cesd) x Anxiety (stai_t) as an explanatory variable in addition to gender, with the outcome variable of mbi_ea score. The adjusted R-squared of this model is 0.2597, suggesting a much better fit than prior models. In this model, anxiety (stai_t) is statistically significant at the 0.05 level. The AIC of 4938 is the lowest of the models so far, suggesting the best fit.\n\n\nCode\n##Model with gender & 3 way interaction NatLang x Depression x Anxiety\nmodel_ea_3 &lt;- lm(mbi_ea ~ gender +  NatLang*cesd*stai_t, data = FinalRecoded2)\nsummary(model_ea_3)\n\n\n\nCall:\nlm(formula = mbi_ea ~ gender + NatLang * cesd * stai_t, data = FinalRecoded2)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.0246  -2.5302   0.0188   2.5821  15.2910 \n\nCoefficients:\n                     Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         33.040892   2.640002  12.515  &lt; 2e-16 ***\ngender               1.138628   0.300263   3.792  0.00016 ***\nNatLang             -0.921610   2.786906  -0.331  0.74096    \ncesd                -0.122730   0.135850  -0.903  0.36655    \nstai_t              -0.176915   0.066095  -2.677  0.00757 ** \nNatLang:cesd        -0.101544   0.143566  -0.707  0.47957    \nNatLang:stai_t       0.016992   0.070349   0.242  0.80920    \ncesd:stai_t          0.001048   0.002286   0.458  0.64674    \nNatLang:cesd:stai_t  0.001434   0.002475   0.579  0.56252    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.965 on 872 degrees of freedom\nMultiple R-squared:  0.2664,    Adjusted R-squared:  0.2597 \nF-statistic: 39.58 on 8 and 872 DF,  p-value: &lt; 2.2e-16\n\n\nCode\n#AIC\nglance(model_ea_3)$AIC\n\n\n[1] 4938.428\n\n\nCode\n#3way interaction without gender - worse fit\nsummary(lm(mbi_ea ~  NatLang*cesd*stai_t, data = FinalRecoded2))\n\n\n\nCall:\nlm(formula = mbi_ea ~ NatLang * cesd * stai_t, data = FinalRecoded2)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.5816  -2.6526   0.1266   2.6563  15.4490 \n\nCoefficients:\n                      Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         33.4081124  2.6583659  12.567   &lt;2e-16 ***\nNatLang             -1.1126095  2.8077232  -0.396   0.6920    \ncesd                -0.1079627  0.1368312  -0.789   0.4303    \nstai_t              -0.1692785  0.0665684  -2.543   0.0112 *  \nNatLang:cesd        -0.1021503  0.1446617  -0.706   0.4803    \nNatLang:stai_t       0.0216380  0.0708758   0.305   0.7602    \ncesd:stai_t          0.0008866  0.0023035   0.385   0.7004    \nNatLang:cesd:stai_t  0.0013891  0.0024937   0.557   0.5776    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.996 on 873 degrees of freedom\nMultiple R-squared:  0.2543,    Adjusted R-squared:  0.2483 \nF-statistic: 42.53 on 7 and 873 DF,  p-value: &lt; 2.2e-16\n\n\nTo determine the best model, I added the control variables of hours studied, having a partner, job status, consulting a psychotherapist in past 12 months, & health satisfaction to see the impact on the adjusted R squared value.\n\n\nCode\n##Control variables\n\n##hours studied strengthens model \nsummary(lm(mbi_ea ~ gender + stud_h + NatLang*cesd*stai_t , data = FinalRecoded2))\n\n\n\nCall:\nlm(formula = mbi_ea ~ gender + stud_h + NatLang * cesd * stai_t, \n    data = FinalRecoded2)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-13.7852  -2.4327   0.0613   2.4029  14.2047 \n\nCoefficients:\n                     Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         31.669058   2.582120  12.265  &lt; 2e-16 ***\ngender               1.238434   0.293149   4.225 2.65e-05 ***\nstud_h               0.056779   0.008359   6.793 2.04e-11 ***\nNatLang             -0.614059   2.717831  -0.226  0.82130    \ncesd                -0.135070   0.132477  -1.020  0.30822    \nstai_t              -0.184615   0.064458  -2.864  0.00428 ** \nNatLang:cesd        -0.100722   0.139988  -0.720  0.47202    \nNatLang:stai_t       0.019902   0.068598   0.290  0.77179    \ncesd:stai_t          0.001318   0.002230   0.591  0.55458    \nNatLang:cesd:stai_t  0.001145   0.002414   0.474  0.63537    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.866 on 871 degrees of freedom\nMultiple R-squared:  0.3033,    Adjusted R-squared:  0.2961 \nF-statistic: 42.13 on 9 and 871 DF,  p-value: &lt; 2.2e-16\n\n\nCode\n##partner - weakens model \nsummary(lm(mbi_ea ~ gender + stud_h + part + NatLang*cesd*stai_t , data = FinalRecoded2))\n\n\n\nCall:\nlm(formula = mbi_ea ~ gender + stud_h + part + NatLang * cesd * \n    stai_t, data = FinalRecoded2)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-13.8188  -2.4552   0.0647   2.4193  14.2476 \n\nCoefficients:\n                     Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         31.579435   2.594144  12.173  &lt; 2e-16 ***\ngender               1.233618   0.293567   4.202 2.92e-05 ***\nstud_h               0.057062   0.008396   6.796 1.99e-11 ***\npart                 0.101340   0.266817   0.380  0.70418    \nNatLang             -0.590223   2.719891  -0.217  0.82826    \ncesd                -0.132307   0.132742  -0.997  0.31918    \nstai_t              -0.184135   0.064502  -2.855  0.00441 ** \nNatLang:cesd        -0.103302   0.140222  -0.737  0.46150    \nNatLang:stai_t       0.019321   0.068648   0.281  0.77844    \ncesd:stai_t          0.001280   0.002233   0.573  0.56664    \nNatLang:cesd:stai_t  0.001188   0.002417   0.491  0.62322    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.868 on 870 degrees of freedom\nMultiple R-squared:  0.3034,    Adjusted R-squared:  0.2954 \nF-statistic:  37.9 on 10 and 870 DF,  p-value: &lt; 2.2e-16\n\n\nCode\n##having job -weakens model\nsummary(lm(mbi_ea ~ gender + stud_h + job + NatLang*cesd*stai_t , data = FinalRecoded2))\n\n\n\nCall:\nlm(formula = mbi_ea ~ gender + stud_h + job + NatLang * cesd * \n    stai_t, data = FinalRecoded2)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-13.6939  -2.3959   0.0343   2.4416  14.1748 \n\nCoefficients:\n                     Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         31.818732   2.593577  12.268  &lt; 2e-16 ***\ngender               1.244022   0.293378   4.240 2.47e-05 ***\nstud_h               0.055724   0.008523   6.538 1.06e-10 ***\njob                 -0.179002   0.279885  -0.640  0.52263    \nNatLang             -0.675599   2.720456  -0.248  0.80393    \ncesd                -0.138143   0.132609  -1.042  0.29783    \nstai_t              -0.185816   0.064507  -2.881  0.00407 ** \nNatLang:cesd        -0.097233   0.140142  -0.694  0.48798    \nNatLang:stai_t       0.021076   0.068645   0.307  0.75889    \ncesd:stai_t          0.001372   0.002232   0.615  0.53903    \nNatLang:cesd:stai_t  0.001080   0.002417   0.447  0.65497    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.868 on 870 degrees of freedom\nMultiple R-squared:  0.3036,    Adjusted R-squared:  0.2956 \nF-statistic: 37.94 on 10 and 870 DF,  p-value: &lt; 2.2e-16\n\n\nCode\n## seen psychiatrist - weakens model\nsummary(lm(mbi_ea ~ gender + stud_h + psyt + NatLang*cesd*stai_t , data = FinalRecoded2))\n\n\n\nCall:\nlm(formula = mbi_ea ~ gender + stud_h + psyt + NatLang * cesd * \n    stai_t, data = FinalRecoded2)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-13.7875  -2.4098   0.0743   2.3958  14.1929 \n\nCoefficients:\n                     Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         31.661813   2.585228  12.247  &lt; 2e-16 ***\ngender               1.240498   0.294482   4.212 2.79e-05 ***\nstud_h               0.056721   0.008396   6.756 2.60e-11 ***\npsyt                -0.026161   0.331809  -0.079  0.93718    \nNatLang             -0.607725   2.720569  -0.223  0.82329    \ncesd                -0.135344   0.132599  -1.021  0.30768    \nstai_t              -0.184271   0.064642  -2.851  0.00447 ** \nNatLang:cesd        -0.100460   0.140107  -0.717  0.47355    \nNatLang:stai_t       0.019657   0.068707   0.286  0.77487    \ncesd:stai_t          0.001320   0.002231   0.592  0.55416    \nNatLang:cesd:stai_t  0.001146   0.002415   0.475  0.63522    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.869 on 870 degrees of freedom\nMultiple R-squared:  0.3033,    Adjusted R-squared:  0.2953 \nF-statistic: 37.88 on 10 and 870 DF,  p-value: &lt; 2.2e-16\n\n\nCode\n## health sat - improved model\nsummary(lm(mbi_ea ~ gender + stud_h + health + NatLang*cesd*stai_t , data = FinalRecoded2))\n\n\n\nCall:\nlm(formula = mbi_ea ~ gender + stud_h + health + NatLang * cesd * \n    stai_t, data = FinalRecoded2)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-13.7305  -2.4347   0.0484   2.4248  13.9423 \n\nCoefficients:\n                     Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         30.460799   2.661451  11.445  &lt; 2e-16 ***\ngender               1.233495   0.292764   4.213 2.78e-05 ***\nstud_h               0.057054   0.008349   6.834 1.55e-11 ***\nhealth               0.243623   0.132835   1.834  0.06699 .  \nNatLang             -0.436979   2.715868  -0.161  0.87221    \ncesd                -0.132613   0.132305  -1.002  0.31646    \nstai_t              -0.180665   0.064406  -2.805  0.00514 ** \nNatLang:cesd        -0.102338   0.139801  -0.732  0.46435    \nNatLang:stai_t       0.015552   0.068546   0.227  0.82057    \ncesd:stai_t          0.001370   0.002227   0.615  0.53857    \nNatLang:cesd:stai_t  0.001220   0.002411   0.506  0.61289    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.861 on 870 degrees of freedom\nMultiple R-squared:  0.306, Adjusted R-squared:  0.298 \nF-statistic: 38.36 on 10 and 870 DF,  p-value: &lt; 2.2e-16\n\n\n\nVariables that increased the adjusted r-squared of the model were:\n\nHours studied (stud_h)\nHealth satisfaction (health)\n\n\nThe best-fitting linear regression model for MBI_cy as determined by adjusted R-squared values includes gender, hours studied, health satisfaction, and the 3-way interaction between native language, depression, and anxiety as explanatory variables. This has an Adjusted R-Squared value of 0.298, suggesting some of the the variance in MBI academic efficacy scores of medical school students is explained by variance in the explanatory variables. The AIC of 4893 is the lowest of any of the linear regression models, supporting the fit of this model.\n\nExplanatory variables significant at the 0.1 level are:\n\nHealth satisfaction (health)\n\nExplanatory variables significant at the 0.05 level are:\n\nAnxiety (stai_t)\n\nExplanatory variables significant at the 0.001 level are:\nGender\nHours studied (stud_h)\n\n\n\nCode\n#Best fit model academic efficacy burnout \n\nMBI_ea_bestfit &lt;- lm(mbi_ea ~ gender + stud_h + health +  NatLang*cesd*stai_t, data = FinalRecoded2)\n\n#AIC\nglance(MBI_ea_bestfit)$AIC\n\n\n[1] 4893.55\n\n\nCode\nsummary(MBI_ea_bestfit)\n\n\n\nCall:\nlm(formula = mbi_ea ~ gender + stud_h + health + NatLang * cesd * \n    stai_t, data = FinalRecoded2)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-13.7305  -2.4347   0.0484   2.4248  13.9423 \n\nCoefficients:\n                     Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         30.460799   2.661451  11.445  &lt; 2e-16 ***\ngender               1.233495   0.292764   4.213 2.78e-05 ***\nstud_h               0.057054   0.008349   6.834 1.55e-11 ***\nhealth               0.243623   0.132835   1.834  0.06699 .  \nNatLang             -0.436979   2.715868  -0.161  0.87221    \ncesd                -0.132613   0.132305  -1.002  0.31646    \nstai_t              -0.180665   0.064406  -2.805  0.00514 ** \nNatLang:cesd        -0.102338   0.139801  -0.732  0.46435    \nNatLang:stai_t       0.015552   0.068546   0.227  0.82057    \ncesd:stai_t          0.001370   0.002227   0.615  0.53857    \nNatLang:cesd:stai_t  0.001220   0.002411   0.506  0.61289    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.861 on 870 degrees of freedom\nMultiple R-squared:  0.306, Adjusted R-squared:  0.298 \nF-statistic: 38.36 on 10 and 870 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "posts/EmmaNarkewicz_FinalProject_CheckIn2.html#diagnostic-models",
    "href": "posts/EmmaNarkewicz_FinalProject_CheckIn2.html#diagnostic-models",
    "title": "Final Project Check in 2",
    "section": "Diagnostic Models",
    "text": "Diagnostic Models\n\nMBI Emotional Exhaustion\n\n\nCode\n#Diagnostic Models Emotional Exhaustion\n\n fit_ex &lt;- lm(mbi_ex ~ sex + stud_h + health + job + NatLang*cesd*stai_t, data = FinalRecoded2)\n\n##MBI_ex Residuals vs. Fitted\n\nplot(fit_ex, which = 1) \n\n\n\n\n\nThe first diagnostic model of residuals vs. fitted value shows a relatively horizontal line that bounces randomly around the zero line, suggesting linearity and constant variance for the MBI_ex linear regression model.\n\n\nCode\n##MBI_Ex Normal Q-Q\nplot(fit_ex, which = 2) \n\n\n\n\n\nMost of the points of the MBI_ex linear regression Q-Q plot fall along the line, supporting the assumption of normality.\n\n\nCode\n##MBI_Ex Scale Location\nplot(fit_ex, which = 3) \n\n\n\n\n\nThe Scale-Location plot of the MBI-ex linear regression model is nearly perfectly horizontal without increasing or decreasing, supporting the assumption of constant variance.\n\n\nCode\n##MBI_Ex Cook's Distance\nplot(fit_ex, which = 4) \n\n\n\n\n\nThe Cook’s distance plot of the MBI_ex linear regression model measures the presence of influential outliers. There appears to be 3 outliers which are influence the model, as shown by at Obs. number at 29, 526, and 756. None of them have a Cook’s distance of 1. However with a sample size of n = 881, 4/n = 0.00454 which means taking a closer look at the outliers here.\n\n\nCode\n##MBI_Ex Residuals vs. leverage \nplot(fit_ex, which = 5) \n\n\n\n\n\nThe Residuals vs. Leverage plot of the MBI_ex linear regression supports the Cook’s distance plot that there are some influential observations in the model (outliers) as the red line deviates from the horizontal line.\n\n\nCode\n##MBI_Ex Cook's Distance vs Leverage\nplot(fit_ex, which = 6) \n\n\n\n\n\nThe final diagnostic plot of Cook’s Distance vs. Leverage for the MBI-ex plot shows that while there are some residuals, they don’t have strong leverage.\nOverall the diagnostic models supports the goodness of fit of the emotional exhaustion MBI model and the assumptions of linearity, constant variance, and normality, but there are some outliers that should be examined.\n\n\nMBI Cynicism\n\n\nCode\n##Diagnostic model\nfit_cy &lt;- lm(mbi_cy ~ gender + stud_h + health +  NatLang*cesd*stai_t, data = FinalRecoded2)\n\n##MBI_ex Residuals vs. Fitted\nplot(fit_cy, which = 1) \n\n\n\n\n\nThe diagnostic plot of residuals vs. fitted values for the MBI_cy linear regression model is mostly horizontal around the 0 line, though there is some slight funneling in the bottom left half of the plot. This suggests linearity but slight violation of constant variance assumption.\n\n\nCode\n##MBI_cy Q-Q plot\nplot(fit_cy, which = 2) \n\n\n\n\n\nThe Q-Q plot for the MBI_cy linear regression supports the assumption of normality, as the points all fall along the line.\n\n\nCode\n##MBI_cy Scale Location\nplot(fit_cy, which = 3) \n\n\n\n\n\nThe Scale-Location plot of the MBI_cy linear regression shows some increasing trend, but is relatively horizontal. The assumption of constant variance is not totally supported but also not totally violated based on the slope of the red line.\n\n\nCode\n##MBI_cy Cook's distance\nplot(fit_cy, which = 4) \n\n\n\n\n\nThe Cook’s distance plot for the MBI-cy linear regression shows 1 influential outlier at observation number 29. This observation does not have a Cook’s distance of 1, but is greater than 4/n, which with n=881 is 0.00454, which means taking a closer look at the potential of an influential outlier.\n\n\nCode\n##MBI_cy Residuals vs Leverage\nplot(fit_cy, which = 5) \n\n\n\n\n\nThe Residuals vs Leverage plot for MBI-cy linear regression model falls within the dashed red line, suggesting that residuals lack leverage to strongly influence the model.\n\n\nCode\n##MBI_cy Cook's dist. vs Leverage\nplot(fit_cy, which = 6) \n\n\n\n\n\nLastly, the diagnostic plot of Cook’s dist. vs. Leverage for the MBI_cy linear regression model suggests that any outliers in the model do not have strong leverage on the model.\nOverall the diagnostic models mostly support the fit of the cynicism MBI model and the assumptions of linearity and normality, as well as the lack of influential outliers. The diagnostic of residuals vs. fitted and residuals vs variance suggesting some slight violation of the assumption of constant variance.\n\n\nMBI Acamemic Efficacy\n\n\nCode\nfit_ea &lt;- lm(mbi_ea ~ gender + stud_h + health +  NatLang*cesd*stai_t, data = FinalRecoded2)\n\n#MBI_ea Residuals vs. Fitted\nplot(fit_ea, which = 1)\n\n\n\n\n\nThe first diagnostic plot of residuals vs. fitted values for the MBI_ea linear regression has a horizontal trend line without any funneling, supporting the assumptions of linearity and constant variance.\n\n\nCode\n#MBI_ea Q-Q plot\nplot(fit_ea, which = 2)\n\n\n\n\n\nThe diagnostic Q-Q plot for the MBI-ea linear regression model supports the assumption of normality, as most of the points fall along the line.\n\n\nCode\n#MBI_ea scale-location\nplot(fit_ea, which = 3)\n\n\n\n\n\nThe scale-location diagnostic plot for the MBI_ea linear regression model is approximately horizontal, supporting the assumption of constant variance.\n\n\nCode\n#MBI_ea\nplot(fit_ea, which = 4)\n\n\n\n\n\nThe Cook’s distance diagnostic plot for the MBI_ea linear regression model suggests the lack of influential observations (outliers). None of the observations have a Cook’s distance near 1, with the highest being less than 0.030, suggesting the outliers don’t strongly impact the model.\n\n\nCode\n#MBI_ea residuals vs. leverage\nplot(fit_ea, which = 5)\n\n\n\n\n\nThe residuals vs. leverage plot for the mbi_ea linear regression model further supports the lack of influential observations (outliers), as does the Cook’s Distances vs. Leverage plot below\n\n\nCode\n#MBI_ea Cooks Distance vs. Leverage\nplot(fit_ea, which = 6)\n\n\n\n\n\nOverall these diagnostic plots support the goodness of fit of the academic efficacy MBI linear regression model, supporting the assumptions of linearity, constant variance, and normality aswell as suggesting the lack of influential outliers.\n\n\nDiscussion\nReturning to my research question:\nWhy are some medical students more likely to experience burnout than others?\nThere was not statistical support for my hypothesis that medical students whose native language was a national language where they are in medical school will have lower burnout than medical students speaking the non-native language. I failed to reject the null-hypothesis that there was no difference in the mean burnout scores of native and non-native language speakers. The only MBI sub scale with any statistically significant difference between native & non-native language speakers was for the emotional exhaustion sub-scale, which was only significant at the 0.1 level.\nI still find my original hypothesis plausible as being a medical student in a different country would increase rates of burnout due to cultural shock and isolation from family & friends. However, it is unclear if “native tongue”, the variable in the data set used to code NatLang, truly measures immigrant status or rather ethnicity. Furthermore, the sample used for analysis was of medical students in Switzerland, so these findings cannot be generalized to medical students in all countries.\nThrough further T-tests discovered that there was statistically significant difference in anxiety & depression levels for native and non-native language speaking medical students in the direction I expected. Through exploration I discovered that the 3-way interaction term of NatLang x Anxiety x Depression was a strong explanatory variable for burnout on all three sub scales: emotional exhaustion, cyncicism, and academic efficacy. Gender, hours studied, and health satisfaction also explain why some medical students are more likely to experience burnout than others on all three sub scales. For emotional exhaustion burnout, having a partner also impacted burnout.\nBased on Adjusted R-squared values, AIC values, and diagnostic plots, the linear regression models are a good fit to explain variance in the outcome variables of burnout on each of the 3 MBI sub-scales.\nFuture research might examine how removing outliers and recoding NatLang to include English speakers might impacts significance of the hypothesis & fit of linear regression. While English is not one of the national languages in Switzerland, it is commonly used and on official signage & documents, so it would be interesting to see if there was a statistically significant difference in burnout between medical students with a native tongue of a commonly used language (national language or English) in Switzerland vs. those who do not. Lastly, testing this hypothesis in a global sample would be interesting and allow for more generalizable findings."
  },
  {
    "objectID": "posts/EmmaNarkewicz_FinalProject_CheckIn2.html#works-cited",
    "href": "posts/EmmaNarkewicz_FinalProject_CheckIn2.html#works-cited",
    "title": "Final Project Check in 2",
    "section": "Works Cited",
    "text": "Works Cited\nCarrard, V., Bourquin, C., Berney, S, Schlegel, K., Gaume, J., Bart, P-A., Preisig M., Mast, M. A., & Berney, A. (2022). The relationship between medical students’ empathy, mental health, and burnout: A cross-sectional study, Medical Teacher, 44:12, 1392-1399, DOI: 10.1080/0142159X.2022.2098708\nDoulougeri, K., Georganta, K., & Montgomery, A. (2016). “Diagnosing” burnout among healthcare professionals: Can we find consensus?, Cogent Medicine, 3:1, DOI: 10.1080/2331205X.2016.1237605\nGradiski, I. P., Borovecki, A., Ćurković, M., San-Martín, M., Delgado Bolton, R. C., & Vivanco, L. (2022). Burnout in International Medical Students: Characterization of Professionalism and Loneliness as Predictive Factors of Burnout. International journal of environmental research and public health, 19(3), 1385. https://doi.org/10.3390/ijerph19031385\nGoebert., D., Thompson., D., Takeshita., J., Beach, C., Bryson, P., Ephgrave, K., Kent. A., Kunkel., M., Schechter., J., Tate., J. (2009). Depressive Symptoms in Medical Students and Residents: A Multischool Study. Academic Medicine 84(2):p 236-241, DOI: 10.1097/ACM.0b013e31819391bb\nHlavac, Marek (2022). stargazer: Well-Formatted Regression and Summary Statistics Tables. R package version 5.2.3. https://CRAN.R-project.org/package=stargazer\nJennings, M.L. Medical Student Burnout: Interdisciplinary Exploration and Analysis. J Med Humanit 30, 253–269 (2009). https://doi.org/10.1007/s10912-009-9093-5\nKużelewska,E. (2016).Language Policy in Switzerland. Studies in Logic, Grammar and Rhetoric,45(1) 125-140. https://doi.org/10.1515/slgr-2016-0020\nMaslach, C., Jackson, S.E., & Jackson, Leiter, M. P. (Eds.) (1996). Maslach Burnout Inventory manual (3rd ed.).\nPaturel, A. (2020). Healing the very youngest healers. American Association of Medical Colleges (AAMC). https://www.aamc.org/news-insights/healing-very-youngest-healers#:~:text=In%20a%20recent%20study%20%2C%209.4,as%20their%20same%2Dage%20peers.\nPeters E. (2018). Compassion fatigue in nursing: A concept analysis. Nursing forum, 53(4), 466–480. https://doi.org/10.1111/nuf.12274\nRadloff, L.S. (1977). The CES-D Scale: a self-report depression scale for research in the general population. Applied Psychological Measurement, 1:385-401.\nRodriguez, S. Y. S., Carlotta, M. S.. (2017). Predictors of Burnout Syndrome in psychologists. Estudos De Psicologia (campinas), 34(Estud. psicol. (Campinas), 2017 34(1)), 141–150. https://doi.org/10.1590/1982-02752017000100014\nShin, H. S., Park, H., & Lee, Y. M. (2022). The relationship between medical students’ empathy and burnout levels by gender and study years. Patient education and counseling, 105(2), 432–439. https://doi.org/10.1016/j.pec.2021.05.036\nTiesman, H., Weissman, D., Stone., D., Quinlan, K., & Chosewood, L. (2021). Suicide Prevention for Healthcare Workers. CDC. https://blogs.cdc.gov/niosh-science-blog/2021/09/17/suicide-prevention-hcw/\nWilliams, B., Beovich, B. Psychometric properties of the Jefferson Scale of Empathy: a COSMIN systematic review protocol. Syst Rev 8, 319 (2019). https://doi.org/10.1186/s13643-019-1240-0\nYahya, M. S., Abutiheen, A. A., & Al- Haidary, A. F. (2021). Burnout among medical students of the University of Kerbala and its correlates. Middle East Current Psychiatry, Ain Shams University, 28(1), 78. https://doi.org/10.1186/s43045-021-00152-2"
  },
  {
    "objectID": "posts/dacss603hw5_LauraCollazo.html",
    "href": "posts/dacss603hw5_LauraCollazo.html",
    "title": "DACSS 603 Homework 4",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(smss)\nlibrary(alr4)"
  },
  {
    "objectID": "posts/dacss603hw5_LauraCollazo.html#a",
    "href": "posts/dacss603hw5_LauraCollazo.html#a",
    "title": "DACSS 603 Homework 4",
    "section": "a",
    "text": "a\nUsing backward elimination, the first variable to be deleted would be beds as it has the highest p-value."
  },
  {
    "objectID": "posts/dacss603hw5_LauraCollazo.html#b",
    "href": "posts/dacss603hw5_LauraCollazo.html#b",
    "title": "DACSS 603 Homework 4",
    "section": "b",
    "text": "b\nUsing forward selection, the first variable to be added would be new as it has the lowest p-value."
  },
  {
    "objectID": "posts/dacss603hw5_LauraCollazo.html#c",
    "href": "posts/dacss603hw5_LauraCollazo.html#c",
    "title": "DACSS 603 Homework 4",
    "section": "c",
    "text": "c\nI think beds has a large p-value in the regression model, even though it’s highly correlated with price, because multicollinearity exists between beds and size."
  },
  {
    "objectID": "posts/dacss603hw5_LauraCollazo.html#d",
    "href": "posts/dacss603hw5_LauraCollazo.html#d",
    "title": "DACSS 603 Homework 4",
    "section": "d",
    "text": "d\nBelow are 2 different models: one with all variables and one with beds removed as it did not have a significant p-value in the original model.\n\n\nCode\nsummary(model_1)\n\n\n\nCall:\nlm(formula = P ~ S + Be + Ba + New, data = house.selling.price.2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-36.212  -9.546   1.277   9.406  71.953 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -41.795     12.104  -3.453 0.000855 ***\nS             64.761      5.630  11.504  &lt; 2e-16 ***\nBe            -2.766      3.960  -0.698 0.486763    \nBa            19.203      5.650   3.399 0.001019 ** \nNew           18.984      3.873   4.902  4.3e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 16.36 on 88 degrees of freedom\nMultiple R-squared:  0.8689,    Adjusted R-squared:  0.8629 \nF-statistic: 145.8 on 4 and 88 DF,  p-value: &lt; 2.2e-16\n\n\nCode\nsummary(model_2)\n\n\n\nCall:\nlm(formula = P ~ S + Ba + New, data = house.selling.price.2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-34.804  -9.496   0.917   7.931  73.338 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -47.992      8.209  -5.847 8.15e-08 ***\nS             62.263      4.335  14.363  &lt; 2e-16 ***\nBa            20.072      5.495   3.653 0.000438 ***\nNew           18.371      3.761   4.885 4.54e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 16.31 on 89 degrees of freedom\nMultiple R-squared:  0.8681,    Adjusted R-squared:  0.8637 \nF-statistic: 195.3 on 3 and 89 DF,  p-value: &lt; 2.2e-16\n\n\n\nR-squared\nIf using only the value for R-squared to determine the best fitting model I would select the model using all variables as it’s higher, albeit barely, than the model without beds as seen above.\n\n\nAdjusted R-squared\nIf using only the value for adjusted R-squared to determine the best fitting model I would select the model without beds as it’s slightly higher than the model with all variables as seen above.\n\n\nPRESS\nIf using only the PRESS statistic to determine the best fitting model, I would select the model without the variable beds. As seen below, P ~ S + Ba + New has the lowest PRESS statistic.\n\n\nCode\n# PRESS function\n\nPRESS &lt;- function(model) {\n    i &lt;- residuals(model)/(1 - lm.influence(model)$hat)\n    sum(i^2)\n}\n\n\nP ~ S + Be + Ba + New\n\n\nCode\nPRESS(model_1)\n\n\n[1] 28390.22\n\n\nP ~ S + Ba + New\n\n\nCode\nPRESS(model_2)\n\n\n[1] 27860.05\n\n\n\n\nAIC\nIf using only the value for AIC to determine the best fitting model, I would select the model without the variable beds. As seen in the summary below, P ~ S + Ba + New has the lowest AIC.\n\n\nCode\nstep(object = model_1, direction = \"backward\")\n\n\nStart:  AIC=524.7\nP ~ S + Be + Ba + New\n\n       Df Sum of Sq   RSS    AIC\n- Be    1       131 23684 523.21\n&lt;none&gt;              23553 524.70\n- Ba    1      3092 26645 534.17\n- New   1      6432 29985 545.15\n- S     1     35419 58972 608.06\n\nStep:  AIC=523.21\nP ~ S + Ba + New\n\n       Df Sum of Sq   RSS    AIC\n&lt;none&gt;              23684 523.21\n- Ba    1      3550 27234 534.20\n- New   1      6349 30033 543.30\n- S     1     54898 78582 632.75\n\n\n\nCall:\nlm(formula = P ~ S + Ba + New, data = house.selling.price.2)\n\nCoefficients:\n(Intercept)            S           Ba          New  \n     -47.99        62.26        20.07        18.37  \n\n\n\n\nBIC\nIf using only the value for BIC to determine the best fitting model, I would select the model without the variable beds. As seen below, P ~ S + Ba + New has the lowest BIC.\nP ~ S + Be + Ba + New\n\n\nCode\nBIC(model_1)\n\n\n[1] 805.8181\n\n\nP ~ S + Ba + New\n\n\nCode\nBIC(model_2)\n\n\n[1] 801.7996"
  },
  {
    "objectID": "posts/dacss603hw5_LauraCollazo.html#e",
    "href": "posts/dacss603hw5_LauraCollazo.html#e",
    "title": "DACSS 603 Homework 4",
    "section": "e",
    "text": "e\nThe model I would select as best fitting is the one without beds, P ~ S + Ba + New. This model is the best fitting using each criterion, except when looking at R-squared. R-squared isn’t the best measure to examine anyways, though, as it increases as predictor variables are added even if the variables aren’t significant. I also believe there is multicollinearity between size and beds, so removing beds addresses this issue."
  },
  {
    "objectID": "posts/dacss603hw5_LauraCollazo.html#a-1",
    "href": "posts/dacss603hw5_LauraCollazo.html#a-1",
    "title": "DACSS 603 Homework 4",
    "section": "a",
    "text": "a\nBelow is the regression model Volume ~ Girth + Height. It has a very high adjusted R-squared and all variables have a significant p-value. This indicates a good fitting model.\n\n\nCode\nmodel_2a &lt;- lm(data = trees, Volume ~ Girth + Height)\n\nsummary(model_2a)\n\n\n\nCall:\nlm(formula = Volume ~ Girth + Height, data = trees)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.4065 -2.6493 -0.2876  2.2003  8.4847 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -57.9877     8.6382  -6.713 2.75e-07 ***\nGirth         4.7082     0.2643  17.816  &lt; 2e-16 ***\nHeight        0.3393     0.1302   2.607   0.0145 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.882 on 28 degrees of freedom\nMultiple R-squared:  0.948, Adjusted R-squared:  0.9442 \nF-statistic:   255 on 2 and 28 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "posts/dacss603hw5_LauraCollazo.html#b-1",
    "href": "posts/dacss603hw5_LauraCollazo.html#b-1",
    "title": "DACSS 603 Homework 4",
    "section": "b",
    "text": "b\nDiagnostic plots for this model are below.\nIn the Residuals vs Fitted plot, both the linearity and constant variance assumptions are violated as the line is not linear and the points are not equally horizontal around the line at 0.\nIn the Normal Q-Q plot, most of the points fall on the line, however towards the ends they begin to curve. This violates the normality assumption.\nThe Scale-Location plot shows a violation of the constant variance assumption and also indicates heteroskedasticity.\nThe Residuals vs Leverage plot shows an outlier outside of Cooks distance which violates the influential observation assumption.\nAfter examining these plots, it appears the model is actually not a great fit as it violates all assumptions.\n\n\nCode\nplot(model_2a)"
  },
  {
    "objectID": "posts/dacss603hw5_LauraCollazo.html#a-2",
    "href": "posts/dacss603hw5_LauraCollazo.html#a-2",
    "title": "DACSS 603 Homework 4",
    "section": "a",
    "text": "a\nBelow is the regression summary and diagnostic plots for the model Buchanan ~ Bush. Based on the diagnostic plots, Palm Beach County is an outlier in every plot as the point for it on each graph is labeled and falls far from all the others.\n\n\nCode\ndata(\"florida\")\n\nmodel_3a &lt;- (lm(data = florida, Buchanan ~ Bush))\n\nsummary(model_3a)\n\n\n\nCall:\nlm(formula = Buchanan ~ Bush, data = florida)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-907.50  -46.10  -29.19   12.26 2610.19 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 4.529e+01  5.448e+01   0.831    0.409    \nBush        4.917e-03  7.644e-04   6.432 1.73e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 353.9 on 65 degrees of freedom\nMultiple R-squared:  0.3889,    Adjusted R-squared:  0.3795 \nF-statistic: 41.37 on 1 and 65 DF,  p-value: 1.727e-08\n\n\nCode\nplot(model_3a)"
  },
  {
    "objectID": "posts/dacss603hw5_LauraCollazo.html#b-2",
    "href": "posts/dacss603hw5_LauraCollazo.html#b-2",
    "title": "DACSS 603 Homework 4",
    "section": "b",
    "text": "b\nThe log of both variables are used in the next model and diagnostic plots. Although this is a better fitting model, Palm Beach County is still an outlier in every diagnostic plot.\n\n\nCode\ndata(\"florida\")\n\nmodel_3b &lt;- (lm(data = florida, log(Buchanan) ~ log(Bush)))\n\nsummary(model_3b)\n\n\n\nCall:\nlm(formula = log(Buchanan) ~ log(Bush), data = florida)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.96075 -0.25949  0.01282  0.23826  1.66564 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -2.57712    0.38919  -6.622 8.04e-09 ***\nlog(Bush)    0.75772    0.03936  19.251  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4673 on 65 degrees of freedom\nMultiple R-squared:  0.8508,    Adjusted R-squared:  0.8485 \nF-statistic: 370.6 on 1 and 65 DF,  p-value: &lt; 2.2e-16\n\n\nCode\nplot(model_3b)"
  },
  {
    "objectID": "posts/abigailbalint_finalpart1.html",
    "href": "posts/abigailbalint_finalpart1.html",
    "title": "Final Project Initial Research",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(dplyr)\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/abigailbalint_finalpart1.html#description-of-data",
    "href": "posts/abigailbalint_finalpart1.html#description-of-data",
    "title": "Final Project Initial Research",
    "section": "Description of data",
    "text": "Description of data\nThe dataset I am using comes from Kaggle https://www.kaggle.com/datasets/kaggle/kaggle-survey-2018 and is a survey titled “2018 Kaggle Machine Learning & Data Science Survey” conducted by Kaggle to capture the current state of machine learning and data science usage, mainly at the enterprise and academic level. The dataset contains survey responses from almost 24,000 respondents from varying backgrounds. The survey contains 50 questions, including 9 demographic questions and 41 questions around machine learning and data science. The questions range from platforms and products used, and tools and methodology, barriers to entry, and more. It also asks respondents about their employee experience working in these fields. I believe that the wide array of types of questions used make this dataset a good fit for research, as there are binary and categorical variables to explore but also some that ask for explicit numeric values like what percentage of their work falls to different tasks. Having several different types of questions provide opportunities for multiple types of models to be performed.\nThis survey was also run in 2017, 2019, and 2020 on Kaggle as part of an annual competition where users could submit code and analysis using this public data. However, I decided to use the 2018 dataset as my focus because certain questions that I think would be really interesting to analyze were omitted in later years/the survey was shortened overall. This survey was hosted by Kaggle, open to anyone in the industry, for one week in October 2018.\nReading in the dataset –\n\n\nCode\nfinal &lt;- read_csv(\"_data/final_project_data.csv\")\n\n\nWarning: One or more parsing issues, see `problems()` for details\n\n\nRows: 23859 Columns: 395\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (343): Q1, Q2, Q3, Q4, Q5, Q6, Q7, Q8, Q9, Q10, Q11_Part_1, Q11_Part_2, ...\ndbl  (48): Time from Start to Finish (seconds), Q1_OTHER_TEXT, Q6_OTHER_TEXT...\nlgl   (4): Q28_Part_24, Q30_Part_15, Q38_Part_19, Q38_Part_20\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nCode\nfinal2 &lt;- read_csv(\"_data/final_project_data2.csv\")\n\n\nWarning: One or more parsing issues, see `problems()` for details\n\n\nRows: 23859 Columns: 395\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (343): What is your gender? - Selected Choice, What is your age (# years...\ndbl  (48): Duration (in seconds), What is your gender? - Prefer to self-desc...\nlgl   (4): Which of the following machine learning products have you used at...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nCode\nhead(final2,10)\n\n\n# A tibble: 10 × 395\n   Duration (i…¹ What …² What …³ What …⁴ In wh…⁵ What …⁶ Which…⁷ Selec…⁸ Selec…⁹\n           &lt;dbl&gt; &lt;chr&gt;     &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;     &lt;dbl&gt;\n 1           710 Female       -1 45-49   United… Doctor… Other   Consul…      -1\n 2           434 Male         -1 30-34   Indone… Bachel… Engine… Other         0\n 3           718 Female       -1 30-34   United… Master… Comput… Data S…      -1\n 4           621 Male         -1 35-39   United… Master… Social… Not em…      -1\n 5           731 Male         -1 22-24   India   Master… Mathem… Data A…      -1\n 6          1142 Male         -1 25-29   Colomb… Bachel… Physic… Data S…      -1\n 7           959 Male         -1 35-39   Chile   Doctor… Inform… Other         1\n 8          1758 Male         -1 18-21   India   Master… Inform… Other         2\n 9           641 Male         -1 25-29   Turkey  Master… Engine… Not em…      -1\n10           751 Male         -1 30-34   Hungary Master… Engine… Softwa…      -1\n# … with 386 more variables:\n#   `In what industry is your current employer/contract (or your most recent employer if retired)? - Selected Choice` &lt;chr&gt;,\n#   `In what industry is your current employer/contract (or your most recent employer if retired)? - Other - Text` &lt;dbl&gt;,\n#   `How many years of experience do you have in your current role?` &lt;chr&gt;,\n#   `What is your current yearly compensation (approximate $USD)?` &lt;chr&gt;,\n#   `Does your current employer incorporate machine learning methods into their business?` &lt;chr&gt;,\n#   `Select any activities that make up an important part of your role at work: (Select all that apply) - Selected Choice - Analyze and understand data to influence product or business decisions` &lt;chr&gt;, …"
  },
  {
    "objectID": "posts/abigailbalint_finalpart1.html#research-question",
    "href": "posts/abigailbalint_finalpart1.html#research-question",
    "title": "Final Project Initial Research",
    "section": "Research Question",
    "text": "Research Question\nUpon doing a cursory search around this data, I see some high level executive-summary style research published about this data set, but I wasn’t able to find anything focused on more specific research questions. It was more demographic data of the state of ML and Data Science. I think there is the opportunity to speak more specifically about the state of machine learning and data science, and look deeper at what tools students and employees are using versus what their time is devoted to.\nTherefore, my main research question is “What is the state of machine learning and data science? What tools are being used in the context of individuals school and work, and how does an individual’s background (age, career, education, etc.) impact how they navigate this tech world? What barriers do users face and are those barriers the same for all users?”\nI plan to use questions like “During a typical data science project at work or school, approximately what proportion of your time is devoted to the following?” or “What percentage of your current machine learning/data science training falls under each category?” to get exact numbers that I can correlate against demos and more general usage of tools and platforms to see if there is any connection between the work one does and the tools they use.\nI am interested in this dataset because a lot of research in my career is in the machine learning space, so I am always interested in contextualizing the employee experience in these areas so that I can better understand the subject of some of my survey research. I also do more general employee engagement research in my career and I think this final is a great opportunity to try my hand at some of the correlations I would like to run at my job now but have never been able to because I don’t have any prior stats knowledge."
  },
  {
    "objectID": "posts/abigailbalint_finalpart1.html#hypothesis",
    "href": "posts/abigailbalint_finalpart1.html#hypothesis",
    "title": "Final Project Initial Research",
    "section": "Hypothesis",
    "text": "Hypothesis\nI would like to test a few different hypotheses that I have. Some of the initial ideas I have currently are:\n-Students are more likely to use free, long-standing coding and ML platforms as opposed to employees using more paid tools with user-friendly features.\n-For the question “How do you perceive the importance of the following topics? - Fairness and bias in ML algorithms, Being able to explain ML model outputs and/or predictions, Reproducibility in data science”, students will perceive this as more important than full-time workers, and younger generations will perceive this as more important than older generations.\n-For the question “During a typical data science project at work or school, approximately what proportion of your time is devoted to the following?” time spent on the analysis end of the process will be reported as a higher percentage of time the older or more experienced the data scientist is.\nThese are just a few ideas of the direction I am thinking, all of course cut by the demographics in this dataset like age, education, industry, years of experience, etc."
  },
  {
    "objectID": "posts/abigailbalint_finalpart1.html#descriptive-statistics",
    "href": "posts/abigailbalint_finalpart1.html#descriptive-statistics",
    "title": "Final Project Initial Research",
    "section": "Descriptive Statistics",
    "text": "Descriptive Statistics\nI described my dataset at the top of this as well as discussed variables of interest in the Research Question section, but here is a little bit of exploratory code:\nI can see the data contains mostly younger males, but because of the sample size can really work with lots of demographic combinations.\n\n\nCode\nggplot(final, aes(x = Q1)) +\n  geom_bar() +\n   labs(x=\"Gender\")\n\n\n\n\n\n\n\nCode\nggplot(final, aes(x = Q2)) +\n  geom_bar() +\n  labs(x=\"Age\") +\n  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust=1))\n\n\n\n\n\nThere is also a range of coding experience in the dataset.\n\n\nCode\nggplot(final, aes(x = Q24)) +\n  geom_bar() +\n   labs(x=\"Years of coding experience\") +\n  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust=1))\n\n\n\n\n\nThe data is split between students, tech industry employees, and other industry employees.\n\n\nCode\nggplot(final, aes(x = Q7)) +\n  geom_bar() +\n   labs(x=\"Industry\") +\n  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust=1))\n\n\n\n\n\nThis is just a quick example of the types of thinking I want to do for my final poster. Here, I made a scatterplot showing where the amount of training from work meets the amount of time spent finding insights instead of cleaning data, coding, etc. I expected this to be much higher for those who received most or all of their training from work, but that isn’t the case. I’m interested to see once I define a few more specific hypotheses if they end up being true or false.\n\n\nCode\nggplot(final, aes(x = Q35_Part_3, y=Q34_Part_6)) +\n  geom_point() +\n   labs(x=\"Percentage of machine learning training from work\", y=\"Percentage of project time spent finding insights\")\n\n\nWarning: Removed 8114 rows containing missing values (geom_point).\n\n\n\n\n\nHere is the glimpse function to show essentially the questionaiire in text form. I’m working with the variable codes in my coding and using a key as the codes are much shorter.\n\n\nCode\nglimpse(final2)\n\n\nRows: 23,859\nColumns: 395\n$ `Duration (in seconds)`                                                                                                                                                                                                                     &lt;dbl&gt; …\n$ `What is your gender? - Selected Choice`                                                                                                                                                                                                    &lt;chr&gt; …\n$ `What is your gender? - Prefer to self-describe - Text`                                                                                                                                                                                     &lt;dbl&gt; …\n$ `What is your age (# years)?`                                                                                                                                                                                                               &lt;chr&gt; …\n$ `In which country do you currently reside?`                                                                                                                                                                                                 &lt;chr&gt; …\n$ `What is the highest level of formal education that you have attained or plan to attain within the next 2 years?`                                                                                                                           &lt;chr&gt; …\n$ `Which best describes your undergraduate major? - Selected Choice`                                                                                                                                                                          &lt;chr&gt; …\n$ `Select the title most similar to your current role (or most recent title if retired): - Selected Choice`                                                                                                                                   &lt;chr&gt; …\n$ `Select the title most similar to your current role (or most recent title if retired): - Other - Text`                                                                                                                                      &lt;dbl&gt; …\n$ `In what industry is your current employer/contract (or your most recent employer if retired)? - Selected Choice`                                                                                                                           &lt;chr&gt; …\n$ `In what industry is your current employer/contract (or your most recent employer if retired)? - Other - Text`                                                                                                                              &lt;dbl&gt; …\n$ `How many years of experience do you have in your current role?`                                                                                                                                                                            &lt;chr&gt; …\n$ `What is your current yearly compensation (approximate $USD)?`                                                                                                                                                                              &lt;chr&gt; …\n$ `Does your current employer incorporate machine learning methods into their business?`                                                                                                                                                      &lt;chr&gt; …\n$ `Select any activities that make up an important part of your role at work: (Select all that apply) - Selected Choice - Analyze and understand data to influence product or business decisions`                                             &lt;chr&gt; …\n$ `Select any activities that make up an important part of your role at work: (Select all that apply) - Selected Choice - Build and/or run a machine learning service that operationally improves my product or workflows`                    &lt;chr&gt; …\n$ `Select any activities that make up an important part of your role at work: (Select all that apply) - Selected Choice - Build and/or run the data infrastructure that my business uses for storing, analyzing, and operationalizing data`   &lt;chr&gt; …\n$ `Select any activities that make up an important part of your role at work: (Select all that apply) - Selected Choice - Build prototypes to explore applying machine learning to new areas`                                                 &lt;chr&gt; …\n$ `Select any activities that make up an important part of your role at work: (Select all that apply) - Selected Choice - Do research that advances the state of the art of machine learning`                                                 &lt;chr&gt; …\n$ `Select any activities that make up an important part of your role at work: (Select all that apply) - Selected Choice - None of these activities are an important part of my role at work`                                                  &lt;chr&gt; …\n$ `Select any activities that make up an important part of your role at work: (Select all that apply) - Selected Choice - Other`                                                                                                              &lt;chr&gt; …\n$ `Select any activities that make up an important part of your role at work: (Select all that apply) - Other - Text`                                                                                                                         &lt;dbl&gt; …\n$ `What is the primary tool that you use at work or school to analyze data? (include text response) - Selected Choice`                                                                                                                        &lt;chr&gt; …\n$ `What is the primary tool that you use at work or school to analyze data? (include text response) - Basic statistical software (Microsoft Excel, Google Sheets, etc.) - Text`                                                               &lt;dbl&gt; …\n$ `What is the primary tool that you use at work or school to analyze data? (include text response) - Advanced statistical software (SPSS, SAS, etc.) - Text`                                                                                 &lt;dbl&gt; …\n$ `What is the primary tool that you use at work or school to analyze data? (include text response) - Business intelligence software (Salesforce, Tableau, Spotfire, etc.) - Text`                                                            &lt;dbl&gt; …\n$ `What is the primary tool that you use at work or school to analyze data? (include text response) - Local or hosted development environments (RStudio, JupyterLab, etc.) - Text`                                                            &lt;dbl&gt; …\n$ `What is the primary tool that you use at work or school to analyze data? (include text response) - Cloud-based data software & APIs (AWS, GCP, Azure, etc.) - Text`                                                                        &lt;dbl&gt; …\n$ `What is the primary tool that you use at work or school to analyze data? (include text response) - Other - Text`                                                                                                                           &lt;dbl&gt; …\n$ `Which of the following integrated development environments (IDE's) have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - Jupyter/IPython`                                                       &lt;chr&gt; …\n$ `Which of the following integrated development environments (IDE's) have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - RStudio`                                                               &lt;chr&gt; …\n$ `Which of the following integrated development environments (IDE's) have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - PyCharm`                                                               &lt;chr&gt; …\n$ `Which of the following integrated development environments (IDE's) have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - Visual Studio Code`                                                    &lt;chr&gt; …\n$ `Which of the following integrated development environments (IDE's) have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - nteract`                                                               &lt;chr&gt; …\n$ `Which of the following integrated development environments (IDE's) have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - Atom`                                                                  &lt;chr&gt; …\n$ `Which of the following integrated development environments (IDE's) have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - MATLAB`                                                                &lt;chr&gt; …\n$ `Which of the following integrated development environments (IDE's) have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - Visual Studio`                                                         &lt;chr&gt; …\n$ `Which of the following integrated development environments (IDE's) have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - Notepad++`                                                             &lt;chr&gt; …\n$ `Which of the following integrated development environments (IDE's) have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - Sublime Text`                                                          &lt;chr&gt; …\n$ `Which of the following integrated development environments (IDE's) have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - Vim`                                                                   &lt;chr&gt; …\n$ `Which of the following integrated development environments (IDE's) have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - IntelliJ`                                                              &lt;chr&gt; …\n$ `Which of the following integrated development environments (IDE's) have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - Spyder`                                                                &lt;chr&gt; …\n$ `Which of the following integrated development environments (IDE's) have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - None`                                                                  &lt;chr&gt; …\n$ `Which of the following integrated development environments (IDE's) have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - Other`                                                                 &lt;chr&gt; …\n$ `Which of the following integrated development environments (IDE's) have you used at work or school in the last 5 years? (Select all that apply) - Other - Text`                                                                            &lt;dbl&gt; …\n$ `Which of the following hosted notebooks have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - Kaggle Kernels`                                                                                   &lt;chr&gt; …\n$ `Which of the following hosted notebooks have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - Google Colab`                                                                                     &lt;chr&gt; …\n$ `Which of the following hosted notebooks have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - Azure Notebook`                                                                                   &lt;chr&gt; …\n$ `Which of the following hosted notebooks have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - Domino Datalab`                                                                                   &lt;chr&gt; …\n$ `Which of the following hosted notebooks have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - Google Cloud Datalab`                                                                             &lt;chr&gt; …\n$ `Which of the following hosted notebooks have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - Paperspace`                                                                                       &lt;chr&gt; …\n$ `Which of the following hosted notebooks have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - Floydhub`                                                                                         &lt;chr&gt; …\n$ `Which of the following hosted notebooks have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - Crestle`                                                                                          &lt;chr&gt; …\n$ `Which of the following hosted notebooks have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - JupyterHub/Binder`                                                                                &lt;chr&gt; …\n$ `Which of the following hosted notebooks have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - None`                                                                                             &lt;chr&gt; …\n$ `Which of the following hosted notebooks have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - Other`                                                                                            &lt;chr&gt; …\n$ `Which of the following hosted notebooks have you used at work or school in the last 5 years? (Select all that apply) - Other - Text`                                                                                                       &lt;dbl&gt; …\n$ `Which of the following cloud computing services have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - Google Cloud Platform (GCP)`                                                              &lt;chr&gt; …\n$ `Which of the following cloud computing services have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - Amazon Web Services (AWS)`                                                                &lt;chr&gt; …\n$ `Which of the following cloud computing services have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - Microsoft Azure`                                                                          &lt;chr&gt; …\n$ `Which of the following cloud computing services have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - IBM Cloud`                                                                                &lt;chr&gt; …\n$ `Which of the following cloud computing services have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - Alibaba Cloud`                                                                            &lt;chr&gt; …\n$ `Which of the following cloud computing services have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - I have not used any cloud providers`                                                      &lt;chr&gt; …\n$ `Which of the following cloud computing services have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - Other`                                                                                    &lt;chr&gt; …\n$ `Which of the following cloud computing services have you used at work or school in the last 5 years? (Select all that apply) - Other - Text`                                                                                               &lt;dbl&gt; …\n$ `What programming languages do you use on a regular basis? (Select all that apply) - Selected Choice - Python`                                                                                                                              &lt;chr&gt; …\n$ `What programming languages do you use on a regular basis? (Select all that apply) - Selected Choice - R`                                                                                                                                   &lt;chr&gt; …\n$ `What programming languages do you use on a regular basis? (Select all that apply) - Selected Choice - SQL`                                                                                                                                 &lt;chr&gt; …\n$ `What programming languages do you use on a regular basis? (Select all that apply) - Selected Choice - Bash`                                                                                                                                &lt;chr&gt; …\n$ `What programming languages do you use on a regular basis? (Select all that apply) - Selected Choice - Java`                                                                                                                                &lt;chr&gt; …\n$ `What programming languages do you use on a regular basis? (Select all that apply) - Selected Choice - Javascript/Typescript`                                                                                                               &lt;chr&gt; …\n$ `What programming languages do you use on a regular basis? (Select all that apply) - Selected Choice - Visual Basic/VBA`                                                                                                                    &lt;chr&gt; …\n$ `What programming languages do you use on a regular basis? (Select all that apply) - Selected Choice - C/C++`                                                                                                                               &lt;chr&gt; …\n$ `What programming languages do you use on a regular basis? (Select all that apply) - Selected Choice - MATLAB`                                                                                                                              &lt;chr&gt; …\n$ `What programming languages do you use on a regular basis? (Select all that apply) - Selected Choice - Scala`                                                                                                                               &lt;chr&gt; …\n$ `What programming languages do you use on a regular basis? (Select all that apply) - Selected Choice - Julia`                                                                                                                               &lt;chr&gt; …\n$ `What programming languages do you use on a regular basis? (Select all that apply) - Selected Choice - Go`                                                                                                                                  &lt;chr&gt; …\n$ `What programming languages do you use on a regular basis? (Select all that apply) - Selected Choice - C#/.NET`                                                                                                                             &lt;chr&gt; …\n$ `What programming languages do you use on a regular basis? (Select all that apply) - Selected Choice - PHP`                                                                                                                                 &lt;chr&gt; …\n$ `What programming languages do you use on a regular basis? (Select all that apply) - Selected Choice - Ruby`                                                                                                                                &lt;chr&gt; …\n$ `What programming languages do you use on a regular basis? (Select all that apply) - Selected Choice - SAS/STATA`                                                                                                                           &lt;chr&gt; …\n$ `What programming languages do you use on a regular basis? (Select all that apply) - Selected Choice - None`                                                                                                                                &lt;chr&gt; …\n$ `What programming languages do you use on a regular basis? (Select all that apply) - Selected Choice - Other`                                                                                                                               &lt;chr&gt; …\n$ `What programming languages do you use on a regular basis? (Select all that apply) - Other - Text`                                                                                                                                          &lt;dbl&gt; …\n$ `What specific programming language do you use most often? - Selected Choice`                                                                                                                                                               &lt;chr&gt; …\n$ `What specific programming language do you use most often? - Other - Text`                                                                                                                                                                  &lt;dbl&gt; …\n$ `What programming language would you recommend an aspiring data scientist to learn first? - Selected Choice`                                                                                                                                &lt;chr&gt; …\n$ `What programming language would you recommend an aspiring data scientist to learn first? - Other - Text`                                                                                                                                   &lt;dbl&gt; …\n$ `What machine learning frameworks have you used in the past 5 years? (Select all that apply) - Selected Choice - Scikit-Learn`                                                                                                              &lt;chr&gt; …\n$ `What machine learning frameworks have you used in the past 5 years? (Select all that apply) - Selected Choice - TensorFlow`                                                                                                                &lt;chr&gt; …\n$ `What machine learning frameworks have you used in the past 5 years? (Select all that apply) - Selected Choice - Keras`                                                                                                                     &lt;chr&gt; …\n$ `What machine learning frameworks have you used in the past 5 years? (Select all that apply) - Selected Choice - PyTorch`                                                                                                                   &lt;chr&gt; …\n$ `What machine learning frameworks have you used in the past 5 years? (Select all that apply) - Selected Choice - Spark MLlib`                                                                                                               &lt;chr&gt; …\n$ `What machine learning frameworks have you used in the past 5 years? (Select all that apply) - Selected Choice - H20`                                                                                                                       &lt;chr&gt; …\n$ `What machine learning frameworks have you used in the past 5 years? (Select all that apply) - Selected Choice - Fastai`                                                                                                                    &lt;chr&gt; …\n$ `What machine learning frameworks have you used in the past 5 years? (Select all that apply) - Selected Choice - Mxnet`                                                                                                                     &lt;chr&gt; …\n$ `What machine learning frameworks have you used in the past 5 years? (Select all that apply) - Selected Choice - Caret`                                                                                                                     &lt;chr&gt; …\n$ `What machine learning frameworks have you used in the past 5 years? (Select all that apply) - Selected Choice - Xgboost`                                                                                                                   &lt;chr&gt; …\n$ `What machine learning frameworks have you used in the past 5 years? (Select all that apply) - Selected Choice - mlr`                                                                                                                       &lt;chr&gt; …\n$ `What machine learning frameworks have you used in the past 5 years? (Select all that apply) - Selected Choice - Prophet`                                                                                                                   &lt;chr&gt; …\n$ `What machine learning frameworks have you used in the past 5 years? (Select all that apply) - Selected Choice - randomForest`                                                                                                              &lt;chr&gt; …\n$ `What machine learning frameworks have you used in the past 5 years? (Select all that apply) - Selected Choice - lightgbm`                                                                                                                  &lt;chr&gt; …\n$ `What machine learning frameworks have you used in the past 5 years? (Select all that apply) - Selected Choice - catboost`                                                                                                                  &lt;chr&gt; …\n$ `What machine learning frameworks have you used in the past 5 years? (Select all that apply) - Selected Choice - CNTK`                                                                                                                      &lt;chr&gt; …\n$ `What machine learning frameworks have you used in the past 5 years? (Select all that apply) - Selected Choice - Caffe`                                                                                                                     &lt;chr&gt; …\n$ `What machine learning frameworks have you used in the past 5 years? (Select all that apply) - Selected Choice - None`                                                                                                                      &lt;chr&gt; …\n$ `What machine learning frameworks have you used in the past 5 years? (Select all that apply) - Selected Choice - Other`                                                                                                                     &lt;chr&gt; …\n$ `What machine learning frameworks have you used in the past 5 years? (Select all that apply) - Other - Text`                                                                                                                                &lt;dbl&gt; …\n$ `Of the choices that you selected in the previous question, which ML library have you used the most? - Selected Choice`                                                                                                                     &lt;chr&gt; …\n$ `Of the choices that you selected in the previous question, which ML library have you used the most? - Other - Text`                                                                                                                        &lt;dbl&gt; …\n$ `What data visualization libraries or tools have you used in the past 5 years? (Select all that apply) - Selected Choice - ggplot2`                                                                                                         &lt;chr&gt; …\n$ `What data visualization libraries or tools have you used in the past 5 years? (Select all that apply) - Selected Choice - Matplotlib`                                                                                                      &lt;chr&gt; …\n$ `What data visualization libraries or tools have you used in the past 5 years? (Select all that apply) - Selected Choice - Altair`                                                                                                          &lt;chr&gt; …\n$ `What data visualization libraries or tools have you used in the past 5 years? (Select all that apply) - Selected Choice - Shiny`                                                                                                           &lt;chr&gt; …\n$ `What data visualization libraries or tools have you used in the past 5 years? (Select all that apply) - Selected Choice - D3`                                                                                                              &lt;chr&gt; …\n$ `What data visualization libraries or tools have you used in the past 5 years? (Select all that apply) - Selected Choice - Plotly`                                                                                                          &lt;chr&gt; …\n$ `What data visualization libraries or tools have you used in the past 5 years? (Select all that apply) - Selected Choice - Bokeh`                                                                                                           &lt;chr&gt; …\n$ `What data visualization libraries or tools have you used in the past 5 years? (Select all that apply) - Selected Choice - Seaborn`                                                                                                         &lt;chr&gt; …\n$ `What data visualization libraries or tools have you used in the past 5 years? (Select all that apply) - Selected Choice - Geoplotlib`                                                                                                      &lt;chr&gt; …\n$ `What data visualization libraries or tools have you used in the past 5 years? (Select all that apply) - Selected Choice - Leaflet`                                                                                                         &lt;chr&gt; …\n$ `What data visualization libraries or tools have you used in the past 5 years? (Select all that apply) - Selected Choice - Lattice`                                                                                                         &lt;chr&gt; …\n$ `What data visualization libraries or tools have you used in the past 5 years? (Select all that apply) - Selected Choice - None`                                                                                                            &lt;chr&gt; …\n$ `What data visualization libraries or tools have you used in the past 5 years? (Select all that apply) - Selected Choice - Other`                                                                                                           &lt;chr&gt; …\n$ `What data visualization libraries or tools have you used in the past 5 years? (Select all that apply) - Other - Text`                                                                                                                      &lt;dbl&gt; …\n$ `Of the choices that you selected in the previous question, which specific data visualization library or tool have you used the most? - Selected Choice`                                                                                    &lt;chr&gt; …\n$ `Of the choices that you selected in the previous question, which specific data visualization library or tool have you used the most? - Other - Text`                                                                                       &lt;dbl&gt; …\n$ `Approximately what percent of your time at work or school is spent actively coding?`                                                                                                                                                       &lt;chr&gt; …\n$ `How long have you been writing code to analyze data?`                                                                                                                                                                                      &lt;chr&gt; …\n$ `For how many years have you used machine learning methods (at work or in school)?`                                                                                                                                                         &lt;chr&gt; …\n$ `Do you consider yourself to be a data scientist?`                                                                                                                                                                                          &lt;chr&gt; …\n$ `Which of the following cloud computing products have you used at work or school in the last 5 years (Select all that apply)? - Selected Choice - AWS Elastic Compute Cloud (EC2)`                                                          &lt;chr&gt; …\n$ `Which of the following cloud computing products have you used at work or school in the last 5 years (Select all that apply)? - Selected Choice - Google Compute Engine`                                                                    &lt;chr&gt; …\n$ `Which of the following cloud computing products have you used at work or school in the last 5 years (Select all that apply)? - Selected Choice - AWS Elastic Beanstalk`                                                                    &lt;chr&gt; …\n$ `Which of the following cloud computing products have you used at work or school in the last 5 years (Select all that apply)? - Selected Choice - Google App Engine`                                                                        &lt;chr&gt; …\n$ `Which of the following cloud computing products have you used at work or school in the last 5 years (Select all that apply)? - Selected Choice - Google Kubernetes Engine`                                                                 &lt;chr&gt; …\n$ `Which of the following cloud computing products have you used at work or school in the last 5 years (Select all that apply)? - Selected Choice - AWS Lambda`                                                                               &lt;chr&gt; …\n$ `Which of the following cloud computing products have you used at work or school in the last 5 years (Select all that apply)? - Selected Choice - Google Cloud Functions`                                                                   &lt;chr&gt; …\n$ `Which of the following cloud computing products have you used at work or school in the last 5 years (Select all that apply)? - Selected Choice - AWS Batch`                                                                                &lt;chr&gt; …\n$ `Which of the following cloud computing products have you used at work or school in the last 5 years (Select all that apply)? - Selected Choice - Azure Virtual Machines`                                                                   &lt;chr&gt; …\n$ `Which of the following cloud computing products have you used at work or school in the last 5 years (Select all that apply)? - Selected Choice - Azure Container Service`                                                                  &lt;chr&gt; …\n$ `Which of the following cloud computing products have you used at work or school in the last 5 years (Select all that apply)? - Selected Choice - Azure Functions`                                                                          &lt;chr&gt; …\n$ `Which of the following cloud computing products have you used at work or school in the last 5 years (Select all that apply)? - Selected Choice - Azure Event Grid`                                                                         &lt;chr&gt; …\n$ `Which of the following cloud computing products have you used at work or school in the last 5 years (Select all that apply)? - Selected Choice - Azure Batch`                                                                              &lt;chr&gt; …\n$ `Which of the following cloud computing products have you used at work or school in the last 5 years (Select all that apply)? - Selected Choice - Azure Kubernetes Service`                                                                 &lt;chr&gt; …\n$ `Which of the following cloud computing products have you used at work or school in the last 5 years (Select all that apply)? - Selected Choice - IBM Cloud Virtual Servers`                                                                &lt;chr&gt; …\n$ `Which of the following cloud computing products have you used at work or school in the last 5 years (Select all that apply)? - Selected Choice - IBM Cloud Container Registry`                                                             &lt;chr&gt; …\n$ `Which of the following cloud computing products have you used at work or school in the last 5 years (Select all that apply)? - Selected Choice - IBM Cloud Kubernetes Service`                                                             &lt;chr&gt; …\n$ `Which of the following cloud computing products have you used at work or school in the last 5 years (Select all that apply)? - Selected Choice - IBM Cloud Foundry`                                                                        &lt;chr&gt; …\n$ `Which of the following cloud computing products have you used at work or school in the last 5 years (Select all that apply)? - Selected Choice - None`                                                                                     &lt;chr&gt; …\n$ `Which of the following cloud computing products have you used at work or school in the last 5 years (Select all that apply)? - Selected Choice - Other`                                                                                    &lt;chr&gt; …\n$ `Which of the following cloud computing products have you used at work or school in the last 5 years (Select all that apply)? - Other - Text`                                                                                               &lt;dbl&gt; …\n$ `Which of the following machine learning products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - Amazon Transcribe`                                                                       &lt;chr&gt; …\n$ `Which of the following machine learning products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - Google Cloud Speech-to-text API`                                                         &lt;chr&gt; …\n$ `Which of the following machine learning products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - Amazon Rekognition`                                                                      &lt;chr&gt; …\n$ `Which of the following machine learning products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - Google Cloud Vision API`                                                                 &lt;chr&gt; …\n$ `Which of the following machine learning products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - Amazon Comprehend`                                                                       &lt;chr&gt; …\n$ `Which of the following machine learning products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - Google Cloud Natural Language API`                                                       &lt;chr&gt; …\n$ `Which of the following machine learning products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - Amazon Translate`                                                                        &lt;chr&gt; …\n$ `Which of the following machine learning products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - Google Cloud Translation API`                                                            &lt;chr&gt; …\n$ `Which of the following machine learning products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - Amazon Lex`                                                                              &lt;chr&gt; …\n$ `Which of the following machine learning products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - Google Dialogflow Enterprise Edition`                                                    &lt;chr&gt; …\n$ `Which of the following machine learning products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - Amazon Rekognition Video`                                                                &lt;chr&gt; …\n$ `Which of the following machine learning products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - Google Cloud Video Intelligence API`                                                     &lt;chr&gt; …\n$ `Which of the following machine learning products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - Google Cloud AutoML`                                                                     &lt;chr&gt; …\n$ `Which of the following machine learning products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - Amazon SageMaker`                                                                        &lt;chr&gt; …\n$ `Which of the following machine learning products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - Google Cloud Machine Learning Engine`                                                    &lt;chr&gt; …\n$ `Which of the following machine learning products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - DataRobot`                                                                               &lt;chr&gt; …\n$ `Which of the following machine learning products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - H20 Driverless AI`                                                                       &lt;chr&gt; …\n$ `Which of the following machine learning products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - Domino Datalab`                                                                          &lt;chr&gt; …\n$ `Which of the following machine learning products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - SAS`                                                                                     &lt;chr&gt; …\n$ `Which of the following machine learning products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - Dataiku`                                                                                 &lt;chr&gt; …\n$ `Which of the following machine learning products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - RapidMiner`                                                                              &lt;chr&gt; …\n$ `Which of the following machine learning products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - Instabase`                                                                               &lt;chr&gt; …\n$ `Which of the following machine learning products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - Algorithmia`                                                                             &lt;chr&gt; …\n$ `Which of the following machine learning products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - Dataversity`                                                                             &lt;lgl&gt; …\n$ `Which of the following machine learning products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - Cloudera`                                                                                &lt;chr&gt; …\n$ `Which of the following machine learning products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - Azure Machine Learning Studio`                                                           &lt;chr&gt; …\n$ `Which of the following machine learning products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - Azure Machine Learning Workbench`                                                        &lt;chr&gt; …\n$ `Which of the following machine learning products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - Azure Cortana Intelligence Suite`                                                        &lt;chr&gt; …\n$ `Which of the following machine learning products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - Azure Bing Speech API`                                                                   &lt;chr&gt; …\n$ `Which of the following machine learning products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - Azure Speaker Recognition API`                                                           &lt;chr&gt; …\n$ `Which of the following machine learning products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - Azure Computer Vision API`                                                               &lt;chr&gt; …\n$ `Which of the following machine learning products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - Azure Face API`                                                                          &lt;chr&gt; …\n$ `Which of the following machine learning products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - Azure Video API`                                                                         &lt;chr&gt; …\n$ `Which of the following machine learning products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - IBM Watson Studio`                                                                       &lt;chr&gt; …\n$ `Which of the following machine learning products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - IBM Watson Knowledge Catalog`                                                            &lt;chr&gt; …\n$ `Which of the following machine learning products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - IBM Watson Assistant`                                                                    &lt;chr&gt; …\n$ `Which of the following machine learning products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - IBM Watson Discovery`                                                                    &lt;chr&gt; …\n$ `Which of the following machine learning products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - IBM Watson Text to Speech`                                                               &lt;chr&gt; …\n$ `Which of the following machine learning products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - IBM Watson Visual Recognition`                                                           &lt;chr&gt; …\n$ `Which of the following machine learning products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - IBM Watson Machine Learning`                                                             &lt;chr&gt; …\n$ `Which of the following machine learning products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - Azure Cognitive Services`                                                                &lt;chr&gt; …\n$ `Which of the following machine learning products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - None`                                                                                    &lt;chr&gt; …\n$ `Which of the following machine learning products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - Other`                                                                                   &lt;chr&gt; …\n$ `Which of the following machine learning products have you used at work or school in the last 5 years? (Select all that apply) - Other - Text`                                                                                              &lt;dbl&gt; …\n$ `Which of the following relational database products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - AWS Relational Database Service`                                                      &lt;chr&gt; …\n$ `Which of the following relational database products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - AWS Aurora`                                                                           &lt;chr&gt; …\n$ `Which of the following relational database products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - Google Cloud SQL`                                                                     &lt;chr&gt; …\n$ `Which of the following relational database products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - Google Cloud Spanner`                                                                 &lt;chr&gt; …\n$ `Which of the following relational database products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - AWS DynamoDB`                                                                         &lt;chr&gt; …\n$ `Which of the following relational database products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - Google Cloud Datastore`                                                               &lt;chr&gt; …\n$ `Which of the following relational database products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - Google Cloud Bigtable`                                                                &lt;chr&gt; …\n$ `Which of the following relational database products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - AWS SimpleDB`                                                                         &lt;chr&gt; …\n$ `Which of the following relational database products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - Microsoft SQL Server`                                                                 &lt;chr&gt; …\n$ `Which of the following relational database products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - MySQL`                                                                                &lt;chr&gt; …\n$ `Which of the following relational database products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - PostgresSQL`                                                                          &lt;chr&gt; …\n$ `Which of the following relational database products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - SQLite`                                                                               &lt;chr&gt; …\n$ `Which of the following relational database products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - Oracle Database`                                                                      &lt;chr&gt; …\n$ `Which of the following relational database products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - Ingres`                                                                               &lt;chr&gt; …\n$ `Which of the following relational database products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - Microsoft Access`                                                                     &lt;chr&gt; …\n$ `Which of the following relational database products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - NexusDB`                                                                              &lt;chr&gt; …\n$ `Which of the following relational database products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - SAP IQ`                                                                               &lt;chr&gt; …\n$ `Which of the following relational database products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - Google Fusion Tables`                                                                 &lt;chr&gt; …\n$ `Which of the following relational database products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - Azure Database for MySQL`                                                             &lt;chr&gt; …\n$ `Which of the following relational database products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - Azure Cosmos DB`                                                                      &lt;chr&gt; …\n$ `Which of the following relational database products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - Azure SQL Database`                                                                   &lt;chr&gt; …\n$ `Which of the following relational database products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - Azure Database for PostgreSQL`                                                        &lt;chr&gt; …\n$ `Which of the following relational database products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - IBM Cloud Compose`                                                                    &lt;chr&gt; …\n$ `Which of the following relational database products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - IBM Cloud Compose for MySQL`                                                          &lt;chr&gt; …\n$ `Which of the following relational database products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - IBM Cloud Compose for PostgreSQL`                                                     &lt;chr&gt; …\n$ `Which of the following relational database products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - IBM Cloud Db2`                                                                        &lt;chr&gt; …\n$ `Which of the following relational database products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - None`                                                                                 &lt;chr&gt; …\n$ `Which of the following relational database products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - Other`                                                                                &lt;chr&gt; …\n$ `Which of the following relational database products have you used at work or school in the last 5 years? (Select all that apply) - Other - Text`                                                                                           &lt;dbl&gt; …\n$ `Which of the following big data and analytics products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - AWS Elastic MapReduce`                                                             &lt;chr&gt; …\n$ `Which of the following big data and analytics products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - AWS Batch`                                                                         &lt;chr&gt; …\n$ `Which of the following big data and analytics products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - Google Cloud Dataproc`                                                             &lt;chr&gt; …\n$ `Which of the following big data and analytics products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - Google Cloud Dataflow`                                                             &lt;chr&gt; …\n$ `Which of the following big data and analytics products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - Google Cloud Dataprep`                                                             &lt;chr&gt; …\n$ `Which of the following big data and analytics products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - AWS Kinesis`                                                                       &lt;chr&gt; …\n$ `Which of the following big data and analytics products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - Google Cloud Pub/Sub`                                                              &lt;chr&gt; …\n$ `Which of the following big data and analytics products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - AWS Athena`                                                                        &lt;chr&gt; …\n$ `Which of the following big data and analytics products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - AWS Redshift`                                                                      &lt;chr&gt; …\n$ `Which of the following big data and analytics products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - Google BigQuery`                                                                   &lt;chr&gt; …\n$ `Which of the following big data and analytics products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - Teradata`                                                                          &lt;chr&gt; …\n$ `Which of the following big data and analytics products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - Microsoft Analysis Services`                                                       &lt;chr&gt; …\n$ `Which of the following big data and analytics products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - Oracle Exadata`                                                                    &lt;chr&gt; …\n$ `Which of the following big data and analytics products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - Oracle Warehouse Builder`                                                          &lt;chr&gt; …\n$ `Which of the following big data and analytics products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - SAP IQ`                                                                            &lt;lgl&gt; …\n$ `Which of the following big data and analytics products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - Snowflake`                                                                         &lt;chr&gt; …\n$ `Which of the following big data and analytics products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - Databricks`                                                                        &lt;chr&gt; …\n$ `Which of the following big data and analytics products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - Azure SQL Data Warehouse`                                                          &lt;chr&gt; …\n$ `Which of the following big data and analytics products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - Azure HDInsight`                                                                   &lt;chr&gt; …\n$ `Which of the following big data and analytics products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - Azure Stream Analytics`                                                            &lt;chr&gt; …\n$ `Which of the following big data and analytics products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - IBM InfoSphere DataStorage`                                                        &lt;chr&gt; …\n$ `Which of the following big data and analytics products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - IBM Cloud Analytics Engine`                                                        &lt;chr&gt; …\n$ `Which of the following big data and analytics products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - IBM Cloud Streaming Analytics`                                                     &lt;chr&gt; …\n$ `Which of the following big data and analytics products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - None`                                                                              &lt;chr&gt; …\n$ `Which of the following big data and analytics products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - Other`                                                                             &lt;chr&gt; …\n$ `Which of the following big data and analytics products have you used at work or school in the last 5 years? (Select all that apply) - Other - Text`                                                                                        &lt;dbl&gt; …\n$ `Which types of data do you currently interact with most often at work or school? (Select all that apply) - Selected Choice - Audio Data`                                                                                                   &lt;chr&gt; …\n$ `Which types of data do you currently interact with most often at work or school? (Select all that apply) - Selected Choice - Categorical Data`                                                                                             &lt;chr&gt; …\n$ `Which types of data do you currently interact with most often at work or school? (Select all that apply) - Selected Choice - Genetic Data`                                                                                                 &lt;chr&gt; …\n$ `Which types of data do you currently interact with most often at work or school? (Select all that apply) - Selected Choice - Geospatial Data`                                                                                              &lt;chr&gt; …\n$ `Which types of data do you currently interact with most often at work or school? (Select all that apply) - Selected Choice - Image Data`                                                                                                   &lt;chr&gt; …\n$ `Which types of data do you currently interact with most often at work or school? (Select all that apply) - Selected Choice - Numerical Data`                                                                                               &lt;chr&gt; …\n$ `Which types of data do you currently interact with most often at work or school? (Select all that apply) - Selected Choice - Sensor Data`                                                                                                  &lt;chr&gt; …\n$ `Which types of data do you currently interact with most often at work or school? (Select all that apply) - Selected Choice - Tabular Data`                                                                                                 &lt;chr&gt; …\n$ `Which types of data do you currently interact with most often at work or school? (Select all that apply) - Selected Choice - Text Data`                                                                                                    &lt;chr&gt; …\n$ `Which types of data do you currently interact with most often at work or school? (Select all that apply) - Selected Choice - Time Series Data`                                                                                             &lt;chr&gt; …\n$ `Which types of data do you currently interact with most often at work or school? (Select all that apply) - Selected Choice - Video Data`                                                                                                   &lt;chr&gt; …\n$ `Which types of data do you currently interact with most often at work or school? (Select all that apply) - Selected Choice - Other Data`                                                                                                   &lt;chr&gt; …\n$ `Which types of data do you currently interact with most often at work or school? (Select all that apply) - Other Data - Text`                                                                                                              &lt;dbl&gt; …\n$ `What is the type of data that you currently interact with most often at work or school? - Selected Choice`                                                                                                                                 &lt;chr&gt; …\n$ `What is the type of data that you currently interact with most often at work or school? - Other Data - Text`                                                                                                                               &lt;dbl&gt; …\n$ `Where do you find public datasets? (Select all that apply) - Selected Choice - Government websites`                                                                                                                                        &lt;chr&gt; …\n$ `Where do you find public datasets? (Select all that apply) - Selected Choice - University research group websites`                                                                                                                         &lt;chr&gt; …\n$ `Where do you find public datasets? (Select all that apply) - Selected Choice - Non-profit research group websites`                                                                                                                         &lt;chr&gt; …\n$ `Where do you find public datasets? (Select all that apply) - Selected Choice - Dataset aggregator/platform (Socrata, Kaggle Public Datasets Platform, etc.)`                                                                               &lt;chr&gt; …\n$ `Where do you find public datasets? (Select all that apply) - Selected Choice - I collect my own data (web-scraping, etc.)`                                                                                                                 &lt;chr&gt; …\n$ `Where do you find public datasets? (Select all that apply) - Selected Choice - Publicly released data from private companies`                                                                                                              &lt;chr&gt; …\n$ `Where do you find public datasets? (Select all that apply) - Selected Choice - Google Search`                                                                                                                                              &lt;chr&gt; …\n$ `Where do you find public datasets? (Select all that apply) - Selected Choice - Google Dataset Search`                                                                                                                                      &lt;chr&gt; …\n$ `Where do you find public datasets? (Select all that apply) - Selected Choice - GitHub`                                                                                                                                                     &lt;chr&gt; …\n$ `Where do you find public datasets? (Select all that apply) - Selected Choice - None/I do not work with public data`                                                                                                                        &lt;chr&gt; …\n$ `Where do you find public datasets? (Select all that apply) - Selected Choice - Other`                                                                                                                                                      &lt;chr&gt; …\n$ `Where do you find public datasets? (Select all that apply) - Other - Text`                                                                                                                                                                 &lt;dbl&gt; …\n$ `During a typical data science project at work or school, approximately what proportion of your time is devoted to the following? (Answers must add up to 100%) - Gathering data`                                                           &lt;dbl&gt; …\n$ `During a typical data science project at work or school, approximately what proportion of your time is devoted to the following? (Answers must add up to 100%) - Cleaning data`                                                            &lt;dbl&gt; …\n$ `During a typical data science project at work or school, approximately what proportion of your time is devoted to the following? (Answers must add up to 100%) - Visualizing data`                                                         &lt;dbl&gt; …\n$ `During a typical data science project at work or school, approximately what proportion of your time is devoted to the following? (Answers must add up to 100%) - Model building/model selection`                                           &lt;dbl&gt; …\n$ `During a typical data science project at work or school, approximately what proportion of your time is devoted to the following? (Answers must add up to 100%) - Putting the model into production`                                        &lt;dbl&gt; …\n$ `During a typical data science project at work or school, approximately what proportion of your time is devoted to the following? (Answers must add up to 100%) - Finding insights in the data and communicating with stakeholders`         &lt;dbl&gt; …\n$ `During a typical data science project at work or school, approximately what proportion of your time is devoted to the following? (Answers must add up to 100%) - Other`                                                                    &lt;dbl&gt; …\n$ `What percentage of your current machine learning/data science training falls under each category? (Answers must add up to 100%) - Self-taught`                                                                                             &lt;dbl&gt; …\n$ `What percentage of your current machine learning/data science training falls under each category? (Answers must add up to 100%) - Online courses (Coursera, Udemy, edX, etc.)`                                                             &lt;dbl&gt; …\n$ `What percentage of your current machine learning/data science training falls under each category? (Answers must add up to 100%) - Work`                                                                                                    &lt;dbl&gt; …\n$ `What percentage of your current machine learning/data science training falls under each category? (Answers must add up to 100%) - University`                                                                                              &lt;dbl&gt; …\n$ `What percentage of your current machine learning/data science training falls under each category? (Answers must add up to 100%) - Kaggle competitions`                                                                                     &lt;dbl&gt; …\n$ `What percentage of your current machine learning/data science training falls under each category? (Answers must add up to 100%) - Other`                                                                                                   &lt;dbl&gt; …\n$ `What percentage of your current machine learning/data science training falls under each category? (Answers must add up to 100%) - Other - Text`                                                                                            &lt;dbl&gt; …\n$ `On which online platforms have you begun or completed data science courses? (Select all that apply) - Selected Choice - Udacity`                                                                                                           &lt;chr&gt; …\n$ `On which online platforms have you begun or completed data science courses? (Select all that apply) - Selected Choice - Coursera`                                                                                                          &lt;chr&gt; …\n$ `On which online platforms have you begun or completed data science courses? (Select all that apply) - Selected Choice - edX`                                                                                                               &lt;chr&gt; …\n$ `On which online platforms have you begun or completed data science courses? (Select all that apply) - Selected Choice - DataCamp`                                                                                                          &lt;chr&gt; …\n$ `On which online platforms have you begun or completed data science courses? (Select all that apply) - Selected Choice - DataQuest`                                                                                                         &lt;chr&gt; …\n$ `On which online platforms have you begun or completed data science courses? (Select all that apply) - Selected Choice - Kaggle Learn`                                                                                                      &lt;chr&gt; …\n$ `On which online platforms have you begun or completed data science courses? (Select all that apply) - Selected Choice - Fast.AI`                                                                                                           &lt;chr&gt; …\n$ `On which online platforms have you begun or completed data science courses? (Select all that apply) - Selected Choice - developers.google.com`                                                                                             &lt;chr&gt; …\n$ `On which online platforms have you begun or completed data science courses? (Select all that apply) - Selected Choice - Udemy`                                                                                                             &lt;chr&gt; …\n$ `On which online platforms have you begun or completed data science courses? (Select all that apply) - Selected Choice - TheSchool.AI`                                                                                                      &lt;chr&gt; …\n$ `On which online platforms have you begun or completed data science courses? (Select all that apply) - Selected Choice - Online University Courses`                                                                                         &lt;chr&gt; …\n$ `On which online platforms have you begun or completed data science courses? (Select all that apply) - Selected Choice - None`                                                                                                              &lt;chr&gt; …\n$ `On which online platforms have you begun or completed data science courses? (Select all that apply) - Selected Choice - Other`                                                                                                             &lt;chr&gt; …\n$ `On which online platforms have you begun or completed data science courses? (Select all that apply) - Other - Text`                                                                                                                        &lt;dbl&gt; …\n$ `On which online platform have you spent the most amount of time? - Selected Choice`                                                                                                                                                        &lt;chr&gt; …\n$ `On which online platform have you spent the most amount of time? - Other - Text`                                                                                                                                                           &lt;dbl&gt; …\n$ `Who/what are your favorite media sources that report on data science topics? (Select all that apply) - Selected Choice - Twitter`                                                                                                          &lt;chr&gt; …\n$ `Who/what are your favorite media sources that report on data science topics? (Select all that apply) - Selected Choice - Hacker News`                                                                                                      &lt;chr&gt; …\n$ `Who/what are your favorite media sources that report on data science topics? (Select all that apply) - Selected Choice - r/machinelearning`                                                                                                &lt;chr&gt; …\n$ `Who/what are your favorite media sources that report on data science topics? (Select all that apply) - Selected Choice - Kaggle forums`                                                                                                    &lt;chr&gt; …\n$ `Who/what are your favorite media sources that report on data science topics? (Select all that apply) - Selected Choice - Fastai forums`                                                                                                    &lt;chr&gt; …\n$ `Who/what are your favorite media sources that report on data science topics? (Select all that apply) - Selected Choice - Siraj Raval YouTube Channel`                                                                                      &lt;chr&gt; …\n$ `Who/what are your favorite media sources that report on data science topics? (Select all that apply) - Selected Choice - DataTau News Aggregator`                                                                                          &lt;chr&gt; …\n$ `Who/what are your favorite media sources that report on data science topics? (Select all that apply) - Selected Choice - Linear Digressions Podcast`                                                                                       &lt;chr&gt; …\n$ `Who/what are your favorite media sources that report on data science topics? (Select all that apply) - Selected Choice - Cloud AI Adventures (YouTube)`                                                                                    &lt;chr&gt; …\n$ `Who/what are your favorite media sources that report on data science topics? (Select all that apply) - Selected Choice - FiveThirtyEight.com`                                                                                              &lt;chr&gt; …\n$ `Who/what are your favorite media sources that report on data science topics? (Select all that apply) - Selected Choice - ArXiv & Preprints`                                                                                                &lt;chr&gt; …\n$ `Who/what are your favorite media sources that report on data science topics? (Select all that apply) - Selected Choice - Journal Publications`                                                                                             &lt;chr&gt; …\n$ `Who/what are your favorite media sources that report on data science topics? (Select all that apply) - Selected Choice - FastML Blog`                                                                                                      &lt;chr&gt; …\n$ `Who/what are your favorite media sources that report on data science topics? (Select all that apply) - Selected Choice - KDnuggets Blog`                                                                                                   &lt;chr&gt; …\n$ `Who/what are your favorite media sources that report on data science topics? (Select all that apply) - Selected Choice - O'Reilly Data Newsletter`                                                                                         &lt;chr&gt; …\n$ `Who/what are your favorite media sources that report on data science topics? (Select all that apply) - Selected Choice - Partially Derivative Podcast`                                                                                     &lt;chr&gt; …\n$ `Who/what are your favorite media sources that report on data science topics? (Select all that apply) - Selected Choice - The Data Skeptic Podcast`                                                                                         &lt;chr&gt; …\n$ `Who/what are your favorite media sources that report on data science topics? (Select all that apply) - Selected Choice - Medium Blog Posts`                                                                                                &lt;chr&gt; …\n$ `Who/what are your favorite media sources that report on data science topics? (Select all that apply) - Selected Choice - Towards Data Science Blog`                                                                                        &lt;lgl&gt; …\n$ `Who/what are your favorite media sources that report on data science topics? (Select all that apply) - Selected Choice - Analytics Vidhya Blog`                                                                                            &lt;lgl&gt; …\n$ `Who/what are your favorite media sources that report on data science topics? (Select all that apply) - Selected Choice - None/I do not know`                                                                                               &lt;chr&gt; …\n$ `Who/what are your favorite media sources that report on data science topics? (Select all that apply) - Selected Choice - Other`                                                                                                            &lt;chr&gt; …\n$ `Who/what are your favorite media sources that report on data science topics? (Select all that apply) - Other - Text`                                                                                                                       &lt;dbl&gt; …\n$ `How do you perceive the quality of online learning platforms and in-person bootcamps as compared to the quality of the education provided by traditional brick and mortar institutions? - Online learning platforms and MOOCs:`            &lt;chr&gt; …\n$ `How do you perceive the quality of online learning platforms and in-person bootcamps as compared to the quality of the education provided by traditional brick and mortar institutions? - In-person bootcamps:`                            &lt;chr&gt; …\n$ `Which better demonstrates expertise in data science: academic achievements or independent projects? - Your views:`                                                                                                                         &lt;chr&gt; …\n$ `How do you perceive the importance of the following topics? - Fairness and bias in ML algorithms:`                                                                                                                                         &lt;chr&gt; …\n$ `How do you perceive the importance of the following topics? - Being able to explain ML model outputs and/or predictions`                                                                                                                   &lt;chr&gt; …\n$ `How do you perceive the importance of the following topics? - Reproducibility in data science`                                                                                                                                             &lt;chr&gt; …\n$ `What metrics do you or your organization use to determine whether or not your models were successful? (Select all that apply) - Selected Choice - Revenue and/or business goals`                                                           &lt;chr&gt; …\n$ `What metrics do you or your organization use to determine whether or not your models were successful? (Select all that apply) - Selected Choice - Metrics that consider accuracy`                                                          &lt;chr&gt; …\n$ `What metrics do you or your organization use to determine whether or not your models were successful? (Select all that apply) - Selected Choice - Metrics that consider unfair bias`                                                       &lt;chr&gt; …\n$ `What metrics do you or your organization use to determine whether or not your models were successful? (Select all that apply) - Selected Choice - Not applicable (I am not involved with an organization that builds ML models)`           &lt;chr&gt; …\n$ `What metrics do you or your organization use to determine whether or not your models were successful? (Select all that apply) - Selected Choice - Other`                                                                                   &lt;chr&gt; …\n$ `What metrics do you or your organization use to determine whether or not your models were successful? (Select all that apply) - Other - Text`                                                                                              &lt;dbl&gt; …\n$ `Approximately what percent of your data projects involved exploring unfair bias in the dataset and/or algorithm?`                                                                                                                          &lt;chr&gt; …\n$ `What do you find most difficult about ensuring that your algorithms are fair and unbiased? (Select all that apply) - Lack of communication between individuals who collect the data and individuals who analyze the data`                  &lt;chr&gt; …\n$ `What do you find most difficult about ensuring that your algorithms are fair and unbiased? (Select all that apply) - Difficulty in identifying groups that are unfairly targeted`                                                          &lt;chr&gt; …\n$ `What do you find most difficult about ensuring that your algorithms are fair and unbiased? (Select all that apply) - Difficulty in collecting enough data about groups that may be unfairly targeted`                                      &lt;chr&gt; …\n$ `What do you find most difficult about ensuring that your algorithms are fair and unbiased? (Select all that apply) - Difficulty in identifying and selecting the appropriate evaluation metrics`                                           &lt;chr&gt; …\n$ `What do you find most difficult about ensuring that your algorithms are fair and unbiased? (Select all that apply) - I have never found any difficulty in this task`                                                                       &lt;chr&gt; …\n$ `What do you find most difficult about ensuring that your algorithms are fair and unbiased? (Select all that apply) - I have never performed this task`                                                                                     &lt;chr&gt; …\n$ `In what circumstances would you explore model insights and interpret your model's predictions? (Select all that apply) - Only for very important models that are already in production`                                                    &lt;chr&gt; …\n$ `In what circumstances would you explore model insights and interpret your model's predictions? (Select all that apply) - For all models right before putting the model in production`                                                      &lt;chr&gt; …\n$ `In what circumstances would you explore model insights and interpret your model's predictions? (Select all that apply) - When determining whether it is worth it to put the model into production`                                         &lt;chr&gt; …\n$ `In what circumstances would you explore model insights and interpret your model's predictions? (Select all that apply) - When building a model that was specifically designed to produce such insights`                                    &lt;chr&gt; …\n$ `In what circumstances would you explore model insights and interpret your model's predictions? (Select all that apply) - When first exploring a new ML model or dataset`                                                                   &lt;chr&gt; …\n$ `In what circumstances would you explore model insights and interpret your model's predictions? (Select all that apply) - I do not explore and interpret model insights and predictions`                                                    &lt;chr&gt; …\n$ `Approximately what percent of your data projects involve exploring model insights?`                                                                                                                                                        &lt;chr&gt; …\n$ `What methods do you prefer for explaining and/or interpreting decisions that are made by ML models? (Select all that apply) - Selected Choice - Examine individual model coefficients`                                                     &lt;chr&gt; …\n$ `What methods do you prefer for explaining and/or interpreting decisions that are made by ML models? (Select all that apply) - Selected Choice - Examine feature correlations`                                                              &lt;chr&gt; …\n$ `What methods do you prefer for explaining and/or interpreting decisions that are made by ML models? (Select all that apply) - Selected Choice - Examine feature importances`                                                               &lt;chr&gt; …\n$ `What methods do you prefer for explaining and/or interpreting decisions that are made by ML models? (Select all that apply) - Selected Choice - Plot decision boundaries`                                                                  &lt;chr&gt; …\n$ `What methods do you prefer for explaining and/or interpreting decisions that are made by ML models? (Select all that apply) - Selected Choice - Create partial dependence plots`                                                           &lt;chr&gt; …\n$ `What methods do you prefer for explaining and/or interpreting decisions that are made by ML models? (Select all that apply) - Selected Choice - Dimensionality reduction techniques`                                                       &lt;chr&gt; …\n$ `What methods do you prefer for explaining and/or interpreting decisions that are made by ML models? (Select all that apply) - Selected Choice - Attention mapping/saliency mapping`                                                        &lt;chr&gt; …\n$ `What methods do you prefer for explaining and/or interpreting decisions that are made by ML models? (Select all that apply) - Selected Choice - Plot predicted vs. actual results`                                                         &lt;chr&gt; …\n$ `What methods do you prefer for explaining and/or interpreting decisions that are made by ML models? (Select all that apply) - Selected Choice - Print out a decision tree`                                                                 &lt;chr&gt; …\n$ `What methods do you prefer for explaining and/or interpreting decisions that are made by ML models? (Select all that apply) - Selected Choice - Sensitivity analysis/perturbation importance`                                              &lt;chr&gt; …\n$ `What methods do you prefer for explaining and/or interpreting decisions that are made by ML models? (Select all that apply) - Selected Choice - LIME functions`                                                                            &lt;chr&gt; …\n$ `What methods do you prefer for explaining and/or interpreting decisions that are made by ML models? (Select all that apply) - Selected Choice - ELI5 functions`                                                                            &lt;chr&gt; …\n$ `What methods do you prefer for explaining and/or interpreting decisions that are made by ML models? (Select all that apply) - Selected Choice - SHAP functions`                                                                            &lt;chr&gt; …\n$ `What methods do you prefer for explaining and/or interpreting decisions that are made by ML models? (Select all that apply) - Selected Choice - None/I do not use these model explanation techniques`                                      &lt;chr&gt; …\n$ `What methods do you prefer for explaining and/or interpreting decisions that are made by ML models? (Select all that apply) - Selected Choice - Other`                                                                                     &lt;chr&gt; …\n$ `What methods do you prefer for explaining and/or interpreting decisions that are made by ML models? (Select all that apply) - Other - Text`                                                                                                &lt;chr&gt; …\n$ `Do you consider ML models to be \"black boxes\" with outputs that are difficult or impossible to explain?`                                                                                                                                   &lt;chr&gt; …\n$ `What tools and methods do you use to make your work easy to reproduce? (Select all that apply) - Selected Choice - Share code on Github or a similar code-sharing repository`                                                              &lt;chr&gt; …\n$ `What tools and methods do you use to make your work easy to reproduce? (Select all that apply) - Selected Choice - Share both data and code on Github or a similar code-sharing repository`                                                &lt;chr&gt; …\n$ `What tools and methods do you use to make your work easy to reproduce? (Select all that apply) - Selected Choice - Share data, code, and environment using a hosted service (Kaggle Kernels, Google Colaboratory, Amazon SageMaker, etc.)` &lt;chr&gt; …\n$ `What tools and methods do you use to make your work easy to reproduce? (Select all that apply) - Selected Choice - Share data, code, and environment using containers (Docker, etc.)`                                                      &lt;chr&gt; …\n$ `What tools and methods do you use to make your work easy to reproduce? (Select all that apply) - Selected Choice - Share code, data, and environment using virtual machines (VirtualBox, etc.)`                                            &lt;chr&gt; …\n$ `What tools and methods do you use to make your work easy to reproduce? (Select all that apply) - Selected Choice - Make sure the code is well documented`                                                                                  &lt;chr&gt; …\n$ `What tools and methods do you use to make your work easy to reproduce? (Select all that apply) - Selected Choice - Make sure the code is human-readable`                                                                                   &lt;chr&gt; …\n$ `What tools and methods do you use to make your work easy to reproduce? (Select all that apply) - Selected Choice - Define all random seeds`                                                                                                &lt;chr&gt; …\n$ `What tools and methods do you use to make your work easy to reproduce? (Select all that apply) - Selected Choice - Define relative rather than absolute file paths`                                                                        &lt;chr&gt; …\n$ `What tools and methods do you use to make your work easy to reproduce? (Select all that apply) - Selected Choice - Include a text file describing all dependencies`                                                                        &lt;chr&gt; …\n$ `What tools and methods do you use to make your work easy to reproduce? (Select all that apply) - Selected Choice - None/I do not make my work easy for others to reproduce`                                                                &lt;chr&gt; …\n$ `What tools and methods do you use to make your work easy to reproduce? (Select all that apply) - Selected Choice - Other`                                                                                                                  &lt;chr&gt; …\n$ `What tools and methods do you use to make your work easy to reproduce? (Select all that apply) - Other - Text`                                                                                                                             &lt;dbl&gt; …\n$ `What barriers prevent you from making your work even easier to reuse and reproduce? (Select all that apply) - Selected Choice - Too expensive`                                                                                             &lt;chr&gt; …\n$ `What barriers prevent you from making your work even easier to reuse and reproduce? (Select all that apply) - Selected Choice - Too time-consuming`                                                                                        &lt;chr&gt; …\n$ `What barriers prevent you from making your work even easier to reuse and reproduce? (Select all that apply) - Selected Choice - Requires too much technical knowledge`                                                                     &lt;chr&gt; …\n$ `What barriers prevent you from making your work even easier to reuse and reproduce? (Select all that apply) - Selected Choice - Afraid that others will use my work without giving proper credit`                                          &lt;chr&gt; …\n$ `What barriers prevent you from making your work even easier to reuse and reproduce? (Select all that apply) - Selected Choice - Not enough incentives to share my work`                                                                    &lt;chr&gt; …\n$ `What barriers prevent you from making your work even easier to reuse and reproduce? (Select all that apply) - Selected Choice - I had never considered making my work easier for others to reproduce`                                      &lt;chr&gt; …\n$ `What barriers prevent you from making your work even easier to reuse and reproduce? (Select all that apply) - Selected Choice - None of these reasons apply to me`                                                                         &lt;chr&gt; …\n$ `What barriers prevent you from making your work even easier to reuse and reproduce? (Select all that apply) - Selected Choice - Other`                                                                                                     &lt;chr&gt; …\n$ `What barriers prevent you from making your work even easier to reuse and reproduce? (Select all that apply) - Other - Text`                                                                                                                &lt;dbl&gt; …"
  },
  {
    "objectID": "posts/abigailbalint_finalpart1.html#bibliography",
    "href": "posts/abigailbalint_finalpart1.html#bibliography",
    "title": "Final Project Initial Research",
    "section": "Bibliography",
    "text": "Bibliography\nKaggle, (2018). “2018 Kaggle Machine Learning & Data Science Survey”, Retrieved 21 March 2023 from https://www.kaggle.com/datasets/kaggle/kaggle-survey-2018.\nCC BY-SA 4.0 : https://creativecommons.org/licenses/by-sa/4.0/ :::"
  },
  {
    "objectID": "posts/Homework_5_Saisrinivas.html#a",
    "href": "posts/Homework_5_Saisrinivas.html#a",
    "title": "Homework_5_Saisrinivas",
    "section": "A",
    "text": "A\n\nlibrary(alr4)\n\nLoading required package: car\n\n\nLoading required package: carData\n\n\nLoading required package: effects\n\n\nlattice theme set by effectsTheme()\nSee ?effectsTheme for details.\n\nlibrary(smss)\n\nx1 &lt;- 1240\nx2 &lt;- 18000\nactual_price &lt;- 145000\n\npredicted_price &lt;- -10536 + 53.8 * x1 + 2.84 * x2\nresidual &lt;- actual_price - predicted_price\ncat(\"A. Predicted selling price:\", predicted_price, \"\\nResidual:\", residual, \"\\n\")\n\nA. Predicted selling price: 107296 \nResidual: 37704 \n\n\nGiven the variable information above (y=selling price, x1=house size, x2=lot size) and the data in the prediction equation (1,240 sqft house on an 18,000 sqft lot), the home’s estimated selling price is $107,296. Now compute the difference between this and the actual selling price ($145,000).\nThe difference in pricing indicates that the house sold for $37,740 more than the projected selling price."
  },
  {
    "objectID": "posts/Homework_5_Saisrinivas.html#b",
    "href": "posts/Homework_5_Saisrinivas.html#b",
    "title": "Homework_5_Saisrinivas",
    "section": "B",
    "text": "B\n\nhome_size_coefficient &lt;- 53.8\ncat(\"B. House selling price predicted to increase for each square-foot increase in home size:\", home_size_coefficient, \"\\n\")\n\nB. House selling price predicted to increase for each square-foot increase in home size: 53.8"
  },
  {
    "objectID": "posts/Homework_5_Saisrinivas.html#c",
    "href": "posts/Homework_5_Saisrinivas.html#c",
    "title": "Homework_5_Saisrinivas",
    "section": "C",
    "text": "C\n\nhome_size_increase &lt;- 1\nlot_size_coefficient &lt;- 2.84\n\nlot_size_increase &lt;- (home_size_coefficient * home_size_increase) / lot_size_coefficient\ncat(\"C. Lot size increase needed to have the same impact as a one-square-foot increase in home size:\", lot_size_increase, \"\\n\")\n\nC. Lot size increase needed to have the same impact as a one-square-foot increase in home size: 18.94366"
  },
  {
    "objectID": "posts/Homework_5_Saisrinivas.html#question-2",
    "href": "posts/Homework_5_Saisrinivas.html#question-2",
    "title": "Homework_5_Saisrinivas",
    "section": "Question 2",
    "text": "Question 2"
  },
  {
    "objectID": "posts/Homework_5_Saisrinivas.html#a-1",
    "href": "posts/Homework_5_Saisrinivas.html#a-1",
    "title": "Homework_5_Saisrinivas",
    "section": "A",
    "text": "A\n\nlibrary(alr4)\ndata(salary)\n\nt_test &lt;- t.test(salary ~ sex, data = salary)\nt_test\n\n\n    Welch Two Sample t-test\n\ndata:  salary by sex\nt = 1.7744, df = 21.591, p-value = 0.09009\nalternative hypothesis: true difference in means between group Male and group Female is not equal to 0\n95 percent confidence interval:\n -567.8539 7247.1471\nsample estimates:\n  mean in group Male mean in group Female \n            24696.79             21357.14 \n\n\nWithout regard to school level or work rank, the average male pay is $24,696.79, while the average female wage is $21,357.14 - a $3,339.65 difference."
  },
  {
    "objectID": "posts/Homework_5_Saisrinivas.html#b-1",
    "href": "posts/Homework_5_Saisrinivas.html#b-1",
    "title": "Homework_5_Saisrinivas",
    "section": "B",
    "text": "B\n\nmodel &lt;- lm(salary ~ ., data = salary)\nconf_int &lt;- confint(model, level = 0.95)\nconf_int\n\n                 2.5 %      97.5 %\n(Intercept) 14134.4059 17357.68946\ndegreePhD    -663.2482  3440.47485\nrankAssoc    2985.4107  7599.31080\nrankProf     8396.1546 13841.37340\nsexFemale    -697.8183  3030.56452\nyear          285.1433   667.47476\nysdeg        -280.6397    31.49105\n\n\nThe pay gap between men and women is between -697.82 and 3030.57, according to a multiple linear regression with a 95% confidence range."
  },
  {
    "objectID": "posts/Homework_5_Saisrinivas.html#c-1",
    "href": "posts/Homework_5_Saisrinivas.html#c-1",
    "title": "Homework_5_Saisrinivas",
    "section": "C",
    "text": "C\n\nsummary(model)\n\n\nCall:\nlm(formula = salary ~ ., data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4045.2 -1094.7  -361.5   813.2  9193.1 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 15746.05     800.18  19.678  &lt; 2e-16 ***\ndegreePhD    1388.61    1018.75   1.363    0.180    \nrankAssoc    5292.36    1145.40   4.621 3.22e-05 ***\nrankProf    11118.76    1351.77   8.225 1.62e-10 ***\nsexFemale    1166.37     925.57   1.260    0.214    \nyear          476.31      94.91   5.018 8.65e-06 ***\nysdeg        -124.57      77.49  -1.608    0.115    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2398 on 45 degrees of freedom\nMultiple R-squared:  0.855, Adjusted R-squared:  0.8357 \nF-statistic: 44.24 on 6 and 45 DF,  p-value: &lt; 2.2e-16\n\n\nTo summarize, wage increases are $1388.61 if the person has a PhD, $5292.36 if the person is an Associate Professor, $11,118.75 if the person is a full/tenured Professor, $1,166.37 if the individual is female, $476.31 for each year the individual remains at their present rank.\nSalary, on the other hand, drops by $124.57 for each year after the individual obtained their maximum degree/rank level. Except for this one, all slopes are positive. Furthermore, an individual’s rank and the number of years spent at their present rank are statistically significant (less than 0.05)."
  },
  {
    "objectID": "posts/Homework_5_Saisrinivas.html#d",
    "href": "posts/Homework_5_Saisrinivas.html#d",
    "title": "Homework_5_Saisrinivas",
    "section": "D",
    "text": "D\n\n# Change the baseline category for rank and rerun the model\nsalary$rank &lt;- relevel(salary$rank, ref = \"Asst\")\nmodel2 &lt;- lm(salary ~ ., data = salary)\nsummary(model2)\n\n\nCall:\nlm(formula = salary ~ ., data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4045.2 -1094.7  -361.5   813.2  9193.1 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 15746.05     800.18  19.678  &lt; 2e-16 ***\ndegreePhD    1388.61    1018.75   1.363    0.180    \nrankAssoc    5292.36    1145.40   4.621 3.22e-05 ***\nrankProf    11118.76    1351.77   8.225 1.62e-10 ***\nsexFemale    1166.37     925.57   1.260    0.214    \nyear          476.31      94.91   5.018 8.65e-06 ***\nysdeg        -124.57      77.49  -1.608    0.115    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2398 on 45 degrees of freedom\nMultiple R-squared:  0.855, Adjusted R-squared:  0.8357 \nF-statistic: 44.24 on 6 and 45 DF,  p-value: &lt; 2.2e-16\n\n\nNone of the figured have changed."
  },
  {
    "objectID": "posts/Homework_5_Saisrinivas.html#e",
    "href": "posts/Homework_5_Saisrinivas.html#e",
    "title": "Homework_5_Saisrinivas",
    "section": "E",
    "text": "E\n\n# Exclude the rank variable and rerun the model\nmodel3 &lt;- lm(salary ~ degree + sex + year + ysdeg, data = salary)\nsummary(model3)\n\n\nCall:\nlm(formula = salary ~ degree + sex + year + ysdeg, data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8146.9 -2186.9  -491.5  2279.1 11186.6 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 17183.57    1147.94  14.969  &lt; 2e-16 ***\ndegreePhD   -3299.35    1302.52  -2.533 0.014704 *  \nsexFemale   -1286.54    1313.09  -0.980 0.332209    \nyear          351.97     142.48   2.470 0.017185 *  \nysdeg         339.40      80.62   4.210 0.000114 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3744 on 47 degrees of freedom\nMultiple R-squared:  0.6312,    Adjusted R-squared:  0.5998 \nF-statistic: 20.11 on 4 and 47 DF,  p-value: 1.048e-09\n\n\nSalary reduces by: when variable ‘rank’ is deleted: $3299.35 if the person holds a PhD, $1,286.54 if the person is female Salary gains, however, are $351.97 for each year spent at their present level, $339.40 for each year after earning their highest degree.\nThe slopes are divided 50/50 in terms of how many are positive and how many are negative. ‘degreePhD’, ‘year’, and ‘ysdeg’ are all statistically significant (less than 0.05), but’sexFemale’ is not."
  },
  {
    "objectID": "posts/Homework_5_Saisrinivas.html#f",
    "href": "posts/Homework_5_Saisrinivas.html#f",
    "title": "Homework_5_Saisrinivas",
    "section": "F",
    "text": "F\n\n# Create a new variable and run the multiple regression model\nsalary$new_dean &lt;- ifelse(salary$ysdeg &lt;= 15, \"New\", \"Old\")\nmodel4 &lt;- lm(salary ~ degree + sex + new_dean + year*ysdeg, data = salary)\nsummary(model4)\n\n\nCall:\nlm(formula = salary ~ degree + sex + new_dean + year * ysdeg, \n    data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8396.1 -2171.9  -352.5  2053.3 11061.3 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 16855.764   1508.902  11.171 1.45e-14 ***\ndegreePhD   -3205.140   1431.791  -2.239   0.0302 *  \nsexFemale   -1222.794   1345.288  -0.909   0.3682    \nnew_deanOld   550.409   2119.912   0.260   0.7963    \nyear          462.292    277.040   1.669   0.1021    \nysdeg         332.120    141.753   2.343   0.0236 *  \nyear:ysdeg     -4.525     10.083  -0.449   0.6557    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3814 on 45 degrees of freedom\nMultiple R-squared:  0.6334,    Adjusted R-squared:  0.5845 \nF-statistic: 12.96 on 6 and 45 DF,  p-value: 1.902e-08\n\n\n‘Year’ and ‘ysdeg’ are likely to be highly correlated."
  },
  {
    "objectID": "posts/Homework_5_Saisrinivas.html#question-3",
    "href": "posts/Homework_5_Saisrinivas.html#question-3",
    "title": "Homework_5_Saisrinivas",
    "section": "Question 3",
    "text": "Question 3"
  },
  {
    "objectID": "posts/Homework_5_Saisrinivas.html#a-2",
    "href": "posts/Homework_5_Saisrinivas.html#a-2",
    "title": "Homework_5_Saisrinivas",
    "section": "A",
    "text": "A\n\nlibrary(smss)\ndata(house.selling.price)\n\nmodel &lt;- lm(Price ~ Size + New, data = house.selling.price)\nsummary(model)\n\n\nCall:\nlm(formula = Price ~ Size + New, data = house.selling.price)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-205102  -34374   -5778   18929  163866 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -40230.867  14696.140  -2.738  0.00737 ** \nSize           116.132      8.795  13.204  &lt; 2e-16 ***\nNew          57736.283  18653.041   3.095  0.00257 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 53880 on 97 degrees of freedom\nMultiple R-squared:  0.7226,    Adjusted R-squared:  0.7169 \nF-statistic: 126.3 on 2 and 97 DF,  p-value: &lt; 2.2e-16\n\n\nWhile controlling for size, predictor variables ‘New’ and ‘Size’ have p-values of 0.00257 and 2e-16 respectively. Both p-values are statistically significant as they are less than 0.05. This indicates that the null hypothesis can be rejected (there is no relationship between ‘New’ and ‘Price’ OR between ‘Size’ and ‘Price’ of new homes). By calculating the correlation, we can see that the correlation between ‘New’ and ‘Size’ is 0.3843, which is a wear relationship."
  },
  {
    "objectID": "posts/Homework_5_Saisrinivas.html#b-2",
    "href": "posts/Homework_5_Saisrinivas.html#b-2",
    "title": "Homework_5_Saisrinivas",
    "section": "B",
    "text": "B\n\ncoefficients &lt;- coef(model)\ncoefficients\n\n(Intercept)        Size         New \n-40230.8668    116.1316  57736.2828"
  },
  {
    "objectID": "posts/Homework_5_Saisrinivas.html#c-2",
    "href": "posts/Homework_5_Saisrinivas.html#c-2",
    "title": "Homework_5_Saisrinivas",
    "section": "C",
    "text": "C\n\n# Calculate the predicted selling price\nsize &lt;- 3000\nnew &lt;- 1\nnot_new &lt;- 0\n\npredicted_new &lt;- coefficients[1] + coefficients[2] * size + coefficients[3] * new\npredicted_not_new &lt;- coefficients[1] + coefficients[2] * size + coefficients[3] * not_new\n\npredicted_new\n\n(Intercept) \n   365900.2 \n\npredicted_not_new\n\n(Intercept) \n   308163.9 \n\n\nIf the house is new, the predicted selling price is $365,900.20 If the house isn’t new, the predicted selling price is $308,163.90"
  },
  {
    "objectID": "posts/Homework_5_Saisrinivas.html#d-1",
    "href": "posts/Homework_5_Saisrinivas.html#d-1",
    "title": "Homework_5_Saisrinivas",
    "section": "D",
    "text": "D\n\nmodel_interaction &lt;- lm(Price ~ Size * New, data = house.selling.price)\nsummary(model_interaction)\n\n\nCall:\nlm(formula = Price ~ Size * New, data = house.selling.price)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-175748  -28979   -6260   14693  192519 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -22227.808  15521.110  -1.432  0.15536    \nSize           104.438      9.424  11.082  &lt; 2e-16 ***\nNew         -78527.502  51007.642  -1.540  0.12697    \nSize:New        61.916     21.686   2.855  0.00527 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 52000 on 96 degrees of freedom\nMultiple R-squared:  0.7443,    Adjusted R-squared:  0.7363 \nF-statistic: 93.15 on 3 and 96 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "posts/Homework_5_Saisrinivas.html#e-1",
    "href": "posts/Homework_5_Saisrinivas.html#e-1",
    "title": "Homework_5_Saisrinivas",
    "section": "E",
    "text": "E\n\ncoefficients_interaction &lt;- coef(model_interaction)\ncoefficients_interaction\n\n (Intercept)         Size          New     Size:New \n-22227.80793    104.43839 -78527.50235     61.91588 \n\nlibrary(ggplot2)\n\nggplot(data=house.selling.price,aes(x=Size,y=Price, color=New))+\n  geom_point()+\n  geom_smooth(method=\"lm\",se=F)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nThe variables in the scatterplot have a linear/correlative connection, as seen in the graph, demonstrating that as size grows, so does the price. However, based on the colors of the dots (which correlate to the age of the home), the link isn’t as simple. New houses (light blue dots) are distributed across the graph, mostly along the slope line. The older residences (dots that aren’t light blue) are primarily clustered towards the bottom right corner of the graph, although there are a handful that outperform the price/size of brand new houses."
  },
  {
    "objectID": "posts/Homework_5_Saisrinivas.html#f-1",
    "href": "posts/Homework_5_Saisrinivas.html#f-1",
    "title": "Homework_5_Saisrinivas",
    "section": "F",
    "text": "F\n\n# Calculate the predicted selling price with interaction\npredicted_new_interaction &lt;- coefficients_interaction[1] + coefficients_interaction[2] * size + coefficients_interaction[3] * new + coefficients_interaction[4] * size * new\npredicted_not_new_interaction &lt;- coefficients_interaction[1] + coefficients_interaction[2] * size + coefficients_interaction[3] * not_new + coefficients_interaction[4] * size * not_new\n\npredicted_new_interaction\n\n(Intercept) \n   398307.5 \n\npredicted_not_new_interaction\n\n(Intercept) \n   291087.4 \n\n\nThe predicted selling price for a New home with the above measurements is $398,307.50.\nThe predicted selling price for a not-new home with the above measurements is $291,087.40."
  },
  {
    "objectID": "posts/Homework_5_Saisrinivas.html#g",
    "href": "posts/Homework_5_Saisrinivas.html#g",
    "title": "Homework_5_Saisrinivas",
    "section": "G",
    "text": "G\n\n# Calculate the predicted selling price for a home of 1500 square feet\nsize_1500 &lt;- 1500\n\npredicted_new_1500 &lt;- coefficients_interaction[1] + coefficients_interaction[2] * size_1500 + coefficients_interaction[3] * new + coefficients_interaction[4] * size_1500 * new\npredicted_not_new_1500 &lt;- coefficients_interaction[1] + coefficients_interaction[2] * size_1500 + coefficients_interaction[3] * not_new + coefficients_interaction[4] * size_1500 * not_new\n\npredicted_new_1500\n\n(Intercept) \n   148776.1 \n\npredicted_not_new_1500\n\n(Intercept) \n   134429.8 \n\n\nThe predicted selling price of a new 1500sqft home is $148,776.10\nThe predicted selling price of a 1500sqft home that ISN’T new is $134,429.80\nIn comparison to the data in Part F (where the property size is doubled to 3000sqft), the expected selling prices in this section (G) are significantly lower. A new 3000sqft house is expected to sell for $398,307.50, while a new 1500sqft house is expected to sell for $148,694.70. The size and price both reduced by half, demonstrating that these two variables are connected and have a linear connection. A 3000sqft house that is NOT new is expected to sell for $291,087.40. A 1500sqft house that is not brand new is expected to sell for $134,429.80. The price difference between the two is $156,657.6. The price is more directly proportional to size than it is with new dwellings."
  },
  {
    "objectID": "posts/Homework_5_Saisrinivas.html#h",
    "href": "posts/Homework_5_Saisrinivas.html#h",
    "title": "Homework_5_Saisrinivas",
    "section": "H",
    "text": "H\n\n# Compare the adjusted R-squared values\nsummary(model)$adj.r.squared\n\n[1] 0.7168767\n\nsummary(model_interaction)$adj.r.squared\n\n[1] 0.7363181\n\n# Compare residuals' distribution\npar(mfrow = c(2, 1))\nhist(residuals(model), main = \"Residuals without Interaction\", xlab = \"Residuals\", col = \"lightblue\")\nhist(residuals(model_interaction), main = \"Residuals with Interaction\", xlab = \"Residuals\", col = \"lightblue\")\n\n\n\n\nI believe that a model with no interaction better describes the relationship between ‘Size’ and ‘New’ with the outcome price; the model with interaction best represents the relationship between ‘Size’ and ‘Price’ rather than ‘Size’ and ‘New’."
  },
  {
    "objectID": "posts/HW1_young.html",
    "href": "posts/HW1_young.html",
    "title": "Homework 1",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.1     ✔ purrr   1.0.1\n✔ tibble  3.1.8     ✔ dplyr   1.1.0\n✔ tidyr   1.3.0     ✔ stringr 1.5.0\n✔ readr   2.1.4     ✔ forcats 1.0.0\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\nCode\nlibrary(dplyr)\nlibrary(readxl)\ndf &lt;- read_excel(\"C:/Users/rotte/Documents/R/603_Spring_2023/posts/_data/LungCapData.xls\")"
  },
  {
    "objectID": "posts/HW1_young.html#a",
    "href": "posts/HW1_young.html#a",
    "title": "Homework 1",
    "section": "a",
    "text": "a\n\n\nCode\n# descriptive statistics\nsummary(df$LungCap)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  0.507   6.150   8.000   7.863   9.800  14.675 \n\n\nCode\nsd(df$LungCap)\n\n\n[1] 2.662008\n\n\n\n\nCode\n# making histogram\nhist(df$LungCap)\n\n\n\n\n\nRange is 0.507~14.675. Median is 8.00. And it’s distribution is looks like normal distribution that mean is 7.863 and sd is 2.662."
  },
  {
    "objectID": "posts/HW1_young.html#b",
    "href": "posts/HW1_young.html#b",
    "title": "Homework 1",
    "section": "b",
    "text": "b\n\n\nCode\n# descriptive statistics\ndf %&gt;%\n  group_by(Gender) %&gt;%\n  summarise(mean(LungCap), sd(LungCap))\n\n\n# A tibble: 2 × 3\n  Gender `mean(LungCap)` `sd(LungCap)`\n  &lt;chr&gt;            &lt;dbl&gt;         &lt;dbl&gt;\n1 female            7.41          2.56\n2 male              8.31          2.68\n\n\n\n\nCode\n# making boxplot\nboxplot(LungCap~Gender, df)\n\n\n\n\n\nMean and sd of female’s LungCap are 7.406 and 2.564, respectively. And Mean and sd of male’s LungCap are 8.309 and 2.683, respectively. Male’s LungCap is bigger than female’s. We can also check this through boxplot."
  },
  {
    "objectID": "posts/HW1_young.html#c",
    "href": "posts/HW1_young.html#c",
    "title": "Homework 1",
    "section": "c",
    "text": "c\n\n\nCode\n# finding lungcap for smokers and non-smokers\ndf %&gt;% group_by(Smoke) %&gt;%\n  summarise(mean(LungCap))\n\n\n# A tibble: 2 × 2\n  Smoke `mean(LungCap)`\n  &lt;chr&gt;           &lt;dbl&gt;\n1 no               7.77\n2 yes              8.65\n\n\nSmokers have bigger Lung cap. It’s a different result from common sense. Through the t-test, I will find out whether this result is statistically significant.\n\n\nCode\nt.test(LungCap~Smoke, df, alternative=\"less\")\n\n\n\n    Welch Two Sample t-test\n\ndata:  LungCap by Smoke\nt = -3.6498, df = 117.72, p-value = 0.0001964\nalternative hypothesis: true difference in means between group no and group yes is less than 0\n95 percent confidence interval:\n       -Inf -0.4776762\nsample estimates:\n mean in group no mean in group yes \n         7.770188          8.645455 \n\n\nAs a result of the one-sided t-test, it was found to be statistically significant at the 95% level of significance.\n##d\nFirst, a new variable cAge is created and a new value is given for each age. For those under the age of 13, “Child”, 14, 15 years of age “Middle”, 16, 17 years of age “High”, and 18 years of age or older, “Adult” will be assigned.\n\n\nCode\ndf&lt;-mutate(df, cAge = ifelse(Age&lt;=13, \"Child\", ifelse(Age %in% 14:15, \"Middle\", ifelse(Age %in% 16:17, \"High\", \"Adult\"))))\n\n\n\n\nCode\ndf %&gt;% group_by(cAge) %&gt;%\n  summarise(mean(LungCap))\n\n\n# A tibble: 4 × 2\n  cAge   `mean(LungCap)`\n  &lt;chr&gt;            &lt;dbl&gt;\n1 Adult            11.0 \n2 Child             6.41\n3 High             10.2 \n4 Middle            9.05\n\n\n\n\nCode\nggplot(df, aes(x=Age, y=LungCap)) +\n  geom_point()\n\n\n\n\n\nLooking at each group’s lung caps, child is 6.41, middle is 9.05, high is 10.25, and adult is 10.96. That is, the lung caps grow with age. Here, it is possible to infer why the lung caps of smokers and non-smokers presented in this data are different from our common sense. The more adults there are, the more smokers there will be, and that may have led to a larger lung cap for smokers.\n##e\nFirst, let’s look at the children’s group.\n\n\nCode\nchilddf&lt;-filter(df, cAge==\"Child\")\ntable(childdf$Smoke)\n\n\n\n no yes \n401  27 \n\n\nCode\nchilddf%&gt;%group_by(Smoke) %&gt;%\n  summarise(mean(LungCap))\n\n\n# A tibble: 2 × 2\n  Smoke `mean(LungCap)`\n  &lt;chr&gt;           &lt;dbl&gt;\n1 no               6.36\n2 yes              7.20\n\n\nSmokers have bigger lung cap. Let’s look at the picture in more detail.\n\n\nCode\nggplot(childdf, aes(x=Age, y=LungCap)) +\n  geom_point(aes(col=factor(Smoke)))\n\n\n\n\n\nFrom the plot, the older the age, the larger the lung cap for non-smokers. In other words, when looking at the entire child group, the growth of natural lung caps with growth is not well revealed, so smokers’ lung caps seem to be larger.\nNext, let’s look at the middle group.\n\n\nCode\nmiddf&lt;-filter(df, cAge==\"Middle\")\ntable(middf$Smoke)\n\n\n\n no yes \n105  15 \n\n\nCode\nmiddf%&gt;%group_by(Smoke) %&gt;%\n  summarise(mean(LungCap))\n\n\n# A tibble: 2 × 2\n  Smoke `mean(LungCap)`\n  &lt;chr&gt;           &lt;dbl&gt;\n1 no               9.14\n2 yes              8.39\n\n\n\n\nCode\nboxplot(LungCap~Smoke, middf)\n\n\n\n\n\nNon-smokers of middle group seem to have bigger lung caps.\nThen, let’s look at the high group.\n\n\nCode\nhighdf&lt;-filter(df, cAge==\"High\")\ntable(highdf$Smoke)\n\n\n\n no yes \n 77  20 \n\n\nCode\nhighdf%&gt;%group_by(Smoke) %&gt;%\n  summarise(mean(LungCap))\n\n\n# A tibble: 2 × 2\n  Smoke `mean(LungCap)`\n  &lt;chr&gt;           &lt;dbl&gt;\n1 no              10.5 \n2 yes              9.38\n\n\n\n\nCode\nboxplot(LungCap~Smoke, highdf)\n\n\n\n\n\nNon-smokers of high group also have bigger lung caps.\nLastly, let me check the adult group.\n\n\nCode\nadultdf&lt;-filter(df, cAge==\"Adult\")\ntable(adultdf$Smoke)\n\n\n\n no yes \n 65  15 \n\n\nCode\nadultdf%&gt;%group_by(Smoke) %&gt;%\n  summarise(mean(LungCap))\n\n\n# A tibble: 2 × 2\n  Smoke `mean(LungCap)`\n  &lt;chr&gt;           &lt;dbl&gt;\n1 no               11.1\n2 yes              10.5\n\n\n\n\nCode\nboxplot(LungCap~Smoke, adultdf)\n\n\n\n\n\nEven in the adult group, non-smokers have a bigger lung cap.\nFinally, I took a look at the overall plot.\n\n\nCode\nggplot(df, aes(x=Age, y=LungCap)) +\n  geom_point(aes(col=factor(Smoke)))\n\n\n\n\n\nOverall, it seems that the lung cap of non-smokers (red) is higher than that of smokers (blue)."
  },
  {
    "objectID": "posts/HW1_young.html#a-1",
    "href": "posts/HW1_young.html#a-1",
    "title": "Homework 1",
    "section": "a",
    "text": "a\nThe probability of selecting inmate has exact 2 priority convictions is the number of inmates with 2 priority convictions divided by the total number of inmates.\n\n\nCode\n160/sum(p_df$freq)\n\n\n[1] 0.1975309\n\n\nThe answer is 0.1975."
  },
  {
    "objectID": "posts/HW1_young.html#b-1",
    "href": "posts/HW1_young.html#b-1",
    "title": "Homework 1",
    "section": "b",
    "text": "b\nIn the same way, the frequency of 0 priority convictions and 1 priority convictions can be combined and divided into the total number of prisoners.\n\n\nCode\n(128+434)/sum(p_df$freq)\n\n\n[1] 0.6938272\n\n\nThe answer is 0.6938."
  },
  {
    "objectID": "posts/HW1_young.html#c-1",
    "href": "posts/HW1_young.html#c-1",
    "title": "Homework 1",
    "section": "c",
    "text": "c\nAdd the probabilities obtained from problems a and b to get the answer.\n\n\nCode\n(160/sum(p_df$freq))+((128+434)/sum(p_df$freq))\n\n\n[1] 0.891358\n\n\nThe answer is 0.8914."
  },
  {
    "objectID": "posts/HW1_young.html#d",
    "href": "posts/HW1_young.html#d",
    "title": "Homework 1",
    "section": "d",
    "text": "d\nThis time, we can get the answer using the probability obtained from c. The total sum of the probabilities is 1, so you can get the answer by subtracting the value obtained from 1 and c.\n\n\nCode\n1-((160/sum(p_df$freq))+((128+434)/sum(p_df$freq)))\n\n\n[1] 0.108642\n\n\nThe answer is 0.1086."
  },
  {
    "objectID": "posts/HW1_young.html#e",
    "href": "posts/HW1_young.html#e",
    "title": "Homework 1",
    "section": "e",
    "text": "e\nTo obtain the expected value, we divide the sum of priority convictions(x) times frequency (freq) by the total number of prisoners.\n\n\nCode\nsum(p_df$x*p_df$freq)/810\n\n\n[1] 1.28642\n\n\nIn another way, the probability of priority conversions can be obtained and the expected value can be obtained by summing each frequency multiplied by this value.\n\n\nCode\np_df&lt;-mutate(p_df, pro=freq/810)\np_df\n\n\n  x freq        pro\n1 0  128 0.15802469\n2 1  434 0.53580247\n3 2  160 0.19753086\n4 3   64 0.07901235\n5 4   24 0.02962963\n\n\n\n\nCode\nsum(p_df$x*p_df$pro)\n\n\n[1] 1.28642\n\n\nThe answer is the same."
  },
  {
    "objectID": "posts/HW1_young.html#f",
    "href": "posts/HW1_young.html#f",
    "title": "Homework 1",
    "section": "f",
    "text": "f\n\n\nCode\nmean&lt;-sum(p_df$x*p_df$pro)\n\n\nFirst, to obtain the variance, get the sum of the squared difference between the x-value and the average value (expected value) and divided by the total number of prisoners.\nThe standard deviation is the square root of the variance.\n\n\nCode\nsum((x-mean)^2*p_df$freq)/810\n\n\n[1] 0.8562353\n\n\nCode\nsqrt(sum((x-mean)^2*p_df$freq)/810)\n\n\n[1] 0.9253298\n\n\nVariance is 0.8562 and standard deviation is 0.925.\nAlternatively, the variance can be obtained by multiplying the square of the difference between the x-value and the average value by each probability.\n\n\nCode\nsum((x-mean)^2*p_df$pro)\n\n\n[1] 0.8562353\n\n\nCode\nsqrt(sum((x-mean)^2*p_df$pro))\n\n\n[1] 0.9253298\n\n\nAs expected, the answer is the same."
  },
  {
    "objectID": "posts/asch_harwood_hw3.html",
    "href": "posts/asch_harwood_hw3.html",
    "title": "Homework 3",
    "section": "",
    "text": "Code\nlibrary(alr4)\nlibrary(smss)\nlibrary(ggplot2)\nlibrary(dplyr)\ndata(UN11)\ndata(water)\ndata(\"Rateprof\")\ndata(\"student.survey\")\n\n\n\nQ1\n\na\nPredictor: PPGDP Response: Fertility\n\n\nb\n\nA straight line does not seem like a good fit for this model as it is heavily skewed to the right\n\n\n\nCode\nggplot(UN11, aes(x = ppgdp, y = fertility)) +\n  geom_point() +\n  geom_smooth(method = 'lm', se = FALSE)\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\nc\n\nBy applying a log transformation to the x and y variables, there now appears to be a linear relationship between log(ppgdp) and log(fertility).\n\n\n\nCode\nggplot(UN11, aes(x = log(ppgdp), y = log(fertility))) +\n  geom_point(color = 'blue') +\n  geom_smooth(method = 'lm', color = 'blue', se = FALSE) \n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nQ2\n\na\nAs we can see in both the scatter plot and the regression coefficients, the slope of the equation does not change. We would not expect it to since we are not altering the relationship between between our variables of interest, simply the unit it is denoted in. The only thing that changes is the intercept, since we shift the x values to the left by converting their units.\n\n\nb\nLikewise, the correlation does not change because we do not change the relationship by changing the unit of measurement.\n\n\nCode\nfit&lt;- lm(log(fertility) ~ log(ppgdp), data = UN11)\nsummary(fit)\n\n\n\nCall:\nlm(formula = log(fertility) ~ log(ppgdp), data = UN11)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.79828 -0.21639  0.02669  0.23424  0.95596 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.66551    0.12057   22.11   &lt;2e-16 ***\nlog(ppgdp)  -0.20715    0.01401  -14.79   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3071 on 197 degrees of freedom\nMultiple R-squared:  0.526, Adjusted R-squared:  0.5236 \nF-statistic: 218.6 on 1 and 197 DF,  p-value: &lt; 2.2e-16\n\n\nCode\ncor(log(UN11$ppgdp), log(UN11$fertility))\n\n\n[1] -0.7252483\n\n\n\n\nCode\nfit&lt;- lm(log(fertility) ~ log(ppgdp/1.33), data = UN11)\nsummary(fit)\n\n\n\nCall:\nlm(formula = log(fertility) ~ log(ppgdp/1.33), data = UN11)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.79828 -0.21639  0.02669  0.23424  0.95596 \n\nCoefficients:\n                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      2.60643    0.11664   22.35   &lt;2e-16 ***\nlog(ppgdp/1.33) -0.20715    0.01401  -14.79   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3071 on 197 degrees of freedom\nMultiple R-squared:  0.526, Adjusted R-squared:  0.5236 \nF-statistic: 218.6 on 1 and 197 DF,  p-value: &lt; 2.2e-16\n\n\nCode\ncor(log(UN11$ppgdp/1.33), log(UN11$fertility))\n\n\n[1] -0.7252483\n\n\n\n\nCode\nggplot(UN11, aes(x = log(ppgdp), y = log(fertility))) +\n  geom_point(color = 'blue') +\n  geom_smooth(method = 'lm', color = 'blue', se = FALSE) +\n  geom_point(aes(x=log(ppgdp/1.33)), color = 'red') +\n  geom_smooth(aes(x=log(ppgdp/1.33)), method = 'lm', color = 'red', se = FALSE)\n\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nQ3\nThere are clear, linear relationships between runoff (BSAAM) and precipitation at OPBPC, OPRC, and OPSLAKE. There also may be linear relationships between APMAM, APSB, and APSLake, but they appear to be right-skewed.\nThese same patterns also hold when comparing precipitation locations. The O locations display multicollinearity with each other, as do the A locations. Similarly, when compared to each other, the A locations and O locations might also be related, but those relations appear to be skewed.\nWhen comparing year to precipitation and runoff, there doesn’t appear to a be a linear relationship.\n\n\nCode\npairs(water)\n\n\n\n\n\n\n\nQ4\n\nQuality, helpfulness, and clarity all exhibit clear, linear relationships\nEasiness also exhibits linear relationships with quality, helpfulness, and clarity but the strength of that relationship appears to be much weaker\nWhile we cannot rule out a linear relationship between raterInterest and the other four variables based on a visual inspection, that relationship, if it exists, is much weaker than the other observed relationships.\n\n\n\nCode\ncol_list &lt;- c('quality', 'helpfulness', 'clarity', 'easiness', 'raterInterest')\npairs(Rateprof[col_list])\n\n\n\n\n\n\n\nQ5\n\ni\nVisually, there does seem to be a linear relationship between religiosity and political ideology. As religious participation increases, respondents move right on the political spectrum. This observed relationship is supported by our regression model. Compared to those who never attend church, and assuming the very liberal is the baseline, those who attend occasionally move 0.25 points to the right. Those who attend most weeks weeks are 2.16 points to the right on the political spectrum, and those who attend every week 2.6 points to the right from those who never attend church. However, occasionally is not statistically significant while most weeks and every week are statistically significant. The R-Squared metric shows us that religiosity explains about 39 percent of variation in political ideology. The entire model is statistically significant.\n\n\nCode\nggplot(data = student.survey, aes(x = re, y = pi)) + \n  geom_count()\n\n\n\n\n\n\n\nCode\nre_dummies &lt;- model.matrix(~ re - 1, data = student.survey)\nre_dummies_df &lt;- as.data.frame(re_dummies)\nre_pi_df &lt;- cbind(student.survey, re_dummies_df)\ncolnames(re_pi_df) &lt;- gsub(\" \", \"_\", colnames(re_pi_df))\npi_mapping &lt;- c(\"very liberal\" = -3, \"liberal\" = -2, \"slightly liberal\" = -1, \"moderate\" = 0, \n                \"slightly conservative\" = 1, \"conservative\" = 2, \"very conservative\" = 3)\nre_pi_df$pi_numeric &lt;- as.numeric(pi_mapping[as.character(re_pi_df$pi)])\nre_pi_df$re &lt;- factor(re_pi_df$re, ordered = FALSE)\nre_pi_df$re &lt;- relevel(factor(re_pi_df$re), ref = \"never\")\nfit &lt;- lm(pi_numeric ~ re, data = re_pi_df)\nsummary(fit)\n\n\n\nCall:\nlm(formula = pi_numeric ~ re, data = re_pi_df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.8889 -0.5172 -0.2667  1.2040  2.7333 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)     -1.7333     0.3394  -5.107 4.08e-06 ***\nreoccasionally   0.2506     0.4181   0.599 0.551374    \nremost weeks     2.1619     0.6017   3.593 0.000691 ***\nreevery week     2.6222     0.5543   4.731 1.56e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.315 on 56 degrees of freedom\nMultiple R-squared:  0.3872,    Adjusted R-squared:  0.3544 \nF-statistic:  11.8 on 3 and 56 DF,  p-value: 4.282e-06\n\n\n\n\nii\nVisually, a relationship is plausible. However, there appear to be a few outliers, which may be exerting undue influence. After dropping those outliers, a relationship seems less likely. The model suggests that for every 1 hour increase in tv, gpa drops by 0.02. However, this finding is not significant.\n\n\nCode\ndata(\"student.survey\")\nggplot(data = student.survey, aes(x = tv, y = hi)) +\n  geom_point() +\n  geom_smooth(method='lm')\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\nCode\n#dropping outliers\ns_survey &lt;- student.survey[student.survey$tv&lt;=20,]\nggplot(data = s_survey, aes(x = tv, y = hi)) +\n  geom_point() +\n  geom_smooth(method='lm')\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\nCode\nfit &lt;- lm(hi ~ tv, data = s_survey)\nsummary(fit)\n\n\n\nCall:\nlm(formula = hi ~ tv, data = s_survey)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.24383 -0.25819  0.05617  0.34192  0.71330 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  3.45819    0.10871  31.811   &lt;2e-16 ***\ntv          -0.02144    0.01486  -1.443    0.155    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4584 on 55 degrees of freedom\nMultiple R-squared:  0.03648,   Adjusted R-squared:  0.01896 \nF-statistic: 2.082 on 1 and 55 DF,  p-value: 0.1547"
  },
  {
    "objectID": "posts/dacss603hw4_LauraCollazo.html",
    "href": "posts/dacss603hw4_LauraCollazo.html",
    "title": "DACSS 603 Homework 4",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(alr4)\nlibrary(smss)"
  },
  {
    "objectID": "posts/dacss603hw4_LauraCollazo.html#a",
    "href": "posts/dacss603hw4_LauraCollazo.html#a",
    "title": "DACSS 603 Homework 4",
    "section": "a",
    "text": "a\nWhen HomeSize = 1240 and LotSize= 18,000, the predicted Price is:\n\n\nCode\nsum(-10,536 + (53.8*1240) + (2.84*1800))\n\n\n[1] 72350\n\n\nSince this home actually sold for $145,000, the residual is:\n\n\nCode\nsum(72350-145000)\n\n\n[1] -72650"
  },
  {
    "objectID": "posts/dacss603hw4_LauraCollazo.html#b",
    "href": "posts/dacss603hw4_LauraCollazo.html#b",
    "title": "DACSS 603 Homework 4",
    "section": "b",
    "text": "b\nWhen the lot size remains fixed, the price is predicted to increase $53.80 for every one-square foot increase in size."
  },
  {
    "objectID": "posts/dacss603hw4_LauraCollazo.html#c",
    "href": "posts/dacss603hw4_LauraCollazo.html#c",
    "title": "DACSS 603 Homework 4",
    "section": "c",
    "text": "c\nGiven this same equation, if home size remains fixed, the lot size would need to increase by the below in order to have the same impact on price as a one-square foot increase in home size:\n\n\nCode\nsum(53.8/2.84)\n\n\n[1] 18.94366"
  },
  {
    "objectID": "posts/dacss603hw4_LauraCollazo.html#a-1",
    "href": "posts/dacss603hw4_LauraCollazo.html#a-1",
    "title": "DACSS 603 Homework 4",
    "section": "a",
    "text": "a\nThe below tests the hypothesis that mean salary for men and women is the same.\n\n\nCode\nfit_2a &lt;- lm(salary ~ sex, data = salary)\n\nsummary(fit_2a)\n\n\n\nCall:\nlm(formula = salary ~ sex, data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8602.8 -4296.6  -100.8  3513.1 16687.9 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    24697        938  26.330   &lt;2e-16 ***\nsexFemale      -3340       1808  -1.847   0.0706 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5782 on 50 degrees of freedom\nMultiple R-squared:  0.0639,    Adjusted R-squared:  0.04518 \nF-statistic: 3.413 on 1 and 50 DF,  p-value: 0.0706\n\n\nThis model does not allow the null hypothesis to be rejected as the p-value is not less than 0.05. The adjusted R-squared is also low and indicates this model explains only 4.52% of the variation between salary and sex. This model also shows that being female results in being paid $3,340 less per year than male faculty."
  },
  {
    "objectID": "posts/dacss603hw4_LauraCollazo.html#b-1",
    "href": "posts/dacss603hw4_LauraCollazo.html#b-1",
    "title": "DACSS 603 Homework 4",
    "section": "b",
    "text": "b\nThe below model adds in degree, rank, year, and ysdeg as additional predictors to the regression model.\n\n\nCode\nfit_2b &lt;- lm(salary ~ sex + degree + rank + year + ysdeg, data = salary)\n\nsummary(fit_2b)\n\n\n\nCall:\nlm(formula = salary ~ sex + degree + rank + year + ysdeg, data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4045.2 -1094.7  -361.5   813.2  9193.1 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 15746.05     800.18  19.678  &lt; 2e-16 ***\nsexFemale    1166.37     925.57   1.260    0.214    \ndegreePhD    1388.61    1018.75   1.363    0.180    \nrankAssoc    5292.36    1145.40   4.621 3.22e-05 ***\nrankProf    11118.76    1351.77   8.225 1.62e-10 ***\nyear          476.31      94.91   5.018 8.65e-06 ***\nysdeg        -124.57      77.49  -1.608    0.115    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2398 on 45 degrees of freedom\nMultiple R-squared:  0.855, Adjusted R-squared:  0.8357 \nF-statistic: 44.24 on 6 and 45 DF,  p-value: &lt; 2.2e-16\n\n\nThe 95% confidence interval for the difference in salary between males and females is below.\n\n\nCode\nconfint(fit_2b)\n\n\n                 2.5 %      97.5 %\n(Intercept) 14134.4059 17357.68946\nsexFemale    -697.8183  3030.56452\ndegreePhD    -663.2482  3440.47485\nrankAssoc    2985.4107  7599.31080\nrankProf     8396.1546 13841.37340\nyear          285.1433   667.47476\nysdeg        -280.6397    31.49105"
  },
  {
    "objectID": "posts/dacss603hw4_LauraCollazo.html#c-1",
    "href": "posts/dacss603hw4_LauraCollazo.html#c-1",
    "title": "DACSS 603 Homework 4",
    "section": "c",
    "text": "c\nThis section interprets the findings for each predictor variable in the above model.\n\nsex\nThe variable sex is not statistically significant in this model. It indicates that when all predictors are held constant, females are paid $1,166.37 more per year more than males.\n\n\ndegree\nThe variable degree is not statistically significant in this model. It indicates that when all predictors are held constant, those with a PhD earn $1,388.61 more than those with a Master’s.\n\n\nrank\nThe variable rank is statistically significant in this model. It indicates that when all predictors are held constant, faculty with a title of Assoc earn $5,292.36 more per year than those with a title of Asst and those with a title of Prof earn $11,118.76 more per year than those with a title of Prof.\n\n\nyear\nThe variable year is statistically significant in this model. It indicates that when all predictors are held constant, for every year increase in faculty’s current rank, salary increases by $476.31.\n\n\nysdeg\nThe variable ysdeg is not statistically significant in this model. It indicates that when all predictors are held constant, for every year increase since the highest degree was earned, salary decreases by -$124.57."
  },
  {
    "objectID": "posts/dacss603hw4_LauraCollazo.html#d",
    "href": "posts/dacss603hw4_LauraCollazo.html#d",
    "title": "DACSS 603 Homework 4",
    "section": "d",
    "text": "d\nBelow the model is updated so that the baseline category for rank is “Prof.”\n\n\nCode\nsalary$rank &lt;- relevel(salary$rank, ref = \"Prof\")\n\nfit_2d &lt;- lm(salary ~ sex + degree + rank + year + ysdeg, data = salary)\n\nsummary(fit_2d)\n\n\n\nCall:\nlm(formula = salary ~ sex + degree + rank + year + ysdeg, data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4045.2 -1094.7  -361.5   813.2  9193.1 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  26864.81    1375.29  19.534  &lt; 2e-16 ***\nsexFemale     1166.37     925.57   1.260    0.214    \ndegreePhD     1388.61    1018.75   1.363    0.180    \nrankAsst    -11118.76    1351.77  -8.225 1.62e-10 ***\nrankAssoc    -5826.40    1012.93  -5.752 7.28e-07 ***\nyear           476.31      94.91   5.018 8.65e-06 ***\nysdeg         -124.57      77.49  -1.608    0.115    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2398 on 45 degrees of freedom\nMultiple R-squared:  0.855, Adjusted R-squared:  0.8357 \nF-statistic: 44.24 on 6 and 45 DF,  p-value: &lt; 2.2e-16\n\n\nThis change to the model does not affect its fit or the coefficients. Notice that rankAsst shows a decrease of -$11,118.76 in salary whereas in the previous model rankProf was an increase of $11,118.76 in salary."
  },
  {
    "objectID": "posts/dacss603hw4_LauraCollazo.html#e",
    "href": "posts/dacss603hw4_LauraCollazo.html#e",
    "title": "DACSS 603 Homework 4",
    "section": "e",
    "text": "e\nThis next model removes the variable rank from the model.\n\n\nCode\nfit_2e &lt;- lm(salary ~ sex + degree + year + ysdeg, data = salary)\n\nsummary(fit_2e)\n\n\n\nCall:\nlm(formula = salary ~ sex + degree + year + ysdeg, data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8146.9 -2186.9  -491.5  2279.1 11186.6 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 17183.57    1147.94  14.969  &lt; 2e-16 ***\nsexFemale   -1286.54    1313.09  -0.980 0.332209    \ndegreePhD   -3299.35    1302.52  -2.533 0.014704 *  \nyear          351.97     142.48   2.470 0.017185 *  \nysdeg         339.40      80.62   4.210 0.000114 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3744 on 47 degrees of freedom\nMultiple R-squared:  0.6312,    Adjusted R-squared:  0.5998 \nF-statistic: 20.11 on 4 and 47 DF,  p-value: 1.048e-09\n\n\nRemoving rank leads to ysdeg and degree now being statistically significant in the model. However, the adjusted R-square is lower than in the previous 2 models and the residual standard error is greater making this model not the best fit."
  },
  {
    "objectID": "posts/dacss603hw4_LauraCollazo.html#f",
    "href": "posts/dacss603hw4_LauraCollazo.html#f",
    "title": "DACSS 603 Homework 4",
    "section": "f",
    "text": "f\nThis final model creates a new variable new_hire using ysdeg. Those who were hired within 15 years or less were coded as 1 and everyone else 0. To avoid multicollinearity, the variable year was removed from the model. This is because it’s possible the years in faculty’s current rank are the same as years since the highest degree was earned (or in other words, years since hired).\n\n\nCode\nsalary$new_hire &lt;- ifelse(salary$ysdeg &lt;= 15, 1, 0)\n\nfit_2f &lt;- lm(salary ~ sex + degree + new_hire, data = salary)\n\nsummary(fit_2f)\n\n\n\nCall:\nlm(formula = salary ~ sex + degree + new_hire, data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8260.4 -3557.7  -462.6  3563.2 12098.5 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    28663       1155  24.821  &lt; 2e-16 ***\nsexFemale      -2716       1433  -1.896    0.064 .  \ndegreePhD      -1227       1372  -0.895    0.375    \nnew_hire       -7418       1306  -5.679 7.74e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4558 on 48 degrees of freedom\nMultiple R-squared:  0.4416,    Adjusted R-squared:  0.4067 \nF-statistic: 12.65 on 3 and 48 DF,  p-value: 3.231e-06\n\n\nThis model shows the null hypothesis should be rejected and indicates that faculty hired by the new dean are actually making a lower salary than those who were hired more than 15 years ago."
  },
  {
    "objectID": "posts/dacss603hw4_LauraCollazo.html#a-2",
    "href": "posts/dacss603hw4_LauraCollazo.html#a-2",
    "title": "DACSS 603 Homework 4",
    "section": "a",
    "text": "a\n\n\nCode\nfit_3a &lt;- lm(Price ~ Size + New, data = house.selling.price)\n\nsummary(fit_3a)\n\n\n\nCall:\nlm(formula = Price ~ Size + New, data = house.selling.price)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-205102  -34374   -5778   18929  163866 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -40230.867  14696.140  -2.738  0.00737 ** \nSize           116.132      8.795  13.204  &lt; 2e-16 ***\nNew          57736.283  18653.041   3.095  0.00257 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 53880 on 97 degrees of freedom\nMultiple R-squared:  0.7226,    Adjusted R-squared:  0.7169 \nF-statistic: 126.3 on 2 and 97 DF,  p-value: &lt; 2.2e-16\n\n\nThis first model examines how the size of a house and being new or not influences price. It reveals the variables are statistically significant. Furthermore, we learn that a 1 unit change in size leads to a $116.13 increase in price and when a house is new, it will cost $57,736.28 more than a house that is old when the size is held constant."
  },
  {
    "objectID": "posts/dacss603hw4_LauraCollazo.html#b-2",
    "href": "posts/dacss603hw4_LauraCollazo.html#b-2",
    "title": "DACSS 603 Homework 4",
    "section": "b",
    "text": "b\nThe equation for the predicted selling price when the home is new is: price = -40230.867 + 116.132Size + 57736.283New"
  },
  {
    "objectID": "posts/dacss603hw4_LauraCollazo.html#c-2",
    "href": "posts/dacss603hw4_LauraCollazo.html#c-2",
    "title": "DACSS 603 Homework 4",
    "section": "c",
    "text": "c\nThe predicted selling price for a home of 3000 square feed that is new is below.\n\n\nCode\ndf_new &lt;- data.frame(Size = 3000, New = 1)\n\npredict(fit_3a, newdata = df_new)\n\n\n       1 \n365900.2 \n\n\nThe predicted selling price for a home of 3000 square feed that is not new is below.\n\n\nCode\ndf_not_new &lt;- data.frame(Size = 3000, New = 0)\n\npredict(fit_3a, newdata = df_not_new)\n\n\n       1 \n308163.9"
  },
  {
    "objectID": "posts/dacss603hw4_LauraCollazo.html#d-1",
    "href": "posts/dacss603hw4_LauraCollazo.html#d-1",
    "title": "DACSS 603 Homework 4",
    "section": "d",
    "text": "d\nThe next model includes an interaction term between size and new.\n\n\nCode\nfit_3d &lt;- lm(Price ~ Size * New, data = house.selling.price)\n\nsummary(fit_3d)\n\n\n\nCall:\nlm(formula = Price ~ Size * New, data = house.selling.price)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-175748  -28979   -6260   14693  192519 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -22227.808  15521.110  -1.432  0.15536    \nSize           104.438      9.424  11.082  &lt; 2e-16 ***\nNew         -78527.502  51007.642  -1.540  0.12697    \nSize:New        61.916     21.686   2.855  0.00527 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 52000 on 96 degrees of freedom\nMultiple R-squared:  0.7443,    Adjusted R-squared:  0.7363 \nF-statistic: 93.15 on 3 and 96 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "posts/dacss603hw4_LauraCollazo.html#e-1",
    "href": "posts/dacss603hw4_LauraCollazo.html#e-1",
    "title": "DACSS 603 Homework 4",
    "section": "e",
    "text": "e\n\n\nCode\nggplot(house.selling.price,aes(y=Price,x=Size,color=factor(New)))+\n  geom_point()+\n  stat_smooth(method=\"lm\",se=TRUE)\n\n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "posts/dacss603hw4_LauraCollazo.html#f-1",
    "href": "posts/dacss603hw4_LauraCollazo.html#f-1",
    "title": "DACSS 603 Homework 4",
    "section": "f",
    "text": "f\nThe predicted selling price, using the model with interaction terms, for a home of 3000 square feed that is new is below.\n\n\nCode\npredict(fit_3d, newdata = df_new)\n\n\n       1 \n398307.5 \n\n\nThe predicted selling price, using the model with interaction terms, for a home of 3000 square feed that is not new is below.\n\n\nCode\npredict(fit_3d, newdata = df_not_new)\n\n\n       1 \n291087.4"
  },
  {
    "objectID": "posts/dacss603hw4_LauraCollazo.html#g",
    "href": "posts/dacss603hw4_LauraCollazo.html#g",
    "title": "DACSS 603 Homework 4",
    "section": "g",
    "text": "g\nThe predicted selling price, using the model with interaction terms, for a home of 1500 square feed that is new is below.\n\n\nCode\ndf_new &lt;- data.frame(Size = 1500, New = 1)\n\npredict(fit_3d, newdata = df_new)\n\n\n       1 \n148776.1 \n\n\nThe predicted selling price, using the model with interaction terms, for a home of 1500 square feed that is not new is below.\n\n\nCode\ndf_not_new &lt;- data.frame(Size = 1500, New = 0)\n\npredict(fit_3d, newdata = df_not_new)\n\n\n       1 \n134429.8 \n\n\nIn comparing the predictions for part F and G, it can be observed that the difference in selling price between a new and not new home increases as the the size of the home increases."
  },
  {
    "objectID": "posts/dacss603hw4_LauraCollazo.html#h",
    "href": "posts/dacss603hw4_LauraCollazo.html#h",
    "title": "DACSS 603 Homework 4",
    "section": "h",
    "text": "h\nI believe the model with the interaction term best represents the relationship of size and new to the outcome price. I’ve come to this conclusion as the model with the interaction term has a higher adjusted R-squared and lower residual standard error than the model without the interaction term."
  },
  {
    "objectID": "posts/FelixBetancourt_HW3.html",
    "href": "posts/FelixBetancourt_HW3.html",
    "title": "Homework 3",
    "section": "",
    "text": "Code\nknitr::opts_chunk$set(echo = TRUE, warning = FALSE)"
  },
  {
    "objectID": "posts/FelixBetancourt_HW3.html#homework-3",
    "href": "posts/FelixBetancourt_HW3.html#homework-3",
    "title": "Homework 3",
    "section": "Homework 3",
    "text": "Homework 3\nDACSS 603, Spring 2023\n\n\nCode\n# Loading packages\n\nsuppressPackageStartupMessages(library(dplyr))\nsuppressPackageStartupMessages(library(tidyverse))\nlibrary(formattable)\nsuppressPackageStartupMessages(library(kableExtra))\nlibrary(ggplot2)\nsuppressPackageStartupMessages(library(alr4))\n\n\nUnited Nations (Data file: UN11 in alr4) The data in the file UN11 contains several variables, including ppgdp, the gross national product per person in U.S. dollars, and fertility, the birth rate per 1000 females, both from the year 2009. The data are for 199 localities, mostly UN member countries, but also other areas such as Hong Kong that are not independent countries.The data were collected from the United Nations (2011). We will study the dependence of fertility on ppgdp.\n\n\nCode\nstr(UN11)\n\n\n'data.frame':   199 obs. of  6 variables:\n $ region   : Factor w/ 8 levels \"Africa\",\"Asia\",..: 2 4 1 1 3 5 2 3 8 4 ...\n $ group    : Factor w/ 3 levels \"oecd\",\"other\",..: 2 2 3 3 2 2 2 2 1 1 ...\n $ fertility: num  5.97 1.52 2.14 5.13 2 ...\n $ ppgdp    : num  499 3677 4473 4322 13750 ...\n $ lifeExpF : num  49.5 80.4 75 53.2 81.1 ...\n $ pctUrban : num  23 53 67 59 100 93 64 47 89 68 ...\n - attr(*, \"na.action\")= 'omit' Named int [1:34] 4 5 8 28 41 67 68 72 79 83 ...\n  ..- attr(*, \"names\")= chr [1:34] \"Am Samoa\" \"Andorra\" \"Antigua and Barbuda\" \"Br Virigin Is\" ...\n\n\n\nIdentify the predictor and the response.\n\nPredictor: gross national product per person (ppgdp) Response: fertility\n\nDraw the scatterplot of fertility on the vertical axis versus ppgdp on the horizontal axis and summarize the information in this graph. Does a straight-line mean function seem to be plausible for a summary of this graph?\n\n\n\nCode\nggplot(UN11, aes(x = ppgdp, y = fertility)) +\n  geom_point()\n\n\n\n\n\nSeems that the fertility rate varies a lot when the gross domestic product per person is between 0 and 12,500, but above that level the fertility rate seems more homogeneous. Doesn’t seem possible to have a straight line mean between these to variables, at least without making changes to the two distributions. The relationship seems non-linear.\n\nDraw the scatterplot of log(fertility) versus log(ppgdp) using natural logarithms. Does the simple linear regression model seem plausible for a summary of this graph? If you use a different base of logarithms, the shape of the graph won’t change, but the values on the axes will change.\n\n\n\nCode\nplot(UN11$ppgdp, UN11$fertility, log = \"xy\", pch = 16, col = \"blue\", main = \"Scatterplot with Log Axes for Ferlitity and GDP per person\", \n     xlab = \"GDP-PP (log scale)\", ylab = \"Fertility  (log scale)\")\n\n\n\n\n\nNow there is a visible negative relationship between Fertility and GDP per person. Fertility rate seems to decrease when the GDP is higher.\n\nAnnual income, in dollars, is an explanatory variable in a regression analysis. For a British version of the report on the analysis, all responses are converted to British pounds sterling (1 pound equals about 1.33 dollars, as of 2016).\n\n\nHow, if at all, does the slope of the prediction equation change?\n\nYes, the slope of the prediction equation will change. This is because the units of measurement for the explanatory variable have changed, which affects the scale and interpretation of the slope.\n\nHow, if at all, does the correlation change?\n\nThe correlation should not change. Converting the unit of measurement does not change the strength and direction of the relation between two variables.\n\nWater runoff in the Sierras (Data file: water in alr4) Can Southern California’s water supply in future years be predicted from past data? One factor affecting water availability is stream runoff.\n\nIf runoff could be predicted, engineers, planners, and policy makers could do their jobs more efficiently. The data file contains 43 years’ worth of precipitation measurements taken at six sites in the Sierra Nevada mountains (labeled APMAM, APSAB, APSLAKE, OPBPC, OPRC, and OPSLAKE) and stream runoff volume at a site near Bishop, California, labeled BSAAM. Draw the scatterplot matrix for these data and summarize the information available from these plots. (Hint: Use the pairs() function.)\n\n\nCode\npairs(water[,1:8])\n\n\n\n\n\nThe water stream runoff in Bishop seems to have a positive correlation with the volume of precipitation in certain sites of the sierra nevada, specifically with OPSLAKE, OPRC and OPBPC. This means that while more precipitation in those sites, the water stream runoff in Bishop is higher.\n\nProfessor ratings (Data file: Rateprof in alr4) In the website and online forum RateMyProfessors.com, students rate and comment on their instructors. Launched in 1999, the site includes millions of ratings on thousands of instructors. The data file includes the summaries of the ratings of 364 instructors at a large campus in the Midwest (Bleske-Rechek and Fritsch, 2011).\n\nEach instructor included in the data had at least 10 ratings over a several year period. Students provided ratings of 1–5 on quality, helpfulness, clarity, easiness of instructor’s courses, and raterInterest in the subject matter covered in the instructor’s courses. The data file provides the averages of these five ratings. Create a scatterplot matrix of these five variables. Provide a brief description of the relationships between the five ratings.\n\n\nCode\nstr(Rateprof)\n\n\n'data.frame':   366 obs. of  17 variables:\n $ gender         : Factor w/ 2 levels \"female\",\"male\": 2 2 2 2 2 2 2 2 2 2 ...\n $ numYears       : int  7 6 10 11 11 10 7 11 11 7 ...\n $ numRaters      : int  11 11 43 24 19 15 17 16 12 18 ...\n $ numCourses     : int  5 5 2 5 7 9 3 3 4 4 ...\n $ pepper         : Factor w/ 2 levels \"no\",\"yes\": 1 1 1 1 1 1 1 1 1 1 ...\n $ discipline     : Factor w/ 4 levels \"Hum\",\"SocSci\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ dept           : Factor w/ 48 levels \"Accounting\",\"Anthropology\",..: 17 42 3 17 45 45 45 17 34 17 ...\n $ quality        : num  4.64 4.32 4.79 4.25 4.68 ...\n $ helpfulness    : num  4.64 4.55 4.72 4.46 4.68 ...\n $ clarity        : num  4.64 4.09 4.86 4.04 4.68 ...\n $ easiness       : num  4.82 4.36 4.6 2.79 4.47 ...\n $ raterInterest  : num  3.55 4 3.43 3.18 4.21 ...\n $ sdQuality      : num  0.552 0.902 0.453 0.933 0.65 ...\n $ sdHelpfulness  : num  0.674 0.934 0.666 0.932 0.82 ...\n $ sdClarity      : num  0.505 0.944 0.413 0.999 0.582 ...\n $ sdEasiness     : num  0.405 0.505 0.541 0.588 0.612 ...\n $ sdRaterInterest: num  1.128 1.074 1.237 1.332 0.975 ...\n\n\nCode\nsubset_rate &lt;- Rateprof[, c(\"quality\", \"helpfulness\", \"clarity\", \"easiness\", \"raterInterest\")]\n\npairs(subset_rate)\n\n\n\n\n\nSeems an strong positive relationship among clarity, helpfulness and quality, then easiness seems to have a moderate positive relationship with clarity, helpfulness and quality, and if we see raterInterest as a dependant variable, it shows some patter of low relationship with those 4 qualities.\n\nFor the student.survey data file in the smss package, conduct regression analyses relating (by convention, y denotes the outcome variable, x denotes the explanatory variable)\n\n\ny = political ideology and x = religiosity, (ii) y = high school GPA and x = hours of TV watching. (You can use student.survey in the R console, after loading the package, to see what each variable means.)**\n\n\n\nCode\nsuppressPackageStartupMessages(library(smss))\ndata(\"student.survey\")\nstudent &lt;- student.survey\nstr(student)\n\n\n'data.frame':   60 obs. of  18 variables:\n $ subj: int  1 2 3 4 5 6 7 8 9 10 ...\n $ ge  : Factor w/ 2 levels \"f\",\"m\": 2 1 1 1 2 2 2 1 2 2 ...\n $ ag  : int  32 23 27 35 23 39 24 31 34 28 ...\n $ hi  : num  2.2 2.1 3.3 3.5 3.1 3.5 3.6 3 3 4 ...\n $ co  : num  3.5 3.5 3 3.2 3.5 3.5 3.7 3 3 3.1 ...\n $ dh  : int  0 1200 1300 1500 1600 350 0 5000 5000 900 ...\n $ dr  : num  5 0.3 1.5 8 10 3 0.2 1.5 2 2 ...\n $ tv  : num  3 15 0 5 6 4 5 5 7 1 ...\n $ sp  : int  5 7 4 5 6 5 12 3 5 1 ...\n $ ne  : int  0 5 3 6 3 7 4 3 3 2 ...\n $ ah  : int  0 6 0 3 0 0 2 1 0 1 ...\n $ ve  : logi  FALSE FALSE FALSE FALSE FALSE FALSE ...\n $ pa  : Factor w/ 3 levels \"d\",\"i\",\"r\": 3 1 1 2 2 1 2 2 2 2 ...\n $ pi  : Ord.factor w/ 7 levels \"very liberal\"&lt;..: 6 2 2 4 1 2 2 2 1 3 ...\n $ re  : Ord.factor w/ 4 levels \"never\"&lt;\"occasionally\"&lt;..: 3 2 3 2 1 2 2 2 2 1 ...\n $ ab  : logi  FALSE FALSE FALSE FALSE FALSE FALSE ...\n $ aa  : logi  FALSE FALSE FALSE FALSE FALSE FALSE ...\n $ ld  : logi  FALSE NA NA FALSE FALSE NA ...\n\n\n\nGraphically portray how the explanatory variable relates to the outcome variable in each of the two cases\n\n\n\nCode\nggplot(student, aes(x=re, y=pi)) + geom_point() +\n  labs(title=\"Scatter Plot Religion and Political Ideology\",\n        x =\"Frequency attending religious services\", y = \"Political ideology\")\n\n\n\n\n\nSeems that there a correlation between frequency of attending relegious services and political ideology. Specifically, seems that being conservative is related to attending more services.\n\n\nCode\nggplot(student, aes(x=tv, y=hi)) + geom_point() +\n  labs(title=\"Scatter Plot Hours watching TV and High School GPA\",\n        x =\"Average hours watching TV / week\", y = \"HS GPA Score\")\n\n\n\n\n\nIn the case of GPA score in High school and average hours watching TV per week, it is difficult to see a clear relationshp, but we can say that seems that when the studnet watch TV 10 hours or less per week, there is no correlation with the GPA, but we can say that there is an light trend to get lower GPA when the student watch TV more than 10 hours per week.\n\nSummarize and interpret results of inferential analyses.\n\n\n\nCode\nstudent &lt;- student.survey %&gt;%\n mutate(pi_num = case_when(\n         pi == \"very liberal\" ~ 1,\n         pi == \"liberal\" ~ 2,\n         pi == \"slightly liberal\" ~ 3,\n         pi == \"moderate\" ~ 4,\n         pi == \"slightly conservative\" ~ 5,\n         pi == \"conservative\" ~ 6,\n         pi == \"very conservative\" ~ 7,\n         ))\n\nstudent$pi_num &lt;- as.numeric(student$pi_num)\n\n\nmodel1 &lt;- lm(pi_num ~ re, data=student)\nsummary(model1)\n\n\n\nCall:\nlm(formula = pi_num ~ re, data = student)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.8889 -0.5172 -0.2667  1.2040  2.7333 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   3.5253     0.1958  18.000  &lt; 2e-16 ***\nre.L          2.1864     0.3919   5.579 7.27e-07 ***\nre.Q          0.1049     0.3917   0.268    0.790    \nre.C         -0.6958     0.3915  -1.777    0.081 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.315 on 56 degrees of freedom\nMultiple R-squared:  0.3872,    Adjusted R-squared:  0.3544 \nF-statistic:  11.8 on 3 and 56 DF,  p-value: 4.282e-06\n\n\nWhen we see the rregression output, we can see that seems there is a significant incidence from frequency to attend religious services and political ideology, for instance, seems that attending to religious services more frequently is associated to being more conservative.\n\n\nCode\nmodel2 &lt;- lm(hi ~ tv, data=student)\nsummary(model2)\n\n\n\nCall:\nlm(formula = hi ~ tv, data = student)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.2583 -0.2456  0.0417  0.3368  0.7051 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  3.441353   0.085345  40.323   &lt;2e-16 ***\ntv          -0.018305   0.008658  -2.114   0.0388 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4467 on 58 degrees of freedom\nMultiple R-squared:  0.07156,   Adjusted R-squared:  0.05555 \nF-statistic: 4.471 on 1 and 58 DF,  p-value: 0.03879\n\n\nSeems that there is a negative relationship between average TV hours per week incidence in the GPA score, where as more TV hours watching TV is related to lower GPA score, specifically for every additional hour of TV per week the score can get lower in 3 points. And even thought the relationship is significant at 0.05, still that incidence from TV hours to score seems weak or low. I would elminate 2-3 outliners (the cases with more than 20 hours per week) and look again at the regression output, but I can anticipate non-significant incidence from hours watching TV.\nAlso, important to note that R squared is very low, so TV hours explain very little of the GPA score."
  },
  {
    "objectID": "posts/FelixBetancourt_HW1_v2.html",
    "href": "posts/FelixBetancourt_HW1_v2.html",
    "title": "Homework 1",
    "section": "",
    "text": "Code\nknitr::opts_chunk$set(echo = TRUE, warning = FALSE)"
  },
  {
    "objectID": "posts/FelixBetancourt_HW1_v2.html#homework-1",
    "href": "posts/FelixBetancourt_HW1_v2.html#homework-1",
    "title": "Homework 1",
    "section": "Homework 1",
    "text": "Homework 1\nDACSS 603, Spring 2023\n\n\nCode\n# Loading packages\n\nsuppressPackageStartupMessages(library(dplyr))\nsuppressPackageStartupMessages(library(tidyverse))\nlibrary(formattable)\n\n\nError in library(formattable): there is no package called 'formattable'\n\n\nCode\nsuppressPackageStartupMessages(library(kableExtra))\n\n\nError in library(kableExtra): there is no package called 'kableExtra'\n\n\nCode\nlibrary(ggplot2)\nlibrary(readxl)\n\n# Setting working directory and loading dataset.\n\n\n\nlung &lt;- read_excel(\"_data/LungCapData.xls\")"
  },
  {
    "objectID": "posts/FelixBetancourt_HW1_v2.html#part-a---lung-capacity-dataset",
    "href": "posts/FelixBetancourt_HW1_v2.html#part-a---lung-capacity-dataset",
    "title": "Homework 1",
    "section": "Part A - Lung Capacity Dataset",
    "text": "Part A - Lung Capacity Dataset\nLet’s first explore the database\nStructure\n\n\nCode\nstr(lung)\n\n\ntibble [725 × 6] (S3: tbl_df/tbl/data.frame)\n $ LungCap  : num [1:725] 6.47 10.12 9.55 11.12 4.8 ...\n $ Age      : num [1:725] 6 18 16 14 5 11 8 11 15 11 ...\n $ Height   : num [1:725] 62.1 74.7 69.7 71 56.9 58.7 63.3 70.4 70.5 59.2 ...\n $ Smoke    : chr [1:725] \"no\" \"yes\" \"no\" \"no\" ...\n $ Gender   : chr [1:725] \"male\" \"female\" \"female\" \"male\" ...\n $ Caesarean: chr [1:725] \"no\" \"no\" \"yes\" \"no\" ...\n\n\nSummary\n\n\nCode\nLC_table1 &lt;- summary(lung)\nprint(LC_table1)\n\n\n    LungCap            Age            Height         Smoke          \n Min.   : 0.507   Min.   : 3.00   Min.   :45.30   Length:725        \n 1st Qu.: 6.150   1st Qu.: 9.00   1st Qu.:59.90   Class :character  \n Median : 8.000   Median :13.00   Median :65.40   Mode  :character  \n Mean   : 7.863   Mean   :12.33   Mean   :64.84                     \n 3rd Qu.: 9.800   3rd Qu.:15.00   3rd Qu.:70.30                     \n Max.   :14.675   Max.   :19.00   Max.   :81.80                     \n    Gender           Caesarean        \n Length:725         Length:725        \n Class :character   Class :character  \n Mode  :character   Mode  :character  \n                                      \n                                      \n                                      \n\n\n\na) What does the distribution of LungCap look like?\n\n\nCode\n# Density histogram\nhist(lung$LungCap, freq = FALSE, col=\"white\", main = \"Histogram of Probability Density for Lung Capacity\", xlab = \"Values\", ylab = \"Density\")\nlines(density(lung$LungCap), col = \"blue\", lwd = 2)\n\n\n\n\n\nDistribution looks like a normal distribution, most of the observation close to the mean and a few cases close to the extremes (0.5 and 14.6).\n\n\nb) Compare the probability distribution of the LungCap with respect to Males and Females? (Hint: make boxplots separated by gender using the boxplot() function)\n\n\nCode\nbox_plot_crop&lt;-ggplot(data=lung, aes(x=Gender, y=LungCap, fill=Gender)) \nbox_plot_crop+ geom_boxplot() +\n  theme(legend.position = \"right\") +\n  theme (axis.text.x=element_blank(),\n        axis.ticks.x=element_blank())+\n  coord_cartesian(ylim =  c(0, 15))+\n  labs(title=\"Box Plot - Lung Capacity\",\n        x =\"Gender\", y = \"Frequency\")\n\n\n\n\n\n\n\nCode\nsuppressPackageStartupMessages (library(hrbrthemes))\n\n\nError in library(hrbrthemes): there is no package called 'hrbrthemes'\n\n\nCode\nsuppressPackageStartupMessages (library(viridis))\n\n\nError in library(viridis): there is no package called 'viridis'\n\n\nCode\nGender_d &lt;- ggplot(data=lung, aes(x=LungCap, group=Gender, fill=Gender)) +\n    geom_density(adjust=1.5, alpha=.4) +\n    theme_ipsum()\n\n\nError in theme_ipsum(): could not find function \"theme_ipsum\"\n\n\nCode\nGender_d\n\n\nError in eval(expr, envir, enclos): object 'Gender_d' not found\n\n\n\n\nCode\nLC_table &lt;- lung %&gt;%  \n group_by(Gender) %&gt;%\n  summarise(N = n(), LC.Mean = mean(LungCap, na.rm=TRUE), LC.Median = median(LungCap, na.rm=TRUE), LC.SD = sd(LungCap, na.rm=TRUE))\n  \nLC_table_o &lt;- LC_table[with (LC_table, order(-LC.Mean)),]\nformattable(LC_table_o) %&gt;% \n  kable(\"html\", escape = F, caption = \"Summary of Lung Capacity Grouped by Gender\", align = c(\"l\", \"c\", \"c\", \"c\", \"c\", \"c\")) %&gt;% \n  kable_classic(full_width = F, html_font = \"Cambria\")\n\n\nError in kable_classic(., full_width = F, html_font = \"Cambria\"): could not find function \"kable_classic\"\n\n\nIt was not possible for me to plot a Box Plot with Density like I did for Histogram but I was able to plot separately the Box Plot and Density. It seems that the distribution for the two genders are similar while the Male distribution shows a bit higher average lung capacity.\n\n\nc. Compare the mean lung capacities for smokers and non-smokers. Does it make sense?\nLet’s visualize the differences\n\n\nCode\nbox_plot_crop &lt;- ggplot(data=lung, aes(x=Smoke, y=LungCap, fill=Smoke)) \nbox_plot_crop+ geom_boxplot() +\n  theme(legend.position = \"right\") +\n  theme (axis.text.x=element_blank(),\n        axis.ticks.x=element_blank())+\n  coord_cartesian(ylim =  c(0, 15))+\n  labs(title=\"Box Plot - Lung Capacity\",\n        x =\"Smoke\", y = \"Density\")\n\n\n\n\n\nNow let’s see the specific numbers in this summary\n\n\nCode\nLC_table_s &lt;- lung %&gt;%  \n group_by(Smoke) %&gt;%\n  summarise(N = n(), LC.Mean = mean(LungCap, na.rm=TRUE), LC.Median = median(LungCap, na.rm=TRUE), LC.SD = sd(LungCap, na.rm=TRUE))\n\nLC_table_s2 &lt;- LC_table_s[with (LC_table_s, order(-LC.Mean)),]\nformattable(LC_table_s2) %&gt;% \n  kable(\"html\", escape = F, caption = \"Summary of Lung Capacity Grouped by Smoker\", align = c(\"l\", \"c\", \"c\", \"c\", \"c\", \"c\")) %&gt;% \n  kable_classic(full_width = F, html_font = \"Cambria\")\n\n\nError in kable_classic(., full_width = F, html_font = \"Cambria\"): could not find function \"kable_classic\"\n\n\nSeems that smokers has more lung capacity than non-smokers, and I would expect the opposite. It is interesting to note that the lung capacity in smokers is more homogeneous distribution (lower SD) compared to non-smokers, even though the number of smokers is significantly smaller than non-smokers.\n\n\nd. Examine the relationship between Smoking and Lung Capacity within age groups: “less than or equal to 13”, “14 to 15”, “16 to 17”, and “greater than or equal to 18”.\n\n\ne. Compare the lung capacities for smokers and non-smokers within each age group. Is your answer different from the one in part c. What could possibly be going on here?\n\n\nCode\n# First, let's create the categories for age\n\nlung2 &lt;-lung %&gt;%\n  mutate(Age_Cat = case_when(\n        Age &gt;= 0 & Age &lt;= 13 ~ \"13 or less\",\n        Age &gt;= 14 & Age &lt;= 15 ~ \"14 to 15\" ,\n        Age &gt;= 16 & Age &lt;= 17 ~ \"16 to 17\" ,\n        Age &gt;= 18 ~ \"18 or more\" ,\n        ))\n\nbox_plot_crop2&lt;-ggplot(data=lung2, aes(x=Smoke, y=LungCap, fill=Smoke)) \nbox_plot_crop2+ geom_boxplot() +\n  theme(legend.position = \"right\") +\n  theme (axis.text.x=element_blank(),\n        axis.ticks.x=element_blank())+\n  coord_cartesian(ylim =  c(0, 15))+\n  labs(title=\"Box Plot - Lung Capacity\",\n        x =\"Smoke\", y = \"Density\")+\n  facet_wrap(.~Age_Cat, scales= \"free\")\n\n\n\n\n\nFrom this graph I can note the following:\n\nExcept for the group of 13 or less, non-smokers shows higher lung capacity compared with smokers.\nOn the other hand, seems that the Lung Capacity increases with the years regardless the smoke condition.\n\nLet’s see more detailed numbers. I want to see the Lung Capacity Means and dispersion for each group. for this we will do a crosstab showing n, mean, median and sd.\nFirst let’s see the frequencies for both variables in a crosstab.\n\n\nCode\nxtabs(~Age_Cat+Smoke, data=lung2)\n\n\n            Smoke\nAge_Cat       no yes\n  13 or less 401  27\n  14 to 15   105  15\n  16 to 17    77  20\n  18 or more  65  15\n\n\nMean Lung Capacity by Age Group and Smoke condition\n\n\nCode\nwith(lung2, tapply(LungCap, list(Age_Group=Age_Cat,Smoker=Smoke), mean) )\n\n\n            Smoker\nAge_Group           no       yes\n  13 or less  6.358746  7.201852\n  14 to 15    9.138810  8.391667\n  16 to 17   10.469805  9.383750\n  18 or more 11.068846 10.513333\n\n\nMedian Lung Capacity by Age Group and Smoke condition\n\n\nCode\nwith(lung2, tapply(LungCap, list(Age_Group=Age_Cat,Smoker=Smoke), median) )\n\n\n            Smoker\nAge_Group        no    yes\n  13 or less  6.575  7.025\n  14 to 15    9.000  8.475\n  16 to 17   10.600  9.550\n  18 or more 10.850 10.475\n\n\nStandard Deviation Lung Capacity by Age Group and Smoke condition\n\n\nCode\nwith(lung2, tapply(LungCap, list(Age_Group=Age_Cat,Smoker=Smoke), sd) )\n\n\n            Smoker\nAge_Group          no      yes\n  13 or less 2.214412 1.577728\n  14 to 15   1.546130 1.437497\n  16 to 17   1.536745 1.326136\n  18 or more 1.555139 1.250959\n\n\nClearly the lung capacity is higher for non smokers within each age group except 13 or less years old group, where the smoker’s mean (and median) is higher than non-smokers.\nAs noted before it is also clear that the lung capacity increases with the age regardless the smoke condition.\nHowever it is interesting to note that:\n\nThe number of cases in the group 13 or less is aout the 50% of the whole sample. So it is explaining the findings in the question part “c” above (higher overall Lung Capacity average for smokers).\nThe lung capacity makes a bigger jump for non-smokers from 13 or less to the next bracket (14-15 years) compared to smokers. In other words, non-smokers close the gap and exceed smokers when they pass the 13 years.\n\nIt seems that smoking in early ages affects lungs capacity and making it falling behind consistently from non-smokers as getting older."
  },
  {
    "objectID": "posts/FelixBetancourt_HW1_v2.html#part-b---let-x-number-of-prior-convictions-for-prisoners-at-a-state-prison-at-which-there-are-810-prisoners.",
    "href": "posts/FelixBetancourt_HW1_v2.html#part-b---let-x-number-of-prior-convictions-for-prisoners-at-a-state-prison-at-which-there-are-810-prisoners.",
    "title": "Homework 1",
    "section": "Part B - Let X = number of prior convictions for prisoners at a state prison at which there are 810 prisoners.",
    "text": "Part B - Let X = number of prior convictions for prisoners at a state prison at which there are 810 prisoners.\n\n\nCode\n# create the frequency table\nconvicted &lt;- data.frame(\n  p_convic = c(0, 1, 2, 3, 4),\n  freq = c(128, 434, 160, 64, 24))\n\n\nFrequency Table and Cumulative Frequency Table:\n\n\nCode\nconv.tbl &lt;- xtabs(freq ~ p_convic, data=convicted)\nprint (conv.tbl)\n\n\np_convic\n  0   1   2   3   4 \n128 434 160  64  24 \n\n\nCode\ncumfreq_data &lt;- cumsum(conv.tbl)\nprint (cumfreq_data)\n\n\n  0   1   2   3   4 \n128 562 722 786 810 \n\n\n\n\nCode\n# \nprob_data &lt;- conv.tbl/810\n\nprob_data2 &lt;- as.vector(prob_data)\nprob_data3 &lt;- data.frame (\n  p_convic = c(convicted$p_convic),\n  prob = c(prob_data2))\n\n\n\na) What is the probability that a randomly selected inmate has exactly 2 prior convictions?\n\n\nCode\nsum(prob_data3[which(prob_data3$p_convic == 2), 2])\n\n\n[1] 0.1975309\n\n\nThe Probability is 19.7%\n\n\nb) What is the probability that a randomly selected inmate has fewer than 2 prior convictions?\n\n\nCode\nsum(prob_data3[which(prob_data3$p_convic &lt; 2), 2])\n\n\n[1] 0.6938272\n\n\nThe Probability is 69.3%\n\n\nc) What is the probability that a randomly selected inmate has 2 or fewer prior convictions?\n\n\nCode\nsum(prob_data3[which(prob_data3$p_convic &lt;= 2), 2])\n\n\n[1] 0.891358\n\n\nThe Probability is 89.1%\n\n\nd) What is the probability that a randomly selected inmate has more than 2 prior convictions?\n\n\nCode\nsum(prob_data3[which(prob_data3$p_convic &gt; 2), 2])\n\n\n[1] 0.108642\n\n\nThe Probability is 10.8%\n\n\ne) What is the expected value for the number of prior convictions?\n\n\nCode\np_convic2 &lt;- as.vector(convicted$p_convic)\nconvicted_prob &lt;- as.vector(prob_data)\n\nExpected_mean &lt;- sum(p_convic2*convicted_prob)\nExpected_mean\n\n\n[1] 1.28642\n\n\nThe long term mean for prior convictions is 1.29\n\n\nf) Calculate the variance and the standard deviation for the Prior Convictions.\nVariance\n\n\nCode\n#Variance\n\nVar_c &lt;- var(rep(convicted$p_convic, convicted$freq))\nVar_c\n\n\n[1] 0.8572937\n\n\nStandard Deviation\n\n\nCode\nSD_c &lt;- sd(rep(convicted$p_convic, convicted$freq))\nSD_c\n\n\n[1] 0.9259016"
  },
  {
    "objectID": "posts/AdithyaParupudi_hw2.html",
    "href": "posts/AdithyaParupudi_hw2.html",
    "title": "Homework 2",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.1     ✔ purrr   0.3.4\n✔ tibble  3.1.8     ✔ dplyr   1.0.9\n✔ tidyr   1.2.1     ✔ stringr 1.4.1\n✔ readr   2.1.3     ✔ forcats 0.5.2\n\n\nWarning: package 'ggplot2' was built under R version 4.2.2\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\nCode\nlibrary(hrbrthemes)\n\n\nNOTE: Either Arial Narrow or Roboto Condensed fonts are required to use these themes.\n      Please use hrbrthemes::import_roboto_condensed() to install Roboto Condensed and\n      if Arial Narrow is not on your system, please see https://bit.ly/arialnarrow\n\n\nCode\nlibrary(viridis)\n\n\nWarning: package 'viridis' was built under R version 4.2.2\n\n\nLoading required package: viridisLite\n\n\nCode\nlibrary(readxl)"
  },
  {
    "objectID": "posts/AdithyaParupudi_hw2.html#a",
    "href": "posts/AdithyaParupudi_hw2.html#a",
    "title": "Homework 2",
    "section": "4A",
    "text": "4A\nWe assume that the sample is random and that the population has a normal distribution. Null hypothesis: H0: μ = 500 Alternative hypothesis: Ha: μ ≠ 500 We will reject the null hypothesis at a p-value, p &lt;= 0.05\n\n\nCode\n# defining variables\ns_mean &lt;- 410\nμ &lt;- 500\ns_sd &lt;- 90\ns_size &lt;- 9\n\n\n\n\nCode\n# Calculating test-statistic\nt_score &lt;- (s_mean-μ)/(s_sd/sqrt(s_size))\nt_score\n\n\n[1] -3\n\n\n\n\nCode\n# Calculating p-value\n\np &lt;- 2*pt(t_score, s_size-1)\np\n\n\n[1] 0.01707168\n\n\nThe test-statistic is -3 and p-value is 0.01707168. As p-value is less than the 0.05, we reject the null hypothesis. Therefore, the mean income of female employees is not equal to $500."
  },
  {
    "objectID": "posts/AdithyaParupudi_hw2.html#b",
    "href": "posts/AdithyaParupudi_hw2.html#b",
    "title": "Homework 2",
    "section": "4B",
    "text": "4B\nWe assume that the sample is random and that the population has a normal distribution. Null hypothesis: H0: μ = 500 Alternative hypothesis: Ha: μ &lt; 500 We will reject the null hypothesis at a p-value less than 0.05\n\n\nCode\np_val &lt;- pt(t_score, s_size-1, lower.tail = TRUE)\np_val\n\n\n[1] 0.008535841\n\n\nThe p-value is 0.008535841. As p-value is less than the 0.05, we reject the null hypothesis. Therefore, the mean income of female employees is less than $500."
  },
  {
    "objectID": "posts/AdithyaParupudi_hw2.html#c",
    "href": "posts/AdithyaParupudi_hw2.html#c",
    "title": "Homework 2",
    "section": "4C",
    "text": "4C\nWe assume that the sample is random and that the population has a normal distribution. Null hypothesis: H0: μ = 500 Alternative hypothesis: Ha: μ &gt; 500 We will reject the null hypothesis at a p-value less than 0.05\n\n\nCode\np_val_c &lt;- pt(t_score, s_size-1, lower.tail = FALSE)\np_val_c\n\n\n[1] 0.9914642\n\n\nThe p-value is 0.9914642. As p-value is less than the 0.05, we reject the null hypothesis. Therefore, the mean income of female employees is greater than $500."
  },
  {
    "objectID": "posts/AdithyaParupudi_hw2.html#a-1",
    "href": "posts/AdithyaParupudi_hw2.html#a-1",
    "title": "Homework 2",
    "section": "5A",
    "text": "5A\nWe assume that the sample is random and that the population has a normal distribution. Null hypothesis: H0: μ = 500 Alternative hypothesis: Ha: μ ≠ 500 We will reject the null hypothesis at a p-value less than 0.05\n\n\nCode\n# Calculating t-statistic and p-value for Jones\ns_mean &lt;- 519.5\nμ &lt;- 500\nse &lt;- 10\ns_size &lt;- 1000\n\njames_t &lt;- (s_mean-μ)/se\njames_t\n\n\n[1] 1.95\n\n\nCode\np &lt;- 2*pt(james_t, s_size-1, lower.tail = FALSE)\np\n\n\n[1] 0.05145555\n\n\n\n\nCode\n# Calculating t-statistic and p-value for Smith\ns_mean &lt;- 519.7\nμ &lt;- 500\nse &lt;- 10\ns_size &lt;- 1000\n\nsmith_t &lt;- (s_mean-μ)/se\nsmith_t\n\n\n[1] 1.97\n\n\nCode\np &lt;- 2*pt(smith_t, s_size-1, lower.tail = FALSE)\np\n\n\n[1] 0.04911426\n\n\nThe test-statistic is 1.95, p-value is 0.05145555 for Jones and the test-statistic is 1.97, p-value is 0.05145555 for Smith."
  },
  {
    "objectID": "posts/AdithyaParupudi_hw2.html#b-1",
    "href": "posts/AdithyaParupudi_hw2.html#b-1",
    "title": "Homework 2",
    "section": "5B",
    "text": "5B\nThe p-value is 0.05145555 for Jones. As p-value is greater than the 0.05, we fail to reject the null hypothesis. The p-value is 0.04911426 for Jones. As p-value is less than the 0.05, we reject the null hypothesis. Therefore, the result is statistically significant for Smith, but not Jones."
  },
  {
    "objectID": "posts/AdithyaParupudi_hw2.html#c-1",
    "href": "posts/AdithyaParupudi_hw2.html#c-1",
    "title": "Homework 2",
    "section": "5C",
    "text": "5C\nIf we fail to report the P-value and simply state whether the P-value is less than/equal to or greater than the defined significance level of the test, one cannot determine the strength of the conclusion. In the Jones/Smith example, reporting the results only as P ≤ 0.05 versus P &gt; 0.05 will lead to different conclusions about very similar results."
  },
  {
    "objectID": "posts/Jerin_Jacob_HW1.html",
    "href": "posts/Jerin_Jacob_HW1.html",
    "title": "Homework 1",
    "section": "",
    "text": "Code\nlibrary(readxl)\nlibrary(dplyr)\nlibrary(magrittr)\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/Jerin_Jacob_HW1.html#loading-the-data",
    "href": "posts/Jerin_Jacob_HW1.html#loading-the-data",
    "title": "Homework 1",
    "section": "Loading the Data",
    "text": "Loading the Data\n\n\nCode\ndf &lt;- read_excel(\"_data/LungCapData.xls\")\ndf\n\n\n# A tibble: 725 × 6\n   LungCap   Age Height Smoke Gender Caesarean\n     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;    \n 1    6.48     6   62.1 no    male   no       \n 2   10.1     18   74.7 yes   female no       \n 3    9.55    16   69.7 no    female yes      \n 4   11.1     14   71   no    male   no       \n 5    4.8      5   56.9 no    male   no       \n 6    6.22    11   58.7 no    female no       \n 7    4.95     8   63.3 no    male   yes      \n 8    7.32    11   70.4 no    male   no       \n 9    8.88    15   70.5 no    male   no       \n10    6.8     11   59.2 no    male   no       \n# … with 715 more rows"
  },
  {
    "objectID": "posts/Jerin_Jacob_HW1.html#a",
    "href": "posts/Jerin_Jacob_HW1.html#a",
    "title": "Homework 1",
    "section": "A",
    "text": "A\n\n\nCode\nsummary(df)\n\n\n    LungCap            Age            Height         Smoke          \n Min.   : 0.507   Min.   : 3.00   Min.   :45.30   Length:725        \n 1st Qu.: 6.150   1st Qu.: 9.00   1st Qu.:59.90   Class :character  \n Median : 8.000   Median :13.00   Median :65.40   Mode  :character  \n Mean   : 7.863   Mean   :12.33   Mean   :64.84                     \n 3rd Qu.: 9.800   3rd Qu.:15.00   3rd Qu.:70.30                     \n Max.   :14.675   Max.   :19.00   Max.   :81.80                     \n    Gender           Caesarean        \n Length:725         Length:725        \n Class :character   Class :character  \n Mode  :character   Mode  :character  \n                                      \n                                      \n                                      \n\n\nCode\nhist(df$LungCap, xlab = \"Lung Capacity\", main = \"\", freq = F)\n\n\n\n\n\nThe histogram shows that the distribution is almost a normal distribution with most of the values close to the mean. ## B\n\n\nCode\nboxplot(LungCap ~ Gender, df)\n\n\n\n\n\nThe minimum, first quartile, median, third quartile and maximum, all of them appear to be slightly higher for males than females."
  },
  {
    "objectID": "posts/Jerin_Jacob_HW1.html#c",
    "href": "posts/Jerin_Jacob_HW1.html#c",
    "title": "Homework 1",
    "section": "C",
    "text": "C\n\n\nCode\ndff&lt;- df |&gt;\n  group_by(Smoke) |&gt;\n  summarise(LungCap = mean(LungCap))\n\n\nLung capacity of non-smokers is higher than that of smokers which is against the expectation!"
  },
  {
    "objectID": "posts/Jerin_Jacob_HW1.html#d",
    "href": "posts/Jerin_Jacob_HW1.html#d",
    "title": "Homework 1",
    "section": "D",
    "text": "D\n\nAge less than or equal to 13\n\n\nCode\ndf1 &lt;- df |&gt;\n  filter(Age &lt;= 13) |&gt;\n  group_by(Smoke)|&gt;\n  summarise(LungCap = mean(LungCap)) |&gt;\n  mutate(Age = \"&lt;=13\")\ndf1\n\n\n# A tibble: 2 × 3\n  Smoke LungCap Age  \n  &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt;\n1 no       6.36 &lt;=13 \n2 yes      7.20 &lt;=13 \n\n\n\n\nAge 14 & 15\n\n\nCode\ndf2 &lt;- df |&gt;\n  filter(Age &gt;= 14 & Age &lt;= 15) |&gt;\n  group_by(Smoke)|&gt;\n  summarise(LungCap = mean(LungCap))|&gt;\n  mutate(Age = \"14&15\")\nclass(df2)\n\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n\n\n\nAge 16 to 17\n\n\nCode\ndf3 &lt;- df |&gt;\n  filter(Age &gt;= 16 & Age &lt;= 17) |&gt;\n  group_by(Smoke)|&gt;\n  summarise(LungCap = mean(LungCap))|&gt;\n  mutate(Age = \"16&17\")\n\n\n\n\nAge greater than or equal to 18\n\n\nCode\ndf4 &lt;- df |&gt;\n  filter(Age &gt;= 18) |&gt;\n  group_by(Smoke)|&gt;\n  summarise(LungCap = mean(LungCap))|&gt;\n  mutate(Age = \"&gt;=18\")"
  },
  {
    "objectID": "posts/Jerin_Jacob_HW1.html#e",
    "href": "posts/Jerin_Jacob_HW1.html#e",
    "title": "Homework 1",
    "section": "E",
    "text": "E\n\n\nCode\nnew_df &lt;- rbind(df1,df2, df3,df4)\nnew_df\n\n\n# A tibble: 8 × 3\n  Smoke LungCap Age  \n  &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt;\n1 no       6.36 &lt;=13 \n2 yes      7.20 &lt;=13 \n3 no       9.14 14&15\n4 yes      8.39 14&15\n5 no      10.5  16&17\n6 yes      9.38 16&17\n7 no      11.1  &gt;=18 \n8 yes     10.5  &gt;=18 \n\n\nCode\nggplot(new_df, aes(fill=Smoke, y=LungCap, x=Age)) +\n  geom_bar(position='dodge', stat='identity')\n\n\nError in ggplot(new_df, aes(fill = Smoke, y = LungCap, x = Age)): could not find function \"ggplot\"\n\n\nCode\nggplot(dff, aes(y=LungCap, x=Smoke, fill = Smoke)) +\n  geom_bar(position='dodge', stat='identity')  \n\n\nError in ggplot(dff, aes(y = LungCap, x = Smoke, fill = Smoke)): could not find function \"ggplot\"\n\n\nOnly age group that showed similar result that of step C is &lt;=13. The analysis could go wrong if the lung capacity of smokers and non smokers are studied without considering the age."
  },
  {
    "objectID": "posts/AlexisGamez_HW2.html",
    "href": "posts/AlexisGamez_HW2.html",
    "title": "Blog Post #2",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/AlexisGamez_HW2.html#a",
    "href": "posts/AlexisGamez_HW2.html#a",
    "title": "Blog Post #2",
    "section": "A)",
    "text": "A)\nTest whether the mean income of female employees differs from $500 per week. Include assumptions, hypotheses, test statistic, and P-value. Interpret the result.\nAssumptions:\n\nThe sample provided is randomly selected and representative of the population\nPopulation is normally distributed\nThe variance of both populations are equal\n\nNull Hypothesis(H0): μ = 500\nAlternative Hypothesis(HA): μ ≠ 500\nt-test Formula: t = (Y bar - μ)/(Standard Deviation/sqrt(n))\n\n\nCode\nt &lt;- (410-500)/(90/sqrt(9))\nt\n\n\n[1] -3\n\n\n\n\nCode\np &lt;- 2*pt(t, 9-1)\np\n\n\n[1] 0.01707168\n\n\nThe test statistic and p-value we receive are equal to -3 and 0.0171 respectively. Because our p-value &lt; 0.05, we are able to reject the null hypothesis meaning that the mean income of female employees differs from $500 per week."
  },
  {
    "objectID": "posts/AlexisGamez_HW2.html#b",
    "href": "posts/AlexisGamez_HW2.html#b",
    "title": "Blog Post #2",
    "section": "B)",
    "text": "B)\nReport the P-value for Ha: μ &lt; 500. Interpret.\nHere we’re being asked to calculate a one sided p-value where the alternative hypothesis is that μ &lt; 500. In order to calculate said p-value, we must specify within our function that we will be look at the lower tail of the data.\nH0: μ &gt;= 500\nHA: μ &lt; 500\n\n\nCode\np1 &lt;- pt(t, 9-1, lower.tail = T)\np1\n\n\n[1] 0.008535841\n\n\nBeing provided such a small p-value tells us that we are able to reject the null hypothesis and embrace the alternative that mean female income within the population is less than $500 per week."
  },
  {
    "objectID": "posts/AlexisGamez_HW2.html#c",
    "href": "posts/AlexisGamez_HW2.html#c",
    "title": "Blog Post #2",
    "section": "C)",
    "text": "C)\nReport and interpret the P-value for Ha: μ &gt; 500. (Hint: The P-values for the two possible one-sided tests must sum to 1.)\nHere, we’ll be functionally doing the same thing as in part b, but instead of using the lower tail we’ll be focusing on the upper. Under these circumstances, we’ll be adopting the following hypothesis.\nH0: μ =&lt; 500\nHA: μ &gt; 500\n\n\nCode\np2 &lt;- pt(t, 9-1, lower.tail = F)\np2\n\n\n[1] 0.9914642\n\n\nIn this case, we receive a p-value &gt; 0.05 meaning that we fail to reject the null and that the mean female income within the population is not greater than $500."
  },
  {
    "objectID": "posts/AlexisGamez_HW2.html#a-1",
    "href": "posts/AlexisGamez_HW2.html#a-1",
    "title": "Blog Post #2",
    "section": "A)",
    "text": "A)\nShow that t = 1.95 and P-value = 0.051 for Jones. Show that t = 1.97 and P-value = 0.049 for Smith.\nt-test Formula: t = (Y bar - μ)/Standard Error\n\nJones\n\n\nCode\ntJones &lt;- (519.5-500)/10\ntJones\n\n\n[1] 1.95\n\n\n\n\nCode\npJones &lt;- 2*pt(tJones, 1000-1, lower.tail = F)\npJones\n\n\n[1] 0.05145555\n\n\n\n\nSmith\n\n\nCode\ntSmith &lt;- (519.7-500)/10\ntSmith\n\n\n[1] 1.97\n\n\n\n\nCode\npSmith &lt;- 2*pt(tSmith, 1000-1, lower.tail = F)\npSmith\n\n\n[1] 0.04911426"
  },
  {
    "objectID": "posts/AlexisGamez_HW2.html#b-1",
    "href": "posts/AlexisGamez_HW2.html#b-1",
    "title": "Blog Post #2",
    "section": "B)",
    "text": "B)\nUsing α = 0.05, for each study indicate whether the result is “statistically significant.”\nAlpha(α) is the level at which a result is considered statistically significant or not.\nWith our significance level defined, we know that pJones, sitting at 0.051, would not be considered statistically significant and we fail to reject the null (μ = 500). On the other hand, pSmith (0.049) would be considered statistically significant resulting in our ability to reject the null (μ ≠ 500)."
  },
  {
    "objectID": "posts/AlexisGamez_HW2.html#c-1",
    "href": "posts/AlexisGamez_HW2.html#c-1",
    "title": "Blog Post #2",
    "section": "C)",
    "text": "C)\nUsing this example, explain the misleading aspects of reporting the result of a test as “P ≤ 0.05” versus “P &gt; 0.05,” or as “reject H0” versus “Do not reject H0,” without reporting the actual P-value.\nStatistical significance is something that must be well defined prior to the gathering and analyzing data. The term holds weight in the credibility of research and analysis and without it being clearly defined, it’s uncertain whether or not the work that was done truly holds up and proves anything. Not reporting the actual P-values could lead to confusion and misinterpretation down the road. This makes it important to begin every varying analysis with the a clear definition of hypothesis and significance level."
  },
  {
    "objectID": "posts/HW_1_Diana_Rinker.html",
    "href": "posts/HW_1_Diana_Rinker.html",
    "title": "Homework 1",
    "section": "",
    "text": "Question 1\nFirst, let’s read in the data from the Excel file:\n\n\nCode\nlibrary(readxl)\nlibrary(dplyr)\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nCode\nlibrary(tidyr)\nlibrary(tidyverse)\n\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2\n──\n\n\n✔ ggplot2 3.4.1     ✔ purrr   1.0.1\n✔ tibble  3.1.8     ✔ stringr 1.5.0\n✔ readr   2.1.4     ✔ forcats 1.0.0\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\nCode\ndf &lt;- read_excel(\"_data/LungCapData.xls\")\n\n\n\na) What does the distribution of LungCap look like? (Hint: Plot a histogram with probability density on the y axis)\n\n\nCode\nhist(df$LungCap)\n\n\n\n\n\nThe histogram suggests that the distribution is close to a normal distribution. Most of the observations are close to the mean. Very few observations are close to the margins (0 and 15).\n\n\nb) Compare the probability distribution of the LungCap with respect to Males and Females? (Hint: make boxplots separated by gender using the boxplot() function)\n\n\nCode\nboxplot( LungCap ~ Gender , data =df)\n\n\n\n\n\n\n\nc) Compare the mean lung capacities for smokers and non-smokers. Does it make sense?\n\n\nCode\ndf.grouped&lt;- df %&gt;%\n  group_by(Smoke)%&gt;%\n  summarize (mean.Lunc.Cap = mean (LungCap))\nknitr::kable(df.grouped)\n\n\n\n\n\nSmoke\nmean.Lunc.Cap\n\n\n\n\nno\n7.770188\n\n\nyes\n8.645454\n\n\n\n\n\nIt is surprising that smoker’s lung capacity mean is larger than for nonsmokers. T o understand the reason, I would need to break up the data into subgroups.\n\n\nd) Examine the relationship between Smoking and Lung Capacity within age groups: “less than or equal to 13”, “14 to 15”, “16 to 17”, and “greater than or equal to 18”.\n\n\nCode\ndf$age.group &lt;- NA  \ndf$age.group&lt;- ifelse(df$Age &lt;=13, \"under13\" , df$age.group )\ndf$age.group&lt;- ifelse(df$Age &gt;=14 & df$Age &lt;=15, \"14-15\" , df$age.group )\ndf$age.group&lt;- ifelse(df$Age &gt;=16 & df$Age &lt;=17, \"16-17\" , df$age.group )\ndf$age.group&lt;- ifelse(df$Age &gt;=18, \"18+\" , df$age.group )\ndf$age.group &lt;-factor (df$age.group, levels = c(\"under13\",  \"14-15\", \"16-17\", \"18+\") )\n\ndf.grouped\n\n\n# A tibble: 2 × 2\n  Smoke mean.Lunc.Cap\n  &lt;chr&gt;         &lt;dbl&gt;\n1 no             7.77\n2 yes            8.65\n\n\nCode\nggplot (df, mapping=aes(y=LungCap, x = Smoke ))+\n  geom_boxplot()+\n  facet_wrap (~ age.group)\n\n\n\n\n\n\n\nd) Compare the lung capacities for smokers and non-smokers within each age group. Is your answer different from the one in part c. What could possibly be going on here?\nIn the table of counts within each age group we can see that the group ’under 13” is over 50 of the records. Within this group, there is no difference between smokers and non-smokers (probably because of the length of smoking and the fact that only 7% are smokers), which contibutes to overall sample mean.\n\n\nCode\ndf.grouped&lt;-df%&gt;%\n  group_by(age.group, Smoke)%&gt;%\nsummarize (count=n())\n\n\n`summarise()` has grouped output by 'age.group'. You can override using the\n`.groups` argument.\n\n\nTo compare smokers and non-smokers accurately, we could exclude the group “under 13”.\n\n\nCode\ndf.filtered&lt;- df %&gt;%\n  filter(age.group != \"under13\")\n\nggplot (df.filtered, mapping=aes(y=LungCap, x = Smoke ))+\n  geom_boxplot()\n\n\n\n\n\nNow we can see the difference between mean, where smokers have smaller Lung capacity.\n\n\n\nQuestion 2\nLet X = number of prior convictions for prisoners at a state prison at which there are 810 prisoners.\n\n\nCode\nX &lt;- c(0, 1, 2, 3, 4)\nFrequency &lt;- c(128,434, 160, 64, 24)\n\ndf&lt;-tibble (X, Frequency) \ndf$Probabilty &lt;- Frequency/sum(Frequency)\nknitr::kable(df)\n\n\n\n\n\nX\nFrequency\nProbabilty\n\n\n\n\n0\n128\n0.1580247\n\n\n1\n434\n0.5358025\n\n\n2\n160\n0.1975309\n\n\n3\n64\n0.0790123\n\n\n4\n24\n0.0296296\n\n\n\n\n\n\na) What is the probability that a randomly selected inmate has exactly 2 prior convictions?\nNumber of prior convictions of inmates has Poisson distribution. Probability of X = is 0.197\n\n\nCode\nchances.of.2&lt;- df$Probabilty[3]\nknitr::kable(chances.of.2)\n\n\n\n\n\nx\n\n\n\n\n0.1975309\n\n\n\n\n\n\n\nb) What is the probability that a randomly selected inmate has fewer than 2 prior convictions?\nTo calculate “fewer than2”, we will use cumulative probability for Poisson distribution with default “lower.tail =T”, for value “1” to exclude value “2”.\n\n\nCode\nprob.under.2 &lt;- sum(df$Probabilty[1:2])\nknitr::kable(prob.under.2   )\n\n\n\n\n\nx\n\n\n\n\n0.6938272\n\n\n\n\n\n\n\nc) What is the probability that a randomly selected inmate has 2 or fewer prior convictions?\nTo calculate “fewer than2”, we will use cumulative probability for Poisson distribution with default “lower.tail =T”: It will include value of “2”.\n\n\nCode\nprob.under.and.2 &lt;- sum(df$Probabilty[1:3])\nknitr::kable(prob.under.and.2)\n\n\n\n\n\nx\n\n\n\n\n0.891358\n\n\n\n\n\n\n\nd) What is the probability that a randomly selected inmate has over 2 prior convictions?\nTo calculate “over than 2”, we will use cumulative probability for Poisson distribution with “lower.tail =F”:\n\n\nCode\nlambda&lt;- mean (df$X)\nprob.over.2 &lt;- sum(df$Probabilty[4:5] )\nknitr::kable(prob.over.2  )\n\n\n\n\n\nx\n\n\n\n\n0.108642\n\n\n\n\n\n\n\ne) What is the expected value for the number of prior convictions\n\\[\nE(X) = \\sum_{all x} x  \\cdot p(x) = \\mu  \n\\]\n\n\nCode\ndf$Probabilty &lt;- Frequency/sum(Frequency)\n(expected.value.of.X &lt;- sum(df$X * df$Probabilty ))\n\n\n[1] 1.28642\n\n\nCode\nknitr::kable(expected.value.of.X )\n\n\n\n\n\nx\n\n\n\n\n1.28642\n\n\n\n\n\n\n\nf) Calculate the variance and the standard deviation for the Prior Convictions.\nVariance of a random variable: \\[\n\\sigma^2 = E[(X-\\mu)^2] = \\sum_{all x}(x-\\mu)^2 \\cdot p(x)\n\\]\n\n\nCode\nvariance.X &lt;- sum (((df$X - expected.value.of.X )^2) * df$Probabilty  )\nknitr::kable(variance.X  )\n\n\n\n\n\nx\n\n\n\n\n0.8562353\n\n\n\n\n\nAlternatively: \\[\n\\sigma^2 = E(X^2)-[E(X)]^2 = E(X^2)-\\mu^2\n\\]\n\n\nCode\nvariance &lt;- (sum((df$X^2) * df$Probabilty )) - ((expected.value.of.X)^2)\nknitr::kable(variance)\n\n\n\n\n\nx\n\n\n\n\n0.8562353\n\n\n\n\n\nStandard deviation is a square root of variance: \\[\n\\sigma = \\sqrt variance\n\\]\n\n\nCode\nsd&lt;- sqrt(variance)\nknitr::kable(sd)\n\n\n\n\n\nx\n\n\n\n\n0.9253298"
  },
  {
    "objectID": "about/PranavKomaravolu.html",
    "href": "about/PranavKomaravolu.html",
    "title": "Pranav Bharadwaj Komaravolu",
    "section": "",
    "text": "A Computer Science Graduate student who is ever ready to take on challenges and eager to leverage my knowledge and passion for computers to create an impact on the organization or project I will be part of."
  },
  {
    "objectID": "about/PranavKomaravolu.html#educationwork-background",
    "href": "about/PranavKomaravolu.html#educationwork-background",
    "title": "Pranav Bharadwaj Komaravolu",
    "section": "Education/Work Background",
    "text": "Education/Work Background\n\nMaster of Science in Computer Science  from University of Massachusetts Amherst (Aug 2022 - )\nIntegrated Master of Technology  in Computer Science from University of Hyderabad, India  (Jul 2017 - Jun 2022)"
  },
  {
    "objectID": "about/PranavKomaravolu.html#r-experience",
    "href": "about/PranavKomaravolu.html#r-experience",
    "title": "Pranav Bharadwaj Komaravolu",
    "section": "R experience",
    "text": "R experience\nLearnt some R as a part of Stat-501 course. Slight acquaintance with mosaic library. But willing to master it through this course."
  },
  {
    "objectID": "about/PranavKomaravolu.html#research-interests",
    "href": "about/PranavKomaravolu.html#research-interests",
    "title": "Pranav Bharadwaj Komaravolu",
    "section": "Research interests",
    "text": "Research interests\nCurrently interested in the following areas:\n\nBio-NLP, which involves the use of natural language techniques and models to interpret genome sequences and also identify protein structures.\nDistributed learning, which involves the task of distributing the training task of a large machine learning model over various nodes in a compute cluster."
  },
  {
    "objectID": "about/PranavKomaravolu.html#hometown",
    "href": "about/PranavKomaravolu.html#hometown",
    "title": "Pranav Bharadwaj Komaravolu",
    "section": "Hometown",
    "text": "Hometown\nHyderabad, India"
  },
  {
    "objectID": "about/PranavKomaravolu.html#hobbies",
    "href": "about/PranavKomaravolu.html#hobbies",
    "title": "Pranav Bharadwaj Komaravolu",
    "section": "Hobbies",
    "text": "Hobbies\n\nSwimming\nTennis\nReading comics\nI also like taking walk (only when the weather is pleasant)"
  },
  {
    "objectID": "about/PranavKomaravolu.html#fun-fact",
    "href": "about/PranavKomaravolu.html#fun-fact",
    "title": "Pranav Bharadwaj Komaravolu",
    "section": "Fun fact",
    "text": "Fun fact\nWill learn some by the end of this semester :)"
  },
  {
    "objectID": "about/AdithyaParupudi_about.html",
    "href": "about/AdithyaParupudi_about.html",
    "title": "Adithya Parupudi",
    "section": "",
    "text": "University Of Massachusetts Amherst Amherst, MA M.S in Data Analytics and Computational Social Science | August 2022 - Present\nJawaharlal Nehru Technological University Hyderabad, Telangana, India B. Tech in Computer Science and Engineering | Aug 2022 – Exp. Dec 2023"
  },
  {
    "objectID": "about/AdithyaParupudi_about.html#educationwork-background",
    "href": "about/AdithyaParupudi_about.html#educationwork-background",
    "title": "Adithya Parupudi",
    "section": "",
    "text": "University Of Massachusetts Amherst Amherst, MA M.S in Data Analytics and Computational Social Science | August 2022 - Present\nJawaharlal Nehru Technological University Hyderabad, Telangana, India B. Tech in Computer Science and Engineering | Aug 2022 – Exp. Dec 2023"
  },
  {
    "objectID": "about/AdithyaParupudi_about.html#r-experience",
    "href": "about/AdithyaParupudi_about.html#r-experience",
    "title": "Adithya Parupudi",
    "section": "R Experience",
    "text": "R Experience\nCompleted 601, Text-As-Data in Summer and Fall."
  },
  {
    "objectID": "about/AdithyaParupudi_about.html#research-interests",
    "href": "about/AdithyaParupudi_about.html#research-interests",
    "title": "Adithya Parupudi",
    "section": "Research interests",
    "text": "Research interests\nInterested in healthcare, finance"
  },
  {
    "objectID": "about/AdithyaParupudi_about.html#hometown",
    "href": "about/AdithyaParupudi_about.html#hometown",
    "title": "Adithya Parupudi",
    "section": "Hometown",
    "text": "Hometown\nHyderabad, India"
  },
  {
    "objectID": "about/AdithyaParupudi_about.html#hobbies",
    "href": "about/AdithyaParupudi_about.html#hobbies",
    "title": "Adithya Parupudi",
    "section": "Hobbies",
    "text": "Hobbies\n\ncooking\nworking out\nanime"
  },
  {
    "objectID": "about/AdithyaParupudi_about.html#fun-fact",
    "href": "about/AdithyaParupudi_about.html#fun-fact",
    "title": "Adithya Parupudi",
    "section": "Fun fact",
    "text": "Fun fact\nError 404"
  },
  {
    "objectID": "about/Rowena.Kosher.html",
    "href": "about/Rowena.Kosher.html",
    "title": "Rowena",
    "section": "",
    "text": "##Instructions"
  },
  {
    "objectID": "about/Rowena.Kosher.html#educationwork-background",
    "href": "about/Rowena.Kosher.html#educationwork-background",
    "title": "Rowena",
    "section": "Education/Work Background",
    "text": "Education/Work Background\n-BA Columbia University in Human Rights & Gender & Sexuality Studies -Background in nonprofit, academic, research and activism -current main jobs: Consultant and data analyst at Unsiloed, DEI Consulting Firm- focuses on psychological safety, inclusive leadership, neurodiversity, antiracism, dialogue for transformational change -Also freelance writer, teaching assistant"
  },
  {
    "objectID": "about/Rowena.Kosher.html#r-experience",
    "href": "about/Rowena.Kosher.html#r-experience",
    "title": "Rowena",
    "section": "R experience",
    "text": "R experience\n-minimal to none, though eager to learn"
  },
  {
    "objectID": "about/Rowena.Kosher.html#research-interests",
    "href": "about/Rowena.Kosher.html#research-interests",
    "title": "Rowena",
    "section": "Research interests",
    "text": "Research interests\n-Sociology PhD in the pipeline (hopefully), with intent to focus on gender and secualtiy studies and queer theory -interests include: trans and nonbinary menstruation (for eg. conducted cyberethnography of language use in YouTube videos), queer identity and familial structures, gender identity development and theory, intersectional queer identity, neurodiversity"
  },
  {
    "objectID": "about/Rowena.Kosher.html#hometown",
    "href": "about/Rowena.Kosher.html#hometown",
    "title": "Rowena",
    "section": "Hometown",
    "text": "Hometown\nGrew up in Connecticut, have lived in New York City since 2017 (currently in Brooklyn)"
  },
  {
    "objectID": "about/Rowena.Kosher.html#hobbies",
    "href": "about/Rowena.Kosher.html#hobbies",
    "title": "Rowena",
    "section": "Hobbies",
    "text": "Hobbies\n-Yoga and movement -spending time with my two italian greyhounds and one cat- an entire zoo in one tiny apartment -spending my time with one foot in the art world thanks to my talented friends and a fun foray into film production"
  },
  {
    "objectID": "about/Rowena.Kosher.html#fun-fact",
    "href": "about/Rowena.Kosher.html#fun-fact",
    "title": "Rowena",
    "section": "Fun fact",
    "text": "Fun fact\nI barista and bartend- my go-to coffee is a Black eye (Black drip w/ 2 shots of espresso) and my favorite cocktail is the negroni"
  },
  {
    "objectID": "about/MiguelCuriel.html",
    "href": "about/MiguelCuriel.html",
    "title": "Miguel Curiel",
    "section": "",
    "text": "Bachelor in Psychology and currently pursuing an MS in Data Analytics and Computational Social Science.\nCurrently working as a Computational Social Scientist at a supply chain technology company. Previously engaged several applied research settings, such as a market research agency, a neuroscience institute, and a non-profit."
  },
  {
    "objectID": "about/MiguelCuriel.html#educationwork-background",
    "href": "about/MiguelCuriel.html#educationwork-background",
    "title": "Miguel Curiel",
    "section": "",
    "text": "Bachelor in Psychology and currently pursuing an MS in Data Analytics and Computational Social Science.\nCurrently working as a Computational Social Scientist at a supply chain technology company. Previously engaged several applied research settings, such as a market research agency, a neuroscience institute, and a non-profit."
  },
  {
    "objectID": "about/MiguelCuriel.html#r-experience",
    "href": "about/MiguelCuriel.html#r-experience",
    "title": "Miguel Curiel",
    "section": "R experience",
    "text": "R experience\n\nStarted getting deep into R around May 2022; before that, I had some months of experience in SQL and Python."
  },
  {
    "objectID": "about/MiguelCuriel.html#research-interests",
    "href": "about/MiguelCuriel.html#research-interests",
    "title": "Miguel Curiel",
    "section": "Research interests",
    "text": "Research interests\n\nInterested in modeling and understanding complex social systems, such as social media, economic ecosystems, and supply chains. Also interested in neuropsychology, neuroscience, and mental health in general."
  },
  {
    "objectID": "about/MiguelCuriel.html#hometown",
    "href": "about/MiguelCuriel.html#hometown",
    "title": "Miguel Curiel",
    "section": "Hometown",
    "text": "Hometown\n\nOriginally from Colima, Mexico; currently living in Washington, DC."
  },
  {
    "objectID": "about/MiguelCuriel.html#hobbies",
    "href": "about/MiguelCuriel.html#hobbies",
    "title": "Miguel Curiel",
    "section": "Hobbies",
    "text": "Hobbies\n\nPlaying guitar and video games, hanging out with family and friends, traveling, and discovering new bars and restaurants."
  },
  {
    "objectID": "about/MiguelCuriel.html#fun-fact",
    "href": "about/MiguelCuriel.html#fun-fact",
    "title": "Miguel Curiel",
    "section": "Fun fact",
    "text": "Fun fact\n\nIn a past life, I attempted to be a YouTube blogger - had one channel creating comedy sketches with friends; and another where I posted guitar covers."
  },
  {
    "objectID": "about/about_abbybalint603.html",
    "href": "about/about_abbybalint603.html",
    "title": "Abby Balint",
    "section": "",
    "text": "I graduated from UMass Amherst SBS in 2017 with a double major in Communications and Sociology. After graduation, I began a career in market research. Throughout the last five years my career has included project management, full cycle survey research, cleaning and weighting data, preparing and writing reports in various formats, and vendor/client management. I currently work at S&P Global Market Intelligence as a Research Analyst with a focus in consumer Digital Endpoints research."
  },
  {
    "objectID": "about/about_abbybalint603.html#educationwork-background",
    "href": "about/about_abbybalint603.html#educationwork-background",
    "title": "Abby Balint",
    "section": "",
    "text": "I graduated from UMass Amherst SBS in 2017 with a double major in Communications and Sociology. After graduation, I began a career in market research. Throughout the last five years my career has included project management, full cycle survey research, cleaning and weighting data, preparing and writing reports in various formats, and vendor/client management. I currently work at S&P Global Market Intelligence as a Research Analyst with a focus in consumer Digital Endpoints research."
  },
  {
    "objectID": "about/about_abbybalint603.html#r-experience",
    "href": "about/about_abbybalint603.html#r-experience",
    "title": "Abby Balint",
    "section": "R experience",
    "text": "R experience\nI have no experience in R at all outside of taking 601 last semester. In my day to day I typically use Excel, SPSS, and various online data processing platforms."
  },
  {
    "objectID": "about/about_abbybalint603.html#research-interests",
    "href": "about/about_abbybalint603.html#research-interests",
    "title": "Abby Balint",
    "section": "Research interests",
    "text": "Research interests\nGiven my background in Comm/Sociology, I am personally interested in social science research and topics like wealth inequality, or any data about the way humans behave. Professionally, I am interested in researching ways we anticipate the world will look different in the coming years - things like fin-tech changing financial planning accessibility, or the pandemic changing the way we work forever."
  },
  {
    "objectID": "about/about_abbybalint603.html#hometown",
    "href": "about/about_abbybalint603.html#hometown",
    "title": "Abby Balint",
    "section": "Hometown",
    "text": "Hometown\nCurrently living in Providence, RI but from Connecticut"
  },
  {
    "objectID": "about/about_abbybalint603.html#hobbies",
    "href": "about/about_abbybalint603.html#hobbies",
    "title": "Abby Balint",
    "section": "Hobbies",
    "text": "Hobbies\nPainting, writing poetry, going to music festivals, and traveling"
  },
  {
    "objectID": "about/about_abbybalint603.html#fun-fact",
    "href": "about/about_abbybalint603.html#fun-fact",
    "title": "Abby Balint",
    "section": "Fun fact",
    "text": "Fun fact\nI have two orange brother cats that are one year old, and they love to make appearances on Zoom calls :)"
  },
  {
    "objectID": "about/RahulSomu.html",
    "href": "about/RahulSomu.html",
    "title": "Rahul Somu",
    "section": "",
    "text": "##Instructions"
  },
  {
    "objectID": "about/RahulSomu.html#educationwork-background",
    "href": "about/RahulSomu.html#educationwork-background",
    "title": "Rahul Somu",
    "section": "Education/Work Background",
    "text": "Education/Work Background\nBachelors in Electrical engineering at BITS Pilani -India 3years of professional work experience as software engineer"
  },
  {
    "objectID": "about/RahulSomu.html#r-experience",
    "href": "about/RahulSomu.html#r-experience",
    "title": "Rahul Somu",
    "section": "R experience",
    "text": "R experience\nBeginner"
  },
  {
    "objectID": "about/RahulSomu.html#research-interests",
    "href": "about/RahulSomu.html#research-interests",
    "title": "Rahul Somu",
    "section": "Research interests",
    "text": "Research interests\nBig Data Analytics"
  },
  {
    "objectID": "about/RahulSomu.html#hometown",
    "href": "about/RahulSomu.html#hometown",
    "title": "Rahul Somu",
    "section": "Hometown",
    "text": "Hometown\nHyderabad - India"
  },
  {
    "objectID": "about/RahulSomu.html#hobbies",
    "href": "about/RahulSomu.html#hobbies",
    "title": "Rahul Somu",
    "section": "Hobbies",
    "text": "Hobbies\nSoccer and gaming"
  },
  {
    "objectID": "about/RahulSomu.html#fun-fact",
    "href": "about/RahulSomu.html#fun-fact",
    "title": "Rahul Somu",
    "section": "Fun fact",
    "text": "Fun fact"
  },
  {
    "objectID": "about/DarronBunt.html",
    "href": "about/DarronBunt.html",
    "title": "Darron Bunt",
    "section": "",
    "text": "My background is in qualitative research - I have an MA, where I focused on sociocultural studies of sport and leisure (my thesis was about the use of narratives about youth, masculinity, and nationalism in the production of advertising, using NHL player Sidney Crosby as a case study). Degree-wise, I also have a BAH in Sociology and a Bachelor of Physical and Health Education (specializing in cultural studies and sociology of sport). Certificate-wise, I have a certificate in Strategic Public Relations and am one course shy of my certificate in Digital Strategy and Communications Management (maybe this summer!)\nAfter spending the first 10ish years of my career in non-profit sport management, I made the decision to refocus myself on communications, research, and analysis work. I worked on campus in marketing and communications before moving to my current role as a social media data analyst for a higher ed adjacent agency. In my day-to-day work I pick apart what’s being posted online about our partner schools to help them reach their strategic marketing and communications goals."
  },
  {
    "objectID": "about/DarronBunt.html#educationwork-background",
    "href": "about/DarronBunt.html#educationwork-background",
    "title": "Darron Bunt",
    "section": "",
    "text": "My background is in qualitative research - I have an MA, where I focused on sociocultural studies of sport and leisure (my thesis was about the use of narratives about youth, masculinity, and nationalism in the production of advertising, using NHL player Sidney Crosby as a case study). Degree-wise, I also have a BAH in Sociology and a Bachelor of Physical and Health Education (specializing in cultural studies and sociology of sport). Certificate-wise, I have a certificate in Strategic Public Relations and am one course shy of my certificate in Digital Strategy and Communications Management (maybe this summer!)\nAfter spending the first 10ish years of my career in non-profit sport management, I made the decision to refocus myself on communications, research, and analysis work. I worked on campus in marketing and communications before moving to my current role as a social media data analyst for a higher ed adjacent agency. In my day-to-day work I pick apart what’s being posted online about our partner schools to help them reach their strategic marketing and communications goals."
  },
  {
    "objectID": "about/DarronBunt.html#r-experience",
    "href": "about/DarronBunt.html#r-experience",
    "title": "Darron Bunt",
    "section": "R experience",
    "text": "R experience\nI started using R in 601, which I took in Fall 2022. I would still very much consider myself a beginner and am looking forward to continuing to build my skills which will hopefully help me to continue to broaden my research and analysis capabilities!"
  },
  {
    "objectID": "about/DarronBunt.html#research-interests",
    "href": "about/DarronBunt.html#research-interests",
    "title": "Darron Bunt",
    "section": "Research interests",
    "text": "Research interests\nCultural studies, higher education, social justice"
  },
  {
    "objectID": "about/DarronBunt.html#hometown",
    "href": "about/DarronBunt.html#hometown",
    "title": "Darron Bunt",
    "section": "Hometown",
    "text": "Hometown\nToronto, Ontario, Canada"
  },
  {
    "objectID": "about/DarronBunt.html#hobbies",
    "href": "about/DarronBunt.html#hobbies",
    "title": "Darron Bunt",
    "section": "Hobbies",
    "text": "Hobbies\nHockey, hiking, canoe tripping, powerlifting, writing, chasing my two kids around!"
  },
  {
    "objectID": "about/DarronBunt.html#fun-fact",
    "href": "about/DarronBunt.html#fun-fact",
    "title": "Darron Bunt",
    "section": "Fun fact",
    "text": "Fun fact\nWhy is this always the most difficult question to answer? I don’t know if it’s fun, but I guess it’s a fact; I default to Canadian english, so I have to make sure I edit all work school/work writing to make sure I’ve eliminated all the extraneous “u”s."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Contributors",
    "section": "",
    "text": "Find out more about our DACSS students who contributed to the blog.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAbby Balint\n\n\n\n\n\n\n\n\n\n\n\n\n\nAdithya Parupudi\n\n\n\n\n\n\n\n\n\n\n\n\n\nDarron Bunt\n\n\n\n\n\n\n\n\n\n\n\n\n\nMiguel Curiel\n\n\n\n\n\n\n\n\n\n\n\n\n\nPranav Bharadwaj Komaravolu\n\n\n\n\n\n\n\n\n\n\n\n\n\nRahul Somu\n\n\n\n\n\n\n\n\n\n\n\n\n\nRowena\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "DACSS 603 Introduction to Quantitative Analysis Spring 2023",
    "section": "",
    "text": "The blog posts here are contributed by students enrolled in DACSS 603, Introduction to Quantitative Analysis.\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nundefined\n\n\nHW_4_Diana_Rinker\n\n\nDiana_Rinker \n\n\n\n\nApr 26, 2023\n\n\nDACSS 603 Homework 4\n\n\nLaura Collazo\n\n\n\n\nApr 25, 2023\n\n\nHomework 4\n\n\nAsch Harwood\n\n\n\n\nApr 25, 2023\n\n\nHomework 4\n\n\nMiguel Curiel\n\n\n\n\nApr 24, 2023\n\n\nFinal Project check-in 2\n\n\nDiana Rinker\n\n\n\n\nApr 24, 2023\n\n\nFinal Project Part 1\n\n\nHannah Rosenbaum\n\n\n\n\nApr 24, 2023\n\n\nHomework 4\n\n\nSaisrinivas Ambatipudi\n\n\n\n\nApr 24, 2023\n\n\nHomework 4\n\n\nAdithya Parupudi\n\n\n\n\nApr 24, 2023\n\n\nHomework 4\n\n\nAlexa Potter\n\n\n\n\nApr 24, 2023\n\n\nFinal Project check-in 2\n\n\nDiana Rinker\n\n\n\n\nApr 24, 2023\n\n\nHomework 4\n\n\nAbigail Balint\n\n\n\n\nApr 24, 2023\n\n\nHomework 4\n\n\nJustine Shakespeare\n\n\n\n\nApr 24, 2023\n\n\nHomework_5_Saisrinivas\n\n\nSaisrinivas Ambatipudi\n\n\n\n\nApr 23, 2023\n\n\nHomework 4\n\n\nAkhilesh Kumar\n\n\n\n\nApr 23, 2023\n\n\nBlog Post #3\n\n\nAlexis Gamez\n\n\n\n\nApr 22, 2023\n\n\nKristin Abijaoude_HW4\n\n\n\n\n\n\n\nApr 22, 2023\n\n\nMaternal Mortality and Women's Political Inclusion\n\n\nJustine Shakespeare\n\n\n\n\nApr 21, 2023\n\n\nFinal Project Check in 2\n\n\nKristin Abijaoude\n\n\n\n\nApr 20, 2023\n\n\nFinal Project Part 2\n\n\nTyler Tewksbury\n\n\n\n\nApr 20, 2023\n\n\nEstimating Per Capita Food Waste in the United States\n\n\nAsch Harwood\n\n\n\n\nApr 20, 2023\n\n\nFinal Project - Check point 2\n\n\nFelix Betanourt\n\n\n\n\nApr 20, 2023\n\n\n603_Final_Project_Check_In_2_Saisrinivas_Ambatipudi\n\n\nSaisrinivas Ambatipudi\n\n\n\n\nApr 20, 2023\n\n\nFinal Project Part 2\n\n\nAbigail Balint\n\n\n\n\nApr 20, 2023\n\n\nFinal Project Part 2\n\n\nAlexa Potter\n\n\n\n\nApr 20, 2023\n\n\n603_Final_Project_Check_In_2_Akhilesh_KUmar\n\n\nAkhilesh Kumar\n\n\n\n\nApr 20, 2023\n\n\nFinal Project Check In 2\n\n\nMiguel Curiel\n\n\n\n\nApr 20, 2023\n\n\nFinal Project Check in 2\n\n\nEmma Narkewicz\n\n\n\n\nApr 19, 2023\n\n\nFinal Project - Post 2\n\n\nAdithya Parupudi\n\n\n\n\nApr 18, 2023\n\n\nDACSS 603 Homework 4\n\n\nLaura Collazo\n\n\n\n\nApr 17, 2023\n\n\nFinal Project Checkin-2\n\n\nXiaoyan \n\n\n\n\nApr 17, 2023\n\n\nHomework 4\n\n\nXiaoyan \n\n\n\n\nApr 16, 2023\n\n\nHomework 2\n\n\nDarron Bunt\n\n\n\n\nApr 15, 2023\n\n\nCheck-in 2: DACSS 603 Final Project\n\n\nLaura Collazo\n\n\n\n\nApr 12, 2023\n\n\nProject Analysis\n\n\nAlexis Gamez\n\n\n\n\nApr 12, 2023\n\n\nHomework 3\n\n\nRosemary Pang\n\n\n\n\nApr 12, 2023\n\n\n603Homework_Saisrinivas_Ambatipudi\n\n\nSaisrinivas Ambatipudi\n\n\n\n\nApr 11, 2023\n\n\nHomework 3\n\n\nEmma Narkewicz\n\n\n\n\nApr 11, 2023\n\n\nHomework 3\n\n\nAbigail Balint\n\n\n\n\nApr 11, 2023\n\n\nHomework 3\n\n\nMiguel Curiel\n\n\n\n\nApr 11, 2023\n\n\nHomework 3\n\n\nYoung Soo Choi\n\n\n\n\nApr 11, 2023\n\n\nHomework 3\n\n\nAkhilesh Kumar Meghwal\n\n\n\n\nApr 11, 2023\n\n\nHomework 3\n\n\nDiana Rinker\n\n\n\n\nApr 11, 2023\n\n\nHomework 3\n\n\nAlexa Potter\n\n\n\n\nApr 11, 2023\n\n\nHW3\n\n\nLiam Tucksmith\n\n\n\n\nApr 11, 2023\n\n\nHomework 3\n\n\nFelix Betanourt\n\n\n\n\nApr 10, 2023\n\n\nHomework 3\n\n\nAdithya Parupudi\n\n\n\n\nApr 10, 2023\n\n\nHomework 3\n\n\nJustine Shakespeare\n\n\n\n\nApr 10, 2023\n\n\nHomework 3\n\n\nAsch Harwood\n\n\n\n\nApr 7, 2023\n\n\nDACSS 603 Homework 3\n\n\nLaura Collazo\n\n\n\n\nApr 5, 2023\n\n\nHomework 3\n\n\nXiaoyan \n\n\n\n\nApr 4, 2023\n\n\nFinal Project Check 2\n\n\nGuanhua Tan\n\n\n\n\nApr 3, 2023\n\n\nHomework 2\n\n\nAbigail Balint\n\n\n\n\nApr 2, 2023\n\n\nHomework 3\n\n\nMeredith Derian-Toth\n\n\n\n\nApr 2, 2023\n\n\nHomework2\n\n\nRahul Somu\n\n\n\n\nApr 2, 2023\n\n\nHomework 2\n\n\nMeredith Derian-Toth\n\n\n\n\nApr 2, 2023\n\n\nHw 3 by Kristin Abijaoude\n\n\nKristin Abijaoude\n\n\n\n\nApr 1, 2023\n\n\nHomework 3\n\n\nGuanhua Tan\n\n\n\n\nMar 29, 2023\n\n\nHomework 2\n\n\nRosemary Pang\n\n\n\n\nMar 28, 2023\n\n\nHomework 2\n\n\nFelix Betanourt\n\n\n\n\nMar 28, 2023\n\n\nHomework 2\n\n\nAsch Harwood\n\n\n\n\nMar 28, 2023\n\n\nHomework 2\n\n\nCaitlin Rowley\n\n\n\n\nMar 28, 2023\n\n\nHomework 2\n\n\nSaisrinivas Ambatipudi\n\n\n\n\nMar 28, 2023\n\n\nHomework 2\n\n\nMiguel Curiel\n\n\n\n\nMar 28, 2023\n\n\nHomework 2\n\n\nAlexa Potter\n\n\n\n\nMar 28, 2023\n\n\nHomework 2\n\n\nEmma Narkewicz\n\n\n\n\nMar 27, 2023\n\n\nHomework 2\n\n\nJustine Shakespeare\n\n\n\n\nMar 27, 2023\n\n\nFinal Project check-in (1)\n\n\nDiana Rinker\n\n\n\n\nMar 27, 2023\n\n\nHomework 2\n\n\nYoung Soo Choi\n\n\n\n\nMar 26, 2023\n\n\nBlog Post #2\n\n\nAlexis Gamez\n\n\n\n\nMar 25, 2023\n\n\nHomework 2\n\n\nDiana Rinker\n\n\n\n\nMar 24, 2023\n\n\nHomework 2\n\n\nXiaoyan \n\n\n\n\nMar 24, 2023\n\n\nFinal Project Check-in 1\n\n\nRahul Somu\n\n\n\n\nMar 23, 2023\n\n\nDACSS 603 Homework 2\n\n\nLaura Collazo\n\n\n\n\nMar 23, 2023\n\n\nHomework 2\n\n\nGuanhua Tan\n\n\n\n\nMar 22, 2023\n\n\nFinal Project check in 1\n\n\nThrishul Pola\n\n\n\n\nMar 22, 2023\n\n\n603_Project_Check_In_Saisrinivas_Ambatipudi\n\n\nAkhilesh Kumar & Saisrinivas Ambatipudi\n\n\n\n\nMar 22, 2023\n\n\nFinal Project Check In\n\n\nYoung Soo Choi\n\n\n\n\nMar 21, 2023\n\n\nFinal Project Proposal\n\n\nEmma Narkewicz\n\n\n\n\nMar 21, 2023\n\n\nFinalPart1\n\n\nLiam Tucksmith\n\n\n\n\nMar 21, 2023\n\n\nFinal Project Check in 1\n\n\nKristin Abijaoude\n\n\n\n\nMar 21, 2023\n\n\nFinal Project Part 1\n\n\nAlexa Potter\n\n\n\n\nMar 21, 2023\n\n\nFinal Project - Check-In 1\n\n\nCaitlin Rowley\n\n\n\n\nMar 21, 2023\n\n\nFinal Project Check In 1\n\n\nMiguel Curiel\n\n\n\n\nMar 21, 2023\n\n\nFinal Project Part 1\n\n\nTyler Tewksbury\n\n\n\n\nMar 21, 2023\n\n\n603_Project_Check_In_Akhilesh\n\n\nAkhilesh Kumar & Saisrinivas Ambatipudi\n\n\n\n\nMar 20, 2023\n\n\nFinal Project - Post 1\n\n\nAdithya Parupudi\n\n\n\n\nMar 20, 2023\n\n\nFinal Project Initial Research\n\n\nAbigail Balint\n\n\n\n\nMar 20, 2023\n\n\nHomework 1\n\n\nJerin Jacob\n\n\n\n\nMar 18, 2023\n\n\nFinal Project - Check point 1\n\n\nFelix Betanourt\n\n\n\n\nMar 18, 2023\n\n\nProject Proposal\n\n\nAlexis Gamez\n\n\n\n\nMar 17, 2023\n\n\nFinal Project Checkin-1\n\n\nXiaoyan \n\n\n\n\nMar 16, 2023\n\n\nHw 2 by Kristin Abijaoude\n\n\nKristin Abijaoude\n\n\n\n\nMar 12, 2023\n\n\nHW2\n\n\nLiam Tucksmith\n\n\n\n\nMar 10, 2023\n\n\nFinal Project Proposal Check-in1\n\n\nMeredith Derian-Toth\n\n\n\n\nMar 10, 2023\n\n\nFinal Project 603\n\n\nJerin Jacob\n\n\n\n\nMar 4, 2023\n\n\nFinal Project Check 1\n\n\nGuanhua Tan\n\n\n\n\nMar 1, 2023\n\n\nHomework 1 Solution\n\n\nRosemary Pang\n\n\n\n\nFeb 28, 2023\n\n\nHomework 1 - Akhilesh Kumar\n\n\nAkhilesh Kumar Meghwal\n\n\n\n\nFeb 28, 2023\n\n\nHomework - 1\n\n\nDarron Bunt\n\n\n\n\nFeb 28, 2023\n\n\nHomework 1\n\n\nChristine Brydges\n\n\n\n\nFeb 28, 2023\n\n\nHomework 1\n\n\nAsch Harwood\n\n\n\n\nFeb 28, 2023\n\n\nHomework 1\n\n\nZhiyuan Zhou\n\n\n\n\nFeb 28, 2023\n\n\nHomework 1\n\n\nOllie Murphy\n\n\n\n\nFeb 28, 2023\n\n\nHomework - 1\n\n\nTyler Tewksbury\n\n\n\n\nFeb 28, 2023\n\n\nHomework 1\n\n\nEmma Narkewicz\n\n\n\n\nFeb 27, 2023\n\n\nHomework 1\n\n\nXiaoyan \n\n\n\n\nFeb 27, 2023\n\n\nHomework 2\n\n\nAkhilesh Kumar Meghwal\n\n\n\n\nFeb 27, 2023\n\n\nHomework1\n\n\nRahul Somu\n\n\n\n\nFeb 27, 2023\n\n\nHomework 1\n\n\nSaisrinivas Ambatipudi\n\n\n\n\nFeb 27, 2023\n\n\nHw 1 by Kristin Abijaoude\n\n\nKristin Abijaoude\n\n\n\n\nFeb 27, 2023\n\n\nHomework 1\n\n\nYoung Soo Choi\n\n\n\n\nFeb 27, 2023\n\n\nHomework 2\n\n\nAdithya Parupudi\n\n\n\n\nFeb 26, 2023\n\n\nHomework 1\n\n\nCaitlin Rowley\n\n\n\n\nFeb 26, 2023\n\n\nHomework 1\n\n\nJustine Shakespeare\n\n\n\n\nFeb 25, 2023\n\n\nHomework 1\n\n\nAlexa Potter\n\n\n\n\nFeb 24, 2023\n\n\nHomework 1\n\n\nFelix Betanourt\n\n\n\n\nFeb 23, 2023\n\n\nHomework1 - EDA of LungCap Data\n\n\nAdithya Parpudi\n\n\n\n\nFeb 23, 2023\n\n\nHomework 1\n\n\nAbigail Balint\n\n\n\n\nFeb 20, 2023\n\n\nBlog Post #1\n\n\nAlexis Gamez\n\n\n\n\nFeb 20, 2023\n\n\nHW1\n\n\nLiam Tucksmith\n\n\n\n\nFeb 20, 2023\n\n\nHomework 1\n\n\nMiguel Curiel\n\n\n\n\nFeb 19, 2023\n\n\nDACSS 603 Homework 1\n\n\nLaura Collazo\n\n\n\n\nFeb 17, 2023\n\n\nHomework 1\n\n\nDiana Rinker\n\n\n\n\nFeb 5, 2023\n\n\nHomework 1\n\n\nGuanhua Tan\n\n\n\n\nFeb 5, 2023\n\n\nHomework - 2\n\n\nThrishul \n\n\n\n\nFeb 5, 2023\n\n\nHomework_One\n\n\nMeredith Derian-Toth\n\n\n\n\nFeb 5, 2023\n\n\nHomework - 2\n\n\nThrishul \n\n\n\n\nFeb 5, 2023\n\n\nHomework3\n\n\nRahul Somu\n\n\n\n\nFeb 5, 2023\n\n\nHomework - 1\n\n\nThrishul \n\n\n\n\nAug 3, 2022\n\n\nBlog Post Template\n\n\nSidharth Saluja\n\n\n\n\nApr 12, 2022\n\n\nHomework 3\n\n\nRosemary Pang\n\n\n\n\n\n\nNo matching items"
  }
]