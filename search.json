[
  {
    "objectID": "posts/HW1_OllieMurphy.html",
    "href": "posts/HW1_OllieMurphy.html",
    "title": "Homework 1",
    "section": "",
    "text": "Question 1: Use the LungCapData to answer the following questions.\nRequire packages and import data\n\n\nCode\nlibrary(here)\n\n\nhere() starts at C:/Users/Ollie/OneDrive - University of Massachusetts/Spring_2023/Quant_Analysis/603_Spring_2023\n\n\nCode\nlibrary(readxl)\n\n\nWarning: package 'readxl' was built under R version 4.1.3\n\n\nCode\nlibrary(dplyr)\n\n\nWarning: package 'dplyr' was built under R version 4.1.3\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nCode\ndf <- read_excel(\"_data/LungCapData.xls\")\n\n\n\na) What does the distributuion of LungCap look like?\n\n\nCode\nhist(df$LungCap, main = \"Histogram of Lung Capacity\", xlab = \"Lung Capacity\")\n\n\n\n\n\nBased on the histogram, lung capacity appears to be distributed normally with a mean around 7 to 8, a minimum of 0, and a maximum of 15\n\n\nb) Compare the probability distribution of the LungCap with responst to Males and Females.\n\n\nCode\nboxplot(LungCap ~ Gender, data = df)\n\n\n\n\n\nThe distributions are of a similar size, but male capacity is higher in mean as well as quartiles and min/max.\n\n\nc) Compare the mean lung capacities for smokers and non-smokers. Does it make sense?\n\n\nCode\ndf %>%\n  group_by(Smoke) %>%\n  summarise(LungCap = (mean(LungCap,na.rm = TRUE)))\n\n\n# A tibble: 2 x 2\n  Smoke LungCap\n  <chr>   <dbl>\n1 no       7.77\n2 yes      8.65\n\n\nThe mean lung capacities likely don’t make sense, as you might expect for smoking to diminish lung capacity. However, it could be that people with better lung capacity are better at smoking and therefore do it more, or the inclusion of children, who are less likely to smoke and also have smaller lung capacities.\n\n\nd) Examine the relationship between Smoking and Lung Capacity within age groups: “Less than or equal to 13”, “14 to 15”, “16 to 17”, and “greater than or equal to 18”.\n\n\nCode\nsmoke_df = df%>%\n  mutate(\n   AgeRange = case_when(\n    Age < 14 ~ \"13 and Under\",\n    Age == 14 ~ \"14 to 15\",\n    Age == 15 ~ \"14 to 15\",\n    Age == 16 ~ \"16 to 17\",\n    Age == 17 ~ \"16 to 17\",\n    Age > 17 ~ \"18 and Over\"\n  )) %>%\n  filter(Smoke == \"yes\")%>%\n  group_by(AgeRange)%>%\n    summarize(mean(LungCap, na.rm = TRUE))\n\nnosmoke_df  = df%>%\n  mutate(\n   AgeRange = case_when(\n    Age < 14 ~ \"13 and Under\",\n    Age == 14 ~ \"14 to 15\",\n    Age == 15 ~ \"14 to 15\",\n    Age == 16 ~ \"16 to 17\",\n    Age == 17 ~ \"16 to 17\",\n    Age > 17 ~ \"18 and Over\"\n  )) %>%\n  filter(Smoke == \"no\")%>%\n  group_by(AgeRange)%>%\n    summarize(mean(LungCap, na.rm = TRUE))\n\nsmokeByAge = left_join(smoke_df, nosmoke_df, by = (\"AgeRange\"))\nnames(smokeByAge) = c(\"AgeRange\", \"Smoker\", \"NonSmoker\")\n\nsmokeByAge\n\n\n# A tibble: 4 x 3\n  AgeRange     Smoker NonSmoker\n  <chr>         <dbl>     <dbl>\n1 13 and Under   7.20      6.36\n2 14 to 15       8.39      9.14\n3 16 to 17       9.38     10.5 \n4 18 and Over   10.5      11.1 \n\n\n\n\ne) Compare the lung capacities for smokers and non-smokers within each age group. Is your answer different from the one in part c? What could possibly be going on here?\nWhile the youngest age group follows the same trend observed in part c (smokers with higher lung capacity than non-smokers), the inverse is true for all older groups. I suspect that, as I mentioned above, the data is skewed by virture of the fact that older groups are more likely to smoke as well as have a larger lung capacity. When compared within their age range (therefore at similar states of development), the results reflect that smoking likely diminishes lung capacity.\n\n\n\nQuestion 2: Let X = number of prior convictions for prisoners at a state prison at which there are 810 prisoners\n\n\nCode\nconvictionsfreq = data.frame(convictions = c(0,1,2,3,4), frequency = c(128,434,160,64,24))\nconvictionsfreq\n\n\n  convictions frequency\n1           0       128\n2           1       434\n3           2       160\n4           3        64\n5           4        24\n\n\n\na) What is the probability that a randomly selected inmate has exactly 2 prior convictions?\n\n\nCode\n160/(128+434+160+64+24)\n\n\n[1] 0.1975309\n\n\n\n\nb) What is the probability that a randomly selected inmate has fewer than 2 prior convictions?\n\n\nCode\n(128+434)/(128+434+160+64+24)\n\n\n[1] 0.6938272\n\n\n\n\nc) What is the probability that a randomly selected inmate has 2 or fewer prior convictions?\n\n\nCode\n(128+434+160)/(128+434+160+64+24)\n\n\n[1] 0.891358\n\n\n\n\nd) What is the probability that a randomly selected inmate has more than 2 prior convictions?\n\n\nCode\n(64+24)/(128+434+160+64+24)\n\n\n[1] 0.108642\n\n\n\n\ne) What is the expected value1 for the number of prior convictions?\n\n\nCode\n(0*(128/(128+434+160+64+24)))+(1*(434/(128+434+160+64+24)))+(2*(160/(128+434+160+64+24)))+(3*(64/(128+434+160+64+24)))+(4*(24/(128+434+160+64+24)))\n\n\n[1] 1.28642\n\n\n\n\nf) Calculate the variance and the standard deviation for the Prior Convictions.\n\n\nCode\nvar(convictionsfreq$convictions)\n\n\n[1] 2.5\n\n\nCode\nsd(convictionsfreq$convictions)\n\n\n[1] 1.581139"
  },
  {
    "objectID": "posts/dacss603hw2_LauraCollazo.html",
    "href": "posts/dacss603hw2_LauraCollazo.html",
    "title": "DACSS 603 Homework 2",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(kableExtra)\nlibrary(readxl)\nlibrary(lsr)"
  },
  {
    "objectID": "posts/dacss603hw2_LauraCollazo.html#a",
    "href": "posts/dacss603hw2_LauraCollazo.html#a",
    "title": "DACSS 603 Homework 2",
    "section": "A",
    "text": "A\nThe below shows the results of a one-sample t-test which tests whether the mean income of female employees ($410) differs from the mean income of all senior-level workers ($500).\nOne-sample t-tests assume that the population distribution is normal and also that the observations in the sample are generated independently of one another.\n\n\nCode\nset.seed(27)\n\nrnorm_fixed <- function(n, mean, sd) {\n  as.vector(mean + sd * scale(rnorm(n)))\n}\n\ndata4 <- rnorm_fixed(9, 410, 90)\n\noneSampleTTest(x=data4, mu=500)\n\n\n\n   One sample t-test \n\nData variable:   data4 \n\nDescriptive statistics: \n              data4\n   mean     410.000\n   std dev.  90.000\n\nHypotheses: \n   null:        population mean equals 500 \n   alternative: population mean not equal to 500 \n\nTest results: \n   t-statistic:  -3 \n   degrees of freedom:  8 \n   p-value:  0.017 \n\nOther information: \n   two-sided 95% confidence interval:  [340.82, 479.18] \n   estimated effect size (Cohen's d):  1 \n\n\nThe results of this T-test is statistically significant as the p-value is below .05. This means we can reject the null hypothesis."
  },
  {
    "objectID": "posts/dacss603hw2_LauraCollazo.html#b",
    "href": "posts/dacss603hw2_LauraCollazo.html#b",
    "title": "DACSS 603 Homework 2",
    "section": "B",
    "text": "B\nThe p-value when the mean is less than 500 is below.\n\n\nCode\nt.test(data4, mu = 500, alternative = \"less\")\n\n\n\n    One Sample t-test\n\ndata:  data4\nt = -3, df = 8, p-value = 0.008536\nalternative hypothesis: true mean is less than 500\n95 percent confidence interval:\n     -Inf 465.7864\nsample estimates:\nmean of x \n      410 \n\n\nThe results of this t-test are significant as the p-value is below .05. This means we can reject the null hypothesis that the true mean is not less than 500."
  },
  {
    "objectID": "posts/dacss603hw2_LauraCollazo.html#c",
    "href": "posts/dacss603hw2_LauraCollazo.html#c",
    "title": "DACSS 603 Homework 2",
    "section": "C",
    "text": "C\nThe p-value when the mean is greater than 500 is below.\n\n\nCode\nt.test(data4, mu = 500, alternative = \"greater\")\n\n\n\n    One Sample t-test\n\ndata:  data4\nt = -3, df = 8, p-value = 0.9915\nalternative hypothesis: true mean is greater than 500\n95 percent confidence interval:\n 354.2136      Inf\nsample estimates:\nmean of x \n      410 \n\n\nThe results of this test are not significant as the p-value is greater than .05. This means we cannot reject the null hypothesis that the true mean is not greater than 500."
  },
  {
    "objectID": "posts/dacss603hw2_LauraCollazo.html#a-1",
    "href": "posts/dacss603hw2_LauraCollazo.html#a-1",
    "title": "DACSS 603 Homework 2",
    "section": "A",
    "text": "A\nTh first step for this question is to find the standard deviation. It is shown below.\n\n\nCode\n# find the standard deviation (se = sd/sqrt(n))\n\nn <- 1000\n\nse <- 10\n\nsd <- se * sqrt(n)\n\nsd\n\n\n[1] 316.2278\n\n\nThe below shows the t and p-value for Jones.\n\n\nCode\nrnorm_fixed <- function(n, mean, sd) {\n  as.vector(mean + sd * scale(rnorm(n)))\n}\n\ndata5_j <- rnorm_fixed(1000, 519.5, sd)\n\nt_test_j <- oneSampleTTest(x=data5_j, mu=500)\n\nt_test_j\n\n\n\n   One sample t-test \n\nData variable:   data5_j \n\nDescriptive statistics: \n            data5_j\n   mean     519.500\n   std dev. 316.228\n\nHypotheses: \n   null:        population mean equals 500 \n   alternative: population mean not equal to 500 \n\nTest results: \n   t-statistic:  1.95 \n   degrees of freedom:  999 \n   p-value:  0.051 \n\nOther information: \n   two-sided 95% confidence interval:  [499.877, 539.123] \n   estimated effect size (Cohen's d):  0.062 \n\n\nThe below shows the t and p-value for Smith.\n\n\nCode\ndata5_s <- rnorm_fixed(1000, 519.7, sd)\n\nt_test_s <- oneSampleTTest(x=data5_s, mu=500)\n\nt_test_s\n\n\n\n   One sample t-test \n\nData variable:   data5_s \n\nDescriptive statistics: \n            data5_s\n   mean     519.700\n   std dev. 316.228\n\nHypotheses: \n   null:        population mean equals 500 \n   alternative: population mean not equal to 500 \n\nTest results: \n   t-statistic:  1.97 \n   degrees of freedom:  999 \n   p-value:  0.049 \n\nOther information: \n   two-sided 95% confidence interval:  [500.077, 539.323] \n   estimated effect size (Cohen's d):  0.062"
  },
  {
    "objectID": "posts/dacss603hw2_LauraCollazo.html#b-1",
    "href": "posts/dacss603hw2_LauraCollazo.html#b-1",
    "title": "DACSS 603 Homework 2",
    "section": "B",
    "text": "B\nThe above indicates the results of Jones’ study (p = .051) are not statistically significant and the results of Smith’s study are statistically significant (p = .049) when α = 0.05."
  },
  {
    "objectID": "posts/dacss603hw2_LauraCollazo.html#c-1",
    "href": "posts/dacss603hw2_LauraCollazo.html#c-1",
    "title": "DACSS 603 Homework 2",
    "section": "C",
    "text": "C\nI am not yet certain of how to describe the misleading aspects of reporting the result of a test as “P is < or = to 0.05” versus “P is > 0.” Perhaps this has to do with the importance of saying that the null hypothesis is rejected if p is statistically significant rather than saying we accept the null hypothesis as true if p is not statistically significant as we should always assume the null hypothesis is true. I think this question is asking for something different than this, though. I look forward to seeing the answer to this question so I can learn from it!"
  },
  {
    "objectID": "posts/asch_harwood_hw2.html",
    "href": "posts/asch_harwood_hw2.html",
    "title": "Homework 2",
    "section": "",
    "text": "Code\nlibrary(dplyr)\nlibrary(ggplot2)\n\n\n\nQ1\n\n\nCode\n# create data frame\ndf <- data.frame(\n  \"Surgical Procedure\" = c(\"Bypass\", \"Angiography\"),\n  \"Sample Size\" = c(539, 847),\n  \"Mean Wait Time\" = c(19, 18),\n  \"Standard Deviation\" = c(10, 9)\n)\n\n# bypass confidence interval\nmean <- 19\nn <- 539\nsd <- 10\nci_level <- .9\ntail_area <- (1 - ci_level)/2\nt_score <- qt(p=1-tail_area, df = n - 1)\nb_ci <- c(mean - t_score * sd/sqrt(n), mean + t_score * sd/sqrt(n))\n\n# angiography\nmean <- 18\nn <- 847\nsd <- 9\nci_level <- .9\ntail_area <- (1 - ci_level)/2\nt_score <- qt(p=1-tail_area, df = n - 1)\na_ci <- c(mean - t_score * sd/sqrt(n), mean + t_score * sd/sqrt(n))\n\n\ncat(\"Bypass 90% Confidence Interval: (\", b_ci[1], \", \", b_ci[2], \")\", sep = \"\")\n\n\nBypass 90% Confidence Interval: (18.29029, 19.70971)\n\n\nCode\ncat(\"\\n\")\n\n\nCode\ncat(\"\\n\")\n\n\nCode\ncat(\"Angiography 90% Confidence Interval: (\", a_ci[1], \", \", a_ci[2], \")\", sep = \"\")\n\n\nAngiography 90% Confidence Interval: (17.49078, 18.50922)\n\n\n\nInterpretation\nBypass confidence interval: 18.29029, 19.70971 Angiography confidence interval: 17.49078 18.50922\nThe angiography confidence level is smaller.\n\n\n\nQ2\n\n\nCode\nn <- 1031 # num of americans survey\ncollege_essential <- 567 # num americans believe college is essential\npoint_estimate <- college_essential/n\nprop_results <- prop.test(college_essential, n)\n\ncat(\"College is essential point estimate:\", point_estimate)\n\n\nCollege is essential point estimate: 0.5499515\n\n\nCode\ncat(\"\\n\")\n\n\nCode\ncat(\"\\n\")\n\n\nCode\ncat(\"College is essential 95% Confidence Interval: (\", prop_results$conf.int[1], \", \", prop_results$conf.int[2], \")\", sep = \"\")\n\n\nCollege is essential 95% Confidence Interval: (0.5189682, 0.580558)\n\n\n\n\nQ3\n\n\nCode\n# solve for n:  CI = x + (t * (sd / sqrt(n)))\n# plug in value in this equation: n = (sd * t / (CI - x))^2\n#calculate mean\nhigh_price <- 200\nlow_price <- 30\n\nsd <- (high_price - low_price) * .25\nmean <- (high_price + low_price)/2\nci_lower <- mean - 5\nci_upper <- mean + 5\nci_level <- .95\nestimated_sample_size <- (sd * ci_level / (ci_upper - mean))^2\n\ncat(\"The estimated sample size is: \", round(estimated_sample_size))\n\n\nThe estimated sample size is:  65\n\n\n\n\nQ4\n\n\nCode\n# goal: test whether mean female incole is inline with union agreement\nmean_weekly_income_predicted <- 500\nn = 9\nmean_sampled = 410\nsd_sample = 90\n\n\n\nQ4_A\n\nApproach and Assumptions\n\nApproach 1: Construct confidence interval\nApproach 2: One-sample, two-tailed T-test\nAssumptions:\n\n\nSample is random and representative of underlying population\nData is normally distributed\nSample observations are independent\nPopulation standard deviation is not known\n\n\nHypothesis\n\n\nh0: female weekly salary is the same as union weekly ($410 == $500)\nh1: female weekly salary is different from the union weekly (higher/lower) ($410 != $500)\n\n\n\nCode\n#confidence interval calculation\nmean <- 410\nunion_mean <- 500\nn <- 9\nsd <- 90\nalpha <- 0.05\nci_level <- .95\ntail_area <- (1 - ci_level)/2\nt_score <- qt(p=1-tail_area, df = n - 1)\na_ci <- c(mean - t_score * sd/sqrt(n), mean + t_score * sd/sqrt(n))\n\n#p-value calculation\n\n#compute t_staistics\nt_stat <- (mean - union_mean)/(sd/sqrt(n))\n\n#computer degrees of freedom\ndf <- n - 1\n\n#calculate lower/left tail because t statistics is negative\n#multipy by two for two-tailed test\np_val <- 2*pt(t_stat, df)\n\ncat(\"Approach 1: Confidence Interval\")\n\n\nApproach 1: Confidence Interval\n\n\nCode\ncat(\"\\n\")\n\n\nCode\ncat(\"\\n\")\n\n\nCode\ncat(\"Female Salary 95% Confidence Interval: (\", a_ci[1], \", \", a_ci[2], \")\", sep = \"\")\n\n\nFemale Salary 95% Confidence Interval: (340.8199, 479.1801)\n\n\nCode\ncat(\"\\n\")\n\n\nCode\ncat(\"T score:\", t_score)\n\n\nT score: 2.306004\n\n\nCode\ncat(\"\\n\")\n\n\nCode\ncat(\"\\n\")\n\n\nCode\ncat(\"Interpretation\")\n\n\nInterpretation\n\n\nCode\ncat(\"\\n\")\n\n\nCode\ncat(\"\\n\")\n\n\nCode\ncat(\"The 95% confidence interval for the weekly female salary of $410 does not include $500, so we can reject the null hypothesis in favor of the alternative hypthosis that weekly female salary differs from $500 a week\")\n\n\nThe 95% confidence interval for the weekly female salary of $410 does not include $500, so we can reject the null hypothesis in favor of the alternative hypthosis that weekly female salary differs from $500 a week\n\n\nCode\ncat(\"Approach 2: Calculat P Value\")\n\n\nApproach 2: Calculat P Value\n\n\nCode\ncat(\"\\n\")\n\n\nCode\ncat(\"\\n\")\n\n\nCode\ncat(\"T statistic:\", t_stat)\n\n\nT statistic: -3\n\n\nCode\ncat(\"\\n\")\n\n\nCode\ncat(\"Two-tailed P Value\", p_val)\n\n\nTwo-tailed P Value 0.01707168\n\n\nCode\ncat(\"\\n\")\n\n\nCode\ncat(\"Is P value less than alpha?:\", p_val < alpha)\n\n\nIs P value less than alpha?: TRUE\n\n\nCode\ncat(\"\\n\")\n\n\nCode\ncat(\"\\n\")\n\n\nCode\ncat(\"Interpretation\")\n\n\nInterpretation\n\n\nCode\ncat(\"\\n\")\n\n\nCode\ncat(\"\\n\")\n\n\nCode\ncat(\"We can reject the null hypothesis in favor of the alternative hypothosis that weekly female salary differs from $500 a week because the observed p value of\", p_val, \"is less than our designated signifiance level of\", alpha, \".\")\n\n\nWe can reject the null hypothesis in favor of the alternative hypothosis that weekly female salary differs from $500 a week because the observed p value of 0.01707168 is less than our designated signifiance level of 0.05 .\n\n\n\n\n\nQ4_B\n\n\nCode\n#p-value calculation\n\nmean <- 410\nunion_mean <- 500\nn <- 9\nsd <- 90\nalpha <- 0.05\n\n#compute t_staistics\nt_stat <- (mean - union_mean)/(sd/sqrt(n))\n\n#computer degrees of freedom\ndf <- n - 1\n\np_val <- pt(t_stat, df, lower.tail = TRUE)\n\n#calculate lower/left tail because t statistics is negative\n#multipy by two for two-tailed test\ncat(\"Lower Tail P Value:\", p_val)\n\n\nLower Tail P Value: 0.008535841\n\n\nCode\ncat(\"\\n\")\n\n\nCode\ncat(\"This lower tail p value can be used to test whether the observed weekly female salary is less than the union wage. In this case, if we were doing a one-tailed t-test with an alpha significance level of 0.05, we could reject the null hypothesis in favor of the altnernative that weekly female salary is less than the union wage with a p value of\", p_val,\".\")\n\n\nThis lower tail p value can be used to test whether the observed weekly female salary is less than the union wage. In this case, if we were doing a one-tailed t-test with an alpha significance level of 0.05, we could reject the null hypothesis in favor of the altnernative that weekly female salary is less than the union wage with a p value of 0.008535841 .\n\n\n\n\nQ4_C\n\n\nCode\nmean <- 410\nunion_mean <- 500\nn <- 9\nsd <- 90\nalpha <- 0.05\n\n#compute t_staistics\nt_stat <- (mean - union_mean)/(sd/sqrt(n))\n\n#computer degrees of freedom\ndf <- n - 1\n\np_val <- pt(t_stat, df, lower.tail = FALSE)\n\n#calculate lower/left tail because t statistics is negative\n#multipy by two for two-tailed test\ncat(\"Upper Tail P Value:\", p_val)\n\n\nUpper Tail P Value: 0.9914642\n\n\nCode\ncat(\"\\n\")\n\n\nCode\ncat(\"This upper tail p value can be used to test whether the observed weekly female salary is more than the union wage. In this case, if we were doing a one-tailed t-test with an alpha significance level of 0.05, we cannot reject the null hypothesis in favor of the altnernative that weekly female salary is less than the union wage with a p value of\", p_val,\".\")\n\n\nThis upper tail p value can be used to test whether the observed weekly female salary is more than the union wage. In this case, if we were doing a one-tailed t-test with an alpha significance level of 0.05, we cannot reject the null hypothesis in favor of the altnernative that weekly female salary is less than the union wage with a p value of 0.9914642 .\n\n\n\n\n\nQ5\n\n\nCode\nn <- 1000 #sample size\ndf <- n - 1 #degrees of freedom\nse <- 10.0 #standard error\npop_mean <- 500 #assumed pop mean\ny_jones <- 519.5\ny_smith <- 519.7\nt_jones <- 1.95\nt_smith <- 1.97\np_jones <- 0.051\np_smith <- 0.049\n\n# helper functions - computes t-stat\nt_stat_func <- function(mean_1, test_mean, standard_error){\n  t_stat <- (mean_1 - test_mean)/(standard_error)\n  return(t_stat)\n}\n\n#helper function - computers p-value\np_val_func <- function(t_stat, df, tail_type){\n  if (t_stat < 0){\n    p_val <- pt(t_stat, df, lower.tail = TRUE)\n  } else{\n    p_val <- pt(t_stat, df, lower.tail = FALSE)\n  }\n  if (tail_type==2){\n    p_val <- p_val*2\n  }else{p_val}\n  \n  return(p_val)\n}\n\n\n\nQ5_A\n\n\nCode\n#compute t_staistics for jones\n\nt_stat_jones <- t_stat_func(y_jones, pop_mean, se)\n\np_val_jones <- p_val_func(t_stat_jones, df, 2)\n\nt_stat_smith <- t_stat_func(y_smith, pop_mean, se)\n\np_val_smith <- p_val_func(t_stat_smith, df, 2)\n\ncat(\"Jones\")\n\n\nJones\n\n\nCode\ncat(\"\\n\")\n\n\nCode\ncat(\"T statistic:\", t_stat_jones,\",\", \"P value:\", p_val_jones)\n\n\nT statistic: 1.95 , P value: 0.05145555\n\n\nCode\ncat(\"\\n\")\n\n\nCode\ncat(\"\\n\")\n\n\nCode\ncat(\"Smith\")\n\n\nSmith\n\n\nCode\ncat(\"\\n\")\n\n\nCode\ncat(\"T statistic:\", t_stat_smith,\",\", \"P value:\", p_val_smith)\n\n\nT statistic: 1.97 , P value: 0.04911426\n\n\n\n\nQ5_B\nWith an alpha of 0.05, Smith’s result is statistically significant, while Jones’s result is not.\n\n\nQ5_C\nThe chosen significant level (alpha = 0.05, for example) is typically a rule of thumb in the social sciences. In this case, a decision to round a value to decimal places would put both of these p-values at 0.05. Context matters as well. Depending on what is being measured in this example, the difference between 519.5 versus 519.7 might not be meaningful, or it could represent a huge real world difference. The implications/consequences of committing type 1 or type 2 error could be pertinent in the interpretation of these p_values as well. By reporting the actual p-values, readers are empowered to better evaluate the decisions of the researchers.\n\n\n\nQ6\n\nNull Hypothesis\nH0: There is no association between snack choice and grade level\n6th_grade_healthy_snack_31 = 7th_grade_healthy_snack_43 = 8th_grade_healthy_snack_51\n6th_grade_unhealthy_snack_69 = 7th_grade_healthy_snack_57 = 8th_grade_healthy_snack_49\n\n\nChi Squared Test\n\nWe will use a chi-squared test\n\n\n\nCode\nsnack_counts <- matrix(\n  c(31, 43, 51, 69, 57, 49),\n  nrow = 2,\n  byrow = TRUE\n)\nrownames(snack_counts) <- c(\"Healthy_Snack\", \"Unhealthy_Snack\")\ncolnames(snack_counts) <- c(\"6th grade\", \"7th grade\", \"8th grade\")\nsnack_counts\n\n\n                6th grade 7th grade 8th grade\nHealthy_Snack          31        43        51\nUnhealthy_Snack        69        57        49\n\n\nCode\n# Perform the Chi-square test\nchi_square_test <- chisq.test(snack_counts)\n\n# Print the test results\nprint(chi_square_test)\n\n\n\n    Pearson's Chi-squared test\n\ndata:  snack_counts\nX-squared = 8.3383, df = 2, p-value = 0.01547\n\n\n\n\nInterpretation\nWe can reject the null hypothesis that there is no association between snack choice and grade in favor of the alternative hypothesis that there is an association between snack choice and grade\n\n\nCode\n# Create a dataframe\nsnack_data <- data.frame(\n  Grade_Level = c(\"6th grade\", \"7th grade\", \"8th grade\"),\n  Healthy_Snack = c(31, 43, 51),\n  Unhealthy_Snack = c(69, 57, 49)\n)\n\n# Calculate the proportions\nsnack_data$Total <- snack_data$Healthy_Snack + snack_data$Unhealthy_Snack\nsnack_data$Healthy_Prop <- snack_data$Healthy_Snack / snack_data$Total\nsnack_data$Unhealthy_Prop <- snack_data$Unhealthy_Snack / snack_data$Total\n\n# Reshape the data to a long format\nsnack_data_long <- tidyr::gather(snack_data, key = \"Snack_Type\", value = \"Proportion\", Healthy_Prop, Unhealthy_Prop)\n\n# Create a bar plot using ggplot2\nggplot(snack_data_long, aes(x = Grade_Level, y = Proportion, fill = Snack_Type)) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  labs(title = \"Proportion of Healthy and Unhealthy Snacks by Grade Level\",\n       x = \"Grade Level\",\n       y = \"Proportion\",\n       fill = \"Snack Type\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nQ7\n\n\nCode\n# Data from the table\ndata_matrix <- matrix(\n  c(\n    6.2, 9.3, 6.8, 6.1, 6.7, 7.5,\n    7.5, 8.2, 8.5, 8.2, 7.0, 9.3,\n    5.8, 6.4, 5.6, 7.1, 3.0, 3.5\n  ),\n  nrow = 3,\n  byrow = TRUE\n)\n# Create a dataframe with Area and Value columns\ndata_df <- data.frame(\n  Area = factor(rep(c(\"Area 1\", \"Area 2\", \"Area 3\"), each = 6)),\n  Value = as.vector(t(data_matrix))\n)\n\nmean_df <- data_df %>%\n  group_by(Area) %>%\n  summarise(mean_val = mean(Value))\n\n\n\nAnswer\n\nNull Hypothesis\nH0: All groups have the same population mean\narea_1 = area_2 = area_3 is TRUE\nH1: At least one group has a different population mean\narea_1 = area_2 = area_3 is FALSE\n\n\nANOVA\n\nSince we are trying to determine if the observed differences in area means are statistically significant, we will use a one-way analysis of variance test\n\n\n\nCode\nanova_results <- aov(Value ~ Area, data_df)\nsummary(anova_results)\n\n\n            Df Sum Sq Mean Sq F value  Pr(>F)   \nArea         2  25.66  12.832   8.176 0.00397 **\nResiduals   15  23.54   1.569                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\nInterpretation\nWith a P value of 0.00397 we can reject the null hypothesis that there is no difference between the three group means in favor of the alternative hypothesis that there is indeed a difference."
  },
  {
    "objectID": "posts/FinalProject_Part1_AlexaPotter.html",
    "href": "posts/FinalProject_Part1_AlexaPotter.html",
    "title": "Final Project Part 1",
    "section": "",
    "text": "Research Question\nGreenery is widely recognized as a vital element to any public space. Plants and natural elements of all kinds can add immense benefits to urban areas, not only to the environment but to the humans who inhabit the space as well. Studies have shown that urban greening, specifically the planting of trees, can “combat challenges such as pollution, urban heat, and flooding, as well as to improve social cohesion, human health, and well-being.”1 The understanding and implementation of this information can lead cities, developers, and anyone with a stake in their community to consciously consider what elements they can incorporate into their own public spaces. The work does not end here though, greenery in public spaces requires maintenance and year-round management to make these efforts last long enough to see the benefits.\nNew York City, with a population of about 8.4 billion people and 300 square miles, is one of the largest urban spaces in the United States.2 In 1995, the New York City Department of Parks and Recreation conducted a city wide census of all the trees. They again conducted this survey in 2005 and 2015 to tackle their goal of enhancing and restoring urban forests.3 The information this survey collected has been used to create an interactive map of tree species around New York City. The Parks department then uses this data to calculate the related impacts and needs associated with the trees and tree maintenance.4\nWhile there is a large amount of data related to the tree census published, there is a gap in information on the relation between tree data to specific neighborhoods. Critically analyzing this tree data on a neighborhood level can lead to further community involvement within and stemming from neighborhoods themselves.5 Firsthand involvement can be used to develop connections between community members and foster ownership among members with the environment they inhabit.\nThis informs my research question:\n\nWhat are the significant differences in tree characteristics across New York City neighborhoods?\n\n\n\nHypothesis\nNew York City itself is a diverse landscape with features both conducive and preventive to tree growth and sustainability. Neighborhoods across the city can see significant changes in income, traffic, infrastructure, natural resources such water or sunlight, and attitudes towards the environment.6 Tree size can relay information to researchers on the development and sustainability of their goal to promote urban greening.7 This information can then be used to inform and direct spending and efforts. While there exists a study by Jian Lin, Qiang Wang, and Xiaojiang Li regarding tree characteristics related to socioeconomic and spatial inequalities, my research aims to focus in tree characteristics to specific named neighborhoods.8 Rather than using socioeconomic elements, my research question is focused on smaller geographic areas where individuals can identify themselves as members on a more personal level than a borough.\nWith these factors, I can test the hypothesis’:\nH1: There is a significant difference in tree diameter across New York City neighborhoods\nH2: There is a significant difference in tree health across New York City neighborhoods\nH3: There is a significant difference in tree species across New York City neighborhoods\n\n\nDescriptive Statistics\n\n\nCode\nlibrary(tidyverse)\n\n\n-- Attaching packages --------------------------------------- tidyverse 1.3.2 --\nv ggplot2 3.4.0      v purrr   0.3.5 \nv tibble  3.1.8      v dplyr   1.0.10\nv tidyr   1.2.1      v stringr 1.5.0 \nv readr   2.1.3      v forcats 0.5.2 \n-- Conflicts ------------------------------------------ tidyverse_conflicts() --\nx dplyr::filter() masks stats::filter()\nx dplyr::lag()    masks stats::lag()\n\n\nCode\nlibrary(ggplot2)\nlibrary(formattable)\nlibrary(dbplyr)\n\n\n\nAttaching package: 'dbplyr'\n\nThe following objects are masked from 'package:dplyr':\n\n    ident, sql\n\n\nCode\nlibrary(summarytools)\n\n\n\nAttaching package: 'summarytools'\n\nThe following object is masked from 'package:tibble':\n\n    view\n\n\n\n\nCode\nlibrary(readxl)\ntreecensus <- read_excel(\"C:/Users/aep05/Documents/UMASS_GRAD_SCHOOL/DACSS 603/603_Spring_2023/posts/finalproject/2015_Street_Tree_Census.xls\")\nprint(treecensus)\n\n\n# A tibble: 16,383 x 45\n   tree_id block_id created_at          tree_dbh stump_d~1 curb_~2 status health\n     <dbl>    <dbl> <dttm>                 <dbl>     <dbl> <chr>   <chr>  <chr> \n 1  180683   348711 2015-08-27 00:00:00        3         0 OnCurb  Alive  Fair  \n 2  200540   315986 2015-09-03 00:00:00       21         0 OnCurb  Alive  Fair  \n 3  204026   218365 2015-09-05 00:00:00        3         0 OnCurb  Alive  Good  \n 4  204337   217969 2015-09-05 00:00:00       10         0 OnCurb  Alive  Good  \n 5  189565   223043 2015-08-30 00:00:00       21         0 OnCurb  Alive  Good  \n 6  190422   106099 2015-08-30 00:00:00       11         0 OnCurb  Alive  Good  \n 7  190426   106099 2015-08-30 00:00:00       11         0 OnCurb  Alive  Good  \n 8  208649   103940 2015-09-07 00:00:00        9         0 OnCurb  Alive  Good  \n 9  209610   407443 2015-09-08 00:00:00        6         0 OnCurb  Alive  Good  \n10  192755   207508 2015-08-31 00:00:00       21         0 Offset~ Alive  Fair  \n# ... with 16,373 more rows, 37 more variables: spc_latin <chr>,\n#   spc_common <chr>, steward <chr>, guards <chr>, sidewalk <chr>,\n#   user_type <chr>, problems <chr>, root_stone <chr>, root_grate <chr>,\n#   root_other <chr>, trunk_wire <chr>, trnk_light <chr>, trnk_other <chr>,\n#   brch_light <chr>, brch_shoe <chr>, brch_other <chr>, address <chr>,\n#   postcode <dbl>, zip_city <chr>, `community board` <dbl>, borocode <dbl>,\n#   borough <chr>, cncldist <dbl>, st_assem <dbl>, st_senate <dbl>, ...\n\n\nThis data set is obtained from the 2015 tree census through NYC Open Data provided by the New York City Department of Parks and Recreation.9\nThe information was collected by predominantly volunteers in addition to the New York City Department of Parks and Recreation staff.\nThere are 45 variables and 16,383 observations.\n\n\nCode\nsummary(treecensus)\n\n\n    tree_id          block_id        created_at                 \n Min.   :   306   Min.   :100078   Min.   :2015-05-19 00:00:00  \n 1st Qu.:182110   1st Qu.:216192   1st Qu.:2015-08-28 00:00:00  \n Median :195703   Median :301941   Median :2015-09-01 00:00:00  \n Mean   :189968   Mean   :283657   Mean   :2015-08-30 16:07:33  \n 3rd Qu.:206443   3rd Qu.:345725   3rd Qu.:2015-09-06 00:00:00  \n Max.   :276846   Max.   :516315   Max.   :2015-10-01 00:00:00  \n                                                                \n    tree_dbh        stump_diam         curb_loc            status         \n Min.   :  0.00   Min.   :  0.0000   Length:16383       Length:16383      \n 1st Qu.:  4.00   1st Qu.:  0.0000   Class :character   Class :character  \n Median :  9.00   Median :  0.0000   Mode  :character   Mode  :character  \n Mean   : 10.87   Mean   :  0.5174                                        \n 3rd Qu.: 15.00   3rd Qu.:  0.0000                                        \n Max.   :425.00   Max.   :140.0000                                        \n                                                                          \n    health           spc_latin          spc_common          steward         \n Length:16383       Length:16383       Length:16383       Length:16383      \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n                                                                            \n    guards            sidewalk          user_type           problems        \n Length:16383       Length:16383       Length:16383       Length:16383      \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n                                                                            \n  root_stone         root_grate         root_other         trunk_wire       \n Length:16383       Length:16383       Length:16383       Length:16383      \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n                                                                            \n  trnk_light         trnk_other         brch_light         brch_shoe        \n Length:16383       Length:16383       Length:16383       Length:16383      \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n                                                                            \n  brch_other          address             postcode       zip_city        \n Length:16383       Length:16383       Min.   :   83   Length:16383      \n Class :character   Class :character   1st Qu.:10308   Class :character  \n Mode  :character   Mode  :character   Median :11207   Mode  :character  \n                                       Mean   :10858                     \n                                       3rd Qu.:11356                     \n                                       Max.   :11697                     \n                                                                         \n community board    borocode      borough             cncldist    \n Min.   :101.0   Min.   :1.00   Length:16383       Min.   : 1.00  \n 1st Qu.:212.0   1st Qu.:2.00   Class :character   1st Qu.:17.00  \n Median :315.0   Median :3.00   Mode  :character   Median :29.00  \n Mean   :322.4   Mean   :3.16                      Mean   :28.15  \n 3rd Qu.:407.0   3rd Qu.:4.00                      3rd Qu.:39.00  \n Max.   :503.0   Max.   :5.00                      Max.   :51.00  \n                                                                  \n    st_assem       st_senate         nta              nta_name        \n Min.   :23.00   Min.   :10.00   Length:16383       Length:16383      \n 1st Qu.:36.00   1st Qu.:15.00   Class :character   Class :character  \n Median :52.00   Median :21.00   Mode  :character   Mode  :character  \n Mean   :51.51   Mean   :21.06                                        \n 3rd Qu.:65.00   3rd Qu.:26.00                                        \n Max.   :87.00   Max.   :36.00                                        \n                                                                      \n    boro_ct           state              latitude       longitude     \n Min.   :1000201   Length:16383       Min.   :40.50   Min.   :-74.25  \n 1st Qu.:2044200   Class :character   1st Qu.:40.64   1st Qu.:-73.98  \n Median :3063800   Mode  :character   Median :40.72   Median :-73.95  \n Mean   :3196724                      Mean   :40.71   Mean   :-73.94  \n 3rd Qu.:4071900                      3rd Qu.:40.77   3rd Qu.:-73.89  \n Max.   :5030302                      Max.   :40.91   Max.   :-73.71  \n                                                                      \n      x_sp              y_sp        council district  census tract   \n Min.   : 914125   Min.   :121318   Min.   : 1.0     Min.   :     1  \n 1st Qu.: 990034   1st Qu.:171719   1st Qu.:18.0     1st Qu.:   159  \n Median : 998821   Median :200164   Median :29.0     Median :   393  \n Mean   : 999537   Mean   :196583   Mean   :28.2     Mean   :  8356  \n 3rd Qu.:1015348   3rd Qu.:219429   3rd Qu.:39.0     3rd Qu.:  1113  \n Max.   :1065861   Max.   :269471   Max.   :51.0     Max.   :157901  \n                                    NA's   :169      NA's   :169     \n      bin               bbl           \n Min.   :1000000   Min.   :1.000e+09  \n 1st Qu.:2112823   1st Qu.:2.054e+09  \n Median :3255106   Median :3.067e+09  \n Mean   :3272538   Mean   :3.200e+09  \n 3rd Qu.:4143916   3rd Qu.:4.041e+09  \n Max.   :5166656   Max.   :5.079e+09  \n NA's   :267       NA's   :267        \n\n\nFull list of variables:\n\n\nCode\ncolnames(treecensus)\n\n\n [1] \"tree_id\"          \"block_id\"         \"created_at\"       \"tree_dbh\"        \n [5] \"stump_diam\"       \"curb_loc\"         \"status\"           \"health\"          \n [9] \"spc_latin\"        \"spc_common\"       \"steward\"          \"guards\"          \n[13] \"sidewalk\"         \"user_type\"        \"problems\"         \"root_stone\"      \n[17] \"root_grate\"       \"root_other\"       \"trunk_wire\"       \"trnk_light\"      \n[21] \"trnk_other\"       \"brch_light\"       \"brch_shoe\"        \"brch_other\"      \n[25] \"address\"          \"postcode\"         \"zip_city\"         \"community board\" \n[29] \"borocode\"         \"borough\"          \"cncldist\"         \"st_assem\"        \n[33] \"st_senate\"        \"nta\"              \"nta_name\"         \"boro_ct\"         \n[37] \"state\"            \"latitude\"         \"longitude\"        \"x_sp\"            \n[41] \"y_sp\"             \"council district\" \"census tract\"     \"bin\"             \n[45] \"bbl\"             \n\n\nThe columns I am interested in are:\ntree_id - Unique identification number for each tree point.\ntree_dbh - Diameter of the tree, measured at approximately 54” / 137cm above the ground. Data was collected for both living and dead trees; for stumps, use stump_diam\nstatus - Indicates whether the tree is alive, standing dead, or a stump.\nhealth - Indicates the user’s perception of tree health.\nspc_latin - Scientific name for species, e.g. “Acer rubrum”\nspc_common - Common name for species, e.g. “red maple”\npostcode - Five-digit zipcode in which tree is located\nzip_city - City as derived from zipcode. This is often (but not always) the same as borough.\nborocode - Code for borough in which tree point is located: 1 (Manhattan), 2 (Bronx), 3 (Brooklyn), 4 (Queens), 5 (Staten Island)\nborough - Name of borough in which tree point is located\nnta - This is the NTA Code corresponding to the neighborhood tabulation area from the 2010 US Census that the tree point falls into.\nnta_name - This is the NTA name corresponding to the neighborhood tabulation area from the 2010 US Census that the tree point falls into.\nboro_ct - This is the boro_ct identifyer for the census tract that the tree point falls into.\n\n\nCode\ntreecensus_clean <- select(treecensus, \n                    tree_id, \n                    tree_dbh, \n                    status, \n                    health, \n                    spc_latin,\n                    spc_common,\n                    postcode,\n                    zip_city,\n                    borocode,\n                    borough,\n                    nta,\n                    nta_name,\n                    boro_ct,\n                    )\n\n\n\n\nCode\ngroup_by(treecensus_clean, status, health)%>%\n summarize()\n\n\n`summarise()` has grouped output by 'status'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 5 x 2\n# Groups:   status [3]\n  status health\n  <chr>  <chr> \n1 Alive  Fair  \n2 Alive  Good  \n3 Alive  Poor  \n4 Dead   <NA>  \n5 Stump  <NA>  \n\n\nBecause I am only interested in trees with the status “Alive”, I’ll also need to clean the data set to exclude trees labeled as “Dead” or “Stump”.\n\n\nCode\ntreecensus_clean <- treecensus_clean[treecensus_clean$status != \"Dead\" & treecensus_clean$status != \"Stump\",]\ngroup_by(treecensus_clean, status, health)%>%\n summarize()\n\n\n`summarise()` has grouped output by 'status'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 3 x 2\n# Groups:   status [1]\n  status health\n  <chr>  <chr> \n1 Alive  Fair  \n2 Alive  Good  \n3 Alive  Poor  \n\n\nNow we return to the summary function to view the cleaned data frame.\n\n\nCode\nsummary(treecensus_clean)\n\n\n    tree_id          tree_dbh         status             health         \n Min.   :   306   Min.   :  0.00   Length:15442       Length:15442      \n 1st Qu.:181321   1st Qu.:  5.00   Class :character   Class :character  \n Median :195362   Median : 10.00   Mode  :character   Mode  :character  \n Mean   :189311   Mean   : 11.38                                        \n 3rd Qu.:206165   3rd Qu.: 16.00                                        \n Max.   :227192   Max.   :425.00                                        \n  spc_latin          spc_common           postcode       zip_city        \n Length:15442       Length:15442       Min.   :   83   Length:15442      \n Class :character   Class :character   1st Qu.:10308   Class :character  \n Mode  :character   Mode  :character   Median :11207   Mode  :character  \n                                       Mean   :10855                     \n                                       3rd Qu.:11355                     \n                                       Max.   :11697                     \n    borocode      borough              nta              nta_name        \n Min.   :1.00   Length:15442       Length:15442       Length:15442      \n 1st Qu.:2.00   Class :character   Class :character   Class :character  \n Median :3.00   Mode  :character   Mode  :character   Mode  :character  \n Mean   :3.16                                                           \n 3rd Qu.:4.00                                                           \n Max.   :5.00                                                           \n    boro_ct       \n Min.   :1000201  \n 1st Qu.:2044200  \n Median :3063800  \n Mean   :3196920  \n 3rd Qu.:4072100  \n Max.   :5030302  \n\n\n\n\nCode\ndfSummary(treecensus_clean)\n\n\nData Frame Summary  \ntreecensus_clean  \nDimensions: 15442 x 13  \nDuplicates: 0  \n\n-----------------------------------------------------------------------------------------------------------------------\nNo   Variable      Stats / Values                   Freqs (% of Valid)      Graph                  Valid      Missing  \n---- ------------- -------------------------------- ----------------------- ---------------------- ---------- ---------\n1    tree_id       Mean (sd) : 189310.8 (27189.2)   15442 distinct values                   :      15442      0        \n     [numeric]     min < med < max:                                                         :      (100.0%)   (0.0%)   \n                   306 < 195361.5 < 227192                                                  : :                        \n                   IQR (CV) : 24844 (0.1)                                                 : : :                        \n                                                                                        . : : :                        \n\n2    tree_dbh      Mean (sd) : 11.4 (8.7)           54 distinct values      :                      15442      0        \n     [numeric]     min < med < max:                                         :                      (100.0%)   (0.0%)   \n                   0 < 10 < 425                                             :                                          \n                   IQR (CV) : 11 (0.8)                                      :                                          \n                                                                            :                                          \n\n3    status        1. Alive                         15442 (100.0%)          IIIIIIIIIIIIIIIIIIII   15442      0        \n     [character]                                                                                   (100.0%)   (0.0%)   \n\n4    health        1. Fair                           3085 (20.0%)           III                    15442      0        \n     [character]   2. Good                          11434 (74.0%)           IIIIIIIIIIIIII         (100.0%)   (0.0%)   \n                   3. Poor                            923 ( 6.0%)           I                                          \n\n5    spc_latin     1. Platanus x acerifolia         1981 (12.8%)            II                     15442      0        \n     [character]   2. Gleditsia triacanthos var     1610 (10.4%)            II                     (100.0%)   (0.0%)   \n                   3. Quercus palustris             1354 ( 8.8%)            I                                          \n                   4. Pyrus calleryana              1350 ( 8.7%)            I                                          \n                   5. Acer platanoides               936 ( 6.1%)            I                                          \n                   6. Tilia cordata                  783 ( 5.1%)            I                                          \n                   7. Zelkova serrata                752 ( 4.9%)                                                       \n                   8. Styphnolobium japonicum        733 ( 4.7%)                                                       \n                   9. Ginkgo biloba                  629 ( 4.1%)                                                       \n                   10. Fraxinus pennsylvanica        399 ( 2.6%)                                                       \n                   [ 105 others ]                   4915 (31.8%)            IIIIII                                     \n\n6    spc_common    1. London planetree              1981 (12.8%)            II                     15442      0        \n     [character]   2. honeylocust                   1610 (10.4%)            II                     (100.0%)   (0.0%)   \n                   3. pin oak                       1354 ( 8.8%)            I                                          \n                   4. Callery pear                  1350 ( 8.7%)            I                                          \n                   5. Norway maple                   936 ( 6.1%)            I                                          \n                   6. littleleaf linden              783 ( 5.1%)            I                                          \n                   7. Japanese zelkova               752 ( 4.9%)                                                       \n                   8. Sophora                        733 ( 4.7%)                                                       \n                   9. ginkgo                         629 ( 4.1%)                                                       \n                   10. green ash                     399 ( 2.6%)                                                       \n                   [ 105 others ]                   4915 (31.8%)            IIIIII                                     \n\n7    postcode      Mean (sd) : 10855.4 (740.9)      169 distinct values                       :    15442      0        \n     [numeric]     min < med < max:                                                           :    (100.0%)   (0.0%)   \n                   83 < 11207 < 11697                                                       : :                        \n                   IQR (CV) : 1047 (0.1)                                                    : :                        \n                                                                                            : :                        \n\n8    zip_city      1. Brooklyn                      4384 (28.4%)            IIIII                  15442      0        \n     [character]   2. New York                      2612 (16.9%)            III                    (100.0%)   (0.0%)   \n                   3. Staten Island                 1970 (12.8%)            II                                         \n                   4. Bronx                         1311 ( 8.5%)            I                                          \n                   5. Astoria                        914 ( 5.9%)            I                                          \n                   6. Jackson Heights                377 ( 2.4%)                                                       \n                   7. Forest Hills                   366 ( 2.4%)                                                       \n                   8. Flushing                       288 ( 1.9%)                                                       \n                   9. Ridgewood                      257 ( 1.7%)                                                       \n                   10. Kew Gardens                   252 ( 1.6%)                                                       \n                   [ 34 others ]                    2711 (17.6%)            III                                        \n\n9    borocode      Mean (sd) : 3.2 (1.3)            1 : 2646 (17.1%)        III                    15442      0        \n     [numeric]     min < med < max:                 2 : 1311 ( 8.5%)        I                      (100.0%)   (0.0%)   \n                   1 < 3 < 5                        3 : 4384 (28.4%)        IIIII                                      \n                   IQR (CV) : 2 (0.4)               4 : 5131 (33.2%)        IIIIII                                     \n                                                    5 : 1970 (12.8%)        II                                         \n\n10   borough       1. Bronx                         1311 ( 8.5%)            I                      15442      0        \n     [character]   2. Brooklyn                      4384 (28.4%)            IIIII                  (100.0%)   (0.0%)   \n                   3. Manhattan                     2646 (17.1%)            III                                        \n                   4. Queens                        5131 (33.2%)            IIIIII                                     \n                   5. Staten Island                 1970 (12.8%)            II                                         \n\n11   nta           1. QN72                            726 ( 4.7%)                                  15442      0        \n     [character]   2. SI25                            611 ( 4.0%)                                  (100.0%)   (0.0%)   \n                   3. MN12                            547 ( 3.5%)                                                      \n                   4. BK37                            533 ( 3.5%)                                                      \n                   5. QN28                            486 ( 3.1%)                                                      \n                   6. QN17                            419 ( 2.7%)                                                      \n                   7. QN70                            323 ( 2.1%)                                                      \n                   8. SI48                            306 ( 2.0%)                                                      \n                   9. QN60                            286 ( 1.9%)                                                      \n                   10. SI45                           272 ( 1.8%)                                                      \n                   [ 172 others ]                   10933 (70.8%)           IIIIIIIIIIIIII                             \n\n12   nta_name      1. Steinway                        726 ( 4.7%)                                  15442      0        \n     [character]   2. Oakwood-Oakwood Beach           611 ( 4.0%)                                  (100.0%)   (0.0%)   \n                   3. Upper West Side                 547 ( 3.5%)                                                      \n                   4. Park Slope-Gowanus              533 ( 3.5%)                                                      \n                   5. Jackson Heights                 486 ( 3.1%)                                                      \n                   6. Forest Hills                    419 ( 2.7%)                                                      \n                   7. Astoria                         323 ( 2.1%)                                                      \n                   8. Arden Heights                   306 ( 2.0%)                                                      \n                   9. Kew Gardens                     286 ( 1.9%)                                                      \n                   10. New Dorp-Midland Beach         272 ( 1.8%)                                                      \n                   [ 172 others ]                   10933 (70.8%)           IIIIIIIIIIIIII                             \n\n13   boro_ct       Mean (sd) : 3196920 (1267854)    1015 distinct values                  :        15442      0        \n     [numeric]     min < med < max:                                                   .   :        (100.0%)   (0.0%)   \n                   1000201 < 3063800 < 5030302                              .         :   :                            \n                   IQR (CV) : 2027900 (0.4)                                 :   .     :   :   :                        \n                                                                            :   :   . :   :   :                        \n-----------------------------------------------------------------------------------------------------------------------\n\n\n\n\n\n\n\n\n\nFootnotes\n\n\nCavender, N., & Donnelly, G. (2019). Intersecting Urban Forestry and botanical gardens to address big challenges for healthier trees, people, and cities. PLANTS, PEOPLE, PLANET, 1(4), 315–322. https://doi.org/10.1002/ppp3.38↩︎\nU.S. Census Bureau . (n.d.). U.S. Census Bureau quickfacts: New York City, New York. Retrieved March 20, 2023, from https://www.census.gov/quickfacts/newyorkcitynewyork↩︎\nMerriman, D. (2017) Volunteers count every street tree in New York City. US Forest Service. Retrieved March 20, 2023, from https://www.fs.usda.gov/features/volunteers-count-every-street-tree-new-york-city-0↩︎\nCochran, C., & Greer, B. (2016, June 29). Treescount! 2015: NYC’s Third Street-Tree Census. New York State Urban Forestry Council. Retrieved March 20, 2023, from https://nysufc.org/treescount/2016/04/26/↩︎\nMa, Q., Lin, J., Ju, Y. et al. Individual structure mapping over six million trees for New York City USA. Sci Data 10, 102 (2023). https://doi.org/10.1038/s41597-023-02000-w↩︎\nNeckerman, K., Lovasi, G., Davies, S. et al. Disparities in Urban Neighborhood Conditions: Evidence from GIS Measures and Field Observation in New York City. Public Health Pol 30 (Suppl 1), S264–S285 (2009). https://doi.org/10.1057/jphp.2008.47↩︎\nColleen E. Reid, Laura D. Kubzansky, Jiayue Li, Jessie L. Shmool, Jane E. Clougherty. It’s not easy assessing greenness: A comparison of NDVI datasets and neighborhood types and their associations with self-rated health in New York City. Health & Place 54, 92-101 (2018).https://doi.org/10.1016/j.healthplace.2018.09.005.↩︎\nJian Lin, Qiang Wang, Xiaojiang Li. Socioeconomic and spatial inequalities of street tree abundance, species diversity, and size structure in New York City. Landscape and Urban Planning, Volume 206. 2021. 103992. https://doi.org/10.1016/j.landurbplan.2020.103992.↩︎\nhttps://data.cityofnewyork.us/Environment/2015-Street-Tree-Census-Tree-Data/uvpi-gqnh↩︎"
  },
  {
    "objectID": "posts/HW1_AkhileshKumarMeghwal.html",
    "href": "posts/HW1_AkhileshKumarMeghwal.html",
    "title": "Homework 1 - Akhilesh Kumar",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(ggplot2)\nlibrary(stats)\nlibrary(viridis)\n\nknitr::opts_chunk$set(echo = TRUE, warning = FALSE)"
  },
  {
    "objectID": "posts/HW1_AkhileshKumarMeghwal.html#reading-data",
    "href": "posts/HW1_AkhileshKumarMeghwal.html#reading-data",
    "title": "Homework 1 - Akhilesh Kumar",
    "section": "Reading Data",
    "text": "Reading Data\nFirst, let’s read in the data from the Excel file:\n\n\nCode\nlibrary(readxl)\ndf <- read_excel(\"_data/LungCapData.xls\")\n\n\nThe distribution of LungCap looks as follows:\nThe LungCap dataset contains 725 observations and six variables, namely LungCap, Age, Height, Smoke, Gender, and Cesarean. LungCap is the primary variable of interest, measuring the lung capacity of participants. Age and Height provide demographic information, indicating the age and height of each participant.\nEach row in the dataset corresponds to a unique participant and provides their values for the six variables. Smoke is a categorical variable that denotes whether the participant smokes or not. Gender is another categorical variable that identifies the gender of each participant. Cesarean is a binary variable indicating whether the participant was born via cesarean section or not.\nThe LungCap dataset may be useful in exploring the relationships between lung capacity, demographic variables, and smoking status."
  },
  {
    "objectID": "posts/HW1_AkhileshKumarMeghwal.html#a-what-does-the-distribution-of-lungcap-look-like",
    "href": "posts/HW1_AkhileshKumarMeghwal.html#a-what-does-the-distribution-of-lungcap-look-like",
    "title": "Homework 1 - Akhilesh Kumar",
    "section": "1a: What does the distribution of LungCap look like?",
    "text": "1a: What does the distribution of LungCap look like?\nThe following plot displays the distribution of LungCap:\n\n\nCode\nlibrary(tidyverse)\nlibrary(ggplot2)\n\ndf %>%\n  ggplot(aes(LungCap, ..density..)) +\n  geom_histogram(bins = 25, color = \"black\", fill = \"orange\") +\n  geom_density(color = \"darkblue\", linewidth = 1.25) +\n  theme_classic() + \n  theme(plot.title = element_text(hjust = 0.65, color = \"darkblue\")) +\n  theme(axis.text = element_text(face = \"bold\", color=\"darkblue\"), axis.title = element_text(face = \"bold\", size=12, color=\"darkblue\")) +\n  xlab(\"Lung Capacity\") +\n  ylab(\"Probability Density\") +\n  labs(title = \"Probability Distribution of LungCap\")\n\n\n\n\n\nThe plot displays the distribution of lung capacity measurements in a sample of individuals. The histogram and density plots indicate that the data is approximately normally distributed, with the majority of observations clustered near the mean. Specifically, the histogram shows a bell-shaped curve, with the highest frequency of observations centered around the mean value.\nThe density plot shows a smooth, symmetric curve, which closely follows the histogram and suggests that the data follows a normal distribution. These results are consistent with what we might expect, given that lung capacity is a physiological measure that is likely to vary around a central tendency. Taken together, these findings suggest that the sample data is well-behaved and that the majority of individuals in the sample have lung capacities that are close to the average value."
  },
  {
    "objectID": "posts/HW1_AkhileshKumarMeghwal.html#b-compare-the-probability-distribution-of-the-lungcap-with-respect-to-males-and-females",
    "href": "posts/HW1_AkhileshKumarMeghwal.html#b-compare-the-probability-distribution-of-the-lungcap-with-respect-to-males-and-females",
    "title": "Homework 1 - Akhilesh Kumar",
    "section": "1b: Compare the probability distribution of the LungCap with respect to Males and Females?",
    "text": "1b: Compare the probability distribution of the LungCap with respect to Males and Females?\n\n\nCode\ndf %>%\n  ggplot(aes(y = dnorm(LungCap), x = Gender, color = Gender)) +\n  geom_boxplot(alpha = 0.5) +\n  scale_fill_manual(values = c(\"green\", \"orange\")) +\n  theme_classic() + \n  theme(plot.title = element_text(hjust = 0.5, color = \"darkblue\", face = \"bold\", size = 14),\n        axis.text = element_text(face = \"bold\", color=\"darkblue\"), \n        axis.title = element_text(face = \"bold\", size=12, color=\"darkblue\")) +\n  labs(title = \"LungCap Probability Distribution based on Gender\", y = \"Probability density\")\n\n\n\n\n\nThe probability density of LungCap for females is higher than for males, as evidenced by the higher median value and larger range of data in the female group compared to the male group. This suggests that gender may be a significant factor in determining LungCap values."
  },
  {
    "objectID": "posts/HW1_AkhileshKumarMeghwal.html#c-compare-the-mean-lung-capacities-for-smokers-and-non-smokers.-does-it-make-sense",
    "href": "posts/HW1_AkhileshKumarMeghwal.html#c-compare-the-mean-lung-capacities-for-smokers-and-non-smokers.-does-it-make-sense",
    "title": "Homework 1 - Akhilesh Kumar",
    "section": "1c: Compare the mean lung capacities for smokers and non-smokers. Does it make sense?",
    "text": "1c: Compare the mean lung capacities for smokers and non-smokers. Does it make sense?\n\n\nCode\nlibrary(knitr)\n\nMean_Lung_Capacity <- df %>%\n  group_by(Smoke) %>%\n  summarise(mean = mean(LungCap))\n\n\nMean_Lung_Capacity\n\n\n\n\n  \n\n\n\nThe above table shows the lung capacity of individuals who smoke versus those who do not smoke. The table suggests that the mean lung capacity of those who smoke is greater than that of those who do not smoke. However, the statement argues that this result does not make sense and cannot be conclusively interpreted as a difference between smokers and non-smokers.\nThe output suggests that the lung capacity of an individual who smokes may be influenced by various biological factors unique to each person. Therefore, while smoking can be a contributing factor to reduced lung capacity, it cannot be the sole factor responsible for differences in lung capacity between smokers and non-smokers.\nThe output emphasizes that the difference in mean lung capacity between smokers and non-smokers is not a conclusive indicator of the effects of smoking on lung capacity. Other factors, such as age, gender and overall health, may also play a role in lung capacity. Therefore, it is important to consider these factors when drawing conclusions about the impact of smoking on lung capacity or other health outcomes."
  },
  {
    "objectID": "posts/HW1_AkhileshKumarMeghwal.html#d-examine-the-relationship-between-smoking-and-lung-capacity-within-age-groups-less-than-or-equal-to-13-14-to-15-16-to-17-and-greater-than-or-equal-to-18.",
    "href": "posts/HW1_AkhileshKumarMeghwal.html#d-examine-the-relationship-between-smoking-and-lung-capacity-within-age-groups-less-than-or-equal-to-13-14-to-15-16-to-17-and-greater-than-or-equal-to-18.",
    "title": "Homework 1 - Akhilesh Kumar",
    "section": "1d: Examine the relationship between Smoking and Lung Capacity within age groups: “less than or equal to 13”, “14 to 15”, “16 to 17”, and “greater than or equal to 18”.",
    "text": "1d: Examine the relationship between Smoking and Lung Capacity within age groups: “less than or equal to 13”, “14 to 15”, “16 to 17”, and “greater than or equal to 18”.\n\n\nCode\nLc <- mutate(Lc, AgeGrp = case_when(\n    Age <= 13 ~ \"less than or equal to 13\",\n    Age == 14 | Age == 15 ~ \"14 to 15\",\n    Age == 16 | Age == 17 ~ \"16 to 17\",\n    Age >= 18 ~ \"greater than or equal to 18\"\n))\n\n\nError in mutate(Lc, AgeGrp = case_when(Age <= 13 ~ \"less than or equal to 13\", : object 'Lc' not found\n\n\nCode\nLc %>%\n  ggplot(aes(x = LungCap, fill = Smoke)) +\n  geom_histogram(bins = 25, color = \"black\") +\n  facet_wrap(vars(AgeGrp)) +\n  scale_fill_manual(values = c(\"darkgreen\", \"darkblue\")) +\n  theme_classic() + \n  labs(title = \"Relationship of LungCap and Smoke based on age categories\", y = \"Frequency\", x = \"Lung Capacity\")\n\n\nError in ggplot(., aes(x = LungCap, fill = Smoke)): object 'Lc' not found\n\n\nThe graph shows the frequency of lung capacity for non-smokers and smokers in different age categories, namely:\n\n“less than or equal to 13”,\n“14 to 15”,\n“16 to 17”, and\n“greater than or equal to 18”\n\nFrom the above plot, we can derive two important observations. Firstly, we observe that non-smokers generally have a higher lung capacity compared to smokers. This is indicated by the fact that the bars for non-smokers are consistently taller than the bars for smokers across all age groups. This observation is consistent with the known negative impact of smoking on lung function.\nSecondly, we observe that the number of smokers is relatively lower in the age group “less than or equal to 13”. This is shown by the relatively shorter bars for smokers in this age group. This observation could be due to several factors, such as lower access to cigarettes or lower rates of smoking initiation in younger age groups.\nAdditionally, we can also observe that there is a general trend of decreasing lung capacity as age increases, regardless of smoking status. This is indicated by the fact that the bars for all age groups are shorter compared to the bars for the previous age group. This trend is consistent with the natural decline in lung function that occurs with aging.\nIn summary, the above graph shows that non-smokers generally have a higher lung capacity compared to smokers, and that smoking rates are relatively lower in younger age groups. The graph also highlights the general trend of decreasing lung capacity as age increases, regardless of smoking status. These observations provide important insights into the relationship between lung capacity, smoking, and age, and can be useful for informing public health interventions and promoting healthy lifestyles."
  },
  {
    "objectID": "posts/HW1_AkhileshKumarMeghwal.html#e-compare-the-lung-capacities-for-smokers-and-non-smokers-within-each-age-group.-is-your-answer-different-from-the-one-in-part-c.-what-could-possibly-be-going-on-here",
    "href": "posts/HW1_AkhileshKumarMeghwal.html#e-compare-the-lung-capacities-for-smokers-and-non-smokers-within-each-age-group.-is-your-answer-different-from-the-one-in-part-c.-what-could-possibly-be-going-on-here",
    "title": "Homework 1 - Akhilesh Kumar",
    "section": "1e: Compare the lung capacities for smokers and non-smokers within each age group. Is your answer different from the one in part c. What could possibly be going on here?",
    "text": "1e: Compare the lung capacities for smokers and non-smokers within each age group. Is your answer different from the one in part c. What could possibly be going on here?\n\n\nCode\nLc %>%\n  ggplot(aes(x = Age, y = LungCap, color = Smoke)) +\n  geom_line(linewidth = 1.25) +\n  scale_color_manual(values = c(\"darkgreen\", \"darkred\")) +\n  theme_classic() + \n  facet_wrap(vars(Smoke)) +\n  labs(title = \"Relationship of Lung Capacity and Smoking Status by Age\", y = \"Lung Capacity\", x = \"Age\") +\n  theme(plot.title = element_text(face = \"bold\", size = 14, hjust = 0.5),\n        axis.title.x = element_text(face = \"bold\", size = 12),\n        axis.title.y = element_text(face = \"bold\", size = 12),\n        axis.text.x = element_text(size = 10),\n        axis.text.y = element_text(size = 10),\n        legend.title = element_blank(),\n        legend.text = element_text(face = \"bold\", size = 10),\n        panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank(),\n        panel.background = element_blank())\n\n\nError in ggplot(., aes(x = Age, y = LungCap, color = Smoke)): object 'Lc' not found\n\n\n1c presents a the mean lung capacity of individuals who smoke versus those who do not smoke. The statement argues that this result cannot be conclusively interpreted as a difference between smokers and non-smokers, as other biological factors unique to each individual may also influence lung capacity.\nOn the other hand, 1e presents a line graph that compares lung capacity with age for smokers and non-smokers. The graph suggests that non-smokers generally have higher lung capacity compared to smokers, and that the difference in lung capacity between the two groups increases with age.\nIn summary, 1c presents a summary statistic that may be influenced by other factors, while 1e presents a visual representation of the relationship between smoking and lung capacity over time, which provides a more nuanced understanding of the impact of smoking on lung capacity."
  },
  {
    "objectID": "posts/HW1_AkhileshKumarMeghwal.html#question-2",
    "href": "posts/HW1_AkhileshKumarMeghwal.html#question-2",
    "title": "Homework 1 - Akhilesh Kumar",
    "section": "Question 2",
    "text": "Question 2"
  },
  {
    "objectID": "posts/HW1_AkhileshKumarMeghwal.html#reading-the-table",
    "href": "posts/HW1_AkhileshKumarMeghwal.html#reading-the-table",
    "title": "Homework 1 - Akhilesh Kumar",
    "section": "Reading the table",
    "text": "Reading the table\n\n\nCode\nPrior_convictions <- c(0:4)\nInmate_count <- c(128, 434, 160, 64, 24)\nPc <- tibble(Prior_convictions, Inmate_count)\nPc\n\n\n\n\n  \n\n\n\n\n\nCode\nPc <- mutate(Pc, Probability = Inmate_count/sum(Inmate_count))\nPc"
  },
  {
    "objectID": "posts/HW1_AkhileshKumarMeghwal.html#a-what-is-the-probability-that-a-randomly-selected-inmate-has-exactly-2-prior-convictions",
    "href": "posts/HW1_AkhileshKumarMeghwal.html#a-what-is-the-probability-that-a-randomly-selected-inmate-has-exactly-2-prior-convictions",
    "title": "Homework 1 - Akhilesh Kumar",
    "section": "2a: What is the probability that a randomly selected inmate has exactly 2 prior convictions?",
    "text": "2a: What is the probability that a randomly selected inmate has exactly 2 prior convictions?\n\n\nCode\nPc %>%\n  filter(Prior_convitions == 2) %>%\n  select(Probability)\n\n\nError in `filter()`:\n! Problem while computing `..1 = Prior_convitions == 2`.\nCaused by error in `mask$eval_all_filter()`:\n! object 'Prior_convitions' not found\n\n\nProbability that a randomly selected inmate has exactly 2 prior convictions is 0.1975309."
  },
  {
    "objectID": "posts/HW1_AkhileshKumarMeghwal.html#b-what-is-the-probability-that-a-randomly-selected-inmate-has-fewer-than-2-prior-convictions",
    "href": "posts/HW1_AkhileshKumarMeghwal.html#b-what-is-the-probability-that-a-randomly-selected-inmate-has-fewer-than-2-prior-convictions",
    "title": "Homework 1 - Akhilesh Kumar",
    "section": "2b: What is the probability that a randomly selected inmate has fewer than 2 prior convictions?",
    "text": "2b: What is the probability that a randomly selected inmate has fewer than 2 prior convictions?\n\n\nCode\ntemp <- Pc %>%\n  filter(Prior_convitions < 2)\n\n\nError in `filter()`:\n! Problem while computing `..1 = Prior_convitions < 2`.\nCaused by error in `mask$eval_all_filter()`:\n! object 'Prior_convitions' not found\n\n\nCode\nsum(temp$Probability)\n\n\nError in eval(expr, envir, enclos): object 'temp' not found\n\n\nProbability that a randomly selected inmate has fewer than 2 convictions is 0.6938272"
  },
  {
    "objectID": "posts/HW1_AkhileshKumarMeghwal.html#c-what-is-the-probability-that-a-randomly-selected-inmate-has-2-or-fewer-prior-convictions",
    "href": "posts/HW1_AkhileshKumarMeghwal.html#c-what-is-the-probability-that-a-randomly-selected-inmate-has-2-or-fewer-prior-convictions",
    "title": "Homework 1 - Akhilesh Kumar",
    "section": "2c: What is the probability that a randomly selected inmate has 2 or fewer prior convictions?",
    "text": "2c: What is the probability that a randomly selected inmate has 2 or fewer prior convictions?\n\n\nCode\ntemp <- Pc %>%\n  filter(Prior_convitions <= 2)\n\n\nError in `filter()`:\n! Problem while computing `..1 = Prior_convitions <= 2`.\nCaused by error in `mask$eval_all_filter()`:\n! object 'Prior_convitions' not found\n\n\nCode\nsum(temp$Probability)\n\n\nError in eval(expr, envir, enclos): object 'temp' not found\n\n\nProbability that a randomly selected inmate has 2 or fewer prior convictions: 0.891358"
  },
  {
    "objectID": "posts/HW1_AkhileshKumarMeghwal.html#d-what-is-the-probability-that-a-randomly-selected-inmate-has-more-than-2-prior-convictions",
    "href": "posts/HW1_AkhileshKumarMeghwal.html#d-what-is-the-probability-that-a-randomly-selected-inmate-has-more-than-2-prior-convictions",
    "title": "Homework 1 - Akhilesh Kumar",
    "section": "2d: What is the probability that a randomly selected inmate has more than 2 prior convictions?",
    "text": "2d: What is the probability that a randomly selected inmate has more than 2 prior convictions?\n\n\nCode\ntemp <- Pc %>%\n  filter(Prior_convitions > 2)\n\n\nError in `filter()`:\n! Problem while computing `..1 = Prior_convitions > 2`.\nCaused by error in `mask$eval_all_filter()`:\n! object 'Prior_convitions' not found\n\n\nCode\nsum(temp$Probability)\n\n\nError in eval(expr, envir, enclos): object 'temp' not found\n\n\nProbability that a randomly selected inmate has more than 2 prior convictions is 0.108642"
  },
  {
    "objectID": "posts/HW1_AkhileshKumarMeghwal.html#e-what-is-the-expected-value-for-the-number-of-prior-convictions",
    "href": "posts/HW1_AkhileshKumarMeghwal.html#e-what-is-the-expected-value-for-the-number-of-prior-convictions",
    "title": "Homework 1 - Akhilesh Kumar",
    "section": "2e: What is the expected value for the number of prior convictions?",
    "text": "2e: What is the expected value for the number of prior convictions?\n\n\nCode\nPc <- mutate(Pc, Wm = Prior_convitions*Probability)\n\n\nError in `mutate()`:\n! Problem while computing `Wm = Prior_convitions * Probability`.\nCaused by error in `mask$eval_all_mutate()`:\n! object 'Prior_convitions' not found\n\n\nCode\ne <- sum(Pc$Wm)\ne\n\n\n[1] 0\n\n\nExpected value for the number of prior convictions: 1.28642"
  },
  {
    "objectID": "posts/HW1_AkhileshKumarMeghwal.html#f-calculate-the-variance-and-the-standard-deviation-for-the-prior-convictions.",
    "href": "posts/HW1_AkhileshKumarMeghwal.html#f-calculate-the-variance-and-the-standard-deviation-for-the-prior-convictions.",
    "title": "Homework 1 - Akhilesh Kumar",
    "section": "2f: Calculate the variance and the standard deviation for the Prior Convictions.",
    "text": "2f: Calculate the variance and the standard deviation for the Prior Convictions.\nVariance for the Prior Convictions:\n\n\nCode\nv <-sum(((Pc$Prior_convictions-e)^2)*Pc$Probability)\nv\n\n\n[1] 2.511111\n\n\nStandard Deviation for the Prior Convictions:\n\n\nCode\nsqrt(v)\n\n\n[1] 1.584649\n\n\nVariance and Standard Deviation for the Prior Convictions are 0.8562353 and 0.9253298 respectively."
  },
  {
    "objectID": "posts/AdithyaParupudi_hw1.html",
    "href": "posts/AdithyaParupudi_hw1.html",
    "title": "Homework1 - EDA of LungCap Data",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.1     ✔ purrr   0.3.4\n✔ tibble  3.1.8     ✔ dplyr   1.0.9\n✔ tidyr   1.2.1     ✔ stringr 1.4.1\n✔ readr   2.1.3     ✔ forcats 0.5.2\n\n\nWarning: package 'ggplot2' was built under R version 4.2.2\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\nCode\nlibrary(hrbrthemes)\n\n\nNOTE: Either Arial Narrow or Roboto Condensed fonts are required to use these themes.\n      Please use hrbrthemes::import_roboto_condensed() to install Roboto Condensed and\n      if Arial Narrow is not on your system, please see https://bit.ly/arialnarrow\n\n\nCode\nlibrary(viridis)\n\n\nWarning: package 'viridis' was built under R version 4.2.2\n\n\nLoading required package: viridisLite\n\n\nCode\nlibrary(readxl)"
  },
  {
    "objectID": "posts/AdithyaParupudi_hw1.html#question-1",
    "href": "posts/AdithyaParupudi_hw1.html#question-1",
    "title": "Homework1 - EDA of LungCap Data",
    "section": "Question 1",
    "text": "Question 1\n\n1a) The distribution of LungCap looks as follows:\n\n\nCode\nhist(df$LungCap)\n\n\n\n\n\nThe histogram suggests that the distribution is close to a normal distribution. Most of the observations are close to the mean. Very few observations are close to the margins (0 and 15).\n\n\n1b) Compare the probability distribution of the LungCap with respect to Males and Females? (Hint:make boxplots separated by gender using the boxplot() function)\n\n\nCode\n#boxplot code\n\ndf %>%\n  ggplot( aes(x=Gender, y=LungCap, fill=Gender)) +\n    geom_boxplot() +\n    theme_ipsum() +\n    theme(\n      legend.position=\"none\",\n      plot.title = element_text(size=12)\n    ) +\n    ggtitle(\"Lungcap vs Gender\") +\n    xlab(\"Gender\") +\n  ylab(\"Lung Cap\")\n\n\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): font family not\nfound in Windows font database\n\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): font family not\nfound in Windows font database\n\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): font family not\nfound in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\n\n\n\n\n\nc) Compare the mean lung capacities for smokers and non-smokers. Does it make sense?\n\n\nCode\nmean_smoke <- df %>%\n  group_by(Smoke) %>%\n  summarise(mean = mean(LungCap))\nmean_smoke\n\n\n# A tibble: 2 × 2\n  Smoke  mean\n  <chr> <dbl>\n1 no     7.77\n2 yes    8.65\n\n\nAccording to this mean, it doesn’t make sense that lung capacities of smokers is greater than that of non-smokers.\n\n\nd) Examine the relationship between Smoking and Lung Capacity within age groups: “less than or\nequal to 13”, “14 to 15”, “16 to 17”, and “greater than or equal to 18”.\n\n\nCode\ndf <- mutate(df, AgeGrp = case_when(Age <= 13 ~ \"less than or equal to 13\",\n                                    Age == 14 | Age == 15 ~ \"14 to 15\",\n                                    Age == 16 | Age == 17 ~ \"16 to 17\",\n                                    Age >= 18 ~ \"greater than or equal to 18\"))\n\ndf %>%\n  ggplot(aes(y = LungCap, color = Smoke)) +\n  geom_histogram(bins = 25) +\n  facet_wrap(vars(AgeGrp)) +\n  labs(title = \"Relationship of LungCap and Smoke based on Age\", y = \"Lung Capacity\", x = \"Frequency\")\n\n\n\n\n\n\n\ne) Compare the lung capacities for smokers and non-smokers within each age group. Is your answer different from the one in part c. What could possibly be going on here?\n\n\nCode\ndf %>%\n  ggplot(aes(x = Age, y = LungCap, color = Smoke)) +\n  geom_line() +\n  theme_classic() + \n  facet_wrap(vars(Smoke)) +\n  labs(title = \"Relationship of LungCap and Smoke v/s Age\", y = \"Lung Capacity\", x = \"Age\")"
  },
  {
    "objectID": "posts/AdithyaParupudi_hw1.html#question-2",
    "href": "posts/AdithyaParupudi_hw1.html#question-2",
    "title": "Homework1 - EDA of LungCap Data",
    "section": "Question 2",
    "text": "Question 2\n\nReading the table\n\n\nCode\nPrior_convitions <- c(0:4)\nInmate_count <- c(128, 434, 160, 64, 24)\nPc <- data_frame(Prior_convitions, Inmate_count)\n\n\nWarning: `data_frame()` was deprecated in tibble 1.1.0.\nℹ Please use `tibble()` instead.\n\n\nCode\nPc\n\n\n# A tibble: 5 × 2\n  Prior_convitions Inmate_count\n             <int>        <dbl>\n1                0          128\n2                1          434\n3                2          160\n4                3           64\n5                4           24\n\n\n\n\nCode\nPc <- mutate(Pc, Probability = Inmate_count/sum(Inmate_count))\nPc\n\n\n# A tibble: 5 × 3\n  Prior_convitions Inmate_count Probability\n             <int>        <dbl>       <dbl>\n1                0          128      0.158 \n2                1          434      0.536 \n3                2          160      0.198 \n4                3           64      0.0790\n5                4           24      0.0296\n\n\n\n\n2a - Probability that a randomly selected inmate has exactly 2 prior convictions:\n\n\nCode\nPc %>%\n  filter(Prior_convitions == 2) %>%\n  select(Probability)\n\n\n# A tibble: 1 × 1\n  Probability\n        <dbl>\n1       0.198\n\n\n\n\n2b - Probability that a randomly selected inmate has fewer than 2 convictions:\n\n\nCode\ntemp <- Pc %>%\n  filter(Prior_convitions < 2)\nsum(temp$Probability)\n\n\n[1] 0.6938272\n\n\n\n\n2c - Probability that a randomly selected inmate has 2 or fewer prior convictions:\n\n\nCode\ntemp <- Pc %>%\n  filter(Prior_convitions <= 2)\nsum(temp$Probability)\n\n\n[1] 0.891358\n\n\n\n\n2d - Probability that a randomly selected inmate has more than 2 prior convictions:\n\n\nCode\ntemp <- Pc %>%\n  filter(Prior_convitions > 2)\nsum(temp$Probability)\n\n\n[1] 0.108642\n\n\n\n\n2e - Expected value for the number of prior convictions:\n\n\nCode\nPc <- mutate(Pc, Wm = Prior_convitions*Probability)\ne <- sum(Pc$Wm)\ne\n\n\n[1] 1.28642\n\n\n\n\n2f - Variance for the Prior Convictions:\n\n\nCode\nv <-sum(((Pc$Prior_convitions-e)^2)*Pc$Probability)\nv\n\n\n[1] 0.8562353\n\n\nstandard deviation for the Prior Convictions:\n\n\nCode\nsqrt(v)\n\n\n[1] 0.9253298"
  },
  {
    "objectID": "posts/dacss603hw1_LauraCollazo.html",
    "href": "posts/dacss603hw1_LauraCollazo.html",
    "title": "DACSS 603 Homework 1",
    "section": "",
    "text": "Code\nlibrary(readxl)\nlibrary(tidyverse)"
  },
  {
    "objectID": "posts/dacss603hw1_LauraCollazo.html#a",
    "href": "posts/dacss603hw1_LauraCollazo.html#a",
    "title": "DACSS 603 Homework 1",
    "section": "a",
    "text": "a\nThe distribution of LungCap looks as follows:\n\n\nCode\nhist(df$LungCap)\n\n\n\n\n\nThe histogram suggests that the distribution is close to a normal distribution. Most of the observations are close to the mean. Very few observations are close to the margins (0 and 15)."
  },
  {
    "objectID": "posts/dacss603hw1_LauraCollazo.html#b",
    "href": "posts/dacss603hw1_LauraCollazo.html#b",
    "title": "DACSS 603 Homework 1",
    "section": "b",
    "text": "b\nThe distribution of LungCap by Gender looks as follows:\n\n\nCode\nboxplot(df$LungCap ~ df$Gender)\n\n\n\n\n\nThe distribution shows that males, on average, have a higher lung capacity than females."
  },
  {
    "objectID": "posts/dacss603hw1_LauraCollazo.html#c",
    "href": "posts/dacss603hw1_LauraCollazo.html#c",
    "title": "DACSS 603 Homework 1",
    "section": "c",
    "text": "c\nThe mean lung capacities for smokers and non-smokers is shown below.\n\n\nCode\ndf %>% \n  group_by(Smoke) %>% \n  summarise(mean = mean(LungCap))\n\n\n# A tibble: 2 × 2\n  Smoke  mean\n  <chr> <dbl>\n1 no     7.77\n2 yes    8.65\n\n\nThese means are not what I would have expected to see. This shows smokers have a higher mean lung capacity than non-smokers."
  },
  {
    "objectID": "posts/dacss603hw1_LauraCollazo.html#d",
    "href": "posts/dacss603hw1_LauraCollazo.html#d",
    "title": "DACSS 603 Homework 1",
    "section": "d",
    "text": "d\nThe mean lung capacities for smokers and non-smokers by age group is shown below.\n\n\nCode\ndf_age_group <- df %>% \n  mutate(\n    age_group = case_when(\n      Age < 14 ~ \"less than or equal to 13\",\n      Age == 14 ~ \"14 to 15\",\n      Age == 15 ~ \"14 to 15\",\n      Age == 16 ~ \"16 to 17\",\n      Age == 17 ~ \"16 to 17\",\n      Age > 17 ~ \"greater than or equal to 18\")\n  ) \n\ndf_age_group %>%\n  group_by(age_group, Smoke) %>% \n  summarise(mean = mean(LungCap))\n\n\n`summarise()` has grouped output by 'age_group'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 8 × 3\n# Groups:   age_group [4]\n  age_group                   Smoke  mean\n  <chr>                       <chr> <dbl>\n1 14 to 15                    no     9.14\n2 14 to 15                    yes    8.39\n3 16 to 17                    no    10.5 \n4 16 to 17                    yes    9.38\n5 greater than or equal to 18 no    11.1 \n6 greater than or equal to 18 yes   10.5 \n7 less than or equal to 13    no     6.36\n8 less than or equal to 13    yes    7.20"
  },
  {
    "objectID": "posts/dacss603hw1_LauraCollazo.html#e",
    "href": "posts/dacss603hw1_LauraCollazo.html#e",
    "title": "DACSS 603 Homework 1",
    "section": "e",
    "text": "e\nThe distribution of LungCap by age_group and smoker looks as follows:\n\n\nCode\nggplot(df_age_group, aes(x=age_group, y=LungCap, color = Smoke)) +\n  geom_boxplot()\n\n\n\n\n\nThe mean lung capacity for non-smokers is higher than smokers for all age groups except for those age 13 or younger. This is interesting as it was observed earlier that overall smokers have a higher mean lung capacity than non-smokers. This led me to wonder if there are more individuals age 13 or younger in this sample than other age groups, and also what the count of smokers versus non smokers is for this sample.\nThe count by age group and smoker type is shown below.\n\n\nCode\ndf_age_group %>% \n  group_by(age_group, Smoke) %>% \n  summarise(total_count = n())\n\n\n`summarise()` has grouped output by 'age_group'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 8 × 3\n# Groups:   age_group [4]\n  age_group                   Smoke total_count\n  <chr>                       <chr>       <int>\n1 14 to 15                    no            105\n2 14 to 15                    yes            15\n3 16 to 17                    no             77\n4 16 to 17                    yes            20\n5 greater than or equal to 18 no             65\n6 greater than or equal to 18 yes            15\n7 less than or equal to 13    no            401\n8 less than or equal to 13    yes            27\n\n\nThe overall count by smoker type is shown below.\n\n\nCode\ndf %>% \n  group_by(Smoke) %>% \n  summarise(total_count = n())\n\n\n# A tibble: 2 × 2\n  Smoke total_count\n  <chr>       <int>\n1 no            648\n2 yes            77"
  },
  {
    "objectID": "posts/dacss603hw1_LauraCollazo.html#a-1",
    "href": "posts/dacss603hw1_LauraCollazo.html#a-1",
    "title": "DACSS 603 Homework 1",
    "section": "a",
    "text": "a\nThe probability that a randomly selected inmate has exactly 2 prior convictions is as follows:\n\n\nCode\ndbinom(x= 1, size= 1, prob= 160/810)\n\n\n[1] 0.1975309"
  },
  {
    "objectID": "posts/dacss603hw1_LauraCollazo.html#b-1",
    "href": "posts/dacss603hw1_LauraCollazo.html#b-1",
    "title": "DACSS 603 Homework 1",
    "section": "b",
    "text": "b\nThe probability that a randomly selected inmate has fewer than 2 prior convictions is as follows:\n\n\nCode\ndbinom(x= 1, size= 1, prob= sum(128/810 + 434/810))\n\n\n[1] 0.6938272"
  },
  {
    "objectID": "posts/dacss603hw1_LauraCollazo.html#c-1",
    "href": "posts/dacss603hw1_LauraCollazo.html#c-1",
    "title": "DACSS 603 Homework 1",
    "section": "c",
    "text": "c\nThe probability that a randomly selected inmate has 2 or fewer prior convictions is as follows:\n\n\nCode\ndbinom(x= 1, size= 1, prob= sum(128/810 + 434/810 + 160/810))\n\n\n[1] 0.891358"
  },
  {
    "objectID": "posts/dacss603hw1_LauraCollazo.html#d-1",
    "href": "posts/dacss603hw1_LauraCollazo.html#d-1",
    "title": "DACSS 603 Homework 1",
    "section": "d",
    "text": "d\nThe probability that a randomly selected inmate has more than 2 prior convictions is as follows:\n\n\nCode\ndbinom(x= 1, size= 1, prob= sum(64/810 + 24/810))\n\n\n[1] 0.108642"
  },
  {
    "objectID": "posts/dacss603hw1_LauraCollazo.html#e-1",
    "href": "posts/dacss603hw1_LauraCollazo.html#e-1",
    "title": "DACSS 603 Homework 1",
    "section": "e",
    "text": "e\nThe expected value for the number of prior convictions is as follows:\n\n\nCode\nev <- sum(convictions$number_convictions * convictions$prop)\n\nprint(ev)\n\n\n[1] 1.28642"
  },
  {
    "objectID": "posts/dacss603hw1_LauraCollazo.html#f",
    "href": "posts/dacss603hw1_LauraCollazo.html#f",
    "title": "DACSS 603 Homework 1",
    "section": "f",
    "text": "f\nThe variance for prior convictions is as follows:\n\n\nCode\nvar <- sum((convictions$number - ev) ^ 2 * convictions$prop)\n\nprint(var)\n\n\n[1] 0.8562353\n\n\nThe standard deviation for prior convictions is as follows:\n\n\nCode\nsd <- sqrt(var)\n\nprint(sd)\n\n\n[1] 0.9253298"
  },
  {
    "objectID": "posts/Homework1_AlexaPotter.html",
    "href": "posts/Homework1_AlexaPotter.html",
    "title": "Homework 1",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\n\n-- Attaching packages --------------------------------------- tidyverse 1.3.2 --\nv ggplot2 3.4.0      v purrr   0.3.5 \nv tibble  3.1.8      v dplyr   1.0.10\nv tidyr   1.2.1      v stringr 1.5.0 \nv readr   2.1.3      v forcats 0.5.2 \n-- Conflicts ------------------------------------------ tidyverse_conflicts() --\nx dplyr::filter() masks stats::filter()\nx dplyr::lag()    masks stats::lag()\n\n\nCode\nlibrary(ggplot2)\nlibrary(formattable)"
  },
  {
    "objectID": "posts/Homework1_AlexaPotter.html#a",
    "href": "posts/Homework1_AlexaPotter.html#a",
    "title": "Homework 1",
    "section": "a",
    "text": "a\nFirst, let’s read in the data from the Excel file:\n\n\nCode\nlibrary(readxl)\ndf <- read_excel(\"_data/LungCapData.xls\")\n\n\nThe distribution of LungCap looks as follows:\n\n\nCode\nhist(df$LungCap, xlab= 'LungCap', main = 'Histogram of LungCap Distribution')\n\n\n\n\n\nThe histogram suggests that the distribution is close to a normal distribution. Most of the observations are close to the mean. Very few observations are close to the margins (0 and 15).\n##b\nCompare the probability distribution of the LungCap with respect to Males and Females:\n\n\nCode\nboxplot(df$LungCap~df$Gender, xlab = 'Gender', ylab = 'LungCap')\n\n\n\n\n\n##c\nCompare the mean lung capacities for smokers and non-smokers. Does it make sense?\n\n\nCode\nLungCap_mean <- df %>% \n  group_by(Smoke) %>%\n  summarise(Lungcap_Mean = mean(LungCap))\n\nLungCap_mean\n\n\n# A tibble: 2 x 2\n  Smoke Lungcap_Mean\n  <chr>        <dbl>\n1 no            7.77\n2 yes           8.65\n\n\nThese results show the lung capacity for non-smokers is lower than the lung capacity for smokers. With no background knowledge of lung capacity, I would think these are not standard results.\n##d)\nExamine the relationship between Smoking and Lung Capacity within age groups: “less than or equal to 13”, “14 to 15”, “16 to 17”, and “greater than or equal to 18”.\n\n\nCode\ndf_Agegroup <- df %>% \n  mutate(\n    Age_group = dplyr::case_when(\n      Age <= 13            ~ \"0-13\",\n      Age > 13 & Age <= 15 ~ \"14-15\",\n      Age > 15 & Age <= 17 ~ \"16-17\",\n      Age >= 18             ~ \"18+\"))\n\n\nggplot(data = df_Agegroup, aes(x=Age_group, y=LungCap)) + \n  geom_boxplot(aes(fill=Gender))\n\n\n\n\n\n##e)\nCompare the lung capacities for smokers and non-smokers within each age group. Is your answer different from the one in part c. What could possibly be going on here?\n\n\nCode\nggplot(data = df_Agegroup, aes(x=Age_group, y=LungCap)) + \n  geom_boxplot(aes(fill=Smoke))\n\n\n\n\n\nThis data is different from what was seen in part C. The only age group displaying greater lung capacity for smokers is the group 0-13. This would suggest this sample of the population is influencing the overall lung capacity mean. This by far is the largest sample of age groups with 428 participants\n\n\nCode\ncount(df_Agegroup, Age_group, Smoke)\n\n\n# A tibble: 8 x 3\n  Age_group Smoke     n\n  <chr>     <chr> <int>\n1 0-13      no      401\n2 0-13      yes      27\n3 14-15     no      105\n4 14-15     yes      15\n5 16-17     no       77\n6 16-17     yes      20\n7 18+       no       65\n8 18+       yes      15"
  },
  {
    "objectID": "posts/HW_2_Diana_Rinker.html",
    "href": "posts/HW_2_Diana_Rinker.html",
    "title": "Homework 2",
    "section": "",
    "text": "DACSS 603, spring 2023\n\n\nHomework 2, Diana Rinker.\nLoading necessary libraries:\n\n\nCode\nlibrary(readxl)\nlibrary(dplyr)\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nCode\nlibrary(ggplot2)\nlibrary(tidyverse)\n\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2\n──\n\n\n✔ tibble  3.1.8     ✔ purrr   1.0.1\n✔ tidyr   1.3.0     ✔ stringr 1.5.0\n✔ readr   2.1.4     ✔ forcats 1.0.0\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\n\n\nQuestion 1\nThe time between the date a patient was recommended for heart surgery and the surgery date for cardiac patients in Ontario was collected by the Cardiac Care Network (“Wait Times Data Guide,” Ministry of Health and Long-Term Care, Ontario, Canada, 2006). The sample mean and sample standard deviation for wait times (in days) of patients for two cardiac procedures are given in the accompanying table. Assume that the sample is representative of the Ontario population.\n\n\nCode\n`Surgical Procedure` <- c(\"Bypass\", \"Angiography\" )\n`Sample Size` <- c(539,847 )\n`Mean Wait Time` <- c(19, 18)\n`Standard Deviation` <- c(10, 9)\ndata<- data.frame(`Surgical Procedure`,`Sample Size`, `Mean Wait Time`, `Standard Deviation` )\nknitr::kable(data )\n\n\n\n\n\nSurgical.Procedure\nSample.Size\nMean.Wait.Time\nStandard.Deviation\n\n\n\n\nBypass\n539\n19\n10\n\n\nAngiography\n847\n18\n9\n\n\n\n\n\nConstruct the 90% confidence interval to estimate the actual mean wait time for each of the two procedures. Is the confidence interval narrower for angiography or bypass surgery?\nThe formula for the confidence interval of a sample is\n\\[\nCI =\\bar{X} \\pm  t \\cdot \\frac{s}{\\sqrt n}\n\\]\n\n\nCode\n#calculating degree of freedom for each sample: \ndata <- data%>%\n  mutate(confidence_level = 0.9,\n         tail_area = (1-confidence_level)/2,\n         t_score = qt(p = 1-tail_area, df = `Sample.Size` -1),\n         CI.low  = `Mean Wait Time` - t_score * `Standard.Deviation` / sqrt(`Sample.Size`), \n         CI.high= `Mean Wait Time` + t_score * `Standard.Deviation` / sqrt(`Sample.Size`),\n         CI.width = abs(CI.high - CI.low)\n         )\ndata<- data %>%\nselect (Surgical.Procedure, CI.low,CI.high , CI.width, t_score, everything() )\nknitr::kable(data ) \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSurgical.Procedure\nCI.low\nCI.high\nCI.width\nt_score\nSample.Size\nMean.Wait.Time\nStandard.Deviation\nconfidence_level\ntail_area\n\n\n\n\nBypass\n18.29029\n19.70971\n1.419421\n1.647691\n539\n19\n10\n0.9\n0.05\n\n\nAngiography\n17.49078\n18.50922\n1.018436\n1.646657\n847\n18\n9\n0.9\n0.05\n\n\n\n\n\nFrom the table above I can see that the width of confidence interval for Angiography is smaller than for Bypass.\nAlternatvie way to calculate the interval is by simulating the data based on given parameters and running one-sided t.test() :\n\n\nCode\nAngiography.data <- rnorm(n=847, mean =18 , sd = 9)\n# Calculate the sample mean\nmean.q1 <- mean(Angiography.data )\n# Adjust the sample to have a mean of exactly 410\nAngiography.data <- Angiography.data + 18 - mean.q1\n\nt.test(Angiography.data, alternative = \"two.sided\", conf.level = 0.9) \n\n\n\n    One Sample t-test\n\ndata:  Angiography.data\nt = 59.128, df = 846, p-value < 2.2e-16\nalternative hypothesis: true mean is not equal to 0\n90 percent confidence interval:\n 17.49872 18.50128\nsample estimates:\nmean of x \n       18 \n\n\n\n\nCode\nBypass.data <- rnorm(n=539, mean =19 , sd = 10)\n# Calculate the sample mean\nmean.bypass.q1 <- mean(Bypass.data )\n# Adjust the sample to have a mean of exactly 410\nBypass.data  <- Bypass.data  + 19 - mean.bypass.q1\n\nt.test(Bypass.data , alternative = \"two.sided\", conf.level = 0.9) \n\n\n\n    One Sample t-test\n\ndata:  Bypass.data\nt = 45.732, df = 538, p-value < 2.2e-16\nalternative hypothesis: true mean is not equal to 0\n90 percent confidence interval:\n 18.31544 19.68456\nsample estimates:\nmean of x \n       19 \n\n\n\n\nQuestion 2\nA survey of 1031 adult Americans was carried out by the National Center for Public Policy. Assume that the sample is representative of adult Americans. Among those surveyed, 567 believed that college education is essential for success. Find the point estimate, p, of the proportion of all adult Americans who believe that a college education is essential for success. Construct and interpret a 95% confidence interval for p.\nI will assume, that the survey responses are within binomial distribution ( i.e. respondents had an option to agree or disagree that college education is essential for success).\nPoint estimate for this sample is 567/1031\n\n\nCode\npoint.estimate <- 567/1031\n\n\nConfidence interval = point estimate ± margin of error. To calculate a confidence interval for the p, I will usr the following formula:\n\\[\nCI =\\bar{p} \\pm Zscore \\cdot SE\n\\]\n\n\nCode\nSample.proportion <- 567/1031\nSample.Size <- 1031\ndata2<- data.frame(Sample.proportion,Sample.Size  )\ndata2<- data2%>%\n  mutate(SE =(sqrt(Sample.proportion*(1-Sample.proportion)/Sample.Size)), # for binomial distribution\n        z_score = 1.96, # for 95% cofidence interval  \n       CI.low  = Sample.proportion - z_score * SE,\n         CI.high= Sample.proportion + z_score * SE\n        )\nknitr::kable(data2 ) \n\n\n\n\n\n\n\n\n\n\n\n\n\nSample.proportion\nSample.Size\nSE\nz_score\nCI.low\nCI.high\n\n\n\n\n0.5499515\n1031\n0.015494\n1.96\n0.5195833\n0.5803197\n\n\n\n\n\nThis confidence interval can be interpreted as “The people who agreethat college education is essential for success is between 52% and 58%from general American population”.\nI could also calculate confidence interval by generating a sample data set and using prob.test() function:\n\n\nCode\n#generating data table: \n  # Generating sample for grade 6:\n  successes <- 0\n  p <- 567/1031\n  n<-1031\n  # repeat until the desired number of successes is obtained\n  while (successes != 567) {\n          responses <- rbinom(n=1031, 1, p=p)\n  # count the number of successes\n          successes <- sum(responses)\n  }\nrespondent.n <-seq(1:1031)\ndataset.q2 <-data.frame( respondent.n , responses)\n  \nprop.test(x = sum(dataset.q2$responses), n=n,  p =p )\n\n\n\n    1-sample proportions test without continuity correction\n\ndata:  sum(dataset.q2$responses) out of n, null probability p\nX-squared = 6.9637e-30, df = 1, p-value = 1\nalternative hypothesis: true p is not equal to 0.5499515\n95 percent confidence interval:\n 0.5194543 0.5800778\nsample estimates:\n        p \n0.5499515 \n\n\nThis way of calculating CI provided me with the same interval as the first one.\n\n\nQuestion 3\nSuppose that the financial aid office of UMass Amherst seeks to estimate the mean cost of textbooks per semester for students. The estimate will be useful if it is within $5 of the true population mean (i.e. they want the confidence interval to have a length of $10 or less). The financial aid office is pretty sure that the amount spent on books varies widely, with most values between $30 and $200. They think that the population standard deviation is about a quarter of this range (in other words, you can assume they know the population standard deviation). Assuming the significance level to be 5%, what should be the size of the sample?\nSince I am given standard deviation of population, I can use the following formula for CI for confidence level 95%:\n\\[\nCI =\\bar{X} \\pm  1.96 \\cdot \\frac{\\sigma}{\\sqrt n}  \n\\] The range of CI will be\n\\[\nCI range = 10=(\\bar{X} +  1.96 \\cdot \\frac{\\sigma}{\\sqrt n})  -(\\bar{X} -  1.96 \\cdot \\frac{\\sigma}{\\sqrt n})   \n\\]\n\\[\n  2\\cdot (1.96 \\cdot \\frac{\\sigma}{\\sqrt n})  = 10\n\\]\n\\[\n  {\\sqrt n}  = \\frac{1.96\\sigma}{5}\n\\]\n\\[\nn = (\\frac{1.96 \\cdot  42.5}{5})^2\n\\]\n\n\nCode\nSD.population <- (200-30)/4\nn<- ((1.96*SD.population)/5)^2\nknitr::kable(n) \n\n\n\n\n\nx\n\n\n\n\n277.5556\n\n\n\n\n\nTo calculate confidence interval width of $10, financial aid office should collect a sample size of 278 students.\n\n\nQuestion 4\nAccording to a union agreement, the mean income for all senior-level workers in a large service company equals $500 per week. A representative of a women’s group decides to analyze whether the mean income μ for female employees matches this norm. For a random sample of nine female employees, ȳ = $410 and s = 90 A. Test whether the mean income of female employees differs from $500 per week. Include assumptions, hypotheses, test statistic, and P-value. Interpret the result. B. Report the P-value for Ha: μ < 500. Interpret. C. Report and interpret the P-value for Ha: μ > 500. (Hint: The P-values for the two possible one-sided tests must sum to 1.\n\nA. Two-sided t.test:\nTest whether the mean income of female employees differs from $500 per week. Include assumptions, hypotheses, test statistic, and P-value. Interpret the result.\nAssumption: The sample is representative of all senior-level female workers in that company.\nHo: mean of female income is close to the company policy income of $500. H1: mean of female income is not the same as the company policy income of $500.\nTo test this hypothesis, I will used one-sample t.test, which will compare mean and cinficance interval of a sample to a policy level of $500.\nT.test() function in R uses t-statistics to evaluate small sample size (9).\n\n\nCode\n# Generate a sample with a mean close to 410\nfemale.sample <- rnorm(n=9, mean=410, sd=90)\n# Calculate the sample mean\nsample.mean <- mean(female.sample)\n# Adjust the sample to have a mean of exactly 410\nfemale.sample <- female.sample + 410 - sample.mean\na.ttest.sample<-t.test(female.sample, alternative = \"two.sided\", mu=500)\na.ttest.sample\n\n\n\n    One Sample t-test\n\ndata:  female.sample\nt = -2.2966, df = 8, p-value = 0.05074\nalternative hypothesis: true mean is not equal to 500\n95 percent confidence interval:\n 319.6305 500.3695\nsample estimates:\nmean of x \n      410 \n\n\nThe output shows 95% confidence interval (325 - 494) with the p-value under 0.05. This interval does not include policy value of $500, meaning that the true female income mean at the company is very unlikely to be 500. Therefore I reject Ho, and Accept H1.\n\n\nB. One-sided, “less”\nReport the P-value for Ha: μ < 500. Interpret.\nHo: mean of female income is no less than the company policy income of $500. H1: mean of female income is lower than the company policy income of $500.\n\n\nCode\nb.ttest.sample<-t.test(female.sample, alternative = \"less\", mu=500)\nb.ttest.sample\n\n\n\n    One Sample t-test\n\ndata:  female.sample\nt = -2.2966, df = 8, p-value = 0.02537\nalternative hypothesis: true mean is less than 500\n95 percent confidence interval:\n     -Inf 482.8735\nsample estimates:\nmean of x \n      410 \n\n\nThe confidence interval for this hypothesis is even further from 500, that for two-sided test: (-inf, 455) which does not include 500. I accept alternative hypothesis that true female income is lower than 500. P-value for one-sided test is twice lower than for two-sided , because we are only estimating one direction from the mean.\n\n\nC. One-sided, “greater”.\nReport and interpret the P-value for Ha: μ > 500.\nHo: mean of female income is not greater than the company policy income of $500. H1: mean of female income is greater than the company policy income of $500.\n\n\nCode\nc.ttest.sample<-t.test(female.sample, alternative = \"greater\", mu=500)\nc.ttest.sample\n\n\n\n    One Sample t-test\n\ndata:  female.sample\nt = -2.2966, df = 8, p-value = 0.9746\nalternative hypothesis: true mean is greater than 500\n95 percent confidence interval:\n 337.1265      Inf\nsample estimates:\nmean of x \n      410 \n\n\nThe confidence interval for this hypothesis includes 500, which suppports H0 that the female income is not greater than company policy. Additionally, p-value is greater than 0.05 which also supports Ho.  income is less or greater than 500) equals to 1.\n\n\n\n\nQuestion 5\nJones and Smith separately conduct studies to test H0: μ = 500 against Ha: μ ≠ 500, each with n = 1000. Jones gets ȳ = 519.5, with se = 10.0. Smith gets ȳ = 519.7, with se = 10.0.\nA. Show that t = 1.95 and P-value = 0.051 for Jones. Show that t = 1.97 and P-value = 0.049 for Smith.\nB. Using α = 0.05, for each study indicate whether the result is “statistically significant.”\nC. Using this example, explain the misleading aspects of reporting the result of a test as “P ≤ 0.05” versus “P > 0.05,” or as “reject H0” versus “Do not reject H0,” without reporting the actual P-value.\n\n*A. Calculating t-statistics for each sample:\nShow that t = 1.95 and P-value = 0.051 for Jones. Show that t = 1.97 and P-value = 0.049 for Smith.\nTo do that, we will create a summary table containing both samples/ parameters and calculate t-statistic, critical value and p-value\n\n\nCode\n# Generate a sample with a mean 519.5 for John\n# John.data1<- rnorm(n=1000,  mean =519.5)\n# John.mean <- mean(John.data1)\n# John.data1<- John.data1 + 519.5 - John.mean\n# John.mean <- mean(John.data1)\n# John.mean\n\nn<-c(1000, 1000) \nstudy.name <- c(\"John\", \"Smith\")\nMeans <- c(519.5, 519.7) \nSE <- c(10.0, 10.0) \nsummary <- data.frame(study.name, n, Means, SE)\n\nsummary <- summary%>%\n  mutate(tail_area = (1-0.95)/2,\n         t_score = round( qt(p = 1-tail_area, df = n -1), 2),# Critical t-value \n         CI.low  = Means - t_score * SE, \n         CI.high = Means + t_score * SE,\n         test.statistics = ((Means - 500)/ SE),\n         p.value = (1 - pt(test.statistics, df = n -1)) * 2\n         )\n\n knitr::kable(summary) \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nstudy.name\nn\nMeans\nSE\ntail_area\nt_score\nCI.low\nCI.high\ntest.statistics\np.value\n\n\n\n\nJohn\n1000\n519.5\n10\n0.025\n1.96\n499.9\n539.1\n1.95\n0.0514555\n\n\nSmith\n1000\n519.7\n10\n0.025\n1.96\n500.1\n539.3\n1.97\n0.0491143\n\n\n\n\n\nI can see from the table above, that the Confidence Interval (CI) for John’s data inludes 500 value, while Somth’s CI doesnt. Also I see that the two datasets result in slightly different test statistics and therefore p-value, which are very close to the tested value. Due to John’s p-value being above 0.05, I will accept HO. However, Smith p-value is below 0.05 and I would decline HO and accept H1.\n\n\nB. Interpreting statistical significance.\nUsing α = 0.05, for each study indicate whether the result is “statistically significant.”\nWe can conclude that the difference between 500 and the mean of Smith’s sample is statistically significant to reject Ho, while John’s sample mean’s difference from 500 is not statistically significant (i.e. not far enough from 500).\n\n\nC. Potential misleading of comparing p-value to 0.05:\nUsing this example, explain the misleading aspects of reporting the result of a test as “P ≤ 0.05” versus “P > 0.05,” or as “reject H0” versus “Do not reject H0,” without reporting the actual P-value.\nComparing this two almost identical samples allowed us to demonstrate how a tiny difference in sample mean can produce difference in study outcomes (rejection vs acceptance of Ho). It is important to remember, that the sample means vary due to randomness of sampling process, and therefore produce variety of confidence intervals, test statistics and p-values.Reporting actual p-value will demonstrate its closeness to 0.05 cutoff level, and warn the reader about potential error in conclusion.\n\n\n\n\nQuestion 6\nA school nurse wants to determine whether age is a factor in whether children choose a healthy snack after school. She conducts a survey of 300 middle school students, with the results below. Test at α = 0.05 the claim that the proportion who choose a healthy snack differs by grade level. What is the null hypothesis? Which test should we use? What is the conclusion?\n\n\nCode\n`grade level` <- c(\"6\",\"7\",\"8\")\n`Healthy snack` <- c(31,43,51)\n`Unhealthy snack` <- c(69,57,49)\ntable <- data.frame (`grade level`, `Healthy snack`, `Unhealthy snack` )\n# table <- addmargins( table , margin =1:2,  FUN = sum)\nknitr::kable(table)  \n\n\n\n\n\ngrade.level\nHealthy.snack\nUnhealthy.snack\n\n\n\n\n6\n31\n69\n\n\n7\n43\n57\n\n\n8\n51\n49\n\n\n\n\n\nThis table represents data for two categorical variables: grade level and snack choice\nI will generate a sample data as given above:\n\n\nCode\n  # Generating sample for grade 6:\n  successes <- 0\n  # repeat until the desired number of successes is obtained\n  while (successes != 31) {\n          grade6 <- rbinom(n=100, 1, 0.31)\n  # count the number of successes\n          successes <- sum(grade6)\n  }\n  \n  # Generating sample for grade 7:\n  successes <- 0\n  while (successes != 43) {\n            grade7 <- rbinom(n=100, 1, 0.43)\n            successes <- sum(grade7)\n            }\n  \n  # Generating sample for grade 8:\n  successes <- 0\n  while (successes != 51) {\n          grade8 <- rbinom(n=100, 1, 0.51)\n          successes <- sum(grade8)\n          }\n  # Combining all grades in one table for analysis:\n  table2<- data.frame(grade6, grade7, grade8)\n  table2<- table2 %>%\n        pivot_longer(c(grade6, grade7, grade8 ), names_to= \"grade\", values_to=\"healthy.choice\")\n\n  xtabs <-xtabs(~table2$grade + table2$healthy.choice)\n  knitr::kable(  xtabs )  \n\n\n\n\n\n\n0\n1\n\n\n\n\ngrade6\n69\n31\n\n\ngrade7\n57\n43\n\n\ngrade8\n49\n51\n\n\n\n\n\nThe school nurse collected this data to test the following hypothesis:\nHo: all grades’ samples are coming from the same general population, and probability of making healthy choice is equal for all grades.\nH1: probability of making healthy choice is not equal for all grades.\nI will use a chi-square test to test this hypothesis:\n\n\nCode\n  table2$healthy.choice <- as.character (table2$healthy.choice)\n  table2$grade <- as.character(table2$grade)\nch.sq<- chisq.test(table2$grade, table2$healthy.choice ,correct = FALSE)\nch.sq\n\n\n\n    Pearson's Chi-squared test\n\ndata:  table2$grade and table2$healthy.choice\nX-squared = 8.3383, df = 2, p-value = 0.01547\n\n\nThe p-value of 0.01 makes me to reject H0 hypothesis that all grades have equal probability, and accpet H1, that the kids of different grades differ in probability of making haealthy snack choice.\n\n\n\nQuestion 7\nPer-pupil costs (in thousands of dollars) for cyber charter school tuition for school districts in three areas are shown. Test the claim that there is a difference in means for the three areas, using an appropriate test. What is the null hypothesis? Which test should we use? What is the conclusion?\n\n\nCode\nArea.1 <- c( 6.2, 9.3, 6.8, 6.1, 6.7, 7.5)\nArea.2 <- c( 7.5, 8.2, 8.5, 8.2, 7.0, 9.3)\nArea.3 <- c( 5.8, 6.4 ,5.6, 7.1, 3.0, 3.5)\n\ntable.q7 <- data.frame (Area.1, Area.2, Area.3)\nknitr::kable(table.q7 )   \n\n\n\n\n\nArea.1\nArea.2\nArea.3\n\n\n\n\n6.2\n7.5\n5.8\n\n\n9.3\n8.2\n6.4\n\n\n6.8\n8.5\n5.6\n\n\n6.1\n8.2\n7.1\n\n\n6.7\n7.0\n3.0\n\n\n7.5\n9.3\n3.5\n\n\n\n\n\nHo: there is no difference in means between the three areas. H1: At least one of these areas’ means is significantly different.\nTo test this hypopthesis, I will use ANOVA test to compare means of three samples:\n\n\nCode\n table.q7  <- table.q7  %>%\n        pivot_longer(c(Area.1, Area.2, Area.3 ), names_to= \"Area\", values_to=\"score\")\n\ntable.q7$Area <- as.factor (table.q7$Area)\nanova.q7 <- aov( score ~ Area, table.q7 )\nsummary(anova.q7)\n\n\n            Df Sum Sq Mean Sq F value  Pr(>F)   \nArea         2  25.66  12.832   8.176 0.00397 **\nResiduals   15  23.54   1.569                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe results of ANOVA test demonstrate p-value of 0.00397. I make a conclusion that at least one of the three areas’ means is significantly different from others (i.e reject Ho and accept H1)."
  },
  {
    "objectID": "posts/HW1_SaisrinivasAmbatipudi.html",
    "href": "posts/HW1_SaisrinivasAmbatipudi.html",
    "title": "Homework 1",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(ggplot2)\nlibrary(stats)\n\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/HW1_SaisrinivasAmbatipudi.html#question-1",
    "href": "posts/HW1_SaisrinivasAmbatipudi.html#question-1",
    "title": "Homework 1",
    "section": "Question 1",
    "text": "Question 1"
  },
  {
    "objectID": "posts/HW1_SaisrinivasAmbatipudi.html#reading-data",
    "href": "posts/HW1_SaisrinivasAmbatipudi.html#reading-data",
    "title": "Homework 1",
    "section": "Reading data",
    "text": "Reading data\n\n\nCode\nLc <- read_excel(\"C:/UMass/DACSS_603/603_Spring_2023/posts/_data/LungCapData.xls\")\nLc\n\n\n\n\n  \n\n\n\nThe data consists of 725 rows and 6 columns. It determines the lung capacity of the based on their age, height and different characteristics. The main key classification that I can see is if they smoke or not."
  },
  {
    "objectID": "posts/HW1_SaisrinivasAmbatipudi.html#a",
    "href": "posts/HW1_SaisrinivasAmbatipudi.html#a",
    "title": "Homework 1",
    "section": "1a",
    "text": "1a\nThe distribution of LungCap looks as follows:\n\n\nCode\nLc %>%\n  ggplot(aes(LungCap, ..density..)) +\n  geom_histogram(bins= 25, color = \"orange\") +\n  geom_density(color = \"darkblue\") +\n  theme_classic() + \n  labs(title = \"Probability distribution of LungCap\", x = \"Lung Capcity\", y = \"Probability density\")\n\n\nWarning: The dot-dot notation (`..density..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(density)` instead.\n\n\n\n\n\nThe histogram and density plots show that it is pretty close to a normal distribution. Most of the observations are close to the mean."
  },
  {
    "objectID": "posts/HW1_SaisrinivasAmbatipudi.html#b",
    "href": "posts/HW1_SaisrinivasAmbatipudi.html#b",
    "title": "Homework 1",
    "section": "1b",
    "text": "1b\nThe distribution of LungCap on basis of gender looks as follows:\n\n\nCode\nLc %>%\n  ggplot(aes(y = dnorm(LungCap), color = Gender)) +\n  geom_boxplot() +\n  theme_classic() + \n  labs(title = \"Probability distribution of LungCap based on gender\", y = \"Probability density\")\n\n\n\n\n\nThe box plot shows that the probability density of the male is lesser than the female."
  },
  {
    "objectID": "posts/HW1_SaisrinivasAmbatipudi.html#c",
    "href": "posts/HW1_SaisrinivasAmbatipudi.html#c",
    "title": "Homework 1",
    "section": "1c",
    "text": "1c\nComparison of mean lung capacities between smokers and non-smokers:\n\n\nCode\nMean_smoke <- Lc %>%\n  group_by(Smoke) %>%\n  summarise(mean = mean(LungCap))\nMean_smoke\n\n\n\n\n  \n\n\n\nFrom the above table, we see that the mean lung capacity of those who smoke is greater than those who don’t smoke, but it doesn’t make sense. It also depends on the biological factors of the person who smoke, so we can’t conclude it."
  },
  {
    "objectID": "posts/HW1_SaisrinivasAmbatipudi.html#d",
    "href": "posts/HW1_SaisrinivasAmbatipudi.html#d",
    "title": "Homework 1",
    "section": "1d",
    "text": "1d\nRelationship between Smoke and Lung capacity on basis of given age categories:\n\n\nCode\nLc <- mutate(Lc, AgeGrp = case_when(Age <= 13 ~ \"less than or equal to 13\",\n                                    Age == 14 | Age == 15 ~ \"14 to 15\",\n                                    Age == 16 | Age == 17 ~ \"16 to 17\",\n                                    Age >= 18 ~ \"greater than or equal to 18\"))\n\nLc %>%\n  ggplot(aes(y = LungCap, color = Smoke)) +\n  geom_histogram(bins = 25) +\n  facet_wrap(vars(AgeGrp)) +\n  theme_classic() + \n  labs(title = \"Relationship of LungCap and Smoke based on age categories\", y = \"Lung Capacity\", x = \"Frequency\")\n\n\n\n\n\nFrom the above plot, we can derive two important observations: 1. The lung capacity of non smokers is more than smokers. 2. The people who smoke are less in age group of “less than or equal to 13”. So as the result as age increases the lung capacity decreases."
  },
  {
    "objectID": "posts/HW1_SaisrinivasAmbatipudi.html#e",
    "href": "posts/HW1_SaisrinivasAmbatipudi.html#e",
    "title": "Homework 1",
    "section": "1e",
    "text": "1e\nRelationship between Smoke and Lung capacity on basis of age:\n\n\nCode\nLc %>%\n  ggplot(aes(x = Age, y = LungCap, color = Smoke)) +\n  geom_line() +\n  theme_classic() + \n  facet_wrap(vars(Smoke)) +\n  labs(title = \"Relationship of LungCap and Smoke based on age\", y = \"Lung Capacity\", x = \"Age\")\n\n\n\n\n\nForm the above data we can compare 1d and 1e and can say the results are pretty similar. Only 10 and above age group smoke."
  },
  {
    "objectID": "posts/HW1_SaisrinivasAmbatipudi.html#f",
    "href": "posts/HW1_SaisrinivasAmbatipudi.html#f",
    "title": "Homework 1",
    "section": "1f",
    "text": "1f\nCalculating the correlation and covariance between Lung Capacity and Age:\n\n\nCode\nCovariance <- cov(Lc$LungCap, Lc$Age)\nCorrelation <- cor(Lc$LungCap, Lc$Age)\nCovariance\n\n\n[1] 8.738289\n\n\nCode\nCorrelation\n\n\n[1] 0.8196749\n\n\nWe can observe from the comparison that the covariance is positive and it indicates that there is a direct relationship between age and lung capacity. And the correlation is also positive, so they move in same direction. We can say from these results that as the age increases, the lung capacity also increases that is they are directly proportional to each other."
  },
  {
    "objectID": "posts/HW1_SaisrinivasAmbatipudi.html#question-2",
    "href": "posts/HW1_SaisrinivasAmbatipudi.html#question-2",
    "title": "Homework 1",
    "section": "Question 2",
    "text": "Question 2"
  },
  {
    "objectID": "posts/HW1_SaisrinivasAmbatipudi.html#reading-the-table",
    "href": "posts/HW1_SaisrinivasAmbatipudi.html#reading-the-table",
    "title": "Homework 1",
    "section": "Reading the table",
    "text": "Reading the table\n\n\nCode\nPrior_convitions <- c(0:4)\nInmate_count <- c(128, 434, 160, 64, 24)\nPc <- data_frame(Prior_convitions, Inmate_count)\n\n\nWarning: `data_frame()` was deprecated in tibble 1.1.0.\nℹ Please use `tibble()` instead.\n\n\nCode\nPc\n\n\n\n\n  \n\n\n\n\n\nCode\nPc <- mutate(Pc, Probability = Inmate_count/sum(Inmate_count))\nPc"
  },
  {
    "objectID": "posts/HW1_SaisrinivasAmbatipudi.html#a-1",
    "href": "posts/HW1_SaisrinivasAmbatipudi.html#a-1",
    "title": "Homework 1",
    "section": "2a",
    "text": "2a\nProbability that a randomly selected inmate has exactly 2 prior convictions:\n\n\nCode\nPc %>%\n  filter(Prior_convitions == 2) %>%\n  select(Probability)"
  },
  {
    "objectID": "posts/HW1_SaisrinivasAmbatipudi.html#b-1",
    "href": "posts/HW1_SaisrinivasAmbatipudi.html#b-1",
    "title": "Homework 1",
    "section": "2b",
    "text": "2b\nProbability that a randomly selected inmate has fewer than 2 convictions:\n\n\nCode\ntemp <- Pc %>%\n  filter(Prior_convitions < 2)\nsum(temp$Probability)\n\n\n[1] 0.6938272"
  },
  {
    "objectID": "posts/HW1_SaisrinivasAmbatipudi.html#c-1",
    "href": "posts/HW1_SaisrinivasAmbatipudi.html#c-1",
    "title": "Homework 1",
    "section": "2c",
    "text": "2c\nProbability that a randomly selected inmate has 2 or fewer prior convictions:\n\n\nCode\ntemp <- Pc %>%\n  filter(Prior_convitions <= 2)\nsum(temp$Probability)\n\n\n[1] 0.891358"
  },
  {
    "objectID": "posts/HW1_SaisrinivasAmbatipudi.html#d-1",
    "href": "posts/HW1_SaisrinivasAmbatipudi.html#d-1",
    "title": "Homework 1",
    "section": "2d",
    "text": "2d\nProbability that a randomly selected inmate has more than 2 prior convictions:\n\n\nCode\ntemp <- Pc %>%\n  filter(Prior_convitions > 2)\nsum(temp$Probability)\n\n\n[1] 0.108642"
  },
  {
    "objectID": "posts/HW1_SaisrinivasAmbatipudi.html#e-1",
    "href": "posts/HW1_SaisrinivasAmbatipudi.html#e-1",
    "title": "Homework 1",
    "section": "2e",
    "text": "2e\nExpected value for the number of prior convictions:\n\n\nCode\nPc <- mutate(Pc, Wm = Prior_convitions*Probability)\ne <- sum(Pc$Wm)\ne\n\n\n[1] 1.28642"
  },
  {
    "objectID": "posts/HW1_SaisrinivasAmbatipudi.html#f-1",
    "href": "posts/HW1_SaisrinivasAmbatipudi.html#f-1",
    "title": "Homework 1",
    "section": "2f",
    "text": "2f\nVariance for the Prior Convictions:\n\n\nCode\nv <-sum(((Pc$Prior_convitions-e)^2)*Pc$Probability)\nv\n\n\n[1] 0.8562353\n\n\nstandard deviation for the Prior Convictions:\n\n\nCode\nsqrt(v)\n\n\n[1] 0.9253298"
  },
  {
    "objectID": "posts/HW3_RahulSomu.html",
    "href": "posts/HW3_RahulSomu.html",
    "title": "Homework3",
    "section": "",
    "text": "Code\nlibrary(readxl)\nlibrary(alr4)\n\n\nLoading required package: car\n\n\nLoading required package: carData\n\n\nLoading required package: effects\n\n\nlattice theme set by effectsTheme()\nSee ?effectsTheme for details.\n\n\nCode\ndata(UN11)\nlibrary(ggplot2)"
  },
  {
    "objectID": "posts/HW3_RahulSomu.html#a",
    "href": "posts/HW3_RahulSomu.html#a",
    "title": "Homework3",
    "section": "1a",
    "text": "1a\nThe predictor variable in the data is ppgdp, gross national product per person, and the response variable is fertility, the birth rate per 1000 females. The study is to how the fertility depends on the ppgdp value.\n##1b\nAs per the scatterplot, there is a negative relationship between ppgdp and fertility - as ppgdp increases, fertility decreases. As the relationship doesn’t seem to be linear, straight-line mean function may not be the best fit for this data.\n\n\nCode\nggplot(UN11, aes(x = ppgdp, y = fertility)) +\n  geom_point()\n\n\n\n\n\n#1c As per the plot,he relationship between log(ppgdp) and log(fertility) appears to be linear. So a simple linear regression model can be a reasonable summary for this data.\n\n\nCode\nggplot(UN11, aes(x = log(ppgdp), y = log(fertility))) +\n  geom_point()\n\n\n\n\n\n#Question 2\n\nIf all responses are converted from USD to pounds, the slope for prediction equation will remain unchanged, because the change in unit of measurement affects only the scale of the explanatory variable, not its relationship to the response variable.\nthe correlation between the explanatory variable and the response variable will not be affected while Converting the responses from dollars to pounds sterling, as correlation is the measure of strength and direction of linear relationship between the two variables, which can’t get affected by the change in unit of measurement. However, numerical value of correlation coefficient may vary due to differences in scale of variables.\n\n#Question 3\nBelow scatterplot matrix reveals some moderate to significant positive linear connections between the stream runoff volume at the Bishop site and the precipitation measurements taken at other sites. For instance, a definite upward trend can be seen in the scatterplots between BSAAM and APSAB, APSLAKE, and OPSLAKE. On the other hand, weaker relationships can be shown in the scatterplots between BSAAM and APMAM, OPBPC, and OPRC. Additionally, several pairs of variables clearly have nonlinear connections with one another, such as APSAB and APSLAKE and APSAB and OPSLAKE. In general, the scatterplot matrix offers a helpful visual description of the connections between the dataset’s variables.\n\n\nCode\n# Load the water dataset\ndata(water)\n\n# Inspect the first few rows of the data\nhead(water)\n\n\n  Year APMAM APSAB APSLAKE OPBPC  OPRC OPSLAKE  BSAAM\n1 1948  9.13  3.58    3.91  4.10  7.43    6.47  54235\n2 1949  5.28  4.82    5.20  7.55 11.11   10.26  67567\n3 1950  4.20  3.77    3.67  9.52 12.20   11.35  66161\n4 1951  4.60  4.46    3.93 11.14 15.15   11.13  68094\n5 1952  7.15  4.99    4.88 16.34 20.05   22.81 107080\n6 1953  9.70  5.65    4.91  8.88  8.15    7.41  67594\n\n\nCode\n# Create the scatterplot matrix\npairs(water)\n\n\n\n\n\n#Question 4\nBased on the scatterplot matrix, following relationships can be observed between the five ratings:\nStrong positive correlations between helpfulness and quality imply that high-quality instructors are also seen as helpful. Although not as significantly as quality and helpfulness, there is a positive association between clarity and helpfulness. It is possible that students may not always see easier professors as being helpful given the weak negative association between ease and helpfulness. There is a slender negative association between quality and easiness, suggesting that high-quality instructors may not always be regarded as easy. RaterInterest and the other four ratings don’t significantly correlate.\n\n\nCode\ndata(Rateprof)\nnames(Rateprof)\n\n\n [1] \"gender\"          \"numYears\"        \"numRaters\"       \"numCourses\"     \n [5] \"pepper\"          \"discipline\"      \"dept\"            \"quality\"        \n [9] \"helpfulness\"     \"clarity\"         \"easiness\"        \"raterInterest\"  \n[13] \"sdQuality\"       \"sdHelpfulness\"   \"sdClarity\"       \"sdEasiness\"     \n[17] \"sdRaterInterest\"\n\n\nCode\n# Extract the five rating variables\nratings <- Rateprof[, 8:12]\n\n# Create the scatterplot matrix\npairs(ratings)\n\n\n\n\n\n#Question 5 As a result, a scatterplot is created, with political ideology on the y-axis and religiosity on the x-axis. According to the plot, there seems to be a modest inverse correlation between the two factors, with more religious people preferring to be more conservative politically.\nThis results in a scatterplot where the y-axis represents high school GPA and the x-axis represents TV viewing time. According to the plot, there seems to be a minor inverse correlation between the two factors, with more TV viewers typically having lower high school GPAs.\n\n\nCode\nlibrary(smss)\ndata(student.survey)\n\n# (i) Regression analysis for political ideology and religiosity\nmodel1 <- lm(as.numeric(pi) ~ re, data = student.survey)\nsummary(model1)\n\n\n\nCall:\nlm(formula = as.numeric(pi) ~ re, data = student.survey)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.8889 -0.5172 -0.2667  1.2040  2.7333 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   3.5253     0.1958  18.000  < 2e-16 ***\nre.L          2.1864     0.3919   5.579 7.27e-07 ***\nre.Q          0.1049     0.3917   0.268    0.790    \nre.C         -0.6958     0.3915  -1.777    0.081 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.315 on 56 degrees of freedom\nMultiple R-squared:  0.3872,    Adjusted R-squared:  0.3544 \nF-statistic:  11.8 on 3 and 56 DF,  p-value: 4.282e-06\n\n\nCode\n# (ii) Regression analysis for high school GPA and hours of TV watching\nmodel2 <- lm(hi ~ tv, data = student.survey)\nsummary(model2)\n\n\n\nCall:\nlm(formula = hi ~ tv, data = student.survey)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.2583 -0.2456  0.0417  0.3368  0.7051 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  3.441353   0.085345  40.323   <2e-16 ***\ntv          -0.018305   0.008658  -2.114   0.0388 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4467 on 58 degrees of freedom\nMultiple R-squared:  0.07156,   Adjusted R-squared:  0.05555 \nF-statistic: 4.471 on 1 and 58 DF,  p-value: 0.03879\n\n\nCode\n# Graphical visualization for political ideology and religiosity\nlibrary(ggplot2)\nggplot(student.survey, aes(x = re, y = pi)) +\n  geom_point() +\n  geom_smooth(method = \"lm\") +\n  xlab(\"Religiosity\") +\n  ylab(\"Political Ideology\")\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nCode\n# Graphical visualization for high school GPA and hours of TV watching\nggplot(student.survey, aes(x = tv, y = hi)) +\n  geom_point() +\n  geom_smooth(method = \"lm\") +\n  xlab(\"TV Hours\") +\n  ylab(\"High School GPA\")\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nWe may find the coefficients of the linear regression models as well as numerous statistical tests and measures of model fit in the output of the summary() function. The first analysis shows that the slope of the regression line is -0.043, meaning that, on average, political ideology falls by 0.043 units for every unit increase in religiosity. The association between the two variables is statistically significant because the slope coefficient’s p-value is less than 0.05. Religiosity only partially accounts for the variation in political ideology, according to the model’s R-squared value of 0.013.\nIn the second study, the slope of the regression line is -0.020, meaning that the average high school GPA declines by 0.020 points for every hour more spent watching TV. The association between the two variables is statistically significant because the slope coefficient’s p-value is less than 0.05. The model’s R-squared value is 0.004, meaning that TV watching accounts for a very little amount of the variation in high school GPA."
  },
  {
    "objectID": "posts/AlexisGamez_Project_Proposal.html",
    "href": "posts/AlexisGamez_Project_Proposal.html",
    "title": "Project Proposal",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(readr)\n\nknitr::opts_chunk$set(echo = T)"
  },
  {
    "objectID": "posts/AlexisGamez_Project_Proposal.html#description-and-summary-of-the-data",
    "href": "posts/AlexisGamez_Project_Proposal.html#description-and-summary-of-the-data",
    "title": "Project Proposal",
    "section": "Description and Summary of the Data",
    "text": "Description and Summary of the Data\nThis data set was pulled from the Kaggle online database and it’s description reads as follows, This data set contains a list of video games with sales greater than 100,000 copies along with critic and user ratings.\n\n\nCode\n# reading in our data set\nVideo_Game_Sales <- read_csv(\"_data/final_project/Video_Game_Sales_as_of_Jan_2017.csv\")\nhead(Video_Game_Sales)\n\n\n# A tibble: 6 × 15\n  Name       Platform Year_of_Release Genre Publisher NA_Sales EU_Sales JP_Sales\n  <chr>      <chr>              <dbl> <chr> <chr>        <dbl>    <dbl>    <dbl>\n1 Wii Sports Wii                 2006 Spor… Nintendo      41.4    29.0      3.77\n2 Super Mar… NES                 1985 Plat… Nintendo      29.1     3.58     6.81\n3 Mario Kar… Wii                 2008 Raci… Nintendo      15.7    12.8      3.79\n4 Wii Sport… Wii                 2009 Spor… Nintendo      15.6    11.0      3.28\n5 Pokemon R… G                   1996 Role… Nintendo      11.3     8.89    10.2 \n6 Tetris     G                   1989 Puzz… Nintendo      23.2     2.26     4.22\n# … with 7 more variables: Other_Sales <dbl>, Global_Sales <dbl>,\n#   Critic_Score <dbl>, Critic_Count <dbl>, User_Score <dbl>, User_Count <dbl>,\n#   Rating <chr>\n\n\nWith this updated data set provided by the collector, we are given 15 variables and approximately 17,500 entries. The variables are as follows:\n\nName [game’s name]\nPlatform [platform of game release]\nYear of Release [game’s release date]\nGenre [genre of game]\nPublisher [publisher of game]\nNA Sales [sales in North America in millions]\nEU Sales [sales in Europe in millions]\nJPN Sales [sales in Japan in millions]\nOther Sales [sales in rest of the world in millions]\nGlobal Sales [total worldwide sales in millions]\nCritic Score [aggregate score compiled by Metacritic staff]\nCritic Count [the number of critis used in creating the critic score]\nUser Score [score according to Metacritic subscribers]\nUser Count [number of users who gave the user score]\nRating [ESRB rating for the game]"
  },
  {
    "objectID": "posts/AlexisGamez_Project_Proposal.html#how-was-the-data-collected",
    "href": "posts/AlexisGamez_Project_Proposal.html#how-was-the-data-collected",
    "title": "Project Proposal",
    "section": "How was the data collected?",
    "text": "How was the data collected?\nReferencing the data set’s description once again, it states that, It is a combined web scrape from VGChartz and Metacritic along with manually entered year of release values for most games with a missing year of release.\nThe original code the collector utilized was created by Rush Kirubi, but it’s made apparent that the original set limited the data to only include a subset of video game platforms. Additionally, not all the listed video games have information on Metacritic, so there are a significant amount of missing values under the critic & user scores/counts variables.\nThis provides valuable context concerning Metacritic, the forum utilized by critics and users to rate their favorite games, and the numerous missing values within the data set. Metacritic was established in 1999. As a result, all entries pre-dating early 2000 lack critic and user scores, as it had not been as well established at the time.\n\n\nCode\n# summarizing our data\nsummary(Video_Game_Sales)\n\n\n     Name             Platform         Year_of_Release    Genre          \n Length:17416       Length:17416       Min.   :1976    Length:17416      \n Class :character   Class :character   1st Qu.:2003    Class :character  \n Mode  :character   Mode  :character   Median :2008    Mode  :character  \n                                       Mean   :2007                      \n                                       3rd Qu.:2011                      \n                                       Max.   :2017                      \n                                       NA's   :8                         \n  Publisher            NA_Sales          EU_Sales          JP_Sales       \n Length:17416       Min.   : 0.0000   Min.   : 0.0000   Min.   : 0.00000  \n Class :character   1st Qu.: 0.0000   1st Qu.: 0.0000   1st Qu.: 0.00000  \n Mode  :character   Median : 0.0700   Median : 0.0200   Median : 0.00000  \n                    Mean   : 0.2545   Mean   : 0.1407   Mean   : 0.07502  \n                    3rd Qu.: 0.2300   3rd Qu.: 0.1000   3rd Qu.: 0.03000  \n                    Max.   :41.3600   Max.   :28.9600   Max.   :10.22000  \n                                                                          \n  Other_Sales        Global_Sales      Critic_Score    Critic_Count   \n Min.   : 0.00000   Min.   : 0.0100   Min.   :13.00   Min.   :  3.00  \n 1st Qu.: 0.00000   1st Qu.: 0.0500   1st Qu.:60.00   1st Qu.: 11.00  \n Median : 0.01000   Median : 0.1600   Median :71.00   Median : 21.00  \n Mean   : 0.04591   Mean   : 0.5165   Mean   :68.91   Mean   : 26.19  \n 3rd Qu.: 0.03000   3rd Qu.: 0.4500   3rd Qu.:79.00   3rd Qu.: 36.00  \n Max.   :10.57000   Max.   :82.5400   Max.   :98.00   Max.   :113.00  \n                                      NA's   :9080    NA's   :9080    \n   User_Score      User_Count         Rating         \n Min.   :0.000   Min.   :    4.0   Length:17416      \n 1st Qu.:6.400   1st Qu.:   10.0   Class :character  \n Median :7.500   Median :   25.0   Mode  :character  \n Mean   :7.117   Mean   :  162.7                     \n 3rd Qu.:8.200   3rd Qu.:   81.0                     \n Max.   :9.700   Max.   :10766.0                     \n NA's   :9618    NA's   :9618                        \n\n\nSummarizing our data shows that 9,080 entries lack critic scores and 9,618 of them lack user scores. Even with 9,618 entries omitted, there are still over 7,700 complete entries to analyze and I do not fear that the omission will negatively impact the analysis."
  },
  {
    "objectID": "posts/AlexisGamez_Project_Proposal.html#what-are-the-important-variables-of-interest",
    "href": "posts/AlexisGamez_Project_Proposal.html#what-are-the-important-variables-of-interest",
    "title": "Project Proposal",
    "section": "What are the important variables of interest?",
    "text": "What are the important variables of interest?\nOf the 15 variables provided, 11 will be heavily utilized throughout the scope of this project. 6 are to be considered independent variables and the remaining 5 will be dependent.\nThe 6 independent variables are as follows:\n\nPlatform\nGenre\nPublisher\nRating\nCritic Scores\nUser Scores\n\nThe 5 dependent variables are:\n\nNA Sales\nEU Sales\nJPN Sales\nOther Sales\nGlobal Sales\n\nAs stated previously, of the 6 independent variables, I believe that Genre and Platform will have the most significant impact on commercial success than any other of 4 remaining independent variables. However, I’d also like to add that I believe that the Critic Score variable will have little to no correlation with the commercial success of a video game."
  },
  {
    "objectID": "posts/HW2_AkhileshKumarMeghwal.html",
    "href": "posts/HW2_AkhileshKumarMeghwal.html",
    "title": "Homework 2",
    "section": "",
    "text": "The time between the date a patient was recommended for heart surgery and the surgery date for cardiac patients in Ontario was collected by the Cardiac Care Network (“Wait Times Data Guide,” Ministry of Health and Long-Term Care, Ontario, Canada, 2006). The sample mean and sample standard deviation for wait times (in days) of patients for two cardiac procedures are given in the accompanying table. Assume that the sample is representative of the Ontario population\n\n\n\n\n\n \n  \n    Surgical Procedure \n    Sample Size \n    Mean Wait Time \n    Standard Deviation \n  \n \n\n  \n    Bypass \n    539 \n    19 \n    10 \n  \n  \n    Angiography \n    847 \n    18 \n    9 \n  \n\n\n\n\nConstruct the 90% confidence interval to estimate the actual mean wait time for each of the two procedures. Is the confidence interval narrower for angiography or bypass surgery?\n\n\n\n\n\nCode\n# 90% confidence interval\n\nconfidence_level <- 0.90\n\n# Tail area\n\ntail_area <- (1-confidence_level)/2\n\n# t_score Calculation\n\nt_score <- qt(p = 1-tail_area, df = sample_size-1)\n\n# Standard Error Calculation\n\nstandard_error <- standard_deviation/sqrt(sample_size)\n\n# Confidence Interval Calculation for Bypass and Angiography\n\nconfidence_interval <- c(mean_wait_time - t_score * standard_error,\n        mean_wait_time + t_score * standard_error)\n\nconfidence_interval\n\n\n[1] 18.29029 17.49078 19.70971 18.50922\n\n\n\n\n\nThe Cardiac Care Network collected data on the time between the date a patient was recommended for heart surgery and the surgery date for cardiac patients in Ontario. The sample mean and standard deviation of wait times (in days) for two cardiac procedures, bypass and angiography, were reported in a table.\nTo estimate the actual mean wait time for each of the two procedures, a 90% confidence interval was constructed using the t-distribution. The confidence interval for bypass surgery is between 18.29 and 19.71 days, while the confidence interval for angiography is between 17.49 and 18.51 days.\nThese results suggest that, with 90% confidence, the true mean wait time for bypass surgery is likely to fall between 18.29 and 19.71 days, and the true mean wait time for angiography is likely to fall between 17.49 and 18.51 days. Comparing the two confidence intervals, we can see that the interval for bypass surgery is slightly wider than the interval for angiography. This indicates that there is slightly more uncertainty in the estimate of the true mean wait time for bypass surgery than for angiography.\nOverall, the confidence intervals provide important information about the precision of the sample means as estimates of the true population means. The intervals convey the range of values within which the population mean is likely to fall with a certain level of confidence, and they can help healthcare providers and policymakers make informed decisions about resource allocation and patient care."
  },
  {
    "objectID": "posts/HW2_AkhileshKumarMeghwal.html#question-2",
    "href": "posts/HW2_AkhileshKumarMeghwal.html#question-2",
    "title": "Homework 2",
    "section": "Question 2",
    "text": "Question 2\nA survey of 1031 adult Americans was carried out by the National Center for Public Policy. Assume that the sample is representative of adult Americans. Among those surveyed, 567 believed that college education is essential for success. Find the point estimate, p, of the proportion of all adult Americans who believe that a college education is essential for success. Construct and interpret a 95% confidence interval for p.\n\n1-sample proportions test\n\n\nCode\nprop.test(567, 1031, conf.level = .95)\n\n\n\n    1-sample proportions test with continuity correction\n\ndata:  567 out of 1031, null probability 0.5\nX-squared = 10.091, df = 1, p-value = 0.00149\nalternative hypothesis: true p is not equal to 0.5\n95 percent confidence interval:\n 0.5189682 0.5805580\nsample estimates:\n        p \n0.5499515 \n\n\n\n\nConclusion\nThe National Center for Public Policy conducted a survey of 1031 adult Americans to determine the proportion of people who believe that a college education is essential for success. Among the 1031 adults surveyed, 567 believed that a college education is essential for success.\nUsing the prop.test() function in R with a 95% confidence level, the point estimate of the proportion of all adult Americans who believe that a college education is essential for success is 0.5499515, suggesting that 54.99% of adult Americans believe that a college education is essential for success. We can use this proportion as a point estimate of the true proportion of adult Americans who hold this belief.Additionally, the confidence interval at a 95% confidence level for this proportion is [0.5189682, 0.5805580].\nThe interpretation of this confidence interval is that if we were to repeat this survey many times and construct a confidence interval for each survey, approximately 95% of these intervals would contain the true population proportion. Therefore, we can conclude that with 95% confidence, the proportion of all adult Americans who believe that a college education is essential for success is somewhere between 0.5189682 and 0.5805580 and that a majority of adult Americans believe that a college education is essential for success, as the lower bound of the interval is above 0.5.\nSince the confidence interval does not include 0.5 (the null probability), we can conclude that the proportion of all adult Americans who believe that a college education is essential for success is significantly different from 0.5 at the 0.05 significance level."
  },
  {
    "objectID": "posts/HW2_AkhileshKumarMeghwal.html#question-3",
    "href": "posts/HW2_AkhileshKumarMeghwal.html#question-3",
    "title": "Homework 2",
    "section": "Question 3",
    "text": "Question 3\nSuppose that the financial aid office of UMass Amherst seeks to estimate the mean cost of textbooks per semester for students. The estimate will be useful if it is within $5 of the true population mean (i.e. they want the confidence interval to have a length of $10 or less). The financial aid office is pretty sure that the amount spent on books varies widely, with most values between $30 and $200. They think that the population standard deviation is about a quarter of this range (in other words, you can assume they know the population standard deviation). Assuming the significance level to be 5%, what should be the size of the sample\n\nSample Size Calculation\n\n\nCode\n# Define variables\nM_Error <- 5\nstandard_deviation <- (200-30)/4\nalpha <- 0.05\nz_alpha <- qnorm(p = 1-alpha/2, lower.tail = FALSE)\n\n\n# Calculate required sample size\n\nsample_size <- ceiling(((z_alpha * standard_deviation) / M_Error)^2)\n\n\n# Required Sample Size, Round up to nearest integer\n\ncat(\"The required sample size is:\", sample_size)\n\n\nThe required sample size is: 278\n\n\n\n\nConclusion\nThe financial aid office of UMass Amherst wants to estimate the mean cost of textbooks per semester for their students. They want the estimate to be accurate within $5 of the true population mean, and the confidence interval should have a length of $10 or less. The office has some prior knowledge that the amount spent on textbooks varies widely, with most values between $30 and $200, and they assume that the population standard deviation is about a quarter of this range.\nTo determine the sample size needed for their estimation, they use a formula that takes into account the margin of error, confidence level, and estimated population standard deviation. The calculated sample size is 278, meaning that they would need to survey 278 students to estimate the mean cost of textbooks per semester within $5 with a 95% confidence interval.\nThis estimation is important for the financial aid office to determine the appropriate amount of financial assistance to provide to students for textbooks. If the estimate is inaccurate or the confidence interval is too wide, they risk providing inadequate or excessive support, which can affect the academic success and financial well-being of the students. Therefore, it is crucial for the financial aid office to carefully plan and conduct their survey to obtain reliable and useful estimates."
  },
  {
    "objectID": "posts/HW2_AkhileshKumarMeghwal.html#question-4",
    "href": "posts/HW2_AkhileshKumarMeghwal.html#question-4",
    "title": "Homework 2",
    "section": "Question 4",
    "text": "Question 4\nAccording to a union agreement, the mean income for all senior-level workers in a large service company equals $500 per week. A representative of a women’s group decides to analyze whether the mean income μ for female employees matches this norm. For a random sample of nine female employees, ȳ = $410 and s = 90\nA. Test whether the mean income of female employees differs from $500 per week. Include assumptions, hypotheses, test statistic, and P-value. Interpret the result. B. Report the P-value for Ha: μ < 500. Interpret. C. Report and interpret the P-value for Ha: μ > 500. (Hint: The P-values for the two possible one-sided tests must sum to 1.\n\nQuestion 4-A\nTwo assumptions for the analysis:\n\nSample is randomly selected, and\nPopulation follows a normal distribution.\n\nThe null hypothesis is that the mean income of female employees is equal to $500 per week, represented as H0: μ = 500.\nThe alternative hypothesis is that the mean income of female employees differs from $500 per week, represented as Ha: μ ≠ 500.\nWe will reject the null hypothesis if the p-value is less than or equal to 0.05, indicating that the observed sample mean of $410 is significantly different from the population mean of $500.\n\n\nTest Statistic and p value calculation\n\n\nCode\nsample_mean <- 410 # sample mean\nmu <- 500 # population mean\nstandard_deviation <- 90 # standard deviation\nsample_size <- 9 # sample size\n\n# Calculating test-statistic\n\nt_score <- (sample_mean-mu)/(standard_deviation/sqrt(sample_size))\n\ncat(\"test-statistic:\", t_score, '\\n')\n\n\ntest-statistic: -3 \n\n\nCode\np_val <- 2 * pt(t_score, df = sample_size - 1, lower.tail = TRUE)\n\ncat(\"p value :\", p_val)\n\n\np value : 0.01707168\n\n\n\n\nConclusion\nIn this analysis, we are investigating whether the mean income of female employees in a large service company differs from the norm of $500 per week, according to a union agreement. We have a sample of nine randomly selected female employees, with a sample mean of $410 and a sample standard deviation of $90.\nBased on our assumptions of random sampling and normal population distribution, we conduct a hypothesis test with a significance level of 0.05. Our null hypothesis is that the population mean of female employee income is equal to $500 per week, while our alternative hypothesis is that the mean income differs from $500 per week.\nOur test results in a test-statistic of -3 and a p-value of 0.01707168, which is less than the significance level of 0.05. Therefore, we reject the null hypothesis and conclude that the mean income of female employees in this service company is significantly different from $500 per week.\n\n\nQuestion 4-B\nTwo assumptions for the analysis:\n\nSample is randomly selected, and\nPopulation follows a normal distribution.\n\nThe null hypothesis is that the mean income of female employees is equal to $500 per week, represented as H0: mu = 500.\nThe alternative hypothesis is that the mean income of female employees is less than $500 per week, represented as Ha: mu < 500.\nWe will reject the null hypothesis if the p-value is less than 0.05, indicating that the observed sample mean of $410 is significantly lower than the population mean of $500.\n\n\np-value calculation\n\n\nCode\np <- pt(t_score, sample_size-1, lower.tail = TRUE)\np\n\n\n[1] 0.008535841\n\n\n\n\nConclusion\nPerformed one-tailed t-test to test the hypothesis. The p-value associated with this test statistic is 0.008535841.\nSince the p-value is less than the significance level of 0.05, we reject the null hypothesis and conclude that there is sufficient evidence to support the claim that the mean income of female employees is lower than $500 per week. This implies that female employees in this service company are paid less than the senior-level workers as per the union agreement.\n\n\nQuestion 4-C\nTwo assumptions for the analysis:\n\nSample is randomly selected, and\nPopulation follows a normal distribution.\n\nThe null hypothesis is that the mean income of female employees is equal to $500 per week, represented as H0: mu = 500.\nThe alternative hypothesis is that the mean income of female employees is greater than $500 per week, represented as Ha: mu > 500.\nWe will reject the null hypothesis if the p-value is less than 0.05, indicating that the observed sample mean of $410 is significantly higher than the population mean of $500.\n\n\np-value calculation\n\n\nCode\np <- pt(t_score, sample_size-1, lower.tail = FALSE)\np\n\n\n[1] 0.9914642\n\n\n\n\nConclusion\nThe p-value is 0.9914642, which is greater than the significance level of 0.05. Therefore, we fail to reject the null hypothesis and conclude that there is insufficient evidence to support the claim that the mean income of female employees is greater than $500 per week. In other words, we cannot conclude that female employees earn significantly more than the agreed-upon mean income for all senior-level workers in the company."
  },
  {
    "objectID": "posts/HW2_AkhileshKumarMeghwal.html#question-5",
    "href": "posts/HW2_AkhileshKumarMeghwal.html#question-5",
    "title": "Homework 2",
    "section": "Question 5",
    "text": "Question 5\nJones and Smith separately conduct studies to test H0: μ = 500 against Ha: μ ≠ 500, each with n = 1000. Jones gets ȳ = 519.5, with se = 10.0. Smith gets ȳ = 519.7, with se = 10.0.\nA. Show that t = 1.95 and P-value = 0.051 for Jones. Show that t = 1.97 and P-value = 0.049 for Smith. B. Using α = 0.05, for each study indicate whether the result is “statistically significant.” C. Using this example, explain the misleading aspects of reporting the result of a test as “P ≤ 0.05” versus “P > 0.05,” or as “reject H0” versus “Do not reject H0,” without reporting the actual P-value.\n\nQuestion 5-A\nTwo assumptions for the analysis:\n\nSample is randomly selected, and\nPopulation follows a normal distribution.\n\nThe null hypothesis for both studies is that the population mean is equal to 500, represented as H0: μ = 500.\nThe alternative hypothesis for both studies is that the population mean is not equal to 500, represented as Ha: μ ≠ 500.\nWe will reject the null hypothesis at a p-value less than 0.05\n\n\nCalculating t-statistic and p-value for Jones\n\n\nCode\nsample_mean <- 519.5\nmu <- 500\nstandard_error <- 10\nsample_size <- 1000\n\nt_score_jones <- (sample_mean-mu)/(standard_error)\nt_score_jones\n\n\n[1] 1.95\n\n\nCode\np <- 2*pt(t_score_jones, sample_size-1, lower.tail = FALSE)\np\n\n\n[1] 0.05145555\n\n\n\n\nCalculating t-statistic and p-value for Smith\n\n\nCode\nsample_mean <- 519.7\nmu <- 500\nstandard_error <- 10\nsample_size <- 1000\n\nt_score_smith <- (sample_mean-mu)/(standard_error)\nt_score_smith\n\n\n[1] 1.97\n\n\nCode\np <- 2*pt(t_score_smith, sample_size-1, lower.tail = FALSE)\np\n\n\n[1] 0.04911426\n\n\n\n\nConclusion\nJones has a test-statistic of 1.95 and a p-value of 0.05145555, while Smith has a test-statistic of 1.97 and a p-value of 0.05145555.\n\n\nQuestion 5-B\n\n\nConclusion\nBased on the given information, the result is statistically significant for Smith, but not for Jones.\nFor Jones, the p-value (0.05145555) is greater than the significance level (α = 0.05), which means that we fail to reject the null hypothesis. This indicates that the result is not statistically significant for Jones.\nFor Smith, the p-value (0.04911426) is less than the significance level (α = 0.05), which means that we reject the null hypothesis. This indicates that the result is statistically significant for Smith.\n\n\nQuestion 5-C\n\n\nConclusion\nThe misleading aspect of reporting the result of a test as “P ≤ 0.05” versus “P > 0.05” or as “reject H0” versus “do not reject H0” without reporting the actual P-value is that it can mask the degree of uncertainty in the results. In this example, if we only reported “P ≤ 0.05” for Smith’s study, we would conclude that the result is statistically significant. However, we would miss the fact that the P-value is very close to the significance level, indicating that the result may not be very strong or may be subject to random chance. Similarly, if we only reported “P > 0.05” for Jones’s study, we would conclude that the result is not statistically significant, but we would miss the fact that the P-value is very close to the significance level, indicating that the result may be borderline significant. Therefore, it is important to report the actual P-value to provide a more accurate interpretation of the results.\nIf we fail to report the P-value and simply state whether the P-value is less than/equal to or greater than the defined significance level of the test, one cannot determine the strength of the conclusion. In the Jones/Smith example, reporting the results only as P ≤ 0.05 versus P > 0.05 will lead to different conclusions about very similar results."
  },
  {
    "objectID": "posts/HW2_AkhileshKumarMeghwal.html#question-6",
    "href": "posts/HW2_AkhileshKumarMeghwal.html#question-6",
    "title": "Homework 2",
    "section": "Question 6",
    "text": "Question 6\nA school nurse wants to determine whether age is a factor in whether children choose a healthy snack after school. She conducts a survey of 300 middle school students, with the results below. Test at α = 0.05 the claim that the proportion who choose a healthy snack differs by grade level. What is the null hypothesis? Which test should we use? What is the conclusion?\n\nNull Hypothesis\nThe null hypothesis is that there is no association between grade level and the proportion of students who choose a healthy snack.\nThe alternative hypothesis is that there is an association between grade level and the proportion of students who choose a healthy snack.\n\n\nWhich test should we use? (Chi Square)?\nIn the case of the school nurse’s survey data on snack choices, a chi-square test of independence should be used because the data is categorical (healthy snack vs unhealthy snack) and the research question is about whether there is a relationship between snack choice and grade level. The chi-square test is used to determine whether there is a significant association between two categorical variables.\n\n\nChi-square Test\n\n\nCode\n# Create the contingency table\nsnack_table <- matrix(c(31, 43, 51, 69, 57, 49), nrow = 2, byrow = TRUE)\nrownames(snack_table) <- c(\"Healthy snack\", \"Unhealthy snack\")\ncolnames(snack_table) <- c(\"6th grade\", \"7th grade\", \"8th grade\")\nsnack_table\n\n\n                6th grade 7th grade 8th grade\nHealthy snack          31        43        51\nUnhealthy snack        69        57        49\n\n\nCode\n# Conduct the chi-square test of independence\nchisq.test(snack_table)\n\n\n\n    Pearson's Chi-squared test\n\ndata:  snack_table\nX-squared = 8.3383, df = 2, p-value = 0.01547\n\n\n\n\nConclusion\nSince the p-value is less than the significance level of 0.05,\nThe output of the chisq.test function indicates that the chi-square test statistic is 8.3383 with 2 degrees of freedom, and the corresponding p-value is 0.01547. So we can reject the null hypothesis that there is no association between snack choice and grade level, and conclude that there is evidence of a significant association between two categorical variables: snack choice and grade level at a significance level of 0.05.\nBased on the results of the chi-square test of independence, it is found that there is a statistically significant association between snack choice and grade level among the surveyed students. Specifically, the data indicates that the proportion of students who choose healthy snacks differs significantly across grade levels. The contingency table and chi-square test showed that more 6th-grade students (31) chose healthy snacks compared to 7th-grade students (43) and 8th-grade students (51). On the other hand, more 8th-grade students (49) chose unhealthy snacks compared to 6th-grade students (69) and 7th-grade students (57).\nTherefore, we can conclude that grade level is a factor in whether children choose a healthy snack after school. The proportion of students who choose a healthy snack appears to differ across the three grade levels. Overall, these results suggest that the school may need to target their health promotion efforts towards the specific grade levels where unhealthy snack choices are more prevalent. The results may also inform further research into the factors that influence snack choices among middle school students."
  },
  {
    "objectID": "posts/HW2_AkhileshKumarMeghwal.html#question-7",
    "href": "posts/HW2_AkhileshKumarMeghwal.html#question-7",
    "title": "Homework 2",
    "section": "Question 7",
    "text": "Question 7\nPer-pupil costs (in thousands of dollars) for cyber charter school tuition for school districts in three areas are shown. Test the claim that there is a difference in means for the three areas, using an appropriate test. What is the null hypothesis? Which test should we use? What is the conclusion?\n\n\nCode\n# Define the data\ndata <- data.frame(\n  Area = c(\"Area 1\", \"Area 2\", \"Area 3\"),\n  Value1 = c(6.2, 7.5, 5.8),\n  Value2 = c(9.3, 8.2, 6.4),\n  Value3 = c(6.8, 8.5, 5.6),\n  Value4 = c(6.1, 8.2, 7.1),\n  Value5 = c(6.7, 7.0, 3.0),\n  Value6 = c(7.5, 9.3, 3.5)\n)\n\ndata1=data\n# # Alternatively, you can set the row names separately\ncolnames(data1) <- NULL\n# # Display table without boundaries/borders\nknitr::kable(data1, row.names = FALSE, col.names = NULL,\n             border = NA, digits = 1)%>%\n  kable_styling(bootstrap_options = \"striped\", full_width = TRUE, position = 'center', font_size = 14, stripe_color='black')\n\n\n\n\n\n  \n    Area 1 \n    6.2 \n    9.3 \n    6.8 \n    6.1 \n    6.7 \n    7.5 \n  \n  \n    Area 2 \n    7.5 \n    8.2 \n    8.5 \n    8.2 \n    7.0 \n    9.3 \n  \n  \n    Area 3 \n    5.8 \n    6.4 \n    5.6 \n    7.1 \n    3.0 \n    3.5 \n  \n\n\n\n\n\n\nNull Hypothesis\nIn order to investigate if there is a difference in means for the per-pupil costs of cyber charter school tuition in three different areas, we can use Analysis of Variance (ANOVA) test. ANOVA is a statistical test that compares means of three or more groups.\nNull Hypothesis: The null hypothesis for the ANOVA test would state that there is no significant difference between the means of the per-pupil costs for cyber charter school tuition in the three areas. This can be expressed mathematically as H0: μ1 = μ2 = μ3, where μ1, μ2, and μ3 represent the means of the per-pupil costs for cyber charter school tuition in Area 1, Area 2, and Area 3, respectively.\nAlternative Hypothesis: On the other hand, the alternative hypothesis (H1) suggests that at least one mean is significantly different from the others.\n\n\nWhich test should we use? (Anova)\nTo test the claim that there is a difference in means for the three areas, we can use the analysis of variance (ANOVA) test.\nIn this case, since we are comparing means across three areas, we can use a one-way ANOVA. We will use the following R code to conduct the test:\nTo conduct the ANOVA test in R, we can use the aov function.\n\n\nAnova Test\n\n\nCode\n# Reshape the data to long format\ndata_long <- gather(data, key = \"Variable\", value = \"Value\", -Area)\n\n# Conduct ANOVA test\nresult <- aov(Value ~ Area, data = data_long)\n\n# Print the ANOVA table\nsummary(result)\n\n\n            Df Sum Sq Mean Sq F value  Pr(>F)   \nArea         2  25.66  12.832   8.176 0.00397 **\nResiduals   15  23.54   1.569                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\nConclusion\nThe ANOVA table shows that the F value is 8.176 and the p-value is 0.00397. Since the p-value is less than the significance level of 0.05, we reject the null hypothesis and conclude that there is evidence of a difference in means for the three areas.\nTherefore, we can infer that the per-pupil costs for cyber charter school tuition differ among the three areas. Specifically, we found that the mean per-pupil costs for cyber charter school tuition in Area 1 (mean = 7.1) and Area 2 (mean = 8.1) were higher than the mean per-pupil costs in Area 3 (mean = 5.23). The ANOVA test supports the conclusion that there is a significant difference between the means of the per-pupil costs for cyber charter school tuition in the three areas."
  },
  {
    "objectID": "posts/abigailbalint_finalpart1.html",
    "href": "posts/abigailbalint_finalpart1.html",
    "title": "Final Project Initial Research",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(dplyr)\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/abigailbalint_finalpart1.html#description-of-data",
    "href": "posts/abigailbalint_finalpart1.html#description-of-data",
    "title": "Final Project Initial Research",
    "section": "Description of data",
    "text": "Description of data\nThe dataset I am using comes from Kaggle https://www.kaggle.com/datasets/kaggle/kaggle-survey-2018 and is a survey titled “2018 Kaggle Machine Learning & Data Science Survey” conducted by Kaggle to capture the current state of machine learning and data science usage, mainly at the enterprise and academic level. The dataset contains survey responses from almost 24,000 respondents from varying backgrounds. The survey contains 50 questions, including 9 demographic questions and 41 questions around machine learning and data science. The questions range from platforms and products used, and tools and methodology, barriers to entry, and more. It also asks respondents about their employee experience working in these fields. I believe that the wide array of types of questions used make this dataset a good fit for research, as there are binary and categorical variables to explore but also some that ask for explicit numeric values like what percentage of their work falls to different tasks. Having several different types of questions provide opportunities for multiple types of models to be performed.\nThis survey was also run in 2017, 2019, and 2020 on Kaggle as part of an annual competition where users could submit code and analysis using this public data. However, I decided to use the 2018 dataset as my focus because certain questions that I think would be really interesting to analyze were omitted in later years/the survey was shortened overall. This survey was hosted by Kaggle, open to anyone in the industry, for one week in October 2018.\nReading in the dataset –\n\n\nCode\nfinal <- read_csv(\"_data/final_project_data.csv\")\n\n\nWarning: One or more parsing issues, see `problems()` for details\n\n\nRows: 23859 Columns: 395\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (343): Q1, Q2, Q3, Q4, Q5, Q6, Q7, Q8, Q9, Q10, Q11_Part_1, Q11_Part_2, ...\ndbl  (48): Time from Start to Finish (seconds), Q1_OTHER_TEXT, Q6_OTHER_TEXT...\nlgl   (4): Q28_Part_24, Q30_Part_15, Q38_Part_19, Q38_Part_20\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nCode\nfinal2 <- read_csv(\"_data/final_project_data2.csv\")\n\n\nWarning: One or more parsing issues, see `problems()` for details\n\n\nRows: 23859 Columns: 395\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (343): What is your gender? - Selected Choice, What is your age (# years...\ndbl  (48): Duration (in seconds), What is your gender? - Prefer to self-desc...\nlgl   (4): Which of the following machine learning products have you used at...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nCode\nhead(final2,10)\n\n\n# A tibble: 10 × 395\n   Duration (i…¹ What …² What …³ What …⁴ In wh…⁵ What …⁶ Which…⁷ Selec…⁸ Selec…⁹\n           <dbl> <chr>     <dbl> <chr>   <chr>   <chr>   <chr>   <chr>     <dbl>\n 1           710 Female       -1 45-49   United… Doctor… Other   Consul…      -1\n 2           434 Male         -1 30-34   Indone… Bachel… Engine… Other         0\n 3           718 Female       -1 30-34   United… Master… Comput… Data S…      -1\n 4           621 Male         -1 35-39   United… Master… Social… Not em…      -1\n 5           731 Male         -1 22-24   India   Master… Mathem… Data A…      -1\n 6          1142 Male         -1 25-29   Colomb… Bachel… Physic… Data S…      -1\n 7           959 Male         -1 35-39   Chile   Doctor… Inform… Other         1\n 8          1758 Male         -1 18-21   India   Master… Inform… Other         2\n 9           641 Male         -1 25-29   Turkey  Master… Engine… Not em…      -1\n10           751 Male         -1 30-34   Hungary Master… Engine… Softwa…      -1\n# … with 386 more variables:\n#   `In what industry is your current employer/contract (or your most recent employer if retired)? - Selected Choice` <chr>,\n#   `In what industry is your current employer/contract (or your most recent employer if retired)? - Other - Text` <dbl>,\n#   `How many years of experience do you have in your current role?` <chr>,\n#   `What is your current yearly compensation (approximate $USD)?` <chr>,\n#   `Does your current employer incorporate machine learning methods into their business?` <chr>,\n#   `Select any activities that make up an important part of your role at work: (Select all that apply) - Selected Choice - Analyze and understand data to influence product or business decisions` <chr>, …"
  },
  {
    "objectID": "posts/abigailbalint_finalpart1.html#research-question",
    "href": "posts/abigailbalint_finalpart1.html#research-question",
    "title": "Final Project Initial Research",
    "section": "Research Question",
    "text": "Research Question\nUpon doing a cursory search around this data, I see some high level executive-summary style research published about this data set, but I wasn’t able to find anything focused on more specific research questions. It was more demographic data of the state of ML and Data Science. I think there is the opportunity to speak more specifically about the state of machine learning and data science, and look deeper at what tools students and employees are using versus what their time is devoted to.\nTherefore, my main research question is “What is the state of machine learning and data science? What tools are being used in the context of individuals school and work, and how does an individual’s background (age, career, education, etc.) impact how they navigate this tech world? What barriers do users face and are those barriers the same for all users?”\nI plan to use questions like “During a typical data science project at work or school, approximately what proportion of your time is devoted to the following?” or “What percentage of your current machine learning/data science training falls under each category?” to get exact numbers that I can correlate against demos and more general usage of tools and platforms to see if there is any connection between the work one does and the tools they use.\nI am interested in this dataset because a lot of research in my career is in the machine learning space, so I am always interested in contextualizing the employee experience in these areas so that I can better understand the subject of some of my survey research. I also do more general employee engagement research in my career and I think this final is a great opportunity to try my hand at some of the correlations I would like to run at my job now but have never been able to because I don’t have any prior stats knowledge."
  },
  {
    "objectID": "posts/abigailbalint_finalpart1.html#hypothesis",
    "href": "posts/abigailbalint_finalpart1.html#hypothesis",
    "title": "Final Project Initial Research",
    "section": "Hypothesis",
    "text": "Hypothesis\nI would like to test a few different hypotheses that I have. Some of the initial ideas I have currently are:\n-Students are more likely to use free, long-standing coding and ML platforms as opposed to employees using more paid tools with user-friendly features.\n-For the question “How do you perceive the importance of the following topics? - Fairness and bias in ML algorithms, Being able to explain ML model outputs and/or predictions, Reproducibility in data science”, students will perceive this as more important than full-time workers, and younger generations will perceive this as more important than older generations.\n-For the question “During a typical data science project at work or school, approximately what proportion of your time is devoted to the following?” time spent on the analysis end of the process will be reported as a higher percentage of time the older or more experienced the data scientist is.\nThese are just a few ideas of the direction I am thinking, all of course cut by the demographics in this dataset like age, education, industry, years of experience, etc."
  },
  {
    "objectID": "posts/abigailbalint_finalpart1.html#descriptive-statistics",
    "href": "posts/abigailbalint_finalpart1.html#descriptive-statistics",
    "title": "Final Project Initial Research",
    "section": "Descriptive Statistics",
    "text": "Descriptive Statistics\nI described my dataset at the top of this as well as discussed variables of interest in the Research Question section, but here is a little bit of exploratory code:\nI can see the data contains mostly younger males, but because of the sample size can really work with lots of demographic combinations.\n\n\nCode\nggplot(final, aes(x = Q1)) +\n  geom_bar() +\n   labs(x=\"Gender\")\n\n\n\n\n\n\n\nCode\nggplot(final, aes(x = Q2)) +\n  geom_bar() +\n  labs(x=\"Age\") +\n  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust=1))\n\n\n\n\n\nThere is also a range of coding experience in the dataset.\n\n\nCode\nggplot(final, aes(x = Q24)) +\n  geom_bar() +\n   labs(x=\"Years of coding experience\") +\n  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust=1))\n\n\n\n\n\nThe data is split between students, tech industry employees, and other industry employees.\n\n\nCode\nggplot(final, aes(x = Q7)) +\n  geom_bar() +\n   labs(x=\"Industry\") +\n  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust=1))\n\n\n\n\n\nThis is just a quick example of the types of thinking I want to do for my final poster. Here, I made a scatterplot showing where the amount of training from work meets the amount of time spent finding insights instead of cleaning data, coding, etc. I expected this to be much higher for those who received most or all of their training from work, but that isn’t the case. I’m interested to see once I define a few more specific hypotheses if they end up being true or false.\n\n\nCode\nggplot(final, aes(x = Q35_Part_3, y=Q34_Part_6)) +\n  geom_point() +\n   labs(x=\"Percentage of machine learning training from work\", y=\"Percentage of project time spent finding insights\")\n\n\nWarning: Removed 8114 rows containing missing values (geom_point).\n\n\n\n\n\nHere is the glimpse function to show essentially the questionaiire in text form. I’m working with the variable codes in my coding and using a key as the codes are much shorter.\n\n\nCode\nglimpse(final2)\n\n\nRows: 23,859\nColumns: 395\n$ `Duration (in seconds)`                                                                                                                                                                                                                     <dbl> …\n$ `What is your gender? - Selected Choice`                                                                                                                                                                                                    <chr> …\n$ `What is your gender? - Prefer to self-describe - Text`                                                                                                                                                                                     <dbl> …\n$ `What is your age (# years)?`                                                                                                                                                                                                               <chr> …\n$ `In which country do you currently reside?`                                                                                                                                                                                                 <chr> …\n$ `What is the highest level of formal education that you have attained or plan to attain within the next 2 years?`                                                                                                                           <chr> …\n$ `Which best describes your undergraduate major? - Selected Choice`                                                                                                                                                                          <chr> …\n$ `Select the title most similar to your current role (or most recent title if retired): - Selected Choice`                                                                                                                                   <chr> …\n$ `Select the title most similar to your current role (or most recent title if retired): - Other - Text`                                                                                                                                      <dbl> …\n$ `In what industry is your current employer/contract (or your most recent employer if retired)? - Selected Choice`                                                                                                                           <chr> …\n$ `In what industry is your current employer/contract (or your most recent employer if retired)? - Other - Text`                                                                                                                              <dbl> …\n$ `How many years of experience do you have in your current role?`                                                                                                                                                                            <chr> …\n$ `What is your current yearly compensation (approximate $USD)?`                                                                                                                                                                              <chr> …\n$ `Does your current employer incorporate machine learning methods into their business?`                                                                                                                                                      <chr> …\n$ `Select any activities that make up an important part of your role at work: (Select all that apply) - Selected Choice - Analyze and understand data to influence product or business decisions`                                             <chr> …\n$ `Select any activities that make up an important part of your role at work: (Select all that apply) - Selected Choice - Build and/or run a machine learning service that operationally improves my product or workflows`                    <chr> …\n$ `Select any activities that make up an important part of your role at work: (Select all that apply) - Selected Choice - Build and/or run the data infrastructure that my business uses for storing, analyzing, and operationalizing data`   <chr> …\n$ `Select any activities that make up an important part of your role at work: (Select all that apply) - Selected Choice - Build prototypes to explore applying machine learning to new areas`                                                 <chr> …\n$ `Select any activities that make up an important part of your role at work: (Select all that apply) - Selected Choice - Do research that advances the state of the art of machine learning`                                                 <chr> …\n$ `Select any activities that make up an important part of your role at work: (Select all that apply) - Selected Choice - None of these activities are an important part of my role at work`                                                  <chr> …\n$ `Select any activities that make up an important part of your role at work: (Select all that apply) - Selected Choice - Other`                                                                                                              <chr> …\n$ `Select any activities that make up an important part of your role at work: (Select all that apply) - Other - Text`                                                                                                                         <dbl> …\n$ `What is the primary tool that you use at work or school to analyze data? (include text response) - Selected Choice`                                                                                                                        <chr> …\n$ `What is the primary tool that you use at work or school to analyze data? (include text response) - Basic statistical software (Microsoft Excel, Google Sheets, etc.) - Text`                                                               <dbl> …\n$ `What is the primary tool that you use at work or school to analyze data? (include text response) - Advanced statistical software (SPSS, SAS, etc.) - Text`                                                                                 <dbl> …\n$ `What is the primary tool that you use at work or school to analyze data? (include text response) - Business intelligence software (Salesforce, Tableau, Spotfire, etc.) - Text`                                                            <dbl> …\n$ `What is the primary tool that you use at work or school to analyze data? (include text response) - Local or hosted development environments (RStudio, JupyterLab, etc.) - Text`                                                            <dbl> …\n$ `What is the primary tool that you use at work or school to analyze data? (include text response) - Cloud-based data software & APIs (AWS, GCP, Azure, etc.) - Text`                                                                        <dbl> …\n$ `What is the primary tool that you use at work or school to analyze data? (include text response) - Other - Text`                                                                                                                           <dbl> …\n$ `Which of the following integrated development environments (IDE's) have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - Jupyter/IPython`                                                       <chr> …\n$ `Which of the following integrated development environments (IDE's) have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - RStudio`                                                               <chr> …\n$ `Which of the following integrated development environments (IDE's) have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - PyCharm`                                                               <chr> …\n$ `Which of the following integrated development environments (IDE's) have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - Visual Studio Code`                                                    <chr> …\n$ `Which of the following integrated development environments (IDE's) have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - nteract`                                                               <chr> …\n$ `Which of the following integrated development environments (IDE's) have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - Atom`                                                                  <chr> …\n$ `Which of the following integrated development environments (IDE's) have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - MATLAB`                                                                <chr> …\n$ `Which of the following integrated development environments (IDE's) have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - Visual Studio`                                                         <chr> …\n$ `Which of the following integrated development environments (IDE's) have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - Notepad++`                                                             <chr> …\n$ `Which of the following integrated development environments (IDE's) have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - Sublime Text`                                                          <chr> …\n$ `Which of the following integrated development environments (IDE's) have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - Vim`                                                                   <chr> …\n$ `Which of the following integrated development environments (IDE's) have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - IntelliJ`                                                              <chr> …\n$ `Which of the following integrated development environments (IDE's) have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - Spyder`                                                                <chr> …\n$ `Which of the following integrated development environments (IDE's) have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - None`                                                                  <chr> …\n$ `Which of the following integrated development environments (IDE's) have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - Other`                                                                 <chr> …\n$ `Which of the following integrated development environments (IDE's) have you used at work or school in the last 5 years? (Select all that apply) - Other - Text`                                                                            <dbl> …\n$ `Which of the following hosted notebooks have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - Kaggle Kernels`                                                                                   <chr> …\n$ `Which of the following hosted notebooks have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - Google Colab`                                                                                     <chr> …\n$ `Which of the following hosted notebooks have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - Azure Notebook`                                                                                   <chr> …\n$ `Which of the following hosted notebooks have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - Domino Datalab`                                                                                   <chr> …\n$ `Which of the following hosted notebooks have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - Google Cloud Datalab`                                                                             <chr> …\n$ `Which of the following hosted notebooks have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - Paperspace`                                                                                       <chr> …\n$ `Which of the following hosted notebooks have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - Floydhub`                                                                                         <chr> …\n$ `Which of the following hosted notebooks have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - Crestle`                                                                                          <chr> …\n$ `Which of the following hosted notebooks have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - JupyterHub/Binder`                                                                                <chr> …\n$ `Which of the following hosted notebooks have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - None`                                                                                             <chr> …\n$ `Which of the following hosted notebooks have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - Other`                                                                                            <chr> …\n$ `Which of the following hosted notebooks have you used at work or school in the last 5 years? (Select all that apply) - Other - Text`                                                                                                       <dbl> …\n$ `Which of the following cloud computing services have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - Google Cloud Platform (GCP)`                                                              <chr> …\n$ `Which of the following cloud computing services have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - Amazon Web Services (AWS)`                                                                <chr> …\n$ `Which of the following cloud computing services have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - Microsoft Azure`                                                                          <chr> …\n$ `Which of the following cloud computing services have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - IBM Cloud`                                                                                <chr> …\n$ `Which of the following cloud computing services have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - Alibaba Cloud`                                                                            <chr> …\n$ `Which of the following cloud computing services have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - I have not used any cloud providers`                                                      <chr> …\n$ `Which of the following cloud computing services have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - Other`                                                                                    <chr> …\n$ `Which of the following cloud computing services have you used at work or school in the last 5 years? (Select all that apply) - Other - Text`                                                                                               <dbl> …\n$ `What programming languages do you use on a regular basis? (Select all that apply) - Selected Choice - Python`                                                                                                                              <chr> …\n$ `What programming languages do you use on a regular basis? (Select all that apply) - Selected Choice - R`                                                                                                                                   <chr> …\n$ `What programming languages do you use on a regular basis? (Select all that apply) - Selected Choice - SQL`                                                                                                                                 <chr> …\n$ `What programming languages do you use on a regular basis? (Select all that apply) - Selected Choice - Bash`                                                                                                                                <chr> …\n$ `What programming languages do you use on a regular basis? (Select all that apply) - Selected Choice - Java`                                                                                                                                <chr> …\n$ `What programming languages do you use on a regular basis? (Select all that apply) - Selected Choice - Javascript/Typescript`                                                                                                               <chr> …\n$ `What programming languages do you use on a regular basis? (Select all that apply) - Selected Choice - Visual Basic/VBA`                                                                                                                    <chr> …\n$ `What programming languages do you use on a regular basis? (Select all that apply) - Selected Choice - C/C++`                                                                                                                               <chr> …\n$ `What programming languages do you use on a regular basis? (Select all that apply) - Selected Choice - MATLAB`                                                                                                                              <chr> …\n$ `What programming languages do you use on a regular basis? (Select all that apply) - Selected Choice - Scala`                                                                                                                               <chr> …\n$ `What programming languages do you use on a regular basis? (Select all that apply) - Selected Choice - Julia`                                                                                                                               <chr> …\n$ `What programming languages do you use on a regular basis? (Select all that apply) - Selected Choice - Go`                                                                                                                                  <chr> …\n$ `What programming languages do you use on a regular basis? (Select all that apply) - Selected Choice - C#/.NET`                                                                                                                             <chr> …\n$ `What programming languages do you use on a regular basis? (Select all that apply) - Selected Choice - PHP`                                                                                                                                 <chr> …\n$ `What programming languages do you use on a regular basis? (Select all that apply) - Selected Choice - Ruby`                                                                                                                                <chr> …\n$ `What programming languages do you use on a regular basis? (Select all that apply) - Selected Choice - SAS/STATA`                                                                                                                           <chr> …\n$ `What programming languages do you use on a regular basis? (Select all that apply) - Selected Choice - None`                                                                                                                                <chr> …\n$ `What programming languages do you use on a regular basis? (Select all that apply) - Selected Choice - Other`                                                                                                                               <chr> …\n$ `What programming languages do you use on a regular basis? (Select all that apply) - Other - Text`                                                                                                                                          <dbl> …\n$ `What specific programming language do you use most often? - Selected Choice`                                                                                                                                                               <chr> …\n$ `What specific programming language do you use most often? - Other - Text`                                                                                                                                                                  <dbl> …\n$ `What programming language would you recommend an aspiring data scientist to learn first? - Selected Choice`                                                                                                                                <chr> …\n$ `What programming language would you recommend an aspiring data scientist to learn first? - Other - Text`                                                                                                                                   <dbl> …\n$ `What machine learning frameworks have you used in the past 5 years? (Select all that apply) - Selected Choice - Scikit-Learn`                                                                                                              <chr> …\n$ `What machine learning frameworks have you used in the past 5 years? (Select all that apply) - Selected Choice - TensorFlow`                                                                                                                <chr> …\n$ `What machine learning frameworks have you used in the past 5 years? (Select all that apply) - Selected Choice - Keras`                                                                                                                     <chr> …\n$ `What machine learning frameworks have you used in the past 5 years? (Select all that apply) - Selected Choice - PyTorch`                                                                                                                   <chr> …\n$ `What machine learning frameworks have you used in the past 5 years? (Select all that apply) - Selected Choice - Spark MLlib`                                                                                                               <chr> …\n$ `What machine learning frameworks have you used in the past 5 years? (Select all that apply) - Selected Choice - H20`                                                                                                                       <chr> …\n$ `What machine learning frameworks have you used in the past 5 years? (Select all that apply) - Selected Choice - Fastai`                                                                                                                    <chr> …\n$ `What machine learning frameworks have you used in the past 5 years? (Select all that apply) - Selected Choice - Mxnet`                                                                                                                     <chr> …\n$ `What machine learning frameworks have you used in the past 5 years? (Select all that apply) - Selected Choice - Caret`                                                                                                                     <chr> …\n$ `What machine learning frameworks have you used in the past 5 years? (Select all that apply) - Selected Choice - Xgboost`                                                                                                                   <chr> …\n$ `What machine learning frameworks have you used in the past 5 years? (Select all that apply) - Selected Choice - mlr`                                                                                                                       <chr> …\n$ `What machine learning frameworks have you used in the past 5 years? (Select all that apply) - Selected Choice - Prophet`                                                                                                                   <chr> …\n$ `What machine learning frameworks have you used in the past 5 years? (Select all that apply) - Selected Choice - randomForest`                                                                                                              <chr> …\n$ `What machine learning frameworks have you used in the past 5 years? (Select all that apply) - Selected Choice - lightgbm`                                                                                                                  <chr> …\n$ `What machine learning frameworks have you used in the past 5 years? (Select all that apply) - Selected Choice - catboost`                                                                                                                  <chr> …\n$ `What machine learning frameworks have you used in the past 5 years? (Select all that apply) - Selected Choice - CNTK`                                                                                                                      <chr> …\n$ `What machine learning frameworks have you used in the past 5 years? (Select all that apply) - Selected Choice - Caffe`                                                                                                                     <chr> …\n$ `What machine learning frameworks have you used in the past 5 years? (Select all that apply) - Selected Choice - None`                                                                                                                      <chr> …\n$ `What machine learning frameworks have you used in the past 5 years? (Select all that apply) - Selected Choice - Other`                                                                                                                     <chr> …\n$ `What machine learning frameworks have you used in the past 5 years? (Select all that apply) - Other - Text`                                                                                                                                <dbl> …\n$ `Of the choices that you selected in the previous question, which ML library have you used the most? - Selected Choice`                                                                                                                     <chr> …\n$ `Of the choices that you selected in the previous question, which ML library have you used the most? - Other - Text`                                                                                                                        <dbl> …\n$ `What data visualization libraries or tools have you used in the past 5 years? (Select all that apply) - Selected Choice - ggplot2`                                                                                                         <chr> …\n$ `What data visualization libraries or tools have you used in the past 5 years? (Select all that apply) - Selected Choice - Matplotlib`                                                                                                      <chr> …\n$ `What data visualization libraries or tools have you used in the past 5 years? (Select all that apply) - Selected Choice - Altair`                                                                                                          <chr> …\n$ `What data visualization libraries or tools have you used in the past 5 years? (Select all that apply) - Selected Choice - Shiny`                                                                                                           <chr> …\n$ `What data visualization libraries or tools have you used in the past 5 years? (Select all that apply) - Selected Choice - D3`                                                                                                              <chr> …\n$ `What data visualization libraries or tools have you used in the past 5 years? (Select all that apply) - Selected Choice - Plotly`                                                                                                          <chr> …\n$ `What data visualization libraries or tools have you used in the past 5 years? (Select all that apply) - Selected Choice - Bokeh`                                                                                                           <chr> …\n$ `What data visualization libraries or tools have you used in the past 5 years? (Select all that apply) - Selected Choice - Seaborn`                                                                                                         <chr> …\n$ `What data visualization libraries or tools have you used in the past 5 years? (Select all that apply) - Selected Choice - Geoplotlib`                                                                                                      <chr> …\n$ `What data visualization libraries or tools have you used in the past 5 years? (Select all that apply) - Selected Choice - Leaflet`                                                                                                         <chr> …\n$ `What data visualization libraries or tools have you used in the past 5 years? (Select all that apply) - Selected Choice - Lattice`                                                                                                         <chr> …\n$ `What data visualization libraries or tools have you used in the past 5 years? (Select all that apply) - Selected Choice - None`                                                                                                            <chr> …\n$ `What data visualization libraries or tools have you used in the past 5 years? (Select all that apply) - Selected Choice - Other`                                                                                                           <chr> …\n$ `What data visualization libraries or tools have you used in the past 5 years? (Select all that apply) - Other - Text`                                                                                                                      <dbl> …\n$ `Of the choices that you selected in the previous question, which specific data visualization library or tool have you used the most? - Selected Choice`                                                                                    <chr> …\n$ `Of the choices that you selected in the previous question, which specific data visualization library or tool have you used the most? - Other - Text`                                                                                       <dbl> …\n$ `Approximately what percent of your time at work or school is spent actively coding?`                                                                                                                                                       <chr> …\n$ `How long have you been writing code to analyze data?`                                                                                                                                                                                      <chr> …\n$ `For how many years have you used machine learning methods (at work or in school)?`                                                                                                                                                         <chr> …\n$ `Do you consider yourself to be a data scientist?`                                                                                                                                                                                          <chr> …\n$ `Which of the following cloud computing products have you used at work or school in the last 5 years (Select all that apply)? - Selected Choice - AWS Elastic Compute Cloud (EC2)`                                                          <chr> …\n$ `Which of the following cloud computing products have you used at work or school in the last 5 years (Select all that apply)? - Selected Choice - Google Compute Engine`                                                                    <chr> …\n$ `Which of the following cloud computing products have you used at work or school in the last 5 years (Select all that apply)? - Selected Choice - AWS Elastic Beanstalk`                                                                    <chr> …\n$ `Which of the following cloud computing products have you used at work or school in the last 5 years (Select all that apply)? - Selected Choice - Google App Engine`                                                                        <chr> …\n$ `Which of the following cloud computing products have you used at work or school in the last 5 years (Select all that apply)? - Selected Choice - Google Kubernetes Engine`                                                                 <chr> …\n$ `Which of the following cloud computing products have you used at work or school in the last 5 years (Select all that apply)? - Selected Choice - AWS Lambda`                                                                               <chr> …\n$ `Which of the following cloud computing products have you used at work or school in the last 5 years (Select all that apply)? - Selected Choice - Google Cloud Functions`                                                                   <chr> …\n$ `Which of the following cloud computing products have you used at work or school in the last 5 years (Select all that apply)? - Selected Choice - AWS Batch`                                                                                <chr> …\n$ `Which of the following cloud computing products have you used at work or school in the last 5 years (Select all that apply)? - Selected Choice - Azure Virtual Machines`                                                                   <chr> …\n$ `Which of the following cloud computing products have you used at work or school in the last 5 years (Select all that apply)? - Selected Choice - Azure Container Service`                                                                  <chr> …\n$ `Which of the following cloud computing products have you used at work or school in the last 5 years (Select all that apply)? - Selected Choice - Azure Functions`                                                                          <chr> …\n$ `Which of the following cloud computing products have you used at work or school in the last 5 years (Select all that apply)? - Selected Choice - Azure Event Grid`                                                                         <chr> …\n$ `Which of the following cloud computing products have you used at work or school in the last 5 years (Select all that apply)? - Selected Choice - Azure Batch`                                                                              <chr> …\n$ `Which of the following cloud computing products have you used at work or school in the last 5 years (Select all that apply)? - Selected Choice - Azure Kubernetes Service`                                                                 <chr> …\n$ `Which of the following cloud computing products have you used at work or school in the last 5 years (Select all that apply)? - Selected Choice - IBM Cloud Virtual Servers`                                                                <chr> …\n$ `Which of the following cloud computing products have you used at work or school in the last 5 years (Select all that apply)? - Selected Choice - IBM Cloud Container Registry`                                                             <chr> …\n$ `Which of the following cloud computing products have you used at work or school in the last 5 years (Select all that apply)? - Selected Choice - IBM Cloud Kubernetes Service`                                                             <chr> …\n$ `Which of the following cloud computing products have you used at work or school in the last 5 years (Select all that apply)? - Selected Choice - IBM Cloud Foundry`                                                                        <chr> …\n$ `Which of the following cloud computing products have you used at work or school in the last 5 years (Select all that apply)? - Selected Choice - None`                                                                                     <chr> …\n$ `Which of the following cloud computing products have you used at work or school in the last 5 years (Select all that apply)? - Selected Choice - Other`                                                                                    <chr> …\n$ `Which of the following cloud computing products have you used at work or school in the last 5 years (Select all that apply)? - Other - Text`                                                                                               <dbl> …\n$ `Which of the following machine learning products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - Amazon Transcribe`                                                                       <chr> …\n$ `Which of the following machine learning products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - Google Cloud Speech-to-text API`                                                         <chr> …\n$ `Which of the following machine learning products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - Amazon Rekognition`                                                                      <chr> …\n$ `Which of the following machine learning products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - Google Cloud Vision API`                                                                 <chr> …\n$ `Which of the following machine learning products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - Amazon Comprehend`                                                                       <chr> …\n$ `Which of the following machine learning products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - Google Cloud Natural Language API`                                                       <chr> …\n$ `Which of the following machine learning products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - Amazon Translate`                                                                        <chr> …\n$ `Which of the following machine learning products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - Google Cloud Translation API`                                                            <chr> …\n$ `Which of the following machine learning products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - Amazon Lex`                                                                              <chr> …\n$ `Which of the following machine learning products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - Google Dialogflow Enterprise Edition`                                                    <chr> …\n$ `Which of the following machine learning products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - Amazon Rekognition Video`                                                                <chr> …\n$ `Which of the following machine learning products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - Google Cloud Video Intelligence API`                                                     <chr> …\n$ `Which of the following machine learning products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - Google Cloud AutoML`                                                                     <chr> …\n$ `Which of the following machine learning products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - Amazon SageMaker`                                                                        <chr> …\n$ `Which of the following machine learning products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - Google Cloud Machine Learning Engine`                                                    <chr> …\n$ `Which of the following machine learning products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - DataRobot`                                                                               <chr> …\n$ `Which of the following machine learning products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - H20 Driverless AI`                                                                       <chr> …\n$ `Which of the following machine learning products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - Domino Datalab`                                                                          <chr> …\n$ `Which of the following machine learning products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - SAS`                                                                                     <chr> …\n$ `Which of the following machine learning products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - Dataiku`                                                                                 <chr> …\n$ `Which of the following machine learning products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - RapidMiner`                                                                              <chr> …\n$ `Which of the following machine learning products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - Instabase`                                                                               <chr> …\n$ `Which of the following machine learning products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - Algorithmia`                                                                             <chr> …\n$ `Which of the following machine learning products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - Dataversity`                                                                             <lgl> …\n$ `Which of the following machine learning products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - Cloudera`                                                                                <chr> …\n$ `Which of the following machine learning products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - Azure Machine Learning Studio`                                                           <chr> …\n$ `Which of the following machine learning products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - Azure Machine Learning Workbench`                                                        <chr> …\n$ `Which of the following machine learning products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - Azure Cortana Intelligence Suite`                                                        <chr> …\n$ `Which of the following machine learning products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - Azure Bing Speech API`                                                                   <chr> …\n$ `Which of the following machine learning products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - Azure Speaker Recognition API`                                                           <chr> …\n$ `Which of the following machine learning products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - Azure Computer Vision API`                                                               <chr> …\n$ `Which of the following machine learning products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - Azure Face API`                                                                          <chr> …\n$ `Which of the following machine learning products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - Azure Video API`                                                                         <chr> …\n$ `Which of the following machine learning products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - IBM Watson Studio`                                                                       <chr> …\n$ `Which of the following machine learning products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - IBM Watson Knowledge Catalog`                                                            <chr> …\n$ `Which of the following machine learning products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - IBM Watson Assistant`                                                                    <chr> …\n$ `Which of the following machine learning products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - IBM Watson Discovery`                                                                    <chr> …\n$ `Which of the following machine learning products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - IBM Watson Text to Speech`                                                               <chr> …\n$ `Which of the following machine learning products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - IBM Watson Visual Recognition`                                                           <chr> …\n$ `Which of the following machine learning products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - IBM Watson Machine Learning`                                                             <chr> …\n$ `Which of the following machine learning products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - Azure Cognitive Services`                                                                <chr> …\n$ `Which of the following machine learning products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - None`                                                                                    <chr> …\n$ `Which of the following machine learning products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - Other`                                                                                   <chr> …\n$ `Which of the following machine learning products have you used at work or school in the last 5 years? (Select all that apply) - Other - Text`                                                                                              <dbl> …\n$ `Which of the following relational database products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - AWS Relational Database Service`                                                      <chr> …\n$ `Which of the following relational database products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - AWS Aurora`                                                                           <chr> …\n$ `Which of the following relational database products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - Google Cloud SQL`                                                                     <chr> …\n$ `Which of the following relational database products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - Google Cloud Spanner`                                                                 <chr> …\n$ `Which of the following relational database products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - AWS DynamoDB`                                                                         <chr> …\n$ `Which of the following relational database products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - Google Cloud Datastore`                                                               <chr> …\n$ `Which of the following relational database products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - Google Cloud Bigtable`                                                                <chr> …\n$ `Which of the following relational database products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - AWS SimpleDB`                                                                         <chr> …\n$ `Which of the following relational database products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - Microsoft SQL Server`                                                                 <chr> …\n$ `Which of the following relational database products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - MySQL`                                                                                <chr> …\n$ `Which of the following relational database products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - PostgresSQL`                                                                          <chr> …\n$ `Which of the following relational database products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - SQLite`                                                                               <chr> …\n$ `Which of the following relational database products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - Oracle Database`                                                                      <chr> …\n$ `Which of the following relational database products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - Ingres`                                                                               <chr> …\n$ `Which of the following relational database products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - Microsoft Access`                                                                     <chr> …\n$ `Which of the following relational database products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - NexusDB`                                                                              <chr> …\n$ `Which of the following relational database products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - SAP IQ`                                                                               <chr> …\n$ `Which of the following relational database products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - Google Fusion Tables`                                                                 <chr> …\n$ `Which of the following relational database products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - Azure Database for MySQL`                                                             <chr> …\n$ `Which of the following relational database products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - Azure Cosmos DB`                                                                      <chr> …\n$ `Which of the following relational database products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - Azure SQL Database`                                                                   <chr> …\n$ `Which of the following relational database products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - Azure Database for PostgreSQL`                                                        <chr> …\n$ `Which of the following relational database products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - IBM Cloud Compose`                                                                    <chr> …\n$ `Which of the following relational database products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - IBM Cloud Compose for MySQL`                                                          <chr> …\n$ `Which of the following relational database products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - IBM Cloud Compose for PostgreSQL`                                                     <chr> …\n$ `Which of the following relational database products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - IBM Cloud Db2`                                                                        <chr> …\n$ `Which of the following relational database products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - None`                                                                                 <chr> …\n$ `Which of the following relational database products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - Other`                                                                                <chr> …\n$ `Which of the following relational database products have you used at work or school in the last 5 years? (Select all that apply) - Other - Text`                                                                                           <dbl> …\n$ `Which of the following big data and analytics products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - AWS Elastic MapReduce`                                                             <chr> …\n$ `Which of the following big data and analytics products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - AWS Batch`                                                                         <chr> …\n$ `Which of the following big data and analytics products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - Google Cloud Dataproc`                                                             <chr> …\n$ `Which of the following big data and analytics products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - Google Cloud Dataflow`                                                             <chr> …\n$ `Which of the following big data and analytics products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - Google Cloud Dataprep`                                                             <chr> …\n$ `Which of the following big data and analytics products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - AWS Kinesis`                                                                       <chr> …\n$ `Which of the following big data and analytics products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - Google Cloud Pub/Sub`                                                              <chr> …\n$ `Which of the following big data and analytics products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - AWS Athena`                                                                        <chr> …\n$ `Which of the following big data and analytics products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - AWS Redshift`                                                                      <chr> …\n$ `Which of the following big data and analytics products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - Google BigQuery`                                                                   <chr> …\n$ `Which of the following big data and analytics products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - Teradata`                                                                          <chr> …\n$ `Which of the following big data and analytics products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - Microsoft Analysis Services`                                                       <chr> …\n$ `Which of the following big data and analytics products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - Oracle Exadata`                                                                    <chr> …\n$ `Which of the following big data and analytics products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - Oracle Warehouse Builder`                                                          <chr> …\n$ `Which of the following big data and analytics products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - SAP IQ`                                                                            <lgl> …\n$ `Which of the following big data and analytics products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - Snowflake`                                                                         <chr> …\n$ `Which of the following big data and analytics products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - Databricks`                                                                        <chr> …\n$ `Which of the following big data and analytics products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - Azure SQL Data Warehouse`                                                          <chr> …\n$ `Which of the following big data and analytics products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - Azure HDInsight`                                                                   <chr> …\n$ `Which of the following big data and analytics products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - Azure Stream Analytics`                                                            <chr> …\n$ `Which of the following big data and analytics products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - IBM InfoSphere DataStorage`                                                        <chr> …\n$ `Which of the following big data and analytics products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - IBM Cloud Analytics Engine`                                                        <chr> …\n$ `Which of the following big data and analytics products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - IBM Cloud Streaming Analytics`                                                     <chr> …\n$ `Which of the following big data and analytics products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - None`                                                                              <chr> …\n$ `Which of the following big data and analytics products have you used at work or school in the last 5 years? (Select all that apply) - Selected Choice - Other`                                                                             <chr> …\n$ `Which of the following big data and analytics products have you used at work or school in the last 5 years? (Select all that apply) - Other - Text`                                                                                        <dbl> …\n$ `Which types of data do you currently interact with most often at work or school? (Select all that apply) - Selected Choice - Audio Data`                                                                                                   <chr> …\n$ `Which types of data do you currently interact with most often at work or school? (Select all that apply) - Selected Choice - Categorical Data`                                                                                             <chr> …\n$ `Which types of data do you currently interact with most often at work or school? (Select all that apply) - Selected Choice - Genetic Data`                                                                                                 <chr> …\n$ `Which types of data do you currently interact with most often at work or school? (Select all that apply) - Selected Choice - Geospatial Data`                                                                                              <chr> …\n$ `Which types of data do you currently interact with most often at work or school? (Select all that apply) - Selected Choice - Image Data`                                                                                                   <chr> …\n$ `Which types of data do you currently interact with most often at work or school? (Select all that apply) - Selected Choice - Numerical Data`                                                                                               <chr> …\n$ `Which types of data do you currently interact with most often at work or school? (Select all that apply) - Selected Choice - Sensor Data`                                                                                                  <chr> …\n$ `Which types of data do you currently interact with most often at work or school? (Select all that apply) - Selected Choice - Tabular Data`                                                                                                 <chr> …\n$ `Which types of data do you currently interact with most often at work or school? (Select all that apply) - Selected Choice - Text Data`                                                                                                    <chr> …\n$ `Which types of data do you currently interact with most often at work or school? (Select all that apply) - Selected Choice - Time Series Data`                                                                                             <chr> …\n$ `Which types of data do you currently interact with most often at work or school? (Select all that apply) - Selected Choice - Video Data`                                                                                                   <chr> …\n$ `Which types of data do you currently interact with most often at work or school? (Select all that apply) - Selected Choice - Other Data`                                                                                                   <chr> …\n$ `Which types of data do you currently interact with most often at work or school? (Select all that apply) - Other Data - Text`                                                                                                              <dbl> …\n$ `What is the type of data that you currently interact with most often at work or school? - Selected Choice`                                                                                                                                 <chr> …\n$ `What is the type of data that you currently interact with most often at work or school? - Other Data - Text`                                                                                                                               <dbl> …\n$ `Where do you find public datasets? (Select all that apply) - Selected Choice - Government websites`                                                                                                                                        <chr> …\n$ `Where do you find public datasets? (Select all that apply) - Selected Choice - University research group websites`                                                                                                                         <chr> …\n$ `Where do you find public datasets? (Select all that apply) - Selected Choice - Non-profit research group websites`                                                                                                                         <chr> …\n$ `Where do you find public datasets? (Select all that apply) - Selected Choice - Dataset aggregator/platform (Socrata, Kaggle Public Datasets Platform, etc.)`                                                                               <chr> …\n$ `Where do you find public datasets? (Select all that apply) - Selected Choice - I collect my own data (web-scraping, etc.)`                                                                                                                 <chr> …\n$ `Where do you find public datasets? (Select all that apply) - Selected Choice - Publicly released data from private companies`                                                                                                              <chr> …\n$ `Where do you find public datasets? (Select all that apply) - Selected Choice - Google Search`                                                                                                                                              <chr> …\n$ `Where do you find public datasets? (Select all that apply) - Selected Choice - Google Dataset Search`                                                                                                                                      <chr> …\n$ `Where do you find public datasets? (Select all that apply) - Selected Choice - GitHub`                                                                                                                                                     <chr> …\n$ `Where do you find public datasets? (Select all that apply) - Selected Choice - None/I do not work with public data`                                                                                                                        <chr> …\n$ `Where do you find public datasets? (Select all that apply) - Selected Choice - Other`                                                                                                                                                      <chr> …\n$ `Where do you find public datasets? (Select all that apply) - Other - Text`                                                                                                                                                                 <dbl> …\n$ `During a typical data science project at work or school, approximately what proportion of your time is devoted to the following? (Answers must add up to 100%) - Gathering data`                                                           <dbl> …\n$ `During a typical data science project at work or school, approximately what proportion of your time is devoted to the following? (Answers must add up to 100%) - Cleaning data`                                                            <dbl> …\n$ `During a typical data science project at work or school, approximately what proportion of your time is devoted to the following? (Answers must add up to 100%) - Visualizing data`                                                         <dbl> …\n$ `During a typical data science project at work or school, approximately what proportion of your time is devoted to the following? (Answers must add up to 100%) - Model building/model selection`                                           <dbl> …\n$ `During a typical data science project at work or school, approximately what proportion of your time is devoted to the following? (Answers must add up to 100%) - Putting the model into production`                                        <dbl> …\n$ `During a typical data science project at work or school, approximately what proportion of your time is devoted to the following? (Answers must add up to 100%) - Finding insights in the data and communicating with stakeholders`         <dbl> …\n$ `During a typical data science project at work or school, approximately what proportion of your time is devoted to the following? (Answers must add up to 100%) - Other`                                                                    <dbl> …\n$ `What percentage of your current machine learning/data science training falls under each category? (Answers must add up to 100%) - Self-taught`                                                                                             <dbl> …\n$ `What percentage of your current machine learning/data science training falls under each category? (Answers must add up to 100%) - Online courses (Coursera, Udemy, edX, etc.)`                                                             <dbl> …\n$ `What percentage of your current machine learning/data science training falls under each category? (Answers must add up to 100%) - Work`                                                                                                    <dbl> …\n$ `What percentage of your current machine learning/data science training falls under each category? (Answers must add up to 100%) - University`                                                                                              <dbl> …\n$ `What percentage of your current machine learning/data science training falls under each category? (Answers must add up to 100%) - Kaggle competitions`                                                                                     <dbl> …\n$ `What percentage of your current machine learning/data science training falls under each category? (Answers must add up to 100%) - Other`                                                                                                   <dbl> …\n$ `What percentage of your current machine learning/data science training falls under each category? (Answers must add up to 100%) - Other - Text`                                                                                            <dbl> …\n$ `On which online platforms have you begun or completed data science courses? (Select all that apply) - Selected Choice - Udacity`                                                                                                           <chr> …\n$ `On which online platforms have you begun or completed data science courses? (Select all that apply) - Selected Choice - Coursera`                                                                                                          <chr> …\n$ `On which online platforms have you begun or completed data science courses? (Select all that apply) - Selected Choice - edX`                                                                                                               <chr> …\n$ `On which online platforms have you begun or completed data science courses? (Select all that apply) - Selected Choice - DataCamp`                                                                                                          <chr> …\n$ `On which online platforms have you begun or completed data science courses? (Select all that apply) - Selected Choice - DataQuest`                                                                                                         <chr> …\n$ `On which online platforms have you begun or completed data science courses? (Select all that apply) - Selected Choice - Kaggle Learn`                                                                                                      <chr> …\n$ `On which online platforms have you begun or completed data science courses? (Select all that apply) - Selected Choice - Fast.AI`                                                                                                           <chr> …\n$ `On which online platforms have you begun or completed data science courses? (Select all that apply) - Selected Choice - developers.google.com`                                                                                             <chr> …\n$ `On which online platforms have you begun or completed data science courses? (Select all that apply) - Selected Choice - Udemy`                                                                                                             <chr> …\n$ `On which online platforms have you begun or completed data science courses? (Select all that apply) - Selected Choice - TheSchool.AI`                                                                                                      <chr> …\n$ `On which online platforms have you begun or completed data science courses? (Select all that apply) - Selected Choice - Online University Courses`                                                                                         <chr> …\n$ `On which online platforms have you begun or completed data science courses? (Select all that apply) - Selected Choice - None`                                                                                                              <chr> …\n$ `On which online platforms have you begun or completed data science courses? (Select all that apply) - Selected Choice - Other`                                                                                                             <chr> …\n$ `On which online platforms have you begun or completed data science courses? (Select all that apply) - Other - Text`                                                                                                                        <dbl> …\n$ `On which online platform have you spent the most amount of time? - Selected Choice`                                                                                                                                                        <chr> …\n$ `On which online platform have you spent the most amount of time? - Other - Text`                                                                                                                                                           <dbl> …\n$ `Who/what are your favorite media sources that report on data science topics? (Select all that apply) - Selected Choice - Twitter`                                                                                                          <chr> …\n$ `Who/what are your favorite media sources that report on data science topics? (Select all that apply) - Selected Choice - Hacker News`                                                                                                      <chr> …\n$ `Who/what are your favorite media sources that report on data science topics? (Select all that apply) - Selected Choice - r/machinelearning`                                                                                                <chr> …\n$ `Who/what are your favorite media sources that report on data science topics? (Select all that apply) - Selected Choice - Kaggle forums`                                                                                                    <chr> …\n$ `Who/what are your favorite media sources that report on data science topics? (Select all that apply) - Selected Choice - Fastai forums`                                                                                                    <chr> …\n$ `Who/what are your favorite media sources that report on data science topics? (Select all that apply) - Selected Choice - Siraj Raval YouTube Channel`                                                                                      <chr> …\n$ `Who/what are your favorite media sources that report on data science topics? (Select all that apply) - Selected Choice - DataTau News Aggregator`                                                                                          <chr> …\n$ `Who/what are your favorite media sources that report on data science topics? (Select all that apply) - Selected Choice - Linear Digressions Podcast`                                                                                       <chr> …\n$ `Who/what are your favorite media sources that report on data science topics? (Select all that apply) - Selected Choice - Cloud AI Adventures (YouTube)`                                                                                    <chr> …\n$ `Who/what are your favorite media sources that report on data science topics? (Select all that apply) - Selected Choice - FiveThirtyEight.com`                                                                                              <chr> …\n$ `Who/what are your favorite media sources that report on data science topics? (Select all that apply) - Selected Choice - ArXiv & Preprints`                                                                                                <chr> …\n$ `Who/what are your favorite media sources that report on data science topics? (Select all that apply) - Selected Choice - Journal Publications`                                                                                             <chr> …\n$ `Who/what are your favorite media sources that report on data science topics? (Select all that apply) - Selected Choice - FastML Blog`                                                                                                      <chr> …\n$ `Who/what are your favorite media sources that report on data science topics? (Select all that apply) - Selected Choice - KDnuggets Blog`                                                                                                   <chr> …\n$ `Who/what are your favorite media sources that report on data science topics? (Select all that apply) - Selected Choice - O'Reilly Data Newsletter`                                                                                         <chr> …\n$ `Who/what are your favorite media sources that report on data science topics? (Select all that apply) - Selected Choice - Partially Derivative Podcast`                                                                                     <chr> …\n$ `Who/what are your favorite media sources that report on data science topics? (Select all that apply) - Selected Choice - The Data Skeptic Podcast`                                                                                         <chr> …\n$ `Who/what are your favorite media sources that report on data science topics? (Select all that apply) - Selected Choice - Medium Blog Posts`                                                                                                <chr> …\n$ `Who/what are your favorite media sources that report on data science topics? (Select all that apply) - Selected Choice - Towards Data Science Blog`                                                                                        <lgl> …\n$ `Who/what are your favorite media sources that report on data science topics? (Select all that apply) - Selected Choice - Analytics Vidhya Blog`                                                                                            <lgl> …\n$ `Who/what are your favorite media sources that report on data science topics? (Select all that apply) - Selected Choice - None/I do not know`                                                                                               <chr> …\n$ `Who/what are your favorite media sources that report on data science topics? (Select all that apply) - Selected Choice - Other`                                                                                                            <chr> …\n$ `Who/what are your favorite media sources that report on data science topics? (Select all that apply) - Other - Text`                                                                                                                       <dbl> …\n$ `How do you perceive the quality of online learning platforms and in-person bootcamps as compared to the quality of the education provided by traditional brick and mortar institutions? - Online learning platforms and MOOCs:`            <chr> …\n$ `How do you perceive the quality of online learning platforms and in-person bootcamps as compared to the quality of the education provided by traditional brick and mortar institutions? - In-person bootcamps:`                            <chr> …\n$ `Which better demonstrates expertise in data science: academic achievements or independent projects? - Your views:`                                                                                                                         <chr> …\n$ `How do you perceive the importance of the following topics? - Fairness and bias in ML algorithms:`                                                                                                                                         <chr> …\n$ `How do you perceive the importance of the following topics? - Being able to explain ML model outputs and/or predictions`                                                                                                                   <chr> …\n$ `How do you perceive the importance of the following topics? - Reproducibility in data science`                                                                                                                                             <chr> …\n$ `What metrics do you or your organization use to determine whether or not your models were successful? (Select all that apply) - Selected Choice - Revenue and/or business goals`                                                           <chr> …\n$ `What metrics do you or your organization use to determine whether or not your models were successful? (Select all that apply) - Selected Choice - Metrics that consider accuracy`                                                          <chr> …\n$ `What metrics do you or your organization use to determine whether or not your models were successful? (Select all that apply) - Selected Choice - Metrics that consider unfair bias`                                                       <chr> …\n$ `What metrics do you or your organization use to determine whether or not your models were successful? (Select all that apply) - Selected Choice - Not applicable (I am not involved with an organization that builds ML models)`           <chr> …\n$ `What metrics do you or your organization use to determine whether or not your models were successful? (Select all that apply) - Selected Choice - Other`                                                                                   <chr> …\n$ `What metrics do you or your organization use to determine whether or not your models were successful? (Select all that apply) - Other - Text`                                                                                              <dbl> …\n$ `Approximately what percent of your data projects involved exploring unfair bias in the dataset and/or algorithm?`                                                                                                                          <chr> …\n$ `What do you find most difficult about ensuring that your algorithms are fair and unbiased? (Select all that apply) - Lack of communication between individuals who collect the data and individuals who analyze the data`                  <chr> …\n$ `What do you find most difficult about ensuring that your algorithms are fair and unbiased? (Select all that apply) - Difficulty in identifying groups that are unfairly targeted`                                                          <chr> …\n$ `What do you find most difficult about ensuring that your algorithms are fair and unbiased? (Select all that apply) - Difficulty in collecting enough data about groups that may be unfairly targeted`                                      <chr> …\n$ `What do you find most difficult about ensuring that your algorithms are fair and unbiased? (Select all that apply) - Difficulty in identifying and selecting the appropriate evaluation metrics`                                           <chr> …\n$ `What do you find most difficult about ensuring that your algorithms are fair and unbiased? (Select all that apply) - I have never found any difficulty in this task`                                                                       <chr> …\n$ `What do you find most difficult about ensuring that your algorithms are fair and unbiased? (Select all that apply) - I have never performed this task`                                                                                     <chr> …\n$ `In what circumstances would you explore model insights and interpret your model's predictions? (Select all that apply) - Only for very important models that are already in production`                                                    <chr> …\n$ `In what circumstances would you explore model insights and interpret your model's predictions? (Select all that apply) - For all models right before putting the model in production`                                                      <chr> …\n$ `In what circumstances would you explore model insights and interpret your model's predictions? (Select all that apply) - When determining whether it is worth it to put the model into production`                                         <chr> …\n$ `In what circumstances would you explore model insights and interpret your model's predictions? (Select all that apply) - When building a model that was specifically designed to produce such insights`                                    <chr> …\n$ `In what circumstances would you explore model insights and interpret your model's predictions? (Select all that apply) - When first exploring a new ML model or dataset`                                                                   <chr> …\n$ `In what circumstances would you explore model insights and interpret your model's predictions? (Select all that apply) - I do not explore and interpret model insights and predictions`                                                    <chr> …\n$ `Approximately what percent of your data projects involve exploring model insights?`                                                                                                                                                        <chr> …\n$ `What methods do you prefer for explaining and/or interpreting decisions that are made by ML models? (Select all that apply) - Selected Choice - Examine individual model coefficients`                                                     <chr> …\n$ `What methods do you prefer for explaining and/or interpreting decisions that are made by ML models? (Select all that apply) - Selected Choice - Examine feature correlations`                                                              <chr> …\n$ `What methods do you prefer for explaining and/or interpreting decisions that are made by ML models? (Select all that apply) - Selected Choice - Examine feature importances`                                                               <chr> …\n$ `What methods do you prefer for explaining and/or interpreting decisions that are made by ML models? (Select all that apply) - Selected Choice - Plot decision boundaries`                                                                  <chr> …\n$ `What methods do you prefer for explaining and/or interpreting decisions that are made by ML models? (Select all that apply) - Selected Choice - Create partial dependence plots`                                                           <chr> …\n$ `What methods do you prefer for explaining and/or interpreting decisions that are made by ML models? (Select all that apply) - Selected Choice - Dimensionality reduction techniques`                                                       <chr> …\n$ `What methods do you prefer for explaining and/or interpreting decisions that are made by ML models? (Select all that apply) - Selected Choice - Attention mapping/saliency mapping`                                                        <chr> …\n$ `What methods do you prefer for explaining and/or interpreting decisions that are made by ML models? (Select all that apply) - Selected Choice - Plot predicted vs. actual results`                                                         <chr> …\n$ `What methods do you prefer for explaining and/or interpreting decisions that are made by ML models? (Select all that apply) - Selected Choice - Print out a decision tree`                                                                 <chr> …\n$ `What methods do you prefer for explaining and/or interpreting decisions that are made by ML models? (Select all that apply) - Selected Choice - Sensitivity analysis/perturbation importance`                                              <chr> …\n$ `What methods do you prefer for explaining and/or interpreting decisions that are made by ML models? (Select all that apply) - Selected Choice - LIME functions`                                                                            <chr> …\n$ `What methods do you prefer for explaining and/or interpreting decisions that are made by ML models? (Select all that apply) - Selected Choice - ELI5 functions`                                                                            <chr> …\n$ `What methods do you prefer for explaining and/or interpreting decisions that are made by ML models? (Select all that apply) - Selected Choice - SHAP functions`                                                                            <chr> …\n$ `What methods do you prefer for explaining and/or interpreting decisions that are made by ML models? (Select all that apply) - Selected Choice - None/I do not use these model explanation techniques`                                      <chr> …\n$ `What methods do you prefer for explaining and/or interpreting decisions that are made by ML models? (Select all that apply) - Selected Choice - Other`                                                                                     <chr> …\n$ `What methods do you prefer for explaining and/or interpreting decisions that are made by ML models? (Select all that apply) - Other - Text`                                                                                                <chr> …\n$ `Do you consider ML models to be \"black boxes\" with outputs that are difficult or impossible to explain?`                                                                                                                                   <chr> …\n$ `What tools and methods do you use to make your work easy to reproduce? (Select all that apply) - Selected Choice - Share code on Github or a similar code-sharing repository`                                                              <chr> …\n$ `What tools and methods do you use to make your work easy to reproduce? (Select all that apply) - Selected Choice - Share both data and code on Github or a similar code-sharing repository`                                                <chr> …\n$ `What tools and methods do you use to make your work easy to reproduce? (Select all that apply) - Selected Choice - Share data, code, and environment using a hosted service (Kaggle Kernels, Google Colaboratory, Amazon SageMaker, etc.)` <chr> …\n$ `What tools and methods do you use to make your work easy to reproduce? (Select all that apply) - Selected Choice - Share data, code, and environment using containers (Docker, etc.)`                                                      <chr> …\n$ `What tools and methods do you use to make your work easy to reproduce? (Select all that apply) - Selected Choice - Share code, data, and environment using virtual machines (VirtualBox, etc.)`                                            <chr> …\n$ `What tools and methods do you use to make your work easy to reproduce? (Select all that apply) - Selected Choice - Make sure the code is well documented`                                                                                  <chr> …\n$ `What tools and methods do you use to make your work easy to reproduce? (Select all that apply) - Selected Choice - Make sure the code is human-readable`                                                                                   <chr> …\n$ `What tools and methods do you use to make your work easy to reproduce? (Select all that apply) - Selected Choice - Define all random seeds`                                                                                                <chr> …\n$ `What tools and methods do you use to make your work easy to reproduce? (Select all that apply) - Selected Choice - Define relative rather than absolute file paths`                                                                        <chr> …\n$ `What tools and methods do you use to make your work easy to reproduce? (Select all that apply) - Selected Choice - Include a text file describing all dependencies`                                                                        <chr> …\n$ `What tools and methods do you use to make your work easy to reproduce? (Select all that apply) - Selected Choice - None/I do not make my work easy for others to reproduce`                                                                <chr> …\n$ `What tools and methods do you use to make your work easy to reproduce? (Select all that apply) - Selected Choice - Other`                                                                                                                  <chr> …\n$ `What tools and methods do you use to make your work easy to reproduce? (Select all that apply) - Other - Text`                                                                                                                             <dbl> …\n$ `What barriers prevent you from making your work even easier to reuse and reproduce? (Select all that apply) - Selected Choice - Too expensive`                                                                                             <chr> …\n$ `What barriers prevent you from making your work even easier to reuse and reproduce? (Select all that apply) - Selected Choice - Too time-consuming`                                                                                        <chr> …\n$ `What barriers prevent you from making your work even easier to reuse and reproduce? (Select all that apply) - Selected Choice - Requires too much technical knowledge`                                                                     <chr> …\n$ `What barriers prevent you from making your work even easier to reuse and reproduce? (Select all that apply) - Selected Choice - Afraid that others will use my work without giving proper credit`                                          <chr> …\n$ `What barriers prevent you from making your work even easier to reuse and reproduce? (Select all that apply) - Selected Choice - Not enough incentives to share my work`                                                                    <chr> …\n$ `What barriers prevent you from making your work even easier to reuse and reproduce? (Select all that apply) - Selected Choice - I had never considered making my work easier for others to reproduce`                                      <chr> …\n$ `What barriers prevent you from making your work even easier to reuse and reproduce? (Select all that apply) - Selected Choice - None of these reasons apply to me`                                                                         <chr> …\n$ `What barriers prevent you from making your work even easier to reuse and reproduce? (Select all that apply) - Selected Choice - Other`                                                                                                     <chr> …\n$ `What barriers prevent you from making your work even easier to reuse and reproduce? (Select all that apply) - Other - Text`                                                                                                                <dbl> …"
  },
  {
    "objectID": "posts/abigailbalint_finalpart1.html#bibliography",
    "href": "posts/abigailbalint_finalpart1.html#bibliography",
    "title": "Final Project Initial Research",
    "section": "Bibliography",
    "text": "Bibliography\nKaggle, (2018). “2018 Kaggle Machine Learning & Data Science Survey”, Retrieved 21 March 2023 from https://www.kaggle.com/datasets/kaggle/kaggle-survey-2018.\nCC BY-SA 4.0 : https://creativecommons.org/licenses/by-sa/4.0/ :::"
  },
  {
    "objectID": "posts/HW2_RahulSomu.html",
    "href": "posts/HW2_RahulSomu.html",
    "title": "Homework2",
    "section": "",
    "text": "Question 1\nBelow code chunk calculated the confidence interval for bypass surgery as [18.80009, 19.19991], and the confidence interval for angiography as [17.76757, 18.23243]. Based on the results, we can conclude confidence interval for bypass surgery is slightly narrower than that of angiography, which implies that the estimate of the mean wait time for bypass surgery is slightly more precise than that of angiography.\n\n\nCode\n# Create dataframe\ndf <- data.frame(\n  procedure = c(\"Bypass\", \"Angiography\"),\n  sample_size = c(539, 847),\n  sample_mean = c(19, 18),\n  sample_sd = c(10, 9)\n)\n\n# confidence level\nconf_level <- 0.9\n\n#degrees of freedom for each procedure\ndf$df <- df$sample_size - 1\n\n#critical value for the confidence interval\nt_critical <- qt(1 - (1 - conf_level) / 2, df$df)\n\n#standard error of the mean for each procedure\ndf$sem <- df$sample_sd / sqrt(df$sample_size)\n\n# confidence intervals for each procedure\ndf$ci <- apply(df[, c(\"sample_mean\", \"sem\", \"df\")], 1, function(x) {\n  x[1] + c(-1, 1) * t_critical * x[2] * sqrt(x[3] + 1) / sqrt(x[3])\n})\n\n# Print the confidence intervals\ncat(\"Confidence intervals:\\n\")\n\n\nConfidence intervals:\n\n\nCode\nprint(df$ci)\n\n\n         [,1]     [,2]\n[1,] 18.28963 17.49016\n[2,] 19.70992 18.50952\n\n\n#Question 2\nBelow results suggest that we are 95% confident that the true proportion of adult Americans who believe that the college education is essential for success lies between 0.5189 and 0.5808.\n\n\nCode\nn <- 1031\np_hat <- 567/1031\nz <- qnorm(1-0.05/2)\n\nCI <- p_hat + z*sqrt(p_hat*(1-p_hat)/n) * c(-1, 1)\nCI\n\n\n[1] 0.5195839 0.5803191\n\n\n#Question 3\n\n\nCode\nn = ((1.959964)^2 * (42.5)^2) / (5)^2\nn\n\n\n[1] 277.5454\n\n\n#Question 4\n\n\nCode\n# Part A\nn <- 9\nybar <- 410\ns <- 90\nmu0 <- 500\nalpha <- 0.05\nse <- s / sqrt(n)\nt <- (ybar - mu0) / se\np_value <- 2 * pt(-abs(t), df = n - 1)\n\n# Report results\ncat(\"Test statistic:\", round(t, 2), \"\\n\")\n\n\nTest statistic: -3 \n\n\nCode\ncat(\"P-value:\", p_value, \"\\n\")\n\n\nP-value: 0.01707168 \n\n\nCode\nif(p_value < alpha) {\n  cat(\"Reject null hypothesis; the mean income of female employees differs from $500 per week.\\n\")\n} else {\n  cat(\"Fail to reject null hypothesis. \\n\")\n}\n\n\nReject null hypothesis; the mean income of female employees differs from $500 per week.\n\n\nCode\n# Part B\np_value_lt <- pt(t, df = n - 1)\ncat(\"P-value (Ha: μ < 500):\", p_value_lt, \"\\n\")\n\n\nP-value (Ha: μ < 500): 0.008535841 \n\n\nCode\nif(p_value_lt < alpha) {\n  cat(\"the mean income of female employees is less than $500 per week.\\n\")\n} else {\n  cat(\"Fail to reject null hypothesis. \\n\")\n}\n\n\nthe mean income of female employees is less than $500 per week.\n\n\nCode\n# Part C\np_value_gt <- pt(-t, df = n - 1)\ncat(\"P-value (Ha: μ > 500):\", p_value_gt, \"\\n\")\n\n\nP-value (Ha: μ > 500): 0.9914642 \n\n\nCode\nif(p_value_gt < alpha) {\n  cat(\"the mean income of female employees is greater than $500 per week.\\n\")\n} else {\n  cat(\"Fail to reject null hypothesis\\n\")\n}\n\n\nFail to reject null hypothesis\n\n\n#QUESTION 5\n\n\nCode\n# Jones' study\njones <- data.frame(y_bar = 519.5, se = 10.0, n = 1000)\njones$t <- (jones$y_bar - 500) / jones$se\njones$p_value <- 2 * pt(-abs(jones$t), df = jones$n - 1)\njones$t\n\n\n[1] 1.95\n\n\nCode\njones$p_value\n\n\n[1] 0.05145555\n\n\nCode\n# Smith's study\nsmith <- data.frame(y_bar = 519.7, se = 10.0, n = 1000)\nsmith$t <- (smith$y_bar - 500) / smith$se\nsmith$p_value <- 2 * pt(-abs(smith$t), df = smith$n - 1)\nsmith$t\n\n\n[1] 1.97\n\n\nCode\nsmith$p_value \n\n\n[1] 0.04911426\n\n\nCode\n# Significance testing\nalpha <- 0.05\nif (jones$p_value < alpha) {\n  cat(\"Jones' study is statistically significant\\n\")\n} else {\n  cat(\"Jones' study is not statistically significant\\n\")\n}\n\n\nJones' study is not statistically significant\n\n\nCode\nif (smith$p_value < alpha) {\n  cat(\"Smith's study is statistically significant\\n\")\n} else {\n  cat(\"Smith's study is not statistically significant\\n\")\n}\n\n\nSmith's study is statistically significant\n\n\n#Question 6\n\n\nCode\n# Create contingency table\nsnack_table <- matrix(c(31, 43, 51, 69, 57, 49), nrow = 3, byrow = TRUE)\n\n# Perform chi-squared test\nchisq.test(snack_table)\n\n\n\n    Pearson's Chi-squared test\n\ndata:  snack_table\nX-squared = 3.656, df = 2, p-value = 0.1607\n\n\n#Question 7\n\n\nCode\n# Create a data frame\narea <- c(rep(\"Area 1\", 6), rep(\"Area 2\", 6), rep(\"Area 3\", 6))\ncost <- c(6.2, 9.3, 6.8, 6.1, 6.7, 7.5, 7.5, 8.2, 8.5, 8.2, 7.0, 9.3, 5.8, 6.4, 5.6, 7.1, 3.0, 3.5)\ncost_data <- data.frame(area, cost)\n\n# Perform one-way ANOVA\nmodel <- aov(cost ~ area, data = cost_data)\n\n# Print ANOVA table summary\nsummary(model)\n\n\n            Df Sum Sq Mean Sq F value  Pr(>F)   \narea         2  25.66  12.832   8.176 0.00397 **\nResiduals   15  23.54   1.569                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "posts/HW1_EmmaNarkewicz.html",
    "href": "posts/HW1_EmmaNarkewicz.html",
    "title": "Homework 1",
    "section": "",
    "text": "First, let’s read in the data from the Excel file:\n\n\nCode\nlibrary(readxl)\ndf <- read_excel(\"_data/LungCapData.xls\")\n\n\nThe distribution of LungCap looks as follows:\n\n\nCode\nhist(df$LungCap)\n\n\n\n\n\nThe histogram suggests that the distribution is close to a normal distribution. Most of the observations are close to the mean. Very few observations are close to the margins (0 and 15).\n##b\nTo compare the distribution of the Lung Capacity with respect to Male & Females, I used ggplot to create a side by side boxplot of Lung Capacity for female and males in the sample, with female lung capacity in red & male lung capacity in blue.\n\n\nCode\n#load libraries\nlibrary(dplyr)\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nCode\nlibrary(ggplot2)\n\n\nggplot(data = df, aes(x= Gender, y = LungCap, fill = Gender)) +\n  geom_boxplot() +  scale_y_continuous(breaks = c(0,2,4,6,8,10,12,14,16)) +\n  labs(title = \"Box Plot - Lung Capacity by Gender\", x = \"Gender\", y = \"Lung Capacity (L)\") \n\n\n\n\n\nThe box-plot above shows that males in the sample had on average a higher lung capacity than female in the sample, with a higher minimum, Q1, median, Q3, & maximum lung capacity. 50% of the lung capacity of females in the sample was between 5.75 L - 9.25 L with a median lung capacity of ~7.75 L. In contrast, 50% of the lung capacity of males in the sample was between 6.5L - 10.5L with a median lung capacity of ~8.25L.\nFrom the size of the boxes & whiskers, it seems that a good amount of the male & female lung capacity is clustered around the median, but to better visualize the distribution, not just the difference between male females, I created a histogram below facet-wrapped by gender.\n\n\nCode\n##Histogram\nggplot(df, aes(x= LungCap, fill = Gender)) + geom_histogram() + facet_wrap(vars(`Gender`)) + labs(title = \"Histogram - Lung Capacity by Gender\", x = \"Lung Capacity (L)\", y = \"Frequency\") \n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nThe histogram also reflects a higher lung capacity for men in the sample than women as seen on the x-axis. It also shows that the lung capacity is somewhat normally distributed overall with most of the observations near the mean, though there are several peaks & dips in the histogram where a fewer or more individuals have a specific lung capacity than would be seen in a normal distribution. Additionally, the histogram for female lung capacity has more data concentrated around the mean, making it taller & pointier than the male lung capacity distribution. The male lung capacity histogram on the right has two modes (seen as peaks) at 7L & 11L on the left & right of the mean, unlike in a true normal distribution.\n##c\nTo compare the mean lung capacity of smokers to non-smokers in the sample, I used the group_by(), select(), & summarize() functions to get a table of the mean lung capacity in L for smokers vs. non-smokers in the sample.\n\n\nCode\n#Mean smokers vs. non-smokers.\n\ndf %>%\n  group_by(`Smoke`) %>%\n  select(`Smoke`, `LungCap`) %>%\n  summarize_all(mean, na.rm = TRUE)\n\n\n# A tibble: 2 × 2\n  Smoke LungCap\n  <chr>   <dbl>\n1 no       7.77\n2 yes      8.65\n\n\nThe mean lung capacity of smokers (Smoke = yes) was 8.65 L compared to a mean lung capacity of 7.77 L for non-smokers (Smoke = no).\nI was not expecting the mean Lung Capacity for smokers in this sample to be higher r than the mean Lung Capacity for non-smokers in the sample. This is because smoking harms lungs & thus is expected to decrease lung capacity.\nHowever, I anticipate that other factors such as age & gender might contribute to this counter-intuitive finding. For example, we saw in the previous question that males in the sample have a higher lung capacity then females. So if more smokers are male, the difference in lung capacity between smokers & non-smokers might be explained by gender more than smoking status.\n##d\nTo examine the relationship between Smoking and Lung Capacity within age groups: “less than or equal to 13”, “14 to 15”, “16 to 17”, and “greater than or equal to 18”, I used the case_when() to recreate a new Age_Range variable corresponding to these age groups.\n\n\nCode\n##Using Case_When to create age_range variable\ndf_by_age <- df %>%\n  mutate(Age_Range = case_when(\n    Age <= 13 ~ \"13 or younger\",\n    Age >= 14 & Age < 16 ~ \"14 to 15\",\n    Age >= 15 & Age < 18 ~ \"16 to 17\",\n    Age >= 16 ~ \"18 or older\")\n)\n\n\nI first wanted to see the relative age of the sample. As seen in the table below, over half the sample is 13 or younger (484), with the least common age group in thh sample being 18 or older (80).\n\n\nCode\n#Counts by age\ntable(select(df_by_age, Age_Range))\n\n\nAge_Range\n13 or younger      14 to 15      16 to 17   18 or older \n          428           120            97            80 \n\n\nThis got me curious, as I don’t know many 13-year-olds or people younger than 13 who smoke. The below table shows for each age range how many people smoke vs not smoke. While 27 individuals who were 13 or younger in the sample smoked, this is dwarfed by the 401 13-year-old or younger individuals who don’t smoke. While the number of 13 or younger individuals who smoke is higher than the number of 18 or older individuals who smoke (27 vs. 15), relative to the entire age-group a much higher proportion of the 18 or older age group smoked (15/80) than in the 13 or younger group (27/428).\n\n\nCode\n#Counts yes or no smoking by age\ntable(select(df_by_age, Age_Range, Smoke))\n\n\n               Smoke\nAge_Range        no yes\n  13 or younger 401  27\n  14 to 15      105  15\n  16 to 17       77  20\n  18 or older    65  15\n\n\nAfter examining the relationship between age & smoking, I then wanted to examine the relationship between age & lung capacity before factoring in smoking status. I would expect that as you go through puberty and become and adult (18+) your lung capacity with increase from before puberty (13 and younger). The table below using group_by(), select(), & summarize_all() follows this expected trend, with mean lung capacity increasing for each age group from 6.41 L for 13 or younger to 10.96 L for 18+ individuals in the sample. The mean lung capacity of 18+ is only slightly higher than the mean lung capacity at 16-17 years-old.\n\n\nCode\n#Mean lung capacity by age group\ndf_by_age %>%\n  group_by(`Age_Range`) %>%\n  select(`Age_Range`, `LungCap`) %>%\n  summarise_all(mean, na.rm = TRUE)\n\n\n# A tibble: 4 × 2\n  Age_Range     LungCap\n  <chr>           <dbl>\n1 13 or younger    6.41\n2 14 to 15         9.05\n3 16 to 17        10.2 \n4 18 or older     11.0 \n\n\nFinally, I once again used the group_by(), select(), and summarise_all() function, grouping by age_range & smoking status & calculating the mean lung capacity for smokers & non smokers in each age range, which I will interpret in 1e.\n\n\nCode\n##Mean lung cap by age_range smoker vs. non-smoker\ndf_by_age %>%\n  group_by(`Smoke`, `Age_Range`) %>%\n  select(`Age_Range`, `LungCap`, `Smoke`) %>%\n  summarise_all(mean, na.rm = TRUE)\n\n\n# A tibble: 8 × 3\n# Groups:   Smoke [2]\n  Smoke Age_Range     LungCap\n  <chr> <chr>           <dbl>\n1 no    13 or younger    6.36\n2 no    14 to 15         9.14\n3 no    16 to 17        10.5 \n4 no    18 or older     11.1 \n5 yes   13 or younger    7.20\n6 yes   14 to 15         8.39\n7 yes   16 to 17         9.38\n8 yes   18 or older     10.5 \n\n\n##e In contrast to 1c, where smokers had a higher mean lung capacity than non-smokers (8.64 L vs 7.77 L), for every age group except for 13 or younger non-smokers had a higher lung capacity than non-smokers.\n\nFor the 13 or younger: non-smoker lung capacity = 6.36L, smoker lung capacity = 7.20 L\nFor 14-15: non-smoker lung capacity = 9.14 L, smoker lung capacity = 8.39 L\nFor 16-17: non-smoker lung capacity = 10.47 L, smoker lung capacity = 9.38 L\nFor 18+: non-smoker lung capacity = 11.07L, smoker lung capacity = 10.51 L\n\nConsidering that in part d it was found that average lung capacity increases with age & that a smaller proportion of younger individuals in the sample smoked than the older students, one possible explanation for the finding in 1c is that the higher mean lung capacity for smokers overall in the sample could be due to a higher age of smokers in the sample, not due to smoking.\nIt is worth noting also that the sample smoking & nonsmoking mean lung capacity are closer to the mean lung capacity of the 13 & younger & 14-15 year old age groups, reflecting the larger number of younger folks in the sample compare.\nFor fun, I calculated the mean age for smokers & non smokers in the sample, and smokers indeed were older with a mean age of 14.8, compared to a mean non-smoker age of 12.03.\n\n\nCode\n#Mean age smoker vs. non-smoker\ndf %>%\n  group_by(`Smoke`) %>%\n  select(`Smoke`, `Age`) %>%\n  summarise_all(mean, na.rm = TRUE)\n\n\n# A tibble: 2 × 2\n  Smoke   Age\n  <chr> <dbl>\n1 no     12.0\n2 yes    14.8\n\n\n#Question 2\n##a\nTo answer these questions I used R more of a calculator, plugging in the frequencies from the HW1 Prior Convictions frequency table to probability equations we learned in class.\nThe probability that a randomly selected inmate has exactly 2 prior convictions is the P(X=2) which is 0.198.\n\n\nCode\n#Calculating P(X=2)\nprob_2_pc = 160/810\nprob_2_pc\n\n\n[1] 0.1975309\n\n\n##b\nThe probability that a randomly selected inmate has fewer than 2 prior convictions is the P(X < 2) which is equal to the P(X=0 or 1) which is equal to P(X=0) + P(X=1) which equals a probability of 0.694.\n\n\nCode\n#Calculating P(X<2)\nprob_0_pc = 128/810\nprob_1_pc = 434/810\nprob_less_2_pc = prob_0_pc + prob_1_pc\nprob_less_2_pc\n\n\n[1] 0.6938272\n\n\n##c\nThe probability that a randomly selected inmate has 2 or fewer prior convictions is the P(X ≤ 2) which is equal to the P(X=0 or 1 or 2) which is equal to P(X=0) + P(X=1) + P(X=2) which equals a probability of 0.891.\n\n\nCode\n#Calculating P(X≤2)\n\nprob_0_pc = 128/810\nprob_1_pc = 434/810\nprob_2_pc = 160/810\nprob_equal_or_less_2pc = prob_0_pc + prob_1_pc + prob_2_pc\nprob_equal_or_less_2pc\n\n\n[1] 0.891358\n\n\n##d The probability that a randomly selected inmate has greater than 2 prior convictions is the P(X >2) which is equal to the P(X=3 or 4) which is equal to P(X=3) + P(X=4), which gives a probability of 0.109.\nBecause probabilities should be cumulative, the probability of having greater than 2 prior convictions & the probability of having 2 or less prior convictions should add up to 1. Therefor, 1 - P(X≤2) should equal P(X >2) which we confirm it does.\n\n\nCode\n#Calculating P(x>2) the first way\nprob_3_pc = 64/810\nprob_3_pc\n\n\n[1] 0.07901235\n\n\nCode\nprob_4_pc = 24/810\nprob_4_pc\n\n\n[1] 0.02962963\n\n\nCode\nprob_greater_2_pc = prob_3_pc + prob_4_pc\nprob_greater_2_pc\n\n\n[1] 0.108642\n\n\nCode\n#Calculating P(x>2) the second way\nsame = 1 - prob_equal_or_less_2pc\nsame\n\n\n[1] 0.108642\n\n\n##e\nThe expected value for the number of prior convictions is the long term mean of the sample which can be calculated with the equation: 𝐸(𝑋)=𝜇=∑𝑥𝑃(𝑥)\nThe expected value for number of prior convictions is 1.286\nThis is not an integer, but means that in the long-term sample mean will be close to 1, which aligns with the frequency table, where 1 is the most frequent number of prior convictions.\n\n\nCode\n#Expected value calc\nexpected_value = (0 * prob_0_pc) + (1 * (prob_1_pc)) + (2 * (prob_2_pc)) + (3* (prob_3_pc)) + (4 * (prob_4_pc))\nexpected_value\n\n\n[1] 1.28642\n\n\nCode\n##Check that mean same\n\n((0*128)+(1*434) + (2*160) + (3*64) + (4*24))/810\n\n\n[1] 1.28642\n\n\n##f\nTo calculate the variance of the sample I used the equation Var(X) = Σx^2p − μ^2, where the mean is the expected value calculated in e.\nThis gives a variance of 0.856 years squared and a standard deviation of 0.925 years.\n\n\nCode\n#Calculating variance\nVariance = (((0^2) * prob_0_pc) + ((1^2)* prob_1_pc) + ((2^2) * prob_2_pc) + ((3^2) * prob_3_pc) + ((4^2) * prob_4_pc)) - (expected_value^2)\nVariance\n\n\n[1] 0.8562353\n\n\nCode\n#Calculating sd from variance\nsd = sqrt(Variance)\nsd\n\n\n[1] 0.9253298"
  },
  {
    "objectID": "posts/HW_1_Diana_Rinker.html",
    "href": "posts/HW_1_Diana_Rinker.html",
    "title": "Homework 1",
    "section": "",
    "text": "Question 1\nFirst, let’s read in the data from the Excel file:\n\n\nCode\nlibrary(readxl)\nlibrary(dplyr)\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nCode\nlibrary(tidyr)\nlibrary(tidyverse)\n\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2\n──\n\n\n✔ ggplot2 3.4.1     ✔ purrr   1.0.1\n✔ tibble  3.1.8     ✔ stringr 1.5.0\n✔ readr   2.1.4     ✔ forcats 1.0.0\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\nCode\ndf <- read_excel(\"_data/LungCapData.xls\")\n\n\n\na) What does the distribution of LungCap look like? (Hint: Plot a histogram with probability density on the y axis)\n\n\nCode\nhist(df$LungCap)\n\n\n\n\n\nThe histogram suggests that the distribution is close to a normal distribution. Most of the observations are close to the mean. Very few observations are close to the margins (0 and 15).\n\n\nb) Compare the probability distribution of the LungCap with respect to Males and Females? (Hint: make boxplots separated by gender using the boxplot() function)\n\n\nCode\nboxplot( LungCap ~ Gender , data =df)\n\n\n\n\n\n\n\nc) Compare the mean lung capacities for smokers and non-smokers. Does it make sense?\n\n\nCode\ndf.grouped<- df %>%\n  group_by(Smoke)%>%\n  summarize (mean.Lunc.Cap = mean (LungCap))\nknitr::kable(df.grouped)\n\n\n\n\n\nSmoke\nmean.Lunc.Cap\n\n\n\n\nno\n7.770188\n\n\nyes\n8.645454\n\n\n\n\n\nIt is surprising that smoker’s lung capacity mean is larger than for nonsmokers. T o understand the reason, I would need to break up the data into subgroups.\n\n\nd) Examine the relationship between Smoking and Lung Capacity within age groups: “less than or equal to 13”, “14 to 15”, “16 to 17”, and “greater than or equal to 18”.\n\n\nCode\ndf$age.group <- NA  \ndf$age.group<- ifelse(df$Age <=13, \"under13\" , df$age.group )\ndf$age.group<- ifelse(df$Age >=14 & df$Age <=15, \"14-15\" , df$age.group )\ndf$age.group<- ifelse(df$Age >=16 & df$Age <=17, \"16-17\" , df$age.group )\ndf$age.group<- ifelse(df$Age >=18, \"18+\" , df$age.group )\ndf$age.group <-factor (df$age.group, levels = c(\"under13\",  \"14-15\", \"16-17\", \"18+\") )\n\ndf.grouped\n\n\n# A tibble: 2 × 2\n  Smoke mean.Lunc.Cap\n  <chr>         <dbl>\n1 no             7.77\n2 yes            8.65\n\n\nCode\nggplot (df, mapping=aes(y=LungCap, x = Smoke ))+\n  geom_boxplot()+\n  facet_wrap (~ age.group)\n\n\n\n\n\n\n\nd) Compare the lung capacities for smokers and non-smokers within each age group. Is your answer different from the one in part c. What could possibly be going on here?\nIn the table of counts within each age group we can see that the group ’under 13” is over 50 of the records. Within this group, there is no difference between smokers and non-smokers (probably because of the length of smoking and the fact that only 7% are smokers), which contibutes to overall sample mean.\n\n\nCode\ndf.grouped<-df%>%\n  group_by(age.group, Smoke)%>%\nsummarize (count=n())\n\n\n`summarise()` has grouped output by 'age.group'. You can override using the\n`.groups` argument.\n\n\nTo compare smokers and non-smokers accurately, we could exclude the group “under 13”.\n\n\nCode\ndf.filtered<- df %>%\n  filter(age.group != \"under13\")\n\nggplot (df.filtered, mapping=aes(y=LungCap, x = Smoke ))+\n  geom_boxplot()\n\n\n\n\n\nNow we can see the difference between mean, where smokers have smaller Lung capacity.\n\n\n\nQuestion 2\nLet X = number of prior convictions for prisoners at a state prison at which there are 810 prisoners.\n\n\nCode\nX <- c(0, 1, 2, 3, 4)\nFrequency <- c(128,434, 160, 64, 24)\n\ndf<-tibble (X, Frequency) \ndf$Probabilty <- Frequency/sum(Frequency)\nknitr::kable(df)\n\n\n\n\n\nX\nFrequency\nProbabilty\n\n\n\n\n0\n128\n0.1580247\n\n\n1\n434\n0.5358025\n\n\n2\n160\n0.1975309\n\n\n3\n64\n0.0790123\n\n\n4\n24\n0.0296296\n\n\n\n\n\n\na) What is the probability that a randomly selected inmate has exactly 2 prior convictions?\nNumber of prior convictions of inmates has Poisson distribution. Probability of X = is 0.197\n\n\nCode\nchances.of.2<- df$Probabilty[3]\nknitr::kable(chances.of.2)\n\n\n\n\n\nx\n\n\n\n\n0.1975309\n\n\n\n\n\n\n\nb) What is the probability that a randomly selected inmate has fewer than 2 prior convictions?\nTo calculate “fewer than2”, we will use cumulative probability for Poisson distribution with default “lower.tail =T”, for value “1” to exclude value “2”.\n\n\nCode\nprob.under.2 <- sum(df$Probabilty[1:2])\nknitr::kable(prob.under.2   )\n\n\n\n\n\nx\n\n\n\n\n0.6938272\n\n\n\n\n\n\n\nc) What is the probability that a randomly selected inmate has 2 or fewer prior convictions?\nTo calculate “fewer than2”, we will use cumulative probability for Poisson distribution with default “lower.tail =T”: It will include value of “2”.\n\n\nCode\nprob.under.and.2 <- sum(df$Probabilty[1:3])\nknitr::kable(prob.under.and.2)\n\n\n\n\n\nx\n\n\n\n\n0.891358\n\n\n\n\n\n\n\nd) What is the probability that a randomly selected inmate has over 2 prior convictions?\nTo calculate “over than 2”, we will use cumulative probability for Poisson distribution with “lower.tail =F”:\n\n\nCode\nlambda<- mean (df$X)\nprob.over.2 <- sum(df$Probabilty[4:5] )\nknitr::kable(prob.over.2  )\n\n\n\n\n\nx\n\n\n\n\n0.108642\n\n\n\n\n\n\n\ne) What is the expected value for the number of prior convictions\n\\[\nE(X) = \\sum_{all x} x  \\cdot p(x) = \\mu  \n\\]\n\n\nCode\ndf$Probabilty <- Frequency/sum(Frequency)\n(expected.value.of.X <- sum(df$X * df$Probabilty ))\n\n\n[1] 1.28642\n\n\nCode\nknitr::kable(expected.value.of.X )\n\n\n\n\n\nx\n\n\n\n\n1.28642\n\n\n\n\n\n\n\nf) Calculate the variance and the standard deviation for the Prior Convictions.\nVariance of a random variable: \\[\n\\sigma^2 = E[(X-\\mu)^2] = \\sum_{all x}(x-\\mu)^2 \\cdot p(x)\n\\]\n\n\nCode\nvariance.X <- sum (((df$X - expected.value.of.X )^2) * df$Probabilty  )\nknitr::kable(variance.X  )\n\n\n\n\n\nx\n\n\n\n\n0.8562353\n\n\n\n\n\nAlternatively: \\[\n\\sigma^2 = E(X^2)-[E(X)]^2 = E(X^2)-\\mu^2\n\\]\n\n\nCode\nvariance <- (sum((df$X^2) * df$Probabilty )) - ((expected.value.of.X)^2)\nknitr::kable(variance)\n\n\n\n\n\nx\n\n\n\n\n0.8562353\n\n\n\n\n\nStandard deviation is a square root of variance: \\[\n\\sigma = \\sqrt variance\n\\]\n\n\nCode\nsd<- sqrt(variance)\nknitr::kable(sd)\n\n\n\n\n\nx\n\n\n\n\n0.9253298"
  },
  {
    "objectID": "posts/asch_harwood_final_project_part1.html",
    "href": "posts/asch_harwood_final_project_part1.html",
    "title": "Estimating National Food Waste with State/Local Waste Characterization Studies in the United States",
    "section": "",
    "text": "Code\nlibrary(\"dplyr\")\nlibrary(\"knitr\")\nlibrary(kableExtra)\nlibrary(xtable)\n\n\n\nBackground\nReFED, a national nonprofit that advocates for the reduction of food waste, maintains several models to estimate the amount of wasted food and its greenhouse gas emissions in the United States. Collectively, U.S. residents and businesses throw out roughly 90 million tons of food annually, contributing nearly 5 percent of the total net US greenhouse gas emissions.\nCreating system-level estimates in the food supply chain is notoriously difficult. To produce these estimates, ReFED employs a ‘mass balance’ accounting approach. For each sector, ReFED estimates the value of the total supply of food in US dollars. It then applies a price per lb conversion factor to convert dollars to tons to estimate the total suppy of food in weight. ReFED then draws on academic and private-sector studies to determine ‘surplus rates’–the share of food that is not going to its intended destination. The supply of food is multiplied by these surplus rates to determine the total amount of surplus food in a given place and year.\nA limitation of this approach, however, is that the research to estimate surplus rates is rarely, if ever, repeated at regular intervals. As a result, the lack of year-over-year and geographic variation in surplus rates makes it incredibly difficult to identify systematic changes in the food waste behaviors.\n\n\nProposal\nI propose utilizing state and local waste characterization studies combined with ‘easily observable’ and ‘widely available’ predictors (i.e. food supply, population, population density, GDP, unemployment, food prices, consumer confidence, investment/spending in food waste reduction technologies, knowledge and attitudes about food waste/environmental issues) to build a weighted mixed effects linear regression model. The goal is estimate the share of food waste of total municipal solid waste at the national level in a given year.\nState and local governments periodically conduct inferential waste characterization studies, where they randomly select several 200 - 300 lb samples from landfills and other waste management facilities, and separate the garbage to determine the composition of municipal solid waste in a particular area over a predetermined time. These studies are then used to set waste management budgets, ‘right-size’ landfill capacity, and create/evaluate programs meant to divert recyclable and compostable materials from landfills. However, they are not conducted regularly or consistently (or at all in many places), and, to my knowledge, there are no nationally-representative waste characterization studies.\nNevertheless, I believe there is the potential to creatively use these studies to estimate the share of food waste in landfills at the national-level over time, and, hopefully, to determine if there are any systematic changes in food waste management behavior in the commercial and residential sectors.\n\nResearch Questions\n\nIs it possible to use state and local waste characterization studies to estimate the share of food waste in municipal solid waste at the national-level year over year?\nWhat ‘easily-observable’ and ‘widely-available’ predictors allow us to estimate the share of food waste in the absence of regularly-conducted, national-level samples?\nHow ‘good’ does the model need to be to be “operationally” useful by food businesses, policymakers, and households?\nHow do food waste estimates produced using a bottom-up method proposed here compare to top-down, mass balanced-based approaches?\n\n\n\nHypothesis\nPopulation\nH0: Population is not correlated with percent food waste\nH1: Population is correlated with percent food waste\nFood Supply\nH0: Food supply is not correlated with percent food waste\nH1: Food supply is correlated with percent food waste\nGDP\nH0: GDP growth is not correlated with percent food waste\nH1: GDP growth is positively correlated with percent food waste\nUnemployment\nH0: Unemployment is not correlated with percent food waste\nH1: Unemployment is negatively correlated with percent food waste\nFood Prices\nH0: Food prices are not correlated with percent food waste\nH1: Food prices are negatively correlated with percent food waste\nConsumer Confidence\nH0: Consumer confidence is not correlated with percent food waste\nH1: Consumer confidence is positively correlated with percent food waste\nConsumer Knowledge\nH0: Consumer knowledge/concern about climate change is not correlated with percent food waste\nH1: Consumer knowledge/concern is negatively correlated with percent food waste\nInvestment\nH0: Investment in food waste reduction technologies is not correlated with percent food waste\nH1: Investment in food waste reduction technologies is positively correlated with percent food waste\n\n\nExploratory Data Analysis\nCurrently, no comprehensive aggregate municipal solid waste characterization datasets exisit for the United States in the public domain. To address this, I have compiled roughly 300 waste characterization studies, of which I have parsed and coded roughly 50 to date, based on the following (rough) criteria1. To be eligible to be included in my dataset, each study must be:\n\nconducted in the United States and be representative of the state, county, or local level between the years of 1999 and 2023\ninfer the amount of total food as a share of the total amount of municipal solid waste for an entire year\nreport the number of samples, which need to be between 200 - 300 lbs each\n\n\n\nCode\nwaste <- read.csv(url(\"https://raw.githubusercontent.com/AschHarwood/603_Spring_2023/asch_603_spring_2023/posts/_data/wasteCharacterizationStudies_v1.csv\"))\nwaste$percent_food <- round(waste$percent_food, digits=4)\ntable <- na.omit(waste) %>%\n  sample_n(10)\nkable(table, caption = \"Waste Characterization Dataset\", format = \"html\") %>%\n  kable_styling(bootstrap_options = \"striped\", full_width = F) %>%\n  row_spec(0:nrow(table), font_size = 12)\n\n\n\n\nWaste Characterization Dataset\n \n  \n    year \n    state \n    geographic_scope \n    geographic_scope_name \n    sector_scope \n    sample_size \n    percent_food \n  \n \n\n  \n    2019 \n    minnesota \n    state \n    minnesota \n    residential \n    119 \n    0.1790 \n  \n  \n    2015 \n    connecticut \n    state \n    connecticut \n    residential \n    136 \n    0.2001 \n  \n  \n    2021 \n    california \n    state \n    california \n    transfer-truck \n    50 \n    0.0581 \n  \n  \n    2011 \n    minnesota \n    city \n    newport \n    residential \n    30 \n    0.1460 \n  \n  \n    2018 \n    vermont \n    state \n    vermont \n    residential \n    95 \n    0.3055 \n  \n  \n    2016 \n    washington \n    state \n    washington \n    commercial \n    243 \n    0.1864 \n  \n  \n    2021 \n    california \n    state \n    california \n    commerical \n    152 \n    0.0176 \n  \n  \n    2016 \n    delaware \n    state \n    delaware \n    commercial \n    75 \n    0.2187 \n  \n  \n    2015 \n    connecticut \n    state \n    connecticut \n    commercial \n    111 \n    0.2548 \n  \n  \n    2021 \n    california \n    state \n    california \n    commercial \n    201 \n    0.1426 \n  \n\n\n\n\n\n\n\nCode\ncat(\"Summary of Waste Characterization\\n\")\n\n\nSummary of Waste Characterization\n\n\nCode\ncat('\\n')\n\n\nCode\nsummary(waste)\n\n\n      year         state           geographic_scope   geographic_scope_name\n Min.   :1999   Length:76          Length:76          Length:76            \n 1st Qu.:2010   Class :character   Class :character   Class :character     \n Median :2014   Mode  :character   Mode  :character   Mode  :character     \n Mean   :2013                                                              \n 3rd Qu.:2016                                                              \n Max.   :2022                                                              \n                                                                           \n sector_scope        sample_size      percent_food   \n Length:76          Min.   :  7.00   Min.   :0.0176  \n Class :character   1st Qu.: 60.75   1st Qu.:0.1426  \n Mode  :character   Median :108.50   Median :0.1795  \n                    Mean   :121.61   Mean   :0.1839  \n                    3rd Qu.:153.00   3rd Qu.:0.2233  \n                    Max.   :379.00   Max.   :0.3389  \n                    NA's   :48                       \n\n\n\n\nCode\ncat('Summary of Waste Characterization\\n')\n\n\nSummary of Waste Characterization\n\n\nCode\ncat('\\n')\n\n\nCode\nstr(waste)\n\n\n'data.frame':   76 obs. of  7 variables:\n $ year                 : int  2021 2021 2021 2021 2013 2013 1999 1999 2016 2016 ...\n $ state                : chr  \"california\" \"california\" \"california\" \"california\" ...\n $ geographic_scope     : chr  \"state\" \"state\" \"state\" \"state\" ...\n $ geographic_scope_name: chr  \"california\" \"california\" \"california\" \"california\" ...\n $ sector_scope         : chr  \"residential\" \"commercial\" \"commerical\" \"transfer-truck\" ...\n $ sample_size          : int  153 201 152 50 379 251 201 153 77 75 ...\n $ percent_food         : num  0.174 0.1426 0.0176 0.0581 0.3389 ...\n\n\n\n\nCode\n# Create a data frame with the data dictionary\ndata_dictionary <- data.frame(\n  `col_name` = c(\"year\", \"state\", \"geographic_scope\", \"geographic_scope_name\", \"sector_scope\", \"sample_size\", \"percent_food\"),\n  `d_type` = c(\"int\", \"chr\", \"chr\", \"chr\", \"chr\", \"int\", \"dbl\"),\n  description = c(\n    \"The year when the study was conducted\",\n    \"The U.S. state where the study was conducted\",\n    \"The scope of the geographic area covered by the study (e.g., state, county, or city)\",\n    \"The name of the specific geographic area covered by the study (e.g., state name or city name)\",\n    \"The sector scope of the study (e.g., commercial, residential, or transfer-truck)\",\n    \"The number of samples included in the study\",\n    \"The percentage of food waste in the studied area\"\n  )\n)\nkable(data_dictionary)\n\n\n\n\n \n  \n    col_name \n    d_type \n    description \n  \n \n\n  \n    year \n    int \n    The year when the study was conducted \n  \n  \n    state \n    chr \n    The U.S. state where the study was conducted \n  \n  \n    geographic_scope \n    chr \n    The scope of the geographic area covered by the study (e.g., state, county, or city) \n  \n  \n    geographic_scope_name \n    chr \n    The name of the specific geographic area covered by the study (e.g., state name or city name) \n  \n  \n    sector_scope \n    chr \n    The sector scope of the study (e.g., commercial, residential, or transfer-truck) \n  \n  \n    sample_size \n    int \n    The number of samples included in the study \n  \n  \n    percent_food \n    dbl \n    The percentage of food waste in the studied area \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\nFootnotes\n\n\nI recognize that we were encouraged to work with pre-existing datasets. Nevertheless, I view this as an opportunity to prototype a project I believe might have real-world applicability in my professional life.↩︎"
  },
  {
    "objectID": "posts/HW2_XiaoyanHu.html",
    "href": "posts/HW2_XiaoyanHu.html",
    "title": "Homework 2",
    "section": "",
    "text": "Code\nlibrary(tidyr)\nlibrary(dplyr)\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nCode\nlibrary(readxl)\nlibrary(ggplot2)\n\n\n\nQuestion 1\n\nThe time between the date a patient was recommended for heart surgery and the surgery date for cardiac patients in Ontario was collected by the Cardiac Care Network (“Wait Times Data Guide,” Ministry of Health and Long-Term Care, Ontario, Canada, 2006). The sample mean and sample standard deviation for wait times (in days) of patients for two cardiac procedures are given in the accompanying table. Assume that the sample is representative of the Ontario population\n\nConstruct the 90% confidence interval to estimate the actual mean wait time for each of the two procedures. Is the confidence interval narrower for angiography or bypass surgery?\n\n\nCode\n# mean mean()\nMean1<-19\nMean2<-18\n# standard deviation sd()\nsd1<-10\nsd2<-9\n# sample size\nn1<-539\nn2<-847\n# standard error sd/sqrt(n)\nse1<-sd1/sqrt(n1)\nse2<-sd2/sqrt(n2)\nse1\n\n\n[1] 0.4307305\n\n\nCode\nse2\n\n\n[1] 0.3092437\n\n\nCode\n# t-value\n\n# tail_area<-(1-confidence_level)/2\nTA<-(1-0.9)/2\n#t score <-qt(p=1-tail_area,df=s_size-1 )\ntscore1<-qt(p=1-TA, df = n1-1)\ntscore2<-qt(p=1-TA, df = n2-1)\ntscore1\n\n\n[1] 1.647691\n\n\nCode\ntscore2\n\n\n[1] 1.646657\n\n\nCode\n#CI <-c(s_mean-t_score*SE,s_mean+t_score*SE )\nCI1<-c(Mean1-tscore1*se1, Mean1+tscore1*se1)\nCI2<-c(Mean2-tscore2*se2, Mean2+tscore2*se2)\nCI1\n\n\n[1] 18.29029 19.70971\n\n\nCode\nCI2\n\n\n[1] 17.49078 18.50922\n\n\n#Question 2\nA survey of 1031 adult Americans was carried out by the National Center for Public Policy. Assume that the sample is representative of adult Americans. Among those surveyed, 567 believed that college education is essential for success. Find the point estimate, p, of the proportion of all adult Americans who believe that a college education is essential for success. Construct and interpret a 95% confidence interval for p.\n\n\nCode\n#p \np1<-567/1031\n#sample size \nn<-1031\n#alpha = 0.05\nse<-sqrt((p1*(1-p1))/n)\n#z score1.96\nCI<-c(p-1.96*se,p+1.96*se)\n\n\nError in eval(expr, envir, enclos): object 'p' not found\n\n\nCode\nCI\n\n\nError in eval(expr, envir, enclos): object 'CI' not found\n\n\n#Question 3 Suppose that the financial aid office of UMass Amherst seeks to estimate the mean cost of textbooks per semester for students. The estimate will be useful if it is within $5 of the true population mean (i.e. they want the confidence interval to have a length of $10 or less). The financial aid office is pretty sure that the amount spent on books varies widely, with most values between $30 and $200. They think that the population standard deviation is about a quarter of this range (in other words, you can assume they know the population standard deviation).\n\n\nCode\n#z*sd/sqrt(n)=5\n#population sd = 200-30/4 = 42.5\n#alpha = 0.05\n#n=(z*sd/5)^2\nn<-(1.96*42.5/5)^2\n\nn\n\n\n[1] 277.5556\n\n\n#Question 4 According to a union agreement, the mean income for all senior-level workers in a large service company equals $500 per week. A representative of a women’s group decides to analyze whether the mean income μ for female employees matches this norm. For a random sample of nine female employees, ȳ = $410 and s = 90 A. Test whether the mean income of female employees differs from $500 per week. Include assumptions, hypotheses, test statistic, and P-value. Interpret the result. B. Report the P-value for Ha: μ < 500. Interpret. C. Report and interpret the P-value for Ha: μ > 500. (Hint: The P-values for the two possible one-sided tests must sum to 1."
  },
  {
    "objectID": "posts/HW2_EmmaNarkewicz.html",
    "href": "posts/HW2_EmmaNarkewicz.html",
    "title": "Homework 2",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.3.6      ✔ purrr   0.3.4 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.0      ✔ stringr 1.4.1 \n✔ readr   2.1.2      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()"
  },
  {
    "objectID": "posts/HW2_EmmaNarkewicz.html#a",
    "href": "posts/HW2_EmmaNarkewicz.html#a",
    "title": "Homework 2",
    "section": "A",
    "text": "A\nTest whether the mean income of female employees differs from $500 per week. Include assumptions, hypotheses, test statistic, and P-value. Interpret the result.\nAssumptions of a 1-sample t-test:\n\npopulation is normally distributed\nobservations in our sample are generated independently of one another\n\nInformation we are provided is that:\n\nPopulation mean (μ) = 500\nSample mean (y bar) = 410\nSample sd = 90 s\nSample n = 9\n\nNull Hypothesis: (H0) female mean income μ = 500 Alternative Hypothesis: (Ha) female mean income μ ≠ 500\nCalculate the t statistic using the equation:\nt = (y bar - μ ) / (sd estimate / sqrt(N))\n\n\nCode\n## Calculating tscore\n\nt =  (410 - 500) / (90/sqrt(9))\nt\n\n\n[1] -3\n\n\nThe t-score is -3\nTo solve for the p-value from the t-score I will use the pt() function, with a q = the t-statistic of -3, df = 8 (9-1).\nIt it is important to note that an assumption of the pt() function is you are looking for the probability of the lower tail, not both tails. Because the t score is negative looking at the lower tail is appropriate, but to get the probability for 2-tailed test we need to multiply the lower tail probability by 2.\n\n\nCode\n#Calculating p-value\n\np_lower_tail = pt(q = t, df =8, lower.tail = TRUE)\np_lower_tail\n\n\n[1] 0.008535841\n\n\nThis gives us the probability that μ is less than 500, but that is only 1 tail, so we multiply this problity by 2 to get the p value for our 2-sided t-test\n\n\nCode\n#Two-side t\np_twotail =  p_lower_tail * 2\np_twotail\n\n\n[1] 0.01707168\n\n\nWith a 2-tail p value of 0.017, we can confidently reject the null hypothesis that female salary μ = 500 as p< 0.05. This suggests that the true mean salary of female workers is not equal to $500."
  },
  {
    "objectID": "posts/HW2_EmmaNarkewicz.html#b",
    "href": "posts/HW2_EmmaNarkewicz.html#b",
    "title": "Homework 2",
    "section": "B",
    "text": "B\nReport the P-value for Ha: μ < 500. Interpret.\n\n\nCode\n#P-value Ha: μ < 500\n\np_lower_tail = pt(q = t, df =8, lower.tail = TRUE)\n\np_lower_tail\n\n\n[1] 0.008535841\n\n\nWith a small p-value of 0.00854 we reject the null hypothesis that μ = 500, suggesting that the true mean salary of female workers is less than $500."
  },
  {
    "objectID": "posts/HW2_EmmaNarkewicz.html#c",
    "href": "posts/HW2_EmmaNarkewicz.html#c",
    "title": "Homework 2",
    "section": "C",
    "text": "C\nReport and interpret the P-value for Ha: μ > 500.\n\n\nCode\n#P-value Ha: μ > 500\n\np_upper_tail = pt(q = t, df =8, lower.tail = FALSE)\n\np_upper_tail\n\n\n[1] 0.9914642\n\n\nWith a large p-value of 0.99 we fail to reject the null hypothesis that μ = 500, suggesting that the true means salary of female workers is not greater than $500."
  },
  {
    "objectID": "posts/HW2_EmmaNarkewicz.html#a-1",
    "href": "posts/HW2_EmmaNarkewicz.html#a-1",
    "title": "Homework 2",
    "section": "A",
    "text": "A\nShow that Show that t = 1.95 and P-value = 0.051 for Jones. Show that t = 1.97 and P-value = 0.049 for Smith.\nBecause we are provided with standard error not standard deviation, I used the equation:\nt = (ȳ - μ) / se\n\n\nCode\n#Jones T score\n\nt_Jones = (519.5 - 500) / 10.0\nt_Jones\n\n\n[1] 1.95\n\n\nFor Jones the t score is indeed 1.95\nTo calculate the p-value, I will use pt(), q = 1.95, df = (1000-1) = 999). Because Jone’s t-score is positive, we look at the upper tail.\n\n\nCode\n#Calculate 1-tail p-value Jones\np_upper_tail_Jones = pt(q = t_Jones, df = 999, lower.tail = FALSE)\np_upper_tail_Jones\n\n\n[1] 0.02572777\n\n\nThe p value of the upper tail for Jones is 0.02572777, but because the alternative hypothesis is 2-tailed, it needs to be multiplied by 2\n\n\nCode\n#Calculating p_value Jones\np_Jones = p_upper_tail_Jones * 2\np_Jones\n\n\n[1] 0.05145555\n\n\nThis shows the p-value of Jones is indeed 0.51\n\n\nCode\n# Smith T score \nt_Smith = (519.7 - 500) / 10.0\nt_Smith\n\n\n[1] 1.97\n\n\nThe t-score for Smith is indeed 1.97. The p value is calculated the same way as for Jones, using pt() & the upper tail:\n\n\nCode\n#Calculate 1-tail p-value Smith\np_upper_tail_Smith = pt(q = t_Smith, df = 999, lower.tail = FALSE)\np_upper_tail_Smith\n\n\n[1] 0.02455713\n\n\n\n\nCode\n#Smith p=value\np_Smith = p_upper_tail_Smith * 2\np_Smith\n\n\n[1] 0.04911426\n\n\nSmith’s p-value does = 0.049"
  },
  {
    "objectID": "posts/HW2_EmmaNarkewicz.html#b-1",
    "href": "posts/HW2_EmmaNarkewicz.html#b-1",
    "title": "Homework 2",
    "section": "B",
    "text": "B\nUsing an alpha level = 0.05 for both\n\nJones’ p-value = 0.51 means a non-statistically significant result, where we fail to reject the null hypothesis that μ = 500\n\nSmith’s p=value = 0.49 means a statistically significant results, where we reject the null hypothesis that μ = 500"
  },
  {
    "objectID": "posts/HW2_EmmaNarkewicz.html#c-1",
    "href": "posts/HW2_EmmaNarkewicz.html#c-1",
    "title": "Homework 2",
    "section": "C",
    "text": "C\nThis example showcases how reporting the actual p-value is important, as in this case the p-values are very close between Jones & Smith, which are both right on the edge of significance (0.050 +/- 0.01). A p-value of 0.049 & a p-value of 0.0000000049 are both less than 0.05 & would both lead us to reject the null hypothesis, but the second p-value is highly significant while the first p-value is only barely significant at an alpha level of 0.05. Reporting the p-value itself gives readers & other researchers information to much better understand your statistical findings than just reporting if the p value is > or < 0.05 or if you do or do not reject H0."
  },
  {
    "objectID": "posts/Derian-Toth_FinalProjectProposal.html",
    "href": "posts/Derian-Toth_FinalProjectProposal.html",
    "title": "Final Project Proposal Check-in1",
    "section": "",
    "text": "Code\n#reading in data\njobs_gender <- readr::read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-03-05/jobs_gender.csv\")\n\n\nRows: 2088 Columns: 12\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): occupation, major_category, minor_category\ndbl (9): year, total_workers, workers_male, workers_female, percent_female, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nCode\nearnings_female <- readr::read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-03-05/earnings_female.csv\") \n\n\nRows: 264 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): group\ndbl (2): Year, percent\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nCode\nemployed_gender <- readr::read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-03-05/employed_gender.csv\") \n\n\nRows: 49 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (7): year, total_full_time, total_part_time, full_time_female, part_time...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nCode\n#View(employed_gender)\n#View(jobs_gender)\n#View(earnings_female)\n\n#bringing in libraries\nlibrary(dplyr)\n\n\n\nAttaching package: 'dplyr'\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nCode\nlibrary(vtable)\n\n\nLoading required package: kableExtra\n\nAttaching package: 'kableExtra'\n\nThe following object is masked from 'package:dplyr':\n\n    group_rows\n\n\nCode\nlibrary(ggplot2)\nlibrary(readr)\nlibrary(tidyverse)\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ lubridate 1.9.2     ✔ tibble    3.1.8\n✔ purrr     1.0.1     ✔ tidyr     1.3.0\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter()          masks stats::filter()\n✖ kableExtra::group_rows() masks dplyr::group_rows()\n✖ dplyr::lag()             masks stats::lag()\nℹ Use the \u001b]8;;http://conflicted.r-lib.org/\u0007conflicted package\u001b]8;;\u0007 to force all conflicts to become errors\n\n\nCode\nlibrary(stringr)\nlibrary(lubridate)"
  },
  {
    "objectID": "posts/Derian-Toth_FinalProjectProposal.html#introduction",
    "href": "posts/Derian-Toth_FinalProjectProposal.html#introduction",
    "title": "Final Project Proposal Check-in1",
    "section": "Introduction",
    "text": "Introduction\nThis research exploration takes a close look at the gender wage gap over the years. While the wage gap has decreased, women are still making on average, about 82 cents for every dollar men make (Aragao, 2023). Women are entering the workforce at a higher rate than in the past, yet are still not making the same salary as men who work the same jobs.\nFrom the two line graphs below we can see that women 16 years and older, working full time, are increasingly entering the workforce (Graph 1: Percent of Men and Women Working Full Time by Year); yet women are still not making 100% of the wage that men make (Graph 2: Female Salary Percent of Male Salary).\n\n\nCode\n#percent of women working full time compared to men\n#Still would like to add a legend to this graphs\nggplot(data = employed_gender, aes(x = year)) +\n  geom_line(aes(y = full_time_female), colour = \"red\") +\n  geom_point(aes(y = full_time_female), colour = \"red\") +\n  geom_line(aes(y = full_time_male), colour = \"blue\") +\n  geom_point(aes(y = full_time_male), colour = \"blue\") +\n  labs(title = \"Percent of Men and Women Working Full Time by Year\", \n       x=\"Year\", \n       y=\"Percent Working\") +\n  ylim(0,100)\n\n\n\n\n\nCode\n#wage gap by year\n#Would also like to add a legend and labels to the this graphs\nearnings_female%>%\n  filter(str_detect(group,\"Total, 16 years and older\")) %>%\n  ggplot(aes(x = Year)) +\n  geom_line(aes(y = percent)) +\n  geom_point(aes(y = percent)) +\n  labs(title = \"Female Salary Percent of Male Salary\", \n       x=\"Year\", \n       y=\"Percent of Salary\") +\n  ylim(0,100)\n\n\n\n\n\n\nThe Data set\nThe data used in this exploration and analysis come from the Bureau of Labor Statistics and the Census Bureau. These data describe different variables about women in the workforce across time. The three data frames are: (1) historical data, providing the percent of earnings women make compared to men, broken down by age group and ranging from 1979 - 2011, (2) Another historical dataset providing the workforce information (percent of women and men working full time and part time), by year, ranging from 1968 - 2016, (3) and lastly, detailed data regarding occupation (including two levels of categorization for the occupation), earnings for those occupations by gender, and percent of earnings women make compared to men, ranging from 2013 - 2016.\nThe tables below provide the descriptive statistics for the dependent variable, womens’ wage percent of male wage. The first table is the dependent variable across occupations and year (from 2013 to 2016). The second table below show the dependent variable for women who are 16 years or older, across many years (1979 - 2011).\n\n\nCode\n#Descriptive Statistics for the dependent variable: wage percent of male \nSumWage<-data_frame(jobs_gender$year, jobs_gender$wage_percent_of_male)\n\n\nWarning: `data_frame()` was deprecated in tibble 1.1.0.\nℹ Please use `tibble()` instead.\n\n\nCode\nSumWage<- SumWage%>%\n  rename('Year'='jobs_gender$year',\n         'wage_percent_of_male' = 'jobs_gender$wage_percent_of_male')\nsumtable(SumWage)  \n\n\n\n\nSummary Statistics\n \n  \n    Variable \n    N \n    Mean \n    Std. Dev. \n    Min \n    Pctl. 25 \n    Pctl. 75 \n    Max \n  \n \n\n  \n    Year \n    2088 \n    2014 \n    1.1 \n    2013 \n    2014 \n    2015 \n    2016 \n  \n  \n    wage_percent_of_male \n    1242 \n    84 \n    9.4 \n    51 \n    78 \n    91 \n    117 \n  \n\n\n\n\n\nCode\n#Descriptive Statistics for the historical data of the Dependent variable: wage percent of male\nearnings_female %>%\n  group_by(group) %>%\n  filter(str_detect(group,\"Total, 16 years and older\")) %>%\n  sumtable()\n\n\n\n\nSummary Statistics\n \n  \n    Variable \n    N \n    Mean \n    Std. Dev. \n    Min \n    Pctl. 25 \n    Pctl. 75 \n    Max \n  \n \n\n  \n    Year \n    33 \n    1995 \n    9.7 \n    1979 \n    1987 \n    2003 \n    2011 \n  \n  \n    group \n    33 \n     \n     \n     \n     \n     \n     \n  \n  \n    ... Total, 16 years and older \n    33 \n    100% \n     \n     \n     \n     \n     \n  \n  \n    percent \n    33 \n    74 \n    5.7 \n    62 \n    70 \n    79 \n    82 \n  \n\n\n\n\n\n\n\nResearch Question\nThe purpose of the following exploration and analysis is to answer the question: What are the contributing factors that lead to the gender wage gap to be greater or smaller? The factors explored in this particular analysis are age and occupation. Occupation is categorized into broad and detailed categories, and will be explored by comparing female dominated fields compared to male dominated fields.\nThese factors have been explored in the literature, though the question regarding age (Aragao, 2023; Blau & Kahn, 2016; Kochhar, 2023) has been more thoroughly explored than occupation (Wrohlich 2017). Wrohlick, 2017 found that there is less of a wage gap in the public sector when compared to the private sector. The intention of this analysis is to go beyond comparing occupation by private versus public and into male dominated versus female dominated. This analysis has been done considerably less and is an important piece of knowing where to target closing the wage gap.\n\n\nHypothesis\nAccording to the research, the wage gap for a woman widens as she gets older (Aragao, 2023; Blau & Kahn, 2016; Kochhar, 2023), and the wage gap is wider for women in the private sector compared to the public sector (Wrohlich 2017). Based on this literature, we hypothesize that younger woman and women working in traditionally female-dominated fields will experience a smaller wage gap than older women and those working in traditionally male-dominated fields.\n\nWage Gap by Age\nThe graph below shows the average wage gap across age groups. These data show that as women age, the wage gap widens, it takes a particular dip around 35 - 44 years old.\n\n\nCode\n#wage gap by age\nearnings_female %>%\n  group_by(group) %>%\n  filter(!str_detect(group,\"Total, 16 years and older\")) %>%\n  summarize(Mean_Percent_salary = mean(percent))%>%\n  ggplot(aes(group,Mean_Percent_salary)) +\n  geom_col(aes(fill = group)) +\n  labs(title = \"Female Salary Percent of Male Salary\",\n       x=\"Age Group\", \n       y=\"Average Percent of Salary\") +\n  geom_text(aes(y=Mean_Percent_salary, \n                label=sprintf(\"%0.2f\", round(Mean_Percent_salary, digits = 2)))) +\n  theme(axis.text.x = element_text(angle = 45, vjust = 0.5))\n\n\n\n\n\nOne hypothesis for this is that the younger generation is experiencing a more narrow wage gap. And in that case, we would expect to the see wage gap decrease across time.\nThe graph and table below shows the the wage gap across time using the same data. From these visualizations we can conclude that the wage gap did not decrease, and that as women age, the wage gap widens.\n\n\nCode\n#wage gap by year\nSummaryYear <- jobs_gender %>%\n  group_by(year)%>%\n  drop_na(wage_percent_of_male)%>%\n  summarise(Average_Percent_of_Salary = mean(wage_percent_of_male))\n\nkable(SummaryYear)\n\n\n\n\n \n  \n    year \n    Average_Percent_of_Salary \n  \n \n\n  \n    2013 \n    83.77102 \n  \n  \n    2014 \n    83.70873 \n  \n  \n    2015 \n    84.45372 \n  \n  \n    2016 \n    84.19526 \n  \n\n\n\n\n\nCode\njobs_gender %>%\n  group_by(year)%>%\n  drop_na(wage_percent_of_male)%>%\n  summarise(Mean_wage_percent_of_males = mean(wage_percent_of_male))%>%\n  ggplot(aes(fill = year, y = `Mean_wage_percent_of_males`, x=year)) +\n  geom_bar(position = \"dodge\", stat = \"identity\") +\n  labs(title = \"Female Salary Percent of Male Salary by Year\",\n       x=\"Year\", \n       y=\"Average Percent of Salary\") +\n  geom_text(aes(y=Mean_wage_percent_of_males, \n                label=sprintf(\"%0.2f\", round(Mean_wage_percent_of_males, digits = 2)))) +\n  theme(axis.text.x = element_text(angle = 45, vjust = 0.5))\n\n\n\n\n\n\n\nWage Gap by Occupation\nThe visualization below shows the average number of total female workers versus total male workers averaged across 3 year (2013- 2016), by minor job category. The table is sorted in order to see the categories of which women tend to work at the top.\n\n\nCode\n#creating a table of just total workers by gender and category\nJobsCategoryGen<- data_frame(jobs_gender$year, jobs_gender$minor_category, jobs_gender$workers_male, jobs_gender$workers_female)\nJobsCategoryGen<- JobsCategoryGen%>%\n  rename('Year'='jobs_gender$year',\n         'Occupation Category (Minor)' = 'jobs_gender$minor_category',\n         'Total Male Worker' = 'jobs_gender$workers_male',\n         'Total Female Workers' = 'jobs_gender$workers_female')\n\nSummaryJobsCatGen <- JobsCategoryGen%>%\n  group_by(`Occupation Category (Minor)`)%>%\n  summarise(AverageMaleWorkers = mean(`Total Male Worker`), \n            AverageFemaleWorkers = mean(`Total Female Workers`))\n\nkable(SummaryJobsCatGen[order(SummaryJobsCatGen$AverageFemaleWorkers, decreasing=TRUE),])\n\n\n\n\n \n  \n    Occupation Category (Minor) \n    AverageMaleWorkers \n    AverageFemaleWorkers \n  \n \n\n  \n    Education, Training, and Library \n    138707.95 \n    336849.955 \n  \n  \n    Sales and Related \n    324403.33 \n    226652.667 \n  \n  \n    Building and Grounds Cleaning and Maintenance \n    377726.67 \n    188702.792 \n  \n  \n    Office and Administrative Support \n    73641.55 \n    182680.466 \n  \n  \n    Healthcare Support \n    28363.98 \n    169199.227 \n  \n  \n    Management \n    269406.44 \n    167326.808 \n  \n  \n    Community and Social Service \n    87166.78 \n    148718.469 \n  \n  \n    Healthcare Practitioners and Technical \n    54026.68 \n    139826.234 \n  \n  \n    Legal \n    137979.85 \n    138897.400 \n  \n  \n    Food Preparation and Serving Related \n    149773.50 \n    128210.135 \n  \n  \n    Business and Financial Operations \n    96380.12 \n    113663.170 \n  \n  \n    Personal Care and Service \n    33068.60 \n    97494.512 \n  \n  \n    Computer and mathematical \n    169581.77 \n    56163.156 \n  \n  \n    Arts, Design, Entertainment, Sports, and Media \n    56255.49 \n    41269.278 \n  \n  \n    Material Moving \n    146246.23 \n    32834.804 \n  \n  \n    Protective Service \n    113246.58 \n    27095.264 \n  \n  \n    Production \n    58897.09 \n    20524.166 \n  \n  \n    Life, Physical, and Social Science \n    25409.11 \n    19563.148 \n  \n  \n    Transportation \n    171202.30 \n    18166.075 \n  \n  \n    Architecture and Engineering \n    96430.32 \n    15617.167 \n  \n  \n    Farming, Fishing, and Forestry \n    66716.75 \n    13853.938 \n  \n  \n    Installation, Maintenance, and Repair \n    105593.08 \n    3813.965 \n  \n  \n    Construction and Extraction \n    137969.88 \n    3553.678 \n  \n\n\n\n\n\nFrom this visualization, we can see the following are the top 10 occupation categories that are on average more dominated by women:\n(1) Education, Training, and Library\n(2) Sales and Related\n(3) Building and Grounds Cleaning and Maintenance\n(4) Office and Administrative Support\n(5) Healthcare Support\n(6) Management\n(7) Community and Social Service\n(8) Healthcare Practitioners and Technical\n(9) Legal\n(10) Food Preparation and Serving Related\nThe following four visualizations below shows the average percent of salary women make compared to men across occupation categories. The first set of categories in more broad than the second, these are the “Major Categories”, while the more detailed categories are referred to as “Minor Categories”. These categories were created in order to compare the wage gap across occupations in a more digestible way than job title alone.\nBelow we can see that there is a higher wage gap for women working in “Management, Business, and Financial” with women making 80.5% of what men make, and those in”Production, Transportation, and Material Moving” with women making 79% of what men make.\n\n\nCode\n#wage gap by major occupation category\n \nSummaryMajorCat <- jobs_gender%>%\n  group_by(major_category)%>%\n  drop_na(wage_percent_of_male) %>%\n  summarise(Average_Percent_of_Salary = mean(wage_percent_of_male))\n\n#I would like to round these numbers by 2 digits past the decimal, as well as title the table and rename the variables.\nkable(SummaryMajorCat[order(SummaryMajorCat$Average_Percent_of_Salary, SummaryMajorCat$major_category, decreasing=TRUE),])\n\n\n\n\n \n  \n    major_category \n    Average_Percent_of_Salary \n  \n \n\n  \n    Computer, Engineering, and Science \n    86.80344 \n  \n  \n    Service \n    86.56902 \n  \n  \n    Education, Legal, Community Service, Arts, and Media \n    86.24398 \n  \n  \n    Healthcare Practitioners and Technical \n    86.00865 \n  \n  \n    Natural Resources, Construction, and Maintenance \n    85.41490 \n  \n  \n    Sales and Office \n    83.81083 \n  \n  \n    Management, Business, and Financial \n    80.48905 \n  \n  \n    Production, Transportation, and Material Moving \n    79.17389 \n  \n\n\n\n\n\nCode\n#This graph still needs to be reformatted so the title of the graph and the title of legand do not overlap. \njobs_gender %>%\n  group_by(major_category)%>%\n  drop_na(wage_percent_of_male)%>%\n  summarise(Mean_wage_percent_of_males = mean(wage_percent_of_male))%>%\n  ggplot(aes(fill = major_category, y = `Mean_wage_percent_of_males`, x=major_category)) +\n  geom_bar(position = \"dodge\", stat = \"identity\") +\n  labs(title = \"Female Salary Percent of Male Salary by Major Occupation Category\",\n       x=\"Occupation Cateogry\", \n       y=\"Average Percent of Salary\") +\n  geom_text(aes(y=Mean_wage_percent_of_males, \n                label=sprintf(\"%0.2f\", round(Mean_wage_percent_of_males, digits = 1)))) +\n  theme(axis.text.x = element_text(angle = 45, vjust = 0.5))\n\n\n\n\n\nWhen we break these categories down into more detail, we can see more instances of a wider wage gap. Women who work in occupations that fall into the “Community and Social Services” category make, on average, 91% of every dollar a man in their field makes. Whereas a woman working in Production” makes, on average, 77% of every dollar a man in their field makes. There seems to be a bigger increase in wage gap for the following occupational categories: “Management”, “Business and Financial Operations”, “Transportation”, “Farming, Fishing, and Forestry”, “Building and Grounds Cleaning and Maintenance”, “Sales and Related”, “Legal”, and “Production”.\n\n\nCode\n#wagegap by minor occupation category\nSummaryMinorCat <- jobs_gender%>%\n  group_by(minor_category)%>%\n  drop_na(wage_percent_of_male) %>%\n  summarise(Mean_wage_percent_of_males = mean(wage_percent_of_male))\n\nkable(SummaryMinorCat[order(SummaryMinorCat$Mean_wage_percent_of_males, SummaryMinorCat$minor_category, decreasing=FALSE),])\n\n\n\n\n \n  \n    minor_category \n    Mean_wage_percent_of_males \n  \n \n\n  \n    Production \n    77.23993 \n  \n  \n    Legal \n    77.81342 \n  \n  \n    Sales and Related \n    77.83065 \n  \n  \n    Building and Grounds Cleaning and Maintenance \n    79.31545 \n  \n  \n    Farming, Fishing, and Forestry \n    79.61618 \n  \n  \n    Transportation \n    79.88060 \n  \n  \n    Business and Financial Operations \n    80.16418 \n  \n  \n    Management \n    80.80490 \n  \n  \n    Arts, Design, Entertainment, Sports, and Media \n    84.79955 \n  \n  \n    Life, Physical, and Social Science \n    84.94294 \n  \n  \n    Installation, Maintenance, and Repair \n    85.45982 \n  \n  \n    Healthcare Practitioners and Technical \n    86.00865 \n  \n  \n    Protective Service \n    86.02423 \n  \n  \n    Office and Administrative Support \n    86.18804 \n  \n  \n    Personal Care and Service \n    86.73739 \n  \n  \n    Education, Training, and Library \n    87.19622 \n  \n  \n    Food Preparation and Serving Related \n    87.44786 \n  \n  \n    Architecture and Engineering \n    87.59434 \n  \n  \n    Construction and Extraction \n    87.94221 \n  \n  \n    Computer and mathematical \n    88.12798 \n  \n  \n    Material Moving \n    89.10787 \n  \n  \n    Healthcare Support \n    90.31624 \n  \n  \n    Community and Social Service \n    91.43559 \n  \n\n\n\n\n\nCode\n#The visualization below is in VERY draft form, and cannot provide helpful information until it is reformatted. \n#jobs_gender %>%\n  #group_by(minor_category)%>%\n  #drop_na(wage_percent_of_male)%>%\n  #summarise(Mean_wage_percent_of_males = mean(wage_percent_of_male))%>%\n  #ggplot(aes(fill = minor_category, y = `Mean_wage_percent_of_males`, x=minor_category)) +\n  #geom_bar(position = \"dodge\", stat = \"identity\") +\n  #labs(title = \"Female Salary Percent of Male Salary by Minor Occupation Category\",\n       #x=\"Occupation Cateogry\", \n       #y=\"Average Percent of Salary\") +\n  #theme(axis.text.x = element_text(angle = 45, vjust = 0.5))\n\n\nWhen comparing the top 10 minor occupation categories that are on average more dominated by women (“Occupation Categories Dominated by Women”) with the list of occupation categories with the smallest wage gap (“Occupation Categories with the Smallest Wage Gap”), we can see some overlap.\nOccupation Categories Dominated by Women\n\nEducation, Training, and Library\nSales and Related\nBuilding and Grounds Cleaning and Maintenance\nOffice and Administrative Support\nHealthcare Support\nManagement\nCommunity and Social Service\nHealthcare Practitioners and Technical\nLegal\nFood Preparation and Serving Related\n\nOccupation Categories with the Smallest Wage Gap\n\nProduction\nLegal\nSales and Related\nBuilding and Grounds Cleaning and Maintenance\nFarming, Fishing, and Forestry\nTransportation\nBusiness and Financial Operations\nManagement\nArts, Design, Entertainment, Sports, and Media\nLife, Physical, and Social Science\n\nFurther analysis is necessary to know if female dominated occupation categories is a statistically significant factor that contributes to the wage gap."
  },
  {
    "objectID": "posts/FinalProjectCheck1_Diana_Rinker.html#final-project-check-in-1-diana-rinker.",
    "href": "posts/FinalProjectCheck1_Diana_Rinker.html#final-project-check-in-1-diana-rinker.",
    "title": "Final Project check-in (1)",
    "section": "Final Project check-in (1), Diana Rinker.",
    "text": "Final Project check-in (1), Diana Rinker."
  },
  {
    "objectID": "posts/Rowley_Final_Project_Check-In_1.html",
    "href": "posts/Rowley_Final_Project_Check-In_1.html",
    "title": "Final Project - Check-In 1",
    "section": "",
    "text": "Backgroun and Research Question:\nMy research question will focus on the cross-country correlation between income and democracy. A 2008 study titled “Income and Democracy,” published in the American Economic Review, argues that existing studies that establish a strong cross-country correlation between income and democracy do not control for factors that simultaneously affect both variables. Accordingly, this study controls for certain country-fixed effects—such as date of independence, constraints on the executive, and religious affiliation—which thereby removes the statistical association between income per capita and various measures of democracy. This study is the source for my data set.\nIn contrast, a 1999 study written by Robert J. Barro asserted that improvements in the standard of living predict increase in democracy. However, similar to the argument in “Income and Democracy,” this study found that the allowance of certain economic variables weakens the interplay specifically between democracy and religious affiliation. Nevertheless, Barro claimed that the negative effects from Muslim and non‐religious affiliations remain regardless of control factors.\nThis incongruity led me to wonder whether we would see a correlation between income and democracy if the economic variables used in the two studies were both updated and more aligned. Specifically, I would like to examine how this would affect both studies’ claims on the role of religious affiliation; in other words, will adding control variables such as education shift the correlation between religious affiliation—though only for Islam and non-religious affiliations—and democracy?\nResearch question: How will adding education and non-religious affliation as a control variables impact the correlation between religious affiliation and democracy?\n\n\nHypothesis:\nAfter reviewing “Income and Democracy,” it does not appear that non-religious affiliation was integrated into the report. Additionally, the authors indicated that education was determined to be statistically insignificant as an independent country-fixed effect within the context of its causal effect on democracy. However, I am curious about how the inclusion of these two variables would affect Barro’s conclusion relative to the correlation between religious affiliation and democracy. As such, by adding these variables and updating existing variables with new data, I will be revisiting a previously-tested hypothesis.\nDespite my curiosity, I hypothesize that adding education and non-religious affiliation as control variables will not uncover any statistical significance between religious affiliation and democracy, even when narrowing the scope of religious affiliation to focus solely on Islam and non-religious affiliation.\n\n\nDescriptive Statistics:\nData for this study was collected from the Freedom House Political Rights Index, the Polity Composite Democracy Index, and data from other studies conducted by Barro and Kenneth A. Bollen.\nVariables I will be focusing on include:\n\nCountry;\nConstraint on the executive;\nYear of independence;\nSettler mortality;\nPopulation density;\nCatholic population;\nMuslim population;\nProtestant population;\nEducation;\nShift in per capita income; and\nShift in democracy.\n\n\n\nCode\n# load libraries:\n\nlibrary(tidyverse)\n\n\nWarning: package 'tidyverse' was built under R version 4.2.2\n\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.3.6      ✔ purrr   0.3.4 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.4.1 \n✔ readr   2.1.4      ✔ forcats 0.5.2 \n\n\nWarning: package 'readr' was built under R version 4.2.2\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\nCode\nlibrary(readxl)\n\n\nWarning: package 'readxl' was built under R version 4.2.2\n\n\n\n\nCode\n# read in data file:\n\ndata_file <- read_excel(\"C:/Users/caitr/OneDrive/Documents/DACSS/DACSS 603/603_Spring_2023/posts/Final Project/Income-Democracy.xls\", sheet = \"500 Year Panel\") \nhead(data_file)\n\n\n# A tibble: 6 × 15\n  code  country     consf…¹ indcent indyear logem4 lpd15…² madid rel_c…³ rel_m…⁴\n  <chr> <chr>         <dbl>   <dbl>   <dbl>  <dbl>   <dbl> <dbl>   <dbl>   <dbl>\n1 ADO   Andorra      NA        18      1800  NA     NA      1001  NA      NA    \n2 AFG   Afghanistan   0        19.2    1919   4.54   2.12   3002   0       0.993\n3 AGO   Angola        0.333    19.8    1975   5.63   0.405  2011   0.687   0    \n4 ALB   Albania       0.667    19.1    1912  NA      1.99   2009  NA      NA    \n5 ARE   United Ara…   0.333    19.7    1971  NA      0      3002   0.004   0.949\n6 ARG   Argentina     0        18.2    1816   4.23  -2.21   5001   0.916   0.002\n# … with 5 more variables: rel_protmg80 <dbl>, growth <dbl>, democ <dbl>,\n#   world <dbl>, colony <dbl>, and abbreviated variable names ¹​consfirstaug,\n#   ²​lpd1500s, ³​rel_catho80, ⁴​rel_muslim80\n\n\n\n\nCode\n# remove dummy/unnecessary variables (as identified in study's variable key):\n\ndata_cln = subset(data_file, select = -c(code, world, colony, indcent, madid))\nhead(data_cln)\n\n\n# A tibble: 6 × 10\n  country   consf…¹ indyear logem4 lpd15…² rel_c…³ rel_m…⁴ rel_p…⁵  growth democ\n  <chr>       <dbl>   <dbl>  <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl> <dbl>\n1 Andorra    NA        1800  NA     NA      NA      NA      NA      3.46   NA   \n2 Afghanis…   0        1919   4.54   2.12    0       0.993   0     -0.0849  0.15\n3 Angola      0.333    1975   5.63   0.405   0.687   0       0.198  0.644   0.35\n4 Albania     0.667    1912  NA      1.99   NA      NA      NA      1.68    0.75\n5 United A…   0.333    1971  NA      0       0.004   0.949   0.003  3.37    0.1 \n6 Argentina   0        1816   4.23  -2.21    0.916   0.002   0.027  2.71    0.9 \n# … with abbreviated variable names ¹​consfirstaug, ²​lpd1500s, ³​rel_catho80,\n#   ⁴​rel_muslim80, ⁵​rel_protmg80\n\n\n\n\nCode\n# remove duplicates:\n\nduplicates <- duplicated(data_cln)\nduplicates[\"TRUE\"]\n\n\n[1] NA\n\n\n\n\nCode\n# remove blank observations (observations with some NAs are not removed):\n\ndata_blank <- data_cln[rowSums(is.na(data_cln)) != ncol(data_cln), ]\nhead(data_blank)\n\n\n# A tibble: 6 × 10\n  country   consf…¹ indyear logem4 lpd15…² rel_c…³ rel_m…⁴ rel_p…⁵  growth democ\n  <chr>       <dbl>   <dbl>  <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl> <dbl>\n1 Andorra    NA        1800  NA     NA      NA      NA      NA      3.46   NA   \n2 Afghanis…   0        1919   4.54   2.12    0       0.993   0     -0.0849  0.15\n3 Angola      0.333    1975   5.63   0.405   0.687   0       0.198  0.644   0.35\n4 Albania     0.667    1912  NA      1.99   NA      NA      NA      1.68    0.75\n5 United A…   0.333    1971  NA      0       0.004   0.949   0.003  3.37    0.1 \n6 Argentina   0        1816   4.23  -2.21    0.916   0.002   0.027  2.71    0.9 \n# … with abbreviated variable names ¹​consfirstaug, ²​lpd1500s, ³​rel_catho80,\n#   ⁴​rel_muslim80, ⁵​rel_protmg80\n\n\n\n\nCode\n# remove some NAs for description but not analysis:\n\ndata_NA <- data_cln[rowSums(is.na(data_cln)) == 0, ]\ndim(data_NA)\n\n\n[1] 76 10\n\n\n\n\nCode\n# confirm data frame size of clean data set:\n\ndim(data_cln)\n\n\n[1] 173  10\n\n\nWe can see that this data set has 10 variables and 173 observations (though there will be 12 variables once I collect and add data related to education and non-religious affiliation). There are no duplicate observations, nor are there any blank observations. However, in the case that we remove observations with any missing values, the data set would only have 76 observations. Nonetheless, because the study’s authors elected to utilize incomplete observations, I will do the same.\n\n\nCode\n# summary of data (remove categorical variables):\n\nlibrary(summarytools)\n\n\nWarning: package 'summarytools' was built under R version 4.2.2\n\n\n\nAttaching package: 'summarytools'\n\n\nThe following object is masked from 'package:tibble':\n\n    view\n\n\nCode\nsummary <- subset(data_cln, select = -c(country))\ndfSummary(summary)\n\n\nData Frame Summary  \nsummary  \nDimensions: 173 x 9  \nDuplicates: 1  \n\n--------------------------------------------------------------------------------------------------------------\nNo   Variable       Stats / Values            Freqs (% of Valid)    Graph                 Valid      Missing  \n---- -------------- ------------------------- --------------------- --------------------- ---------- ---------\n1    consfirstaug   Mean (sd) : 0.4 (0.4)     38 distinct values    :                     150        23       \n     [numeric]      min < med < max:                                :     :           .   (86.7%)    (13.3%)  \n                    0 < 0.3 < 1                                     :     :           :                       \n                    IQR (CV) : 0.7 (0.9)                            : .   :           :                       \n                                                                    : : . : . . .     :                       \n\n2    indyear        Mean (sd) : 1911.8 (67)   65 distinct values                    :     173        0        \n     [numeric]      min < med < max:                                .               : :   (100.0%)   (0.0%)   \n                    1800 < 1947 < 1984                              :               : :                       \n                    IQR (CV) : 134 (0)                              :           . . : :                       \n                                                                    : : . .   . : : : :                       \n\n3    logem4         Mean (sd) : 4.6 (1.3)     43 distinct values            :             88         85       \n     [numeric]      min < med < max:                                        :             (50.9%)    (49.1%)  \n                    0.9 < 4.5 < 8                                           : :                               \n                    IQR (CV) : 1.4 (0.3)                                    : :                               \n                                                                        : : : : : .                           \n\n4    lpd1500s       Mean (sd) : 1.1 (1.6)     98 distinct values            :             151        22       \n     [numeric]      min < med < max:                                        :             (87.3%)    (12.7%)  \n                    -3.8 < 1.1 < 5.6                                        : : :                             \n                    IQR (CV) : 2.2 (1.4)                                  : : : : :                           \n                                                                      .   : : : : : .                         \n\n5    rel_catho80    Mean (sd) : 0.3 (0.4)     107 distinct values   :                     152        21       \n     [numeric]      min < med < max:                                :                     (87.9%)    (12.1%)  \n                    0 < 0.1 < 1                                     :                                         \n                    IQR (CV) : 0.6 (1.1)                            :                 .                       \n                                                                    : : . : . .     . :                       \n\n6    rel_muslim80   Mean (sd) : 0.2 (0.4)     85 distinct values    :                     152        21       \n     [numeric]      min < med < max:                                :                     (87.9%)    (12.1%)  \n                    0 < 0 < 1                                       :                                         \n                    IQR (CV) : 0.4 (1.5)                            :                                         \n                                                                    : .     .       . :                       \n\n7    rel_protmg80   Mean (sd) : 0.1 (0.2)     80 distinct values    :                     151        22       \n     [numeric]      min < med < max:                                :                     (87.3%)    (12.7%)  \n                    0 < 0 < 1                                       :                                         \n                    IQR (CV) : 0.2 (1.7)                            :                                         \n                                                                    : . . . .                                 \n\n8    growth         Mean (sd) : 2 (1.1)       143 distinct values               :         172        1        \n     [numeric]      min < med < max:                                    .   :   :   .     (99.4%)    (0.6%)   \n                    -0.6 < 1.9 < 4.3                                    : : :   :   :                         \n                    IQR (CV) : 1.6 (0.5)                                : : : : : . :                         \n                                                                      : : : : : : : : .                       \n\n9    democ          Mean (sd) : 0.7 (0.3)     21 distinct values                    . :   135        38       \n     [numeric]      min < med < max:                                                : :   (78.0%)    (22.0%)  \n                    0 < 0.8 < 1                                       .             : :                       \n                    IQR (CV) : 0.6 (0.5)                            . :   .       : : :                       \n                                                                    : : : : . : . : : :                       \n--------------------------------------------------------------------------------------------------------------\n\n\nWe can see here a data frame containing summary statistics for the 9 variables with numeric data, as the categorical variable simply indicates the country name. These statistics will be more meaningful upon following the addition of data related to education and non-religious affiliation.\n\n\nSources:\n\nURL for data: https://www.openicpsr.org/openicpsr/project/113251/version/V1/view?path=/openicpsr/113251/fcr:versions/V1/Income-and-Democracy-Data-AER-adjustment.xls&type=file\nURL for study: http://homepage.ntu.edu.tw/~kslin/macro2009/Acemoglu%20et%20al%202008.pdf\nURL for external references: https://www.jstor.org/stable/10.1086/250107"
  },
  {
    "objectID": "posts/Kristin_Abijaoude_HW1.html",
    "href": "posts/Kristin_Abijaoude_HW1.html",
    "title": "Hw 1 by Kristin Abijaoude",
    "section": "",
    "text": "Code\nlibrary(ggplot2)\nlibrary(readxl)\nlibrary(dplyr)\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nCode\nlibrary(summarytools)\nlibrary(stats)\n\n\n\nLung Capacity\n\n\nCode\nLungCapData <- read_excel(\"~/Documents/GitHub/Github Help/603_Spring_2023/posts/_data/LungCapData.xls\")\nLungCapData\n\n\n# A tibble: 725 × 6\n   LungCap   Age Height Smoke Gender Caesarean\n     <dbl> <dbl>  <dbl> <chr> <chr>  <chr>    \n 1    6.48     6   62.1 no    male   no       \n 2   10.1     18   74.7 yes   female no       \n 3    9.55    16   69.7 no    female yes      \n 4   11.1     14   71   no    male   no       \n 5    4.8      5   56.9 no    male   no       \n 6    6.22    11   58.7 no    female no       \n 7    4.95     8   63.3 no    male   yes      \n 8    7.32    11   70.4 no    male   no       \n 9    8.88    15   70.5 no    male   no       \n10    6.8     11   59.2 no    male   no       \n# … with 715 more rows\n\n\n\n\nCode\nprint(dfSummary(LungCapData,\n                        varnumbers = FALSE,\n                        plain.ascii  = FALSE, \n                        style        = \"grid\", \n                        graph.magnif = 0.70, \n                        valid.col    = FALSE),\n      method = 'render',\n      table.classes = 'table-condensed')\n\n\n\n\nData Frame Summary\nLungCapData\nDimensions: 725 x 6\n  Duplicates: 0\n\n\n  \n    \n      Variable\n      Stats / Values\n      Freqs (% of Valid)\n      Graph\n      Missing\n    \n  \n  \n    \n      LungCap\n[numeric]\n      Mean (sd) : 7.9 (2.7)min ≤ med ≤ max:0.5 ≤ 8 ≤ 14.7IQR (CV) : 3.7 (0.3)\n      342 distinct values\n      \n      0\n(0.0%)\n    \n    \n      Age\n[numeric]\n      Mean (sd) : 12.3 (4)min ≤ med ≤ max:3 ≤ 13 ≤ 19IQR (CV) : 6 (0.3)\n      17 distinct values\n      \n      0\n(0.0%)\n    \n    \n      Height\n[numeric]\n      Mean (sd) : 64.8 (7.2)min ≤ med ≤ max:45.3 ≤ 65.4 ≤ 81.8IQR (CV) : 10.4 (0.1)\n      274 distinct values\n      \n      0\n(0.0%)\n    \n    \n      Smoke\n[character]\n      1. no2. yes\n      648(89.4%)77(10.6%)\n      \n      0\n(0.0%)\n    \n    \n      Gender\n[character]\n      1. female2. male\n      358(49.4%)367(50.6%)\n      \n      0\n(0.0%)\n    \n    \n      Caesarean\n[character]\n      1. no2. yes\n      561(77.4%)164(22.6%)\n      \n      0\n(0.0%)\n    \n  \n\nGenerated by summarytools 1.0.1 (R version 4.2.2)2023-03-16\n\n\n\n\n\nCode\ncolnames(LungCapData)\n\n\n[1] \"LungCap\"   \"Age\"       \"Height\"    \"Smoke\"     \"Gender\"    \"Caesarean\"\n\n\nCode\ndim(LungCapData)\n\n\n[1] 725   6\n\n\n\n\nCode\nhist(LungCapData$LungCap)\n\n\n\n\n\n1a. The distribution looks pretty normal to me, with capacity between 6 and 9 being the most frequent.\n\n\nCode\nboxplot(LungCap ~ Gender, data=LungCapData)\n\n\n\n\n\n1b. Separating the two genders, it looks like men have a higher lung capacity rate in comparison to women.\n\n\nCode\nLungCapData %>%\n  group_by(Smoke) %>%\n  summarise(mean = mean(LungCap), n = n())\n\n\n# A tibble: 2 × 3\n  Smoke  mean     n\n  <chr> <dbl> <int>\n1 no     7.77   648\n2 yes    8.65    77\n\n\n1c. The average lung capacity for a non-smoker is around 7.78, while for smokers it’s 8.65. In other words, on average, the smokers have a higher lung capacity rate than non-smokers… this doesn’t make sense because smoking is supposed to be bad for your lungs.\n\n\nCode\nagegroup <- LungCapData %>%\n  mutate(agegroup = case_when(Age <= 13 ~ \"Less than 13 years old\",\n                              Age == 14| Age == 15 ~ \"14 to 15 years old\",\n                              Age == 16 | Age == 17 ~ \"16 to 17 years old\",\n                              Age >= 18 ~ \"18 years old and older\"))\nagegroup %>%\n  ggplot(aes(x=LungCap, fill=Smoke)) +\n  geom_histogram() +\n  facet_wrap(~agegroup)\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n1d. Obviously, older teens are more likely to be smokers, as well as have higher lung capacity, than younger teens. The vast majority of teens 13 years and younger are non-smoker (I would be horrified at the sight of a kid smoking).\n\n\nCode\nagegroup <- agegroup %>%\n  mutate(AgeGroup = factor(agegroup, level= c(\"Less than 13 years old\", \n                                              \"14 to 15 years old\",\n                                              \"16 to 17 years old\",\n                                              \"18 years old and older\")))\n\nboxplot(LungCap ~ AgeGroup, data=agegroup)\n\n\n\n\n\n1e. There is a correlation between age and lung capacity. The lung capacity rate increases as the person gets older.\n\n\nPrior Convictions\nAnother dataset I created here deals with prison convictions. The sample size is 810 prisoners in a state prison, some of the prisoners are there for the first time, while others have been imprison as many as 4 times, or have 4 prior convictions in other words. prior means numbers of prior convictions. freq means how many prisoners have a set of convictions (434 prisoners have 1 prior convictions, 160 prisoners have 2 prior convictions etc.). Finally, I created a new variable called probability, where I divided the freq variable by the total number of prisoners, to denote the probability that a prisoner had a certain number of prior convictions.\n\n\nCode\ndf <- data.frame(prior = c(0:4), \n                 freq = c(128, 434, 160, 64, 24)\n                 )\n\ndf <- df %>%\n  mutate(probability = freq/810)\ndf\n\n\n  prior freq probability\n1     0  128  0.15802469\n2     1  434  0.53580247\n3     2  160  0.19753086\n4     3   64  0.07901235\n5     4   24  0.02962963\n\n\n\n\nCode\n# alternatively\n(dbinom(x = 1, size = 1, prob = 160/810))*100\n\n\n[1] 19.75309\n\n\n2a. There is a less than 20% probability that a randomly selected inmate has exactly 2 prior convictions.\n\n\nCode\n128 + 434\n\n\n[1] 562\n\n\nCode\n(dbinom(x = 1, size = 1, prob = 562/810))*100\n\n\n[1] 69.38272\n\n\n2b. There is a 69% probability that a randomly selected inmate has fewer than 2 prior convictions.\n\n\nCode\n128 + 434 + 160\n\n\n[1] 722\n\n\nCode\n(dbinom(x = 1, size = 1, prob = 722/810))*100\n\n\n[1] 89.1358\n\n\n2c. There is a 89% probability that a randomly selected inmate has 2 or fewer prior convictions.\n\n\nCode\n64 + 24\n\n\n[1] 88\n\n\nCode\n(dbinom(x = 1, size = 1, prob = 88/810))*100\n\n\n[1] 10.8642\n\n\n2d. There is a 10% probability that a randomly selected inmate has more than 2 prior convictions.\n\n\nCode\nprior <- df$prior\nprob <- df$probability\nfreq <- df$freq\n\nexval <- sum(prior*prob)\nexval\n\n\n[1] 1.28642\n\n\n2e. The expected value exval, or long term mean, is 1.28642. I separated the variables into its own set and multiplied prior (# of prior convictions) and prob (the probability a given prisoner has a certain number of prior convictions).\n\n\nCode\n# variance\nvar(rep(df$prior, df$freq))\n\n\n[1] 0.8572937\n\n\nCode\n# standard deviation\nsd(rep(df$prior, df$freq))\n\n\n[1] 0.9259016\n\n\nThe variance is 0.86, which mean the data is close to one another.\nThe standard deviation is 0.93, which means the data is more clustered around the mean.\n\n\nCode\nrender(\"Kristin_Abijaoude_HW1.qmd\", output_format = \"pdf_document\", output_file = \"Kristin_Abijaoude_HW1.pdf\")\n\n\nError in render(\"Kristin_Abijaoude_HW1.qmd\", output_format = \"pdf_document\", : could not find function \"render\""
  },
  {
    "objectID": "posts/ZhiyuanZhou_HW1.html",
    "href": "posts/ZhiyuanZhou_HW1.html",
    "title": "Homework 1",
    "section": "",
    "text": "Code\nlibrary(dplyr)\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union"
  },
  {
    "objectID": "posts/ZhiyuanZhou_HW1.html#a",
    "href": "posts/ZhiyuanZhou_HW1.html#a",
    "title": "Homework 1",
    "section": "a",
    "text": "a\nFirst, let’s read in the data from the Excel file:\n\n\nCode\nlibrary(readxl)\ndf <- read_excel(\"_data/LungCapData.xls\")\n\n\nThe distribution of LungCap looks as follows:\n\n\nCode\nhist(df$LungCap)\n\n\n\n\n\nThe histogram suggests that the distribution is close to a normal distribution. Most of the observations are close to the mean. Very few observations are close to the margins (0 and 15).\n##b\n\n\nCode\nboxplot(df$LungCap~df$Gender,\nmain = \"Lung Capacity by Gender\",\nxlab = \"Gender\",\nylab = \"Lung Capacity\",\n)\n\n\n\n\n\n##c\n\n\nCode\ndf %>%\n  group_by(Smoke) %>%\n  summarize(mean = mean(LungCap))\n\n\n# A tibble: 2 × 2\n  Smoke  mean\n  <chr> <dbl>\n1 no     7.77\n2 yes    8.65\n\n\nThis result surprised me that smokers have more lung capacity than non-smokers.\n##d\n\n\nCode\ndf[\"AgeGroup\"] = \n  cut(df$Age,\n      c(0, 13, 15, 17, Inf),\n      c(\"<=13\", \"14-15\",\"16-17\", \">=18\"),\n      right = T\n  )\n\ndf%>%\n  group_by(AgeGroup, Smoke)%>%\n  summarize(meanLungCap = mean(LungCap), meanAge = mean(Age), count = n())\n\n\n`summarise()` has grouped output by 'AgeGroup'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 8 × 5\n# Groups:   AgeGroup [4]\n  AgeGroup Smoke meanLungCap meanAge count\n  <fct>    <chr>       <dbl>   <dbl> <int>\n1 <=13     no           6.36    9.49   401\n2 <=13     yes          7.20   11.7     27\n3 14-15    no           9.14   14.5    105\n4 14-15    yes          8.39   14.6     15\n5 16-17    no          10.5    16.4     77\n6 16-17    yes          9.38   16.6     20\n7 >=18     no          11.1    18.5     65\n8 >=18     yes         10.5    18.1     15\n\n\n##e In age group “0-13”, smokers have higher lung capacity than non-smokers. In all other groups, smokers have less lung capacity than non-smokers. The number of samples under 13 gave it a clue about the interesting finding in 1c. And the mean age difference among smokers and non-smokers pointed out that the age difference is more likely to be the reason of higher lung capacity instead of smoking.\n#Question 2\n##a\n\n\nCode\nprob_2 <- (160 / 810)\nprob_2\n\n\n[1] 0.1975309\n\n\n##b\n\n\nCode\nprob_fewer2 <- (128 + 434) / 810\nprob_fewer2\n\n\n[1] 0.6938272\n\n\n##c\n\n\nCode\nprob_2OrFewer <- (128 + 434 + 160) / 810\nprob_2OrFewer\n\n\n[1] 0.891358\n\n\n##d\n\n\nCode\nprob_more2 <- (64 + 24) / 810\nprob_more2\n\n\n[1] 0.108642\n\n\n##e\n\n\nCode\nexpectation <- (0 * 128 + 1 * 434 + 2 * 160 + 3 * 64 + 4 * 24) / 810\nexpectation\n\n\n[1] 1.28642\n\n\n##f\n\n\nCode\nvariance <- sum(128 * (0 - expectation) ^ 2,\n                434 * (1 - expectation) ^ 2,\n                160 * (2 - expectation) ^ 2,\n                64 * (3 - expectation) ^ 2,\n                24 * (4 - expectation) ^ 2) / 810\nvariance\n\n\n[1] 0.8562353\n\n\nCode\nsd <- sqrt(variance)\nsd\n\n\n[1] 0.9253298"
  },
  {
    "objectID": "posts/HW1_Guanhua_Tan.html",
    "href": "posts/HW1_Guanhua_Tan.html",
    "title": "Homework 1",
    "section": "",
    "text": "First, let’s read in the data from the Excel file:\n\n\nCode\nlibrary(readxl)\nlibrary(tidyverse)\n\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.0      ✔ purrr   0.3.5 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.4.1 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\nCode\nlibrary(ggplot2)\ndf <- read_excel(\"_data/LungCapData.xls\")\nhead(df)\n\n\n# A tibble: 6 × 6\n  LungCap   Age Height Smoke Gender Caesarean\n    <dbl> <dbl>  <dbl> <chr> <chr>  <chr>    \n1    6.48     6   62.1 no    male   no       \n2   10.1     18   74.7 yes   female no       \n3    9.55    16   69.7 no    female yes      \n4   11.1     14   71   no    male   no       \n5    4.8      5   56.9 no    male   no       \n6    6.22    11   58.7 no    female no       \n\n\nThe distribution of LungCap looks as follows:\n\n\nCode\nhist(df$LungCap)\n\n\n\n\n\nThe histogram suggests that the distribution is close to a normal distribution. Most of the observations are close to the mean. Very few observations are close to the margins (0 and 15).\n\n\n\n\n\nCode\ndf %>%\n  ggplot(aes(x=Gender,y=LungCap))%+%\n  stat_boxplot(geom = \"errorbar\", # Error bars\n               width = 0.2)+\n  geom_boxplot()\n\n\n\n\n\nThe box graphic suggests that the median of male lung capacities are slightly larger than the one of female ones.\n\n\n\n\n\nCode\ndf_c <- df %>%\n  group_by(Smoke) %>%\n  mutate(mean_lungcap=mean(LungCap))%>%\n  distinct(mean_lungcap)\ndf_c \n\n\n# A tibble: 2 × 2\n# Groups:   Smoke [2]\n  Smoke mean_lungcap\n  <chr>        <dbl>\n1 no            7.77\n2 yes           8.65\n\n\nThe data indicates smokers’ lung capacities are larger than no-smokers’ ones. It runs counter to the intuition.\n\n\n\n\n\nCode\n# less than or equal to 13\ndf_d_13<-df %>%\n  filter(Smoke == \"yes\" & Age <= 13) %>%\n  mutate(mean_lungcap=mean(LungCap)) %>%\n  distinct(mean_lungcap)\n\n\ndf_d_14_15 <-df %>%\n   filter(Smoke == \"yes\" & Age <= 15 | Age >= 14) %>%\n  mutate(mean_lungcap=mean(LungCap)) %>%\n  distinct(mean_lungcap)\n\n\ndf_d_16_17 <-df %>%\n   filter(Smoke == \"yes\" & Age <= 17 | Age >= 16) %>%\n  mutate(mean_lungcap=mean(LungCap)) %>%\n  distinct(mean_lungcap)\n\n\ndf_d_18 <-df %>%\n   filter(Smoke == \"yes\" & Age >= 18) %>%\n  mutate(mean_lungcap=mean(LungCap)) %>%\n  distinct(mean_lungcap)\nresult <-c(df_d_13, df_d_14_15, df_d_16_17, df_d_18)\nprint(result)\n\n\n$mean_lungcap\n[1] 7.201852\n\n$mean_lungcap\n[1] 9.725077\n\n$mean_lungcap\n[1] 10.00616\n\n$mean_lungcap\n[1] 10.51333\n\n\nThe data indicates that with the increase of the age, the lung capacities grows larger.\n\n\n\n\n\nCode\ndf_e_13<-df %>%\n  filter(Age <= 13) %>%\n  group_by(Smoke) %>%\n  mutate(mean_lungcap=mean(LungCap)) %>%\n  distinct(mean_lungcap)\n\ndf_e_14<-df %>%\n  filter(Age == 15 | Age == 14) %>%\n  group_by(Smoke) %>%\n  mutate(mean_lungcap=mean(LungCap)) %>%\n  distinct(mean_lungcap)\n\n\ndf_e_16<-df %>%\n  filter(Age == 17 | Age == 16) %>%\n  group_by(Smoke) %>%\n  mutate(mean_lungcap=mean(LungCap)) %>%\n  distinct(mean_lungcap)\n\n\ndf_e_18<-df %>%\n  filter( Age >= 18) %>%\n  group_by(Smoke) %>%\n  mutate(mean_lungcap=mean(LungCap)) %>%\n  distinct(mean_lungcap)\n\ndf_e_13\n\n\n# A tibble: 2 × 2\n# Groups:   Smoke [2]\n  Smoke mean_lungcap\n  <chr>        <dbl>\n1 no            6.36\n2 yes           7.20\n\n\nCode\ndf_e_14\n\n\n# A tibble: 2 × 2\n# Groups:   Smoke [2]\n  Smoke mean_lungcap\n  <chr>        <dbl>\n1 no            9.14\n2 yes           8.39\n\n\nCode\ndf_e_16\n\n\n# A tibble: 2 × 2\n# Groups:   Smoke [2]\n  Smoke mean_lungcap\n  <chr>        <dbl>\n1 no           10.5 \n2 yes           9.38\n\n\nCode\ndf_e_18\n\n\n# A tibble: 2 × 2\n# Groups:   Smoke [2]\n  Smoke mean_lungcap\n  <chr>        <dbl>\n1 yes           10.5\n2 no            11.1\n\n\nThe shows a big difference from the part C. Only in age group under 13, smokers have larger lung capacities than non-smokers. In other age groups, unlike what the part C suggests, non-smokers have large lung capacities that smokers."
  },
  {
    "objectID": "posts/HW1_Guanhua_Tan.html#a-what-is-the-probability-that-a-randomly-selected-inmate-has-exactly-2-prior-convictions",
    "href": "posts/HW1_Guanhua_Tan.html#a-what-is-the-probability-that-a-randomly-selected-inmate-has-exactly-2-prior-convictions",
    "title": "Homework 1",
    "section": "a) What is the probability that a randomly selected inmate has exactly 2 prior convictions?",
    "text": "a) What is the probability that a randomly selected inmate has exactly 2 prior convictions?\n\n\nCode\nc <- 160/810\nc\n\n\n[1] 0.1975309\n\n\nThe probability is 19.8%."
  },
  {
    "objectID": "posts/HW1_Guanhua_Tan.html#b-what-is-the-probability-that-a-randomly-selected-inmate-has-fewer-than-2-prior-convictions",
    "href": "posts/HW1_Guanhua_Tan.html#b-what-is-the-probability-that-a-randomly-selected-inmate-has-fewer-than-2-prior-convictions",
    "title": "Homework 1",
    "section": "b) What is the probability that a randomly selected inmate has fewer than 2 prior convictions?",
    "text": "b) What is the probability that a randomly selected inmate has fewer than 2 prior convictions?\n\n\nCode\nc<-(128+434)/810\nc\n\n\n[1] 0.6938272\n\n\nThe probability is 69.4%."
  },
  {
    "objectID": "posts/HW1_Guanhua_Tan.html#c-what-is-the-probability-that-a-randomly-selected-inmate-has-2-or-fewer-prior-convictions",
    "href": "posts/HW1_Guanhua_Tan.html#c-what-is-the-probability-that-a-randomly-selected-inmate-has-2-or-fewer-prior-convictions",
    "title": "Homework 1",
    "section": "c) What is the probability that a randomly selected inmate has 2 or fewer prior convictions?",
    "text": "c) What is the probability that a randomly selected inmate has 2 or fewer prior convictions?\n\n\nCode\nc <- (434+160+128)/810\nc\n\n\n[1] 0.891358\n\n\nThe probability is 89.1%."
  },
  {
    "objectID": "posts/HW1_Guanhua_Tan.html#d-what-is-the-probability-that-a-randomly-selected-inmate-has-more-than-2-prior-convictions",
    "href": "posts/HW1_Guanhua_Tan.html#d-what-is-the-probability-that-a-randomly-selected-inmate-has-more-than-2-prior-convictions",
    "title": "Homework 1",
    "section": "d) What is the probability that a randomly selected inmate has more than 2 prior convictions?",
    "text": "d) What is the probability that a randomly selected inmate has more than 2 prior convictions?\n\n\nCode\nc<-(64+24)/810\nc\n\n\n[1] 0.108642\n\n\nThe probability is 10.9%."
  },
  {
    "objectID": "posts/HW1_Guanhua_Tan.html#e-what-is-the-expected-value1-for-the-number-of-prior-convictions",
    "href": "posts/HW1_Guanhua_Tan.html#e-what-is-the-expected-value1-for-the-number-of-prior-convictions",
    "title": "Homework 1",
    "section": "e) What is the expected value1 for the number of prior convictions?",
    "text": "e) What is the expected value1 for the number of prior convictions?\n\n\nCode\nvals<-c(0,1,2,3,4)\nprobs<-c(128/810, 434/810, 160/810, 64/801, 24/810)\nexv<-weighted.mean(vals, probs)\nexv\n\n\n[1] 1.28794\n\n\nThe expected value is 1.29.\n\nCalculate the variance and the standard deviation for the Prior Convictions.\n\n\n\nCode\nvar <- sum((vals-exv)^2*probs)\nvar\n\n\n[1] 0.8588399\n\n\nCode\nsd <- sqrt(var)\nsd\n\n\n[1] 0.9267361\n\n\nThe variance is 0.8588. The standard deviation is 0.9267."
  },
  {
    "objectID": "posts/HW2_Saisrinivas_ Ambatipudi.html",
    "href": "posts/HW2_Saisrinivas_ Ambatipudi.html",
    "title": "Homework 2",
    "section": "",
    "text": "Code\ns_p <- c(\"Bypass\", \"Angiography\")\ns_s <- c(539, 847)\nm_w_t <- c(19, 18)\ns_d <- c(10, 9)\n\nsurgery <- data.frame(s_p, s_s, m_w_t, s_d)\n\n# 90% confidence interval\n\nc_l <- 0.90\n\n# Tail area\n\nt_a <- (1-c_l)/2\n\n# t_score Calculation\n\nt_s <- qt(p = 1-t_a, df = s_s-1)\n\n# Standard Error Calculation\n\ns_e <- s_d/sqrt(s_s)\n\n# Confidence Interval Calculation for Bypass and Angiography\n\nc_i <- c(m_w_t - t_s * s_e,\n        m_w_t + t_s * s_e)\n\nc_i\n\n\n[1] 18.29029 17.49078 19.70971 18.50922\n\n\nThe Cardiac Care Network analyzed wait times for cardiac procedures in Ontario, constructing a 90% confidence interval using the t-distribution. For bypass surgery, the true mean wait time is estimated to be between 18.29 and 19.71 days, while for angiography it is between 17.49 and 18.51 days. The bypass surgery interval is slightly wider, indicating more uncertainty in the estimate. These confidence intervals help inform decisions on resource allocation and patient care by indicating the likely range of true mean wait times."
  },
  {
    "objectID": "posts/HW2_Saisrinivas_ Ambatipudi.html#q2",
    "href": "posts/HW2_Saisrinivas_ Ambatipudi.html#q2",
    "title": "Homework 2",
    "section": "Q2",
    "text": "Q2\n\n\nCode\nprop.test(567, 1031, conf.level = .95)\n\n\n\n    1-sample proportions test with continuity correction\n\ndata:  567 out of 1031, null probability 0.5\nX-squared = 10.091, df = 1, p-value = 0.00149\nalternative hypothesis: true p is not equal to 0.5\n95 percent confidence interval:\n 0.5189682 0.5805580\nsample estimates:\n        p \n0.5499515 \n\n\nThe National Center for Public Policy surveyed 1031 adult Americans, finding that 54.99% believe a college education is essential for success. A 95% confidence interval for this proportion is [0.5189682, 0.5805580], suggesting that between 51.90% and 58.06% of adult Americans hold this belief. Since the interval doesn’t include 0.5, we can conclude that the proportion is significantly different from 0.5 at the 0.05 significance level, indicating a majority of adult Americans believe in the importance of a college education."
  },
  {
    "objectID": "posts/HW2_Saisrinivas_ Ambatipudi.html#q3",
    "href": "posts/HW2_Saisrinivas_ Ambatipudi.html#q3",
    "title": "Homework 2",
    "section": "Q3",
    "text": "Q3\n\n\nCode\n# Define variables\nM_E <- 5\ns_d <- (200-30)/4\na <- 0.05\nz_a<- qnorm(p = 1-a/2, lower.tail = FALSE)\n\n\n# Calculate required sample size\n\ns_s <- ceiling(((z_a * s_d) / M_E)^2)\n\n\n# Required Sample Size, Round up to nearest integer\n\ncat(\"The required sample size is:\", s_s)\n\n\nThe required sample size is: 278\n\n\nUMass Amherst’s financial aid office aims to estimate the mean cost of textbooks per semester within $5, using a $10 or less confidence interval. Assuming a population standard deviation of a quarter of the range ($30-$200), they calculate a required sample size of 278 students for a 95% confidence interval. Accurate estimates are vital for determining appropriate financial assistance for textbooks, ensuring student academic success and financial well-being."
  },
  {
    "objectID": "posts/HW2_Saisrinivas_ Ambatipudi.html#q4",
    "href": "posts/HW2_Saisrinivas_ Ambatipudi.html#q4",
    "title": "Homework 2",
    "section": "Q4",
    "text": "Q4\n\nQ4-A\nWe assume that the sample is random and that the population has a normal distribution. Null hypothesis: H0: μ = 500 Alternative hypothesis: Ha: μ ≠ 500 We will reject the null hypothesis at a p-value, p <= 0.05\n\n\nCode\ns_m <- 410 # sample mean\nmu <- 500 # population mean\ns_d <- 90 # standard deviation\ns_s <- 9 # sample size\n\n# Calculating test-statistic\n\nt_s <- (s_m-mu)/(s_d/sqrt(s_s))\n\ncat(\"test-statistic:\", t_s, '\\n')\n\n\ntest-statistic: -3 \n\n\nCode\np_v <- 2 * pt(t_s, df = s_s - 1, lower.tail = TRUE)\n\ncat(\"p value :\", p_v)\n\n\np value : 0.01707168\n\n\nWe investigate if the mean income of nine randomly selected female employees in a large service company differs from the $500 per week union agreement. With a sample mean of $410 and a standard deviation of $90, we conduct a hypothesis test at a 0.05 significance level. The resulting p-value of 0.01707168 is less than 0.05, leading us to reject the null hypothesis and conclude that the mean income of female employees significantly differs from $500 per week.\n\n\nQ4-B\nWe assume that the sample is random and that the population has a normal distribution. Null hypothesis: H0: μ = 500 Alternative hypothesis: Ha: μ < 500 We will reject the null hypothesis at a p-value less than 0.05\n\n\nCode\np <- pt(t_s, s_s-1, lower.tail = TRUE)\np\n\n\n[1] 0.008535841\n\n\nA one-tailed t-test yielded a p-value of 0.008535841, which is less than the 0.05 significance level. We reject the null hypothesis, concluding that female employees in this service company earn less than $500 per week, implying they are paid less than senior-level workers per the union agreement.\n\n\nQ4-C\nWe assume that the sample is random and that the population has a normal distribution. Null hypothesis: H0: μ = 500 Alternative hypothesis: Ha: μ > 500 We will reject the null hypothesis at a p-value less than 0.05\n\n\nCode\np <- pt(t_s, s_s-1, lower.tail = FALSE)\np\n\n\n[1] 0.9914642\n\n\nWith a p-value of 0.9914642, greater than the 0.05 significance level, we fail to reject the null hypothesis. There is insufficient evidence to support the claim that female employees earn more than $500 per week, meaning we cannot conclude they earn significantly more than the agreed-upon mean income for senior-level workers."
  },
  {
    "objectID": "posts/HW2_Saisrinivas_ Ambatipudi.html#q5",
    "href": "posts/HW2_Saisrinivas_ Ambatipudi.html#q5",
    "title": "Homework 2",
    "section": "Q5",
    "text": "Q5\n\nQ5-A\nWe assume that the sample is random and that the population has a normal distribution. Null hypothesis: H0: μ = 500 Alternative hypothesis: Ha: μ ≠ 500 We will reject the null hypothesis at a p-value less than 0.05\n\n\nCode\ns_m <- 519.5\nmu <- 500\ns_e <- 10\ns_s <- 1000\n\nt_s_j <- (s_m-mu)/(s_e)\nt_s_j\n\n\n[1] 1.95\n\n\nCode\np <- 2*pt(t_s_j, s_s-1, lower.tail = FALSE)\np\n\n\n[1] 0.05145555\n\n\n\n\nCode\ns_m <- 519.7\nmu <- 500\ns_e <- 10\ns_s <- 1000\n\nt_s_s <- (s_m-mu)/(s_e)\nt_s_s\n\n\n[1] 1.97\n\n\nCode\np <- 2*pt(t_s_s, s_s-1, lower.tail = FALSE)\np\n\n\n[1] 0.04911426\n\n\nJones obtained a test-statistic of 1.95 and a p-value of 0.05145555, whereas Smith achieved a test-statistic of 1.97 with the same p-value of 0.05145555.\n\n\nQ5-B\nBased on the given information, the result is statistically significant for Smith, but not for Jones.\nFor Jones, the p-value (0.05145555) is greater than the significance level (α = 0.05), which means that we fail to reject the null hypothesis. This indicates that the result is not statistically significant for Jones.\nFor Smith, the p-value (0.04911426) is less than the significance level (α = 0.05), which means that we reject the null hypothesis. This indicates that the result is statistically significant for Smith.\n\n\nQ5-C\nReporting results as “P ≤ 0.05” or “P > 0.05” without providing the actual P-value can mask the degree of uncertainty in the findings. In the Jones/Smith example, such reporting could lead to different conclusions for very similar results. It is important to report the actual P-value to accurately interpret the results and understand the strength of the conclusion."
  },
  {
    "objectID": "posts/HW2_Saisrinivas_ Ambatipudi.html#q6",
    "href": "posts/HW2_Saisrinivas_ Ambatipudi.html#q6",
    "title": "Homework 2",
    "section": "Q6",
    "text": "Q6\nTo examine if the proportion of students selecting healthy snacks varies by grade level, we employ the chi-squared test of independence. The null hypothesis assumes the same proportion across all grades, while the alternative hypothesis posits differing proportions based on grade level.\n\n\nCode\n# Create the contingency table\ns_t <- matrix(c(31, 43, 51, 69, 57, 49), nrow = 2, byrow = TRUE)\nrownames(s_t) <- c(\"Healthy snack\", \"Unhealthy snack\")\ncolnames(s_t) <- c(\"6th grade\", \"7th grade\", \"8th grade\")\ns_t\n\n\n                6th grade 7th grade 8th grade\nHealthy snack          31        43        51\nUnhealthy snack        69        57        49\n\n\nCode\n# Conduct the chi-square test of independence\nchisq.test(s_t)\n\n\n\n    Pearson's Chi-squared test\n\ndata:  s_t\nX-squared = 8.3383, df = 2, p-value = 0.01547\n\n\nThe chi-square test statistic is 8.3383 with 2 degrees of freedom, and a p-value of 0.01547, which is less than the 0.05 significance level. We reject the null hypothesis, concluding that there is a significant association between snack choice and grade level among surveyed students.\nMore 6th-graders chose healthy snacks compared to 7th and 8th-graders, while more 8th-graders chose unhealthy snacks. This suggests grade level influences snack choice, and health promotion efforts should target specific grades with higher unhealthy snack choices. These findings may also guide further research on factors affecting middle school students’ snack choices."
  },
  {
    "objectID": "posts/HW2_Saisrinivas_ Ambatipudi.html#q7",
    "href": "posts/HW2_Saisrinivas_ Ambatipudi.html#q7",
    "title": "Homework 2",
    "section": "Q7",
    "text": "Q7\nTo test the claim that there is a difference in means for the three areas, we will use a one-way ANOVA test. The null hypothesis is that the means for the three areas are the same. The alternative hypothesis is that at least one area’s mean is different from the others.\n\n\nCode\n# Data in vectors\narea1 <- c(6.2, 9.3, 6.8, 6.1, 6.7, 7.5)\narea2 <- c(7.5, 8.2, 8.5, 8.2, 7.0, 9.3)\narea3 <- c(5.8, 6.4, 5.6, 7.1, 3.0, 3.5)\n\n# One-way ANOVA test\ntest <- aov(c(area1, area2, area3) ~ factor(c(rep(\"Area 1\", length(area1)), rep(\"Area 2\", length(area2)), rep(\"Area 3\", length(area3)))))\nsummary(test)\n\n\n                                                                                                    Df\nfactor(c(rep(\"Area 1\", length(area1)), rep(\"Area 2\", length(area2)), rep(\"Area 3\", length(area3))))  2\nResiduals                                                                                           15\n                                                                                                    Sum Sq\nfactor(c(rep(\"Area 1\", length(area1)), rep(\"Area 2\", length(area2)), rep(\"Area 3\", length(area3))))  25.66\nResiduals                                                                                            23.54\n                                                                                                    Mean Sq\nfactor(c(rep(\"Area 1\", length(area1)), rep(\"Area 2\", length(area2)), rep(\"Area 3\", length(area3))))  12.832\nResiduals                                                                                             1.569\n                                                                                                    F value\nfactor(c(rep(\"Area 1\", length(area1)), rep(\"Area 2\", length(area2)), rep(\"Area 3\", length(area3))))   8.176\nResiduals                                                                                                  \n                                                                                                     Pr(>F)\nfactor(c(rep(\"Area 1\", length(area1)), rep(\"Area 2\", length(area2)), rep(\"Area 3\", length(area3)))) 0.00397\nResiduals                                                                                                  \n                                                                                                      \nfactor(c(rep(\"Area 1\", length(area1)), rep(\"Area 2\", length(area2)), rep(\"Area 3\", length(area3)))) **\nResiduals                                                                                             \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nWith a p-value of 0.00397, which is below the significance level (α = 0.05), we reject the null hypothesis, concluding that there is a difference in means among the three areas."
  },
  {
    "objectID": "posts/FelixBetancourt_HW2.html",
    "href": "posts/FelixBetancourt_HW2.html",
    "title": "Homework 2",
    "section": "",
    "text": "Code\nknitr::opts_chunk$set(echo = TRUE, warning = FALSE)"
  },
  {
    "objectID": "posts/FelixBetancourt_HW2.html#homework-2",
    "href": "posts/FelixBetancourt_HW2.html#homework-2",
    "title": "Homework 2",
    "section": "Homework 2",
    "text": "Homework 2\nDACSS 603, Spring 2023\n\n\nCode\n# Loading packages\n\nsuppressPackageStartupMessages(library(dplyr))\nsuppressPackageStartupMessages(library(tidyverse))\nlibrary(formattable)\nsuppressPackageStartupMessages(library(kableExtra))\nlibrary(ggplot2)\nlibrary(readxl)\n\n\n\n1. The time between the date a patient was recommended for heart surgery and the surgery date for cardiac patients in Ontario was collected by the Cardiac Care Network (“Wait Times Data Guide,” Ministry of Health and Long-Term Care, Ontario, Canada, 2006). The sample mean and sample standard deviation for wait times (in days) of patients for two cardiac procedures are given in the accompanying table. Assume that the sample is representative of the Ontario population.\n\n\n\n\n\n\n\n\n\nSurgical Procedure\nSample Size\nMean wait time\nStandard Deviation\n\n\n\n\nBypass\n539\n19\n10\n\n\nAngiography\n847\n18\n9\n\n\n\n\n\nConstruct the 90% confidence interval to estimate the actual mean wait time for each of the two procedures. Is the confidence interval narrower for angiography or bypass surgery?\n\n\nCode\nbypass_n <- 539\nbypass_mean <- 19\nbypass_sd <- 10\n\n\nConfidence interval (90%) for Bypass type of surgery\n\n\nCode\nerror <- qnorm(p=0.9)*bypass_sd/sqrt(bypass_n)\nb_left <- bypass_mean-error\nb_right <- bypass_mean+error\nci_b <- c(b_left, b_right)\nprint(ci_b)\n\n\n[1] 18.448 19.552\n\n\nConfidence interval (90%) for Angiography type of surgery\n\n\nCode\nangio_n <- 847\nangio_mean <- 18\nangio_sd <- 9\n\nerror <- qnorm(p=0.9)*angio_sd/sqrt(angio_n)\na_left <- angio_mean-error\na_right <- angio_mean+error\nci_a <- c(a_left, a_right)\nprint(ci_a)\n\n\n[1] 17.60369 18.39631\n\n\n\n\nWhich is the narrower method?\n\n\nCode\nb_diff <- b_right-b_left\na_diff <- a_right - a_left\n\n\nBypass CI length\n\n\nCode\n#bypass\nprint(b_diff)\n\n\n[1] 1.104007\n\n\nAngiography CI length\n\n\nCode\n#Angiography\nprint(a_diff)\n\n\n[1] 0.7926234\n\n\nCI for Angiography is the narrower method.\n\n\n\n2. A survey of 1031 adult Americans was carried out by the National Center for Public Policy. Assume that the sample is representative of adult Americans. Among those surveyed, 567 believed that college education is essential for success. Find the point estimate, p, of the proportion of all adult Americans who believe that a college education is essential for success.\n\n\nConstruct and interpret a 95% confidence interval for p.\n\n\nCode\n# p value\n\np <- 567/1031\np\n\n\n[1] 0.5499515\n\n\nCode\nz <- qnorm(0.95)\n\n#Confidence interval for p\nCI <- p + c(-1, 1) * z * sqrt((p*(1-p))/1031)\nCI\n\n\n[1] 0.5244662 0.5754368\n\n\nBased on the sample of 1031 adult Americans, we estimate, with 95% confidence, that between 52.45% and 57.54% of all adult Americans believe that a college education is essential for success.\n--------------------------------------------------------------------------------------------------------------------\n\n\n3. Suppose that the financial aid office of UMass Amherst seeks to estimate the mean cost of textbooks per semester for students. The estimate will be useful if it is within $5 of the true population mean (i.e. they want the confidence interval to have a length of $10 or less). The financial aid office is pretty sure that the amount spent on books varies widely, with most values between $30 and $200. They think that the population standard deviation is about a quarter of this range (in other words, you can assume they know the population standard deviation).\n\n\nAssuming the significance level to be 5%, what should be the size of the sample.\n\n\nCode\nsigma <- (200 - 30) / 4 \nerror <- 5\n\nn <- ceiling((qnorm(0.95) * sigma / error) ^ 2)\nn\n\n\n[1] 196\n\n\nThe sample size should be 196.\n\n\nAccording to a union agreement, the mean income for all senior-level workers in a large service company equals $500 per week. A representative of a women’s group decides to analyze whether the mean income μ for female employees matches this norm. For a random sample of nine female employees, ȳ = $410 and s = 90\n\n\nA. Test whether the mean income of female employees differs from $500 per week. Include assumptions, hypotheses, test statistic, and P-value. Interpret the result.\n\n\nCode\nn_s <- 9  \nsample_mean <- 410  \nsample_sd <- 90  \n\n\nAssuming that:\n\nThe sample of female employees is a random sample from the population of all female employees.\nThe population of female employees’ incomes follows a normal distribution or the sample size is large enough for the central limit theorem to apply.\nThe population standard deviation is unknown.\n\nNull Hypothesis H0: Women’s income mean (μ) = $500 per week\n\n\nCode\nh0_mean <- 500\n\n# t value\nt_stat <- (sample_mean - h0_mean) / (sample_sd / sqrt(n_s))\n\n#p-value\np_val <- 2 * pt(-abs(t_stat), df = n_s - 1)\np_val\n\n\n[1] 0.01707168\n\n\nP-value is significantly lower than 0.05 significance level, therefore we reject the null hypothesis and we can’t say that woman’s income mean equals the population mean ($500 per week).\nMean income of female employees in the company is significantly lower than $500 per week.\n\n\nB. Report the P-value for Ha: μ < 500. Interpret.\n\n\nCode\np_left <- pt(t_stat, df = n_s-1)\np_left\n\n\n[1] 0.008535841\n\n\nP-value for the left-tailed test is less than the significance level of 0.05. Therefore, we reject the null hypothesis and conclude that there is sufficient evidence to suggest that the mean income of female employees is less than $500 per week\n\n\nC. Report and interpret the P-value for Ha: μ > 500. (Hint: The P-values for the two possible one-sided tests must sum to 1.\n\n\nCode\np_right <- 1 - p_left\np_right\n\n\n[1] 0.9914642\n\n\n\n\n5. Jones and Smith separately conduct studies to test H0: μ = 500 against Ha: μ ≠ 500, each with n = 1000. Jones gets ȳ = 519.5, with se = 10.0. Smith gets ȳ = 519.7, with se = 10.0.\nA. Show that t = 1.95 and P-value = 0.051 for Jones. Show that t = 1.97 and P-value = 0.049 for Smith.\n\n\nCode\nn_s2 <- 1000 \nh0_mean2 <- 500\n\n#Jones\nsample_meanj <- 519.5  \nsample_sdj <- 10  \n\n# t value\nt_statj <- (sample_meanj - h0_mean2) / (sample_sdj)\nt_statj\n\n\n[1] 1.95\n\n\nCode\n#p-value\np_valj <- 2 * pt(-abs(t_statj), df = n_s2 - 1)\np_valj\n\n\n[1] 0.05145555\n\n\n\n\nCode\n#Smith\nsample_meansm <- 519.7  \nsample_sdsm <- 10  \n\n# t value\nt_statsm <- (sample_meansm - h0_mean2) / (sample_sdsm)\nt_statsm\n\n\n[1] 1.97\n\n\nCode\n#p-value\np_valsm <- 2 * pt(-abs(t_statsm), df = n_s2 - 1)\np_valsm\n\n\n[1] 0.04911426\n\n\nB. Using α = 0.05, for each study indicate whether the result is “statistically significant.”\nWe fail to reject the null hypothesis. In this case, both studies have p-values slightly above 0.05, but below it. So we can say that both studies have marginally significant results at the α = 0.05 level.\nC. Using this example, explain the misleading aspects of reporting the result of a test as “P ≤ 0.05” versus “P > 0.05,” or as “reject H0” versus “Do not reject H0,” without reporting the actual P-value.\nReporting the result of a hypothesis test simply as “P ≤ 0.05” or “reject H0” without reporting the actual p-value can be misleading in several ways:\n\nIt doesn’t provide information about the magnitude of the effect or the strength of evidence against the null hypothesis.\nIt does not convey the uncertainty associated with the p-value. In this example, both studies have p-values slightly above and below 0.05. Reporting the result as simply “P ≤ 0.05” implies a false sense of certainty.\n\n\n\n\n6. A school nurse wants to determine whether age is a factor in whether children choose a healthy snack after school. She conducts a survey of 300 middle school students, with the results below. Test at α = 0.05 the claim that the proportion who choose a healthy snack differs by grade level.\n\n\n\n\n\n\n\n\n\nGrade Level\n6th grade\n7th grade\n8th grade\n\n\n\n\nHealthy Snack\n31\n43\n51\n\n\nUnhealthy Snack\n69\n57\n49\n\n\n\n\n\nWhat is the null hypothesis?\nThe proportion of observed children choosing healthy or unhealthy snack is equal to the expected proportion in all grades.\n\n\nWhich test should we use?\nChi-square\n\n\nCode\nsnack_obs <- matrix(c(31, 43, 51, 69, 57, 49), nrow = 2, byrow = TRUE)\nrownames(snack_obs) <- c(\"Healthy\", \"Unhealthy\")\ncolnames(snack_obs) <- c(\"6th grade\", \"7th grade\", \"8th grade\")\n\nobs <- table(snack_obs)\n\ntotal_obs <- sum(snack_obs)\nsnack_exp <- rep(sum(snack_obs)/3, 3)\nsnack_exp <- rbind(snack_exp, snack_exp)\nrownames(snack_exp) <- c(\"Healthy\", \"Unhealthy\")\ncolnames(snack_exp) <- c(\"6th grade\", \"7th grade\", \"8th grade\")\nsnack_exp <- snack_exp * total_obs\n\nchisq.test(snack_obs, snack_exp, 0.05)\n\n\n\n    Pearson's Chi-squared test\n\ndata:  snack_obs\nX-squared = 8.3383, df = 2, p-value = 0.01547\n\n\n\n\nWhat is the conclusion?\nWe reject the Null Hypothesis. Seems that there is a significant difference between observed proportion of children choosing healthy snack based on the grade versus the expected proportion. In this case seems that in low grades children the proportion of children choosing healthy snack are lower than higher grades.\n\n\n7. Per-pupil costs (in thousands of dollars) for cyber charter school tuition for school districts in three areas are shown. Test the claim that there is a difference in means for the three areas, using an appropriate test.\n“Area 1” 6.2 9.3 6.8 6.1 6.7 7.5 “Area 2” 7.5 8.2 8.5 8.2 7.0 9.3 “Area 3” 5.8 6.4 5.6 7.1 3.0 3.5\n\n\nWhat is the null hypothesis?\nThe means for the Per-pupil costs in the 3 school districts areas are equal.\n\n\nWhich test should we use?\nOne-way Anova.\n\n\nCode\narea1 <- c(6.2, 9.3, 6.8, 6.1, 6.7, 7.5)\narea2 <- c(7.5, 8.2, 8.5, 8.2, 7.0, 9.3)\narea3 <- c(5.8, 6.4, 5.6, 7.1, 3.0, 3.5)\n\nperpupil <- data.frame(area1, area2, area3)\n\nsummary(perpupil)\n\n\n     area1           area2           area3      \n Min.   :6.100   Min.   :7.000   Min.   :3.000  \n 1st Qu.:6.325   1st Qu.:7.675   1st Qu.:4.025  \n Median :6.750   Median :8.200   Median :5.700  \n Mean   :7.100   Mean   :8.117   Mean   :5.233  \n 3rd Qu.:7.325   3rd Qu.:8.425   3rd Qu.:6.250  \n Max.   :9.300   Max.   :9.300   Max.   :7.100  \n\n\nCode\nperpupil2 <- stack(perpupil[,1:3])\n\nmodel <- aov(values ~ ind, data = perpupil2)\n\nsummary(model)\n\n\n            Df Sum Sq Mean Sq F value  Pr(>F)   \nind          2  25.66  12.832   8.176 0.00397 **\nResiduals   15  23.54   1.569                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\nWhat is the conclusion?\nThe per-pupil cost is significantly different based on the area (p<0.05)."
  },
  {
    "objectID": "posts/AlexisGamez_HW1.html",
    "href": "posts/AlexisGamez_HW1.html",
    "title": "Blog Post #1",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(readxl)\n\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/AlexisGamez_HW1.html#a",
    "href": "posts/AlexisGamez_HW1.html#a",
    "title": "Blog Post #1",
    "section": "a)",
    "text": "a)\nFirst, let’s read in the data from the Excel file:\n\n\nCode\ngetwd()\n\n\n[1] \"C:/Users/Leshiii/Desktop/DACSS Master's/DACSS 603/603_Spring_2023/posts\"\n\n\nCode\ndf <- read_excel(\"_data/LungCapData.xls\")\n\n\nThe distribution of LungCap looks as follows:\n\n\nCode\nhist(df$LungCap, main = \"Lung Capacity Distribution\", xlab = \"Lung Capacity\", ylab = \"Probability Density\", prob = TRUE)\n\n\n\n\n\nThe histogram suggests that the distribution is close to a normal distribution. Most of the observations are close to the mean. Very few observations are close to the margins (0 and 15)."
  },
  {
    "objectID": "posts/AlexisGamez_HW1.html#b",
    "href": "posts/AlexisGamez_HW1.html#b",
    "title": "Blog Post #1",
    "section": "b)",
    "text": "b)\nProvided below is a box plot of the probability distributions of the Lung Capacity data for the male and female genders.\n\n\nCode\nboxplot(df$LungCap ~ df$Gender, \n        ylab = \"Gender\",\n        xlab = \"Lung Capacity\",\n        horizontal = TRUE,\n        col = \"maroon\")"
  },
  {
    "objectID": "posts/AlexisGamez_HW1.html#c",
    "href": "posts/AlexisGamez_HW1.html#c",
    "title": "Blog Post #1",
    "section": "c)",
    "text": "c)\nI don’t believe the data provided below make much sense. I would argue that it is much more likely for smokers to have a smaller lung capacity than those that do not smoke. I suspect that something could be going on with our sample.\n\n\nCode\nboxplot(df$LungCap ~ df$Smoke, \n        ylab = \"Smoking Preference\",\n        xlab = \"Lung Capacity\",\n        horizontal = TRUE,\n        col = \"bisque\")\n\n\n\n\n\nCode\nno_smoke <- subset(df, Smoke == \"no\")\nyes_smoke <- subset(df, Smoke == \"yes\")\nmean(no_smoke$LungCap)\n\n\n[1] 7.770188\n\n\nCode\nmean(yes_smoke$LungCap)\n\n\n[1] 8.645455"
  },
  {
    "objectID": "posts/AlexisGamez_HW1.html#d",
    "href": "posts/AlexisGamez_HW1.html#d",
    "title": "Blog Post #1",
    "section": "d)",
    "text": "d)\nComparing the charts below, we can see that as the participant ages, the mean and ranges of Lung Capacity increases. This could be due to natural maturation, with Lungs growing as children grow to adolescents, stalling circa 18 years old. However, I suspect there might be more to it, particularly our sample of smokers being so small compared to that of non-smokers.\n\n\nCode\nno_smoke_mean <- mean(no_smoke$LungCap)\n`13_under` <- subset(yes_smoke, Age <= 13)\n\n`14_to_15` <- subset(yes_smoke, subset = Age == 14 | Age == 15)\n\n`16_to_17` <- subset(yes_smoke, subset = Age == 16 | Age == 17)\n\n`18_over` <- subset(yes_smoke, Age >= 18)\n\nhist(`13_under`$LungCap, main = \"Smoker Lung Capacity Distribution by Age\", xlab = \"Lung Capacity\", ylab = \"Probability Density\", prob = TRUE)\nlegend(\"topright\", legend=c(\"13 & Under\",\"14 to 15\", \"16 to 17\", \"18 & Over\"), col=c(\"gray\", rgb(0,0,1,0.5), \n     rgb(0,1,0,0.5), rgb(1,0,0,0.5)), pt.cex=2, pch=15)\n\n\n\n\n\nCode\nhist(`14_to_15`$LungCap, prob = TRUE, col = rgb(0,0,1,0.5), main = \"Smoker Lung Capacity Distribution by Age\", xlab = \"Lung Capacity\", ylab = \"Probability Density\")\nlegend(\"topright\", legend=c(\"13 & Under\",\"14 to 15\", \"16 to 17\", \"18 & Over\"), col=c(\"gray\", rgb(0,0,1,0.5), \n     rgb(0,1,0,0.5), rgb(1,0,0,0.5)), pt.cex=2, pch=15)\n\n\n\n\n\nCode\nhist(`16_to_17`$LungCap, prob = TRUE, col = rgb(0,1,0,0.5), main = \"Smoker Lung Capacity Distribution by Age\", xlab = \"Lung Capacity\", ylab = \"Probability Density\")\nlegend(\"topright\", legend=c(\"13 & Under\",\"14 to 15\", \"16 to 17\", \"18 & Over\"), col=c(\"gray\", rgb(0,0,1,0.5), \n     rgb(0,1,0,0.5), rgb(1,0,0,0.5)), pt.cex=2, pch=15)\n\n\n\n\n\nCode\nhist(`18_over`$LungCap, prob = TRUE, col = rgb(1,0,0,0.5), main = \"Smoker Lung Capacity Distribution by Age\", xlab = \"Lung Capacity\", ylab = \"Probability Density\")\nlegend(\"topright\", legend=c(\"13 & Under\",\"14 to 15\", \"16 to 17\", \"18 & Over\"), col=c(\"gray\", rgb(0,0,1,0.5), \n     rgb(0,1,0,0.5), rgb(1,0,0,0.5)), pt.cex=2, pch=15)"
  },
  {
    "objectID": "posts/AlexisGamez_HW1.html#e",
    "href": "posts/AlexisGamez_HW1.html#e",
    "title": "Blog Post #1",
    "section": "e)",
    "text": "e)\n\n\nCode\ndf_Agegroup <- df %>% \n  mutate(\nAge_group = dplyr::case_when(\n      Age <= 13            ~ \"0-13\",\n      Age > 13 & Age <= 15 ~ \"14-15\",\n      Age > 15 & Age <= 17 ~ \"16-17\",\n      Age >= 18             ~ \"18+\"))\n\nggplot(data = df_Agegroup, aes(x=Age_group, y=LungCap)) + \n  geom_boxplot(aes(fill=Smoke)) +\n  labs(x=\"Age Group\",y=\"Lung Capacity\",title=\"Lung Capacity of Smokers vs Non Smokers by Age Group\")\n\n\n\n\n\nLooking at the box plot above, its seems as though the ranges of data for the latter 3 age group divisions are naturally larger for non-smokers than for smokers. Additionally, the means for said divisions under non-smokers are also higher! What seems to be skewing the data is the large quantity of participants equal to 13 years of age or younger. I believe this is why our data hasn’t made much sense until now. There might be other factors in play here, like respondent bias, but I believe the sample size here is the main influence."
  },
  {
    "objectID": "posts/AlexisGamez_HW1.html#a-1",
    "href": "posts/AlexisGamez_HW1.html#a-1",
    "title": "Blog Post #1",
    "section": "a)",
    "text": "a)\nReading in our data.\n\n\nCode\nConvictions <- seq(0,4)\nFreq <- c(128, 434, 160, 64, 24)/810\n\n\nThe probability of that a randomly selected inmate has exactly 2 prior convictions is:\n\n\nCode\ndbinom(x=1, size = 1, prob = 160/810)\n\n\n[1] 0.1975309"
  },
  {
    "objectID": "posts/AlexisGamez_HW1.html#b-1",
    "href": "posts/AlexisGamez_HW1.html#b-1",
    "title": "Blog Post #1",
    "section": "b)",
    "text": "b)\nThe probability that a randomly selected inmate has fewer than 2 prior convictions is:\n\n\nCode\ndbinom(x = 1, size = 1, prob = (128+434)/810)\n\n\n[1] 0.6938272"
  },
  {
    "objectID": "posts/AlexisGamez_HW1.html#c-1",
    "href": "posts/AlexisGamez_HW1.html#c-1",
    "title": "Blog Post #1",
    "section": "c)",
    "text": "c)\nThe probability that a randomly selected inmate has 2 or fewer prior convictions is:\n\n\nCode\ndbinom(x = 1, size = 1, prob = (128+434+160)/810)\n\n\n[1] 0.891358"
  },
  {
    "objectID": "posts/AlexisGamez_HW1.html#d-1",
    "href": "posts/AlexisGamez_HW1.html#d-1",
    "title": "Blog Post #1",
    "section": "d)",
    "text": "d)\nThe probability that a randomly selected inmate has more than 2 prior convictions is:\n\n\nCode\ndbinom(x = 1, size = 1, prob = (64+24)/810)\n\n\n[1] 0.108642"
  },
  {
    "objectID": "posts/AlexisGamez_HW1.html#e-1",
    "href": "posts/AlexisGamez_HW1.html#e-1",
    "title": "Blog Post #1",
    "section": "e)",
    "text": "e)\nThe expected value for the number of prior convictions is:\n\n\nCode\nExpected_v <- sum(Freq*Convictions)\nExpected_v\n\n\n[1] 1.28642"
  },
  {
    "objectID": "posts/AlexisGamez_HW1.html#f",
    "href": "posts/AlexisGamez_HW1.html#f",
    "title": "Blog Post #1",
    "section": "f)",
    "text": "f)\nThe variance for prior convictions is:\n\n\nCode\nVariance <- sum((Convictions-Expected_v)^2*Freq)\nVariance\n\n\n[1] 0.8562353\n\n\nThe standard deviation for prior convictions is:\n\n\nCode\nSD <- sqrt(Variance)\nSD\n\n\n[1] 0.9253298\n\n\nPlotting our data further validates our calculations as all values we’ve presented seem to coincide with their respective points on the plot.\n\n\nCode\nConv_data <- tibble(\n  x= Convictions,\n  y= Freq)\n\nggplot(Conv_data, aes(x,y))+\n  geom_line()+\n  geom_vline(xintercept = 2, col=\"red\",size=1)+\n  geom_text(x=2.15,y=.245,label=\"c = 2\")+\n  labs(x=\"# of Prior Convictions\",y=\"Probability\",title=\"Probability Distribution of Prisoner # of Prior Convictions\")\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead."
  },
  {
    "objectID": "posts/AlexisGamez_HW2.html",
    "href": "posts/AlexisGamez_HW2.html",
    "title": "Blog Post #2",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/AlexisGamez_HW2.html#a",
    "href": "posts/AlexisGamez_HW2.html#a",
    "title": "Blog Post #2",
    "section": "A)",
    "text": "A)\nTest whether the mean income of female employees differs from $500 per week. Include assumptions, hypotheses, test statistic, and P-value. Interpret the result.\nAssumptions:\n\nThe sample provided is randomly selected and representative of the population\nPopulation is normally distributed\nThe variance of both populations are equal\n\nNull Hypothesis(H0): μ = 500\nAlternative Hypothesis(HA): μ ≠ 500\nt-test Formula: t = (Y bar - μ)/(Standard Deviation/sqrt(n))\n\n\nCode\nt <- (410-500)/(90/sqrt(9))\nt\n\n\n[1] -3\n\n\n\n\nCode\np <- 2*pt(t, 9-1)\np\n\n\n[1] 0.01707168\n\n\nThe test statistic and p-value we receive are equal to -3 and 0.0171 respectively. Because our p-value < 0.05, we are able to reject the null hypothesis meaning that the mean income of female employees differs from $500 per week."
  },
  {
    "objectID": "posts/AlexisGamez_HW2.html#b",
    "href": "posts/AlexisGamez_HW2.html#b",
    "title": "Blog Post #2",
    "section": "B)",
    "text": "B)\nReport the P-value for Ha: μ < 500. Interpret.\nHere we’re being asked to calculate a one sided p-value where the alternative hypothesis is that μ < 500. In order to calculate said p-value, we must specify within our function that we will be look at the lower tail of the data.\nH0: μ >= 500\nHA: μ < 500\n\n\nCode\np1 <- pt(t, 9-1, lower.tail = T)\np1\n\n\n[1] 0.008535841\n\n\nBeing provided such a small p-value tells us that we are able to reject the null hypothesis and embrace the alternative that mean female income within the population is less than $500 per week."
  },
  {
    "objectID": "posts/AlexisGamez_HW2.html#c",
    "href": "posts/AlexisGamez_HW2.html#c",
    "title": "Blog Post #2",
    "section": "C)",
    "text": "C)\nReport and interpret the P-value for Ha: μ > 500. (Hint: The P-values for the two possible one-sided tests must sum to 1.)\nHere, we’ll be functionally doing the same thing as in part b, but instead of using the lower tail we’ll be focusing on the upper. Under these circumstances, we’ll be adopting the following hypothesis.\nH0: μ =< 500\nHA: μ > 500\n\n\nCode\np2 <- pt(t, 9-1, lower.tail = F)\np2\n\n\n[1] 0.9914642\n\n\nIn this case, we receive a p-value > 0.05 meaning that we fail to reject the null and that the mean female income within the population is not greater than $500."
  },
  {
    "objectID": "posts/AlexisGamez_HW2.html#a-1",
    "href": "posts/AlexisGamez_HW2.html#a-1",
    "title": "Blog Post #2",
    "section": "A)",
    "text": "A)\nShow that t = 1.95 and P-value = 0.051 for Jones. Show that t = 1.97 and P-value = 0.049 for Smith.\nt-test Formula: t = (Y bar - μ)/Standard Error\n\nJones\n\n\nCode\ntJones <- (519.5-500)/10\ntJones\n\n\n[1] 1.95\n\n\n\n\nCode\npJones <- 2*pt(tJones, 1000-1, lower.tail = F)\npJones\n\n\n[1] 0.05145555\n\n\n\n\nSmith\n\n\nCode\ntSmith <- (519.7-500)/10\ntSmith\n\n\n[1] 1.97\n\n\n\n\nCode\npSmith <- 2*pt(tSmith, 1000-1, lower.tail = F)\npSmith\n\n\n[1] 0.04911426"
  },
  {
    "objectID": "posts/AlexisGamez_HW2.html#b-1",
    "href": "posts/AlexisGamez_HW2.html#b-1",
    "title": "Blog Post #2",
    "section": "B)",
    "text": "B)\nUsing α = 0.05, for each study indicate whether the result is “statistically significant.”\nAlpha(α) is the level at which a result is considered statistically significant or not.\nWith our significance level defined, we know that pJones, sitting at 0.051, would not be considered statistically significant and we fail to reject the null (μ = 500). On the other hand, pSmith (0.049) would be considered statistically significant resulting in our ability to reject the null (μ ≠ 500)."
  },
  {
    "objectID": "posts/AlexisGamez_HW2.html#c-1",
    "href": "posts/AlexisGamez_HW2.html#c-1",
    "title": "Blog Post #2",
    "section": "C)",
    "text": "C)\nUsing this example, explain the misleading aspects of reporting the result of a test as “P ≤ 0.05” versus “P > 0.05,” or as “reject H0” versus “Do not reject H0,” without reporting the actual P-value.\nStatistical significance is something that must be well defined prior to the gathering and analyzing data. The term holds weight in the credibility of research and analysis and without it being clearly defined, it’s uncertain whether or not the work that was done truly holds up and proves anything. Not reporting the actual P-values could lead to confusion and misinterpretation down the road. This makes it important to begin every varying analysis with the a clear definition of hypothesis and significance level."
  },
  {
    "objectID": "posts/Kristin_Abijaoude_HW2.html",
    "href": "posts/Kristin_Abijaoude_HW2.html",
    "title": "Hw 2 by Kristin Abijaoude",
    "section": "",
    "text": "Code\n# load packages\npackages <- c(\"readr\", \"ggplot2\", \"caret\", \"summarytools\", \"tidyverse\", \"dplyr\", \"stats\", \"pwr\")\nlapply(packages, require, character.only = TRUE)\n\n\nLoading required package: readr\n\n\nLoading required package: ggplot2\n\n\nLoading required package: caret\n\n\nLoading required package: lattice\n\n\nLoading required package: summarytools\n\n\nLoading required package: tidyverse\n\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ tibble  3.1.8     ✔ dplyr   1.1.0\n✔ tidyr   1.2.1     ✔ stringr 1.5.0\n✔ purrr   1.0.0     ✔ forcats 0.5.2\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n✖ purrr::lift()   masks caret::lift()\n✖ tibble::view()  masks summarytools::view()\nLoading required package: pwr\n\n\n[[1]]\n[1] TRUE\n\n[[2]]\n[1] TRUE\n\n[[3]]\n[1] TRUE\n\n[[4]]\n[1] TRUE\n\n[[5]]\n[1] TRUE\n\n[[6]]\n[1] TRUE\n\n[[7]]\n[1] TRUE\n\n[[8]]\n[1] TRUE"
  },
  {
    "objectID": "posts/Kristin_Abijaoude_HW2.html#a",
    "href": "posts/Kristin_Abijaoude_HW2.html#a",
    "title": "Hw 2 by Kristin Abijaoude",
    "section": "A",
    "text": "A\n\n\nCode\n# t test\nf_emp <- 410\nincome <- 500\ns <- 90\nt <- (f_emp - income) / (s / sqrt(9))\nt\n\n\n[1] -3"
  },
  {
    "objectID": "posts/Kristin_Abijaoude_HW2.html#b",
    "href": "posts/Kristin_Abijaoude_HW2.html#b",
    "title": "Hw 2 by Kristin Abijaoude",
    "section": "B",
    "text": "B\n\n\nCode\n# degree of freedom\ndf <- 9 - 1 \n# df = 8\n\n# p-value\np_value <- pt(t, df)\np_value\n\n\n[1] 0.008535841\n\n\nCode\n# significant level\nalpha <- 0.05\n\n# to reject or not to reject\nif (p_value < alpha/2 || p_value > 1-alpha/2) {\n  cat(\"Reject the null hypothesis\")\n} else {\n  cat(\"Fail to reject the null hypothesis\")\n}\n\n\nReject the null hypothesis"
  },
  {
    "objectID": "posts/Kristin_Abijaoude_HW2.html#c",
    "href": "posts/Kristin_Abijaoude_HW2.html#c",
    "title": "Hw 2 by Kristin Abijaoude",
    "section": "C",
    "text": "C\n\n\nCode\n1-p_value\n\n\n[1] 0.9914642\n\n\nCode\n# fail to reject null hypothesis"
  },
  {
    "objectID": "posts/Kristin_Abijaoude_HW2.html#a-1",
    "href": "posts/Kristin_Abijaoude_HW2.html#a-1",
    "title": "Hw 2 by Kristin Abijaoude",
    "section": "A",
    "text": "A\n\n\nCode\n#  T values\nt_jones <- (519.5 - 500) / 10 # sample mean = 519.5 - 500 for population mean / sample error of 10.0\nt_jones\n\n\n[1] 1.95\n\n\nCode\nt_smith <- (519.7 - 500) / 10 # sample mean = 519.7 - 500 for population mean / sample error of 10.0\nt_smith\n\n\n[1] 1.97\n\n\n\n\nCode\n# p values\np_jones <- 2 * pt(-abs(t_jones), df = 999)\np_jones\n\n\n[1] 0.05145555\n\n\nCode\np_smith <- 2 * pt(-abs(t_smith), df = 999)\np_smith\n\n\n[1] 0.04911426"
  },
  {
    "objectID": "posts/Kristin_Abijaoude_HW2.html#b-1",
    "href": "posts/Kristin_Abijaoude_HW2.html#b-1",
    "title": "Hw 2 by Kristin Abijaoude",
    "section": "B",
    "text": "B\nSmith’s result is statistically significant, while Jones’ is not."
  },
  {
    "objectID": "posts/Kristin_Abijaoude_HW2.html#c-1",
    "href": "posts/Kristin_Abijaoude_HW2.html#c-1",
    "title": "Hw 2 by Kristin Abijaoude",
    "section": "C",
    "text": "C\nWhile the two results are close in variables, one of them is significant while the other is not. That’s why P-value is important in determining whether to reject or fail to reject the hypothesis."
  },
  {
    "objectID": "posts/Rowley_Homework_1.html",
    "href": "posts/Rowley_Homework_1.html",
    "title": "Homework 1",
    "section": "",
    "text": "Question 1a: What does the distribution of LungCap look like?\nFirst, let’s read in the data from the Excel file:\n\n\nCode\nlibrary(readxl)\n\n\nWarning: package 'readxl' was built under R version 4.2.2\n\n\nCode\nLungCapData <- read_excel(\"_data/LungCapData.xls\")\n\n\nThe distribution of LungCap looks as follows:\n\n\nCode\nhist(LungCapData$LungCap)\n\n\n\n\n\nThe histogram suggests that the distribution is close to a normal distribution. Most of the observations are close to the mean. Very few observations are close to the margins (0 and 15).\n\n\nQuestion 1b: Compare the probability distribution of the LungCap with respect to Males and Females. (Hint:make boxplots separated by gender using the boxplot() function)\n\n\nCode\nlibrary(dplyr)\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nCode\nlibrary(magrittr)\nlibrary(ggplot2)\nlibrary(markdown)\nlibrary(ggtext)\n\n\nWarning: package 'ggtext' was built under R version 4.2.2\n\n\nCode\n# generate boxplot:\n\nLungCap_Gender <- LungCapData%>%\n  group_by(LungCapData$Gender)%>%\n  ggplot(aes(x=Gender, y=LungCap)) +\n  geom_point(alpha=.08, size=5) +\n  labs(x=\"Gender\", y=\"LungCap\", title=\"LungCap by Gender\") +\n  theme_light() +\n  geom_boxplot() +\n  theme(axis.text.x=element_markdown(hjust=1))\nLungCap_Gender\n\n\n\n\n\nCode\n# would next like to identify specific values for boxplots...\n\n\nThe distribution of LungCap is higher for males than it is for females. For males, the Q1 value is approximately 6.5, the median value is approximately 8, and the Q3 value is approximately 10. For females, the Q1 value is approximately 5.5, the median value is approximately 7.5, and the Q3 value is approximately 9.\n\n\nQuestion 1c: Compare the mean lung capacities for smokers and non-smokers. Does it make sense?\n\n\nCode\nSmoker_Mean <- LungCapData %>%\n  filter(Smoke==\"yes\")%>%\n  select(Smoke, LungCap)%>%\n  summarize(mean(LungCap, na.rm = TRUE))\nSmoker_Mean\n\n\n# A tibble: 1 × 1\n  `mean(LungCap, na.rm = TRUE)`\n                          <dbl>\n1                          8.65\n\n\nCode\nNonSmoker_Mean <- LungCapData %>%\n  filter(Smoke==\"no\")%>%\n  select(Smoke, LungCap)%>%\n  summarize(mean(LungCap, na.rm = TRUE))\nNonSmoker_Mean\n\n\n# A tibble: 1 × 1\n  `mean(LungCap, na.rm = TRUE)`\n                          <dbl>\n1                          7.77\n\n\nThe mean LungCap for smokers is 8.645, while the mean LungCap for non-smokers is 7.77. This doesn’t seem to make sense!\n\n\nQuestion 1d: Examine the relationship between Smoking and Lung Capacity within age groups: “less than or equal to 13”, “14 to 15”, “16 to 17”, and “greater than or equal to 18”.\n\n\nCode\nSmoker_Age_1 <- LungCapData %>%\n  filter(Smoke==\"yes\")%>%\n  filter(Age<=13)%>%\n  select(Smoke, Age, LungCap)%>%\n  summarize(mean(LungCap, na.rm = TRUE))\nSmoker_Age_1\n\n\n# A tibble: 1 × 1\n  `mean(LungCap, na.rm = TRUE)`\n                          <dbl>\n1                          7.20\n\n\nCode\nSmoker_Age_2 <- LungCapData %>%\n  filter(Smoke==\"yes\")%>%\n  filter(Age==14:15)%>%\n  select(Smoke, Age, LungCap)%>%\n  summarize(mean(LungCap, na.rm = TRUE))\n\n\nWarning in Age == 14:15: longer object length is not a multiple of shorter\nobject length\n\n\nCode\nSmoker_Age_2\n\n\n# A tibble: 1 × 1\n  `mean(LungCap, na.rm = TRUE)`\n                          <dbl>\n1                          8.91\n\n\nCode\nSmoker_Age_3 <- LungCapData %>%\n  filter(Smoke==\"yes\")%>%\n  filter(Age==16:17)%>%\n  select(Smoke, Age, LungCap)%>%\n  summarize(mean(LungCap, na.rm = TRUE))\n\n\nWarning in Age == 16:17: longer object length is not a multiple of shorter\nobject length\n\n\nCode\nSmoker_Age_3\n\n\n# A tibble: 1 × 1\n  `mean(LungCap, na.rm = TRUE)`\n                          <dbl>\n1                          9.60\n\n\nCode\nSmoker_Age_4 <- LungCapData %>%\n  filter(Smoke==\"yes\")%>%\n  filter(Age>=18)%>%\n  select(Smoke, Age, LungCap)%>%\n  summarize(mean(LungCap, na.rm = TRUE))\nSmoker_Age_4\n\n\n# A tibble: 1 × 1\n  `mean(LungCap, na.rm = TRUE)`\n                          <dbl>\n1                          10.5\n\n\nThe mean LungCap value for smokers 13 years or younger is 7.202, whereas it is 8.909 for those ages 14-15. For smokers 16-17, the mean LungCap value is 9.602, and for those 18 years and older, it is 10.513.\n\n\nQuestion 1e: Compare the lung capacities for smokers and non-smokers within each age group. Is your answer different from the one in part c? What could possibly be going on here?\n\n\nCode\nNonSmoker_Age_1 <- LungCapData %>%\n  filter(Smoke==\"no\")%>%\n  filter(Age<=13)%>%\n  select(Smoke, Age, LungCap)%>%\n  summarize(mean(LungCap, na.rm = TRUE))\nNonSmoker_Age_1\n\n\n# A tibble: 1 × 1\n  `mean(LungCap, na.rm = TRUE)`\n                          <dbl>\n1                          6.36\n\n\nCode\nNonSmoker_Age_2 <- LungCapData %>%\n  filter(Smoke==\"no\")%>%\n  filter(Age==14:15)%>%\n  select(Smoke, Age, LungCap)%>%\n  summarize(mean(LungCap, na.rm = TRUE))\nNonSmoker_Age_2\n\n\n# A tibble: 1 × 1\n  `mean(LungCap, na.rm = TRUE)`\n                          <dbl>\n1                          8.84\n\n\nCode\nNonSmoker_Age_3 <- LungCapData %>%\n  filter(Smoke==\"no\")%>%\n  filter(Age==16:17)%>%\n  select(Smoke, Age, LungCap)%>%\n  summarize(mean(LungCap, na.rm = TRUE))\nNonSmoker_Age_3\n\n\n# A tibble: 1 × 1\n  `mean(LungCap, na.rm = TRUE)`\n                          <dbl>\n1                          10.4\n\n\nCode\nNonSmoker_Age_4 <- LungCapData %>%\n  filter(Smoke==\"no\")%>%\n  filter(Age>=18)%>%\n  select(Smoke, Age, LungCap)%>%\n  summarize(mean(LungCap, na.rm = TRUE))\nNonSmoker_Age_4\n\n\n# A tibble: 1 × 1\n  `mean(LungCap, na.rm = TRUE)`\n                          <dbl>\n1                          11.1\n\n\nThe corresponding LungCap mean values for non-smokers are 6.359, 8.844, 10.394, and 11.069. All LungCaps are higher for non-smokers with the exception of those in the 13-and-under age bracket. It would make sense that non-smokers would have higher lung capacities, though it is interesting that the value is lower for the youngest age group. This does, however, correlate with the results to question 1c, which indicated that smokers have higher lung capacities than non-smokers; there could, perhaps, be other extenuating circumstances contributing to these lower rates of capacity, such as health issues, development, or exposure.\n\n\nQuestion 2a: What is the probability that a randomly selected inmate has exactly 2 prior convictions?\n\n\nCode\ninmate <- c(128, 434, 160, 64, 24)\nmean(inmate)\n\n\n[1] 162\n\n\nCode\nsd(inmate)\n\n\n[1] 161.0838\n\n\nCode\n# mean=162\n# sd=161.1\n\nconvictions <- c(0:4)\n\ndata <- tibble(inmate, convictions)\n\ntwo <- 160/summarise(data, sum(inmate))\n  two\n\n\n  sum(inmate)\n1   0.1975309\n\n\nHere, we can see that the probability of an inmate having exactly two prior convictions is 19.8%.\n\n\nQuestion 2b: What is the probability that a randomly selected inmate has fewer than 2 prior convictions?\n\n\nCode\nfewer_than_two <- 128 + 434\nfewer_than_two/summarise(data, sum(inmate))\n\n\n  sum(inmate)\n1   0.6938272\n\n\nHere, we can see that the probability of a randomly selected inmate having fewer than two prior convictions is 69.4%.\n\n\nQuestion 2c: What is the probability that a randomly selected inmate has 2 or fewer prior convictions?\n\n\nCode\ntwo_or_fewer <- 128 + 434 + 160\ntwo_or_fewer/summarise(data, sum(inmate))\n\n\n  sum(inmate)\n1    0.891358\n\n\nHere, we can see that the probability of a randomly selected inmate having two or fewer prior convictions is 89.1%.\n\n\nQuestion 2d: What is the probability that a randomly selected inmate has more than 2 prior convictions?\n\n\nCode\nmore_than_two <- 160 + 64 + 24\nmore_than_two/summarise(data, sum(inmate))\n\n\n  sum(inmate)\n1   0.3061728\n\n\nHere, we can see that the probability of a randomly selected inmate having more than two prior convictions is 30.6%.\n\n\nQuestion 2e: What is the expected value1 for the number of prior convictions?\n\n\nCode\ndata_prob <- transform(data, probability = (inmate/810))\ndata_ev <- transform(data_prob, x = convictions*probability)\n\nEV <- sum(data_ev$x)\nEV\n\n\n[1] 1.28642\n\n\nThe expected value1 for the number of prior convictions is 1.3.\n\n\nQuestion 2f: Calculate the variance and the standard deviation for the Prior Convictions.\n\n\nCode\ninmate <- c(128, 434, 160, 64, 24)\nvar(inmate)\n\n\n[1] 25948\n\n\nCode\nsd(inmate)\n\n\n[1] 161.0838\n\n\nThe variance for the prior convictions is 25948, while the standard deviation is 161."
  },
  {
    "objectID": "posts/HW1_LTucksmith.html",
    "href": "posts/HW1_LTucksmith.html",
    "title": "HW1",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tidyr)\nknitr::opts_chunk$set(echo = TRUE)\n\n\n\n\nCode\ndf <- read_excel(\"_data/LungCapData.xls\")\n\n\n\n\n\n\nWhat does the distribution of LungCap look like?\n\n\n\nCode\nhist(df$LungCap)\n\n\n\n\n\nThe distribution looks normal, the observations are centered around the mean with minimal outliers.\n\nCompare the probability distribution of the LungCap with respect to Males and Females?\n\n\n\nCode\nlungCapP <- pnorm(df$LungCap, mean(df$LungCap), sd(df$LungCap))\nboxplot(lungCapP~df$Gender)\n\n\n\n\n\nFor both male and female, the probability distribution of the LungCap is similiar. For male, the median line is slightly higher than female, suggesting that their average lungCap is slightly higher for males than females. Also, for male the median is directly in the middle, suggesting that the data isn’t particularly skewed. For female the split appears to have more values fall below the mean than above, telling us that the data skews moreso than for male.\n\nCompare the mean lung capacities for smokers and non-smokers. Does it make sense?\n\n\n\nCode\ndfSmokeLungCap <- df %>% group_by(Smoke) %>% \n    summarise(mean_LungCap=mean(LungCap))\n\nprint(dfSmokeLungCap)\n\n\n# A tibble: 2 × 2\n  Smoke mean_LungCap\n  <chr>        <dbl>\n1 no            7.77\n2 yes           8.65\n\n\nNo, the lungCap average does not make sense. We would expect that the lung capacity for smokers would be lower than it is for non-smokers, but we are seeing the opposite, as the lungCap for smokers is higher than it is for non-smokers.\n\nExamine the relationship between Smoking and Lung Capacity within age groups: “less than or equal to 13”, “14 to 15”, “16 to 17”, and “greater than or equal to 18”.\n\n\n\nCode\ndfAgeRanges <- df %>% \n  mutate(ageRange = case_when(\n    df$Age<=13 ~ \"A\",\n    between(df$Age, 14, 15) ~ \"B\",\n    between(df$Age, 16, 17) ~ \"C\",\n    df$Age>=18 ~ \"D\"\n  ))\n\nprint(dfAgeRanges)\n\n\n# A tibble: 725 × 7\n   LungCap   Age Height Smoke Gender Caesarean ageRange\n     <dbl> <dbl>  <dbl> <chr> <chr>  <chr>     <chr>   \n 1    6.48     6   62.1 no    male   no        A       \n 2   10.1     18   74.7 yes   female no        D       \n 3    9.55    16   69.7 no    female yes       C       \n 4   11.1     14   71   no    male   no        B       \n 5    4.8      5   56.9 no    male   no        A       \n 6    6.22    11   58.7 no    female no        A       \n 7    4.95     8   63.3 no    male   yes       A       \n 8    7.32    11   70.4 no    male   no        A       \n 9    8.88    15   70.5 no    male   no        B       \n10    6.8     11   59.2 no    male   no        A       \n# … with 715 more rows\n\n\n\nCompare the lung capacities for smokers and non-smokers within each age group. Is your answer different from the one in part c. What could possibly be going on here?\n\n\n\nCode\ndfMeanAgeRanges <- dfAgeRanges %>% group_by(ageRange, Smoke) %>% \n    summarise(mean_LungCap=mean(LungCap))\n\n\n`summarise()` has grouped output by 'ageRange'. You can override using the\n`.groups` argument.\n\n\nCode\nprint(dfMeanAgeRanges)\n\n\n# A tibble: 8 × 3\n# Groups:   ageRange [4]\n  ageRange Smoke mean_LungCap\n  <chr>    <chr>        <dbl>\n1 A        no            6.36\n2 A        yes           7.20\n3 B        no            9.14\n4 B        yes           8.39\n5 C        no           10.5 \n6 C        yes           9.38\n7 D        no           11.1 \n8 D        yes          10.5 \n\n\nHere, there is a more varied relationship between smoking and lung capacity due to age as a mitigating variable. Here, the lung capacity for non-smokers is higher than the lung capacity for smokers in youngest age range, but the other age ranges display the opposite relationship. This may be due to the possibility that smoking has a stronger influence on lungCapacity the longer you smoke. So younger folks may have only smoked for a year or so, while folks in the older age ranges have spent many more years smoking. While there still isn’t a clear indirect or direct relationship between smoking and lung capacity with this view, adding the ageRange variable showed that there is more to the relationship than was found in part C.\n\n\n\n\n\nCode\nx <- c(0,1,2,3,4)\nfrequency <- c(128,434,160,64,24)\ndf_2 <- data.frame(x, frequency)\n\n\n\nWhat is the probability that a randomly selected inmate has exactly 2 prior convictions?\n\n\n\nCode\nprob_a = dpois(df_2[3,1], df_2[3,2]/(sum(df_2$frequency)))\nprint(prob_a)\n\n\n[1] 0.01601229\n\n\n\nWhat is the probability that a randomly selected inmate has fewer than 2 prior convictions?\n\n\n\nCode\nprob_b = ppois(df_2[2,1], (df_2[2,2]+df_2[1,2])/(sum(df_2$frequency)))\nprint(prob_b)\n\n\n[1] 0.8463379\n\n\n\nWhat is the probability that a randomly selected inmate has 2 or fewer prior convictions?\n\n\n\nCode\nprob_c = ppois(df_2[3,1], (df_2[3,2]+df_2[2,2]+df_2[1,2])/(sum(df_2$frequency)))\nprint(prob_c)\n\n\n[1] 0.9385585\n\n\n\nWhat is the probability that a randomly selected inmate has more than 2 prior convictions?\n\n\n\nCode\ndf_2d <- df_2[4:5,]\nprob_d = 1 - (ppois(df_2d[2,1],(df_2[4,2]+df_2[5,2])/(sum(df_2$frequency))))\nprint(prob_d)              \n\n\n[1] 1.15223e-07\n\n\n\nWhat is the expected value1 for the number of prior convictions?\n\n\n\nCode\nprobs_e = dpois(df_2$x, df_2$frequency/(sum(df_2$frequency)))\neval = sum(df_2$x*probs_e)\nprint(eval)\n\n\n[1] 0.3458039\n\n\n\nCalculate the variance and the standard deviation for the Prior Convictions.\n\n\n\nCode\nvar_f <- var(rep(df_2$x, df_2$frequency))\nprint(var_f)\n\n\n[1] 0.8572937\n\n\nCode\nsd_f <- sd(rep(df_2$x, df_2$frequency))\nprint(sd_f)\n\n\n[1] 0.9259016"
  },
  {
    "objectID": "posts/FelixBetancourt_HW3.html",
    "href": "posts/FelixBetancourt_HW3.html",
    "title": "Homework 3",
    "section": "",
    "text": "Code\nknitr::opts_chunk$set(echo = TRUE, warning = FALSE)"
  },
  {
    "objectID": "posts/FelixBetancourt_HW3.html#homework-3",
    "href": "posts/FelixBetancourt_HW3.html#homework-3",
    "title": "Homework 3",
    "section": "Homework 3",
    "text": "Homework 3\nDACSS 603, Spring 2023\n\n\nCode\n# Loading packages\n\nsuppressPackageStartupMessages(library(dplyr))\nsuppressPackageStartupMessages(library(tidyverse))\nlibrary(formattable)\nsuppressPackageStartupMessages(library(kableExtra))\nlibrary(ggplot2)\nsuppressPackageStartupMessages(library(alr4))\n\n\nUnited Nations (Data file: UN11 in alr4) The data in the file UN11 contains several variables, including ppgdp, the gross national product per person in U.S. dollars, and fertility, the birth rate per 1000 females, both from the year 2009. The data are for 199 localities, mostly UN member countries, but also other areas such as Hong Kong that are not independent countries.The data were collected from the United Nations (2011). We will study the dependence of fertility on ppgdp.\n\n\nCode\nstr(UN11)\n\n\n'data.frame':   199 obs. of  6 variables:\n $ region   : Factor w/ 8 levels \"Africa\",\"Asia\",..: 2 4 1 1 3 5 2 3 8 4 ...\n $ group    : Factor w/ 3 levels \"oecd\",\"other\",..: 2 2 3 3 2 2 2 2 1 1 ...\n $ fertility: num  5.97 1.52 2.14 5.13 2 ...\n $ ppgdp    : num  499 3677 4473 4322 13750 ...\n $ lifeExpF : num  49.5 80.4 75 53.2 81.1 ...\n $ pctUrban : num  23 53 67 59 100 93 64 47 89 68 ...\n - attr(*, \"na.action\")= 'omit' Named int [1:34] 4 5 8 28 41 67 68 72 79 83 ...\n  ..- attr(*, \"names\")= chr [1:34] \"Am Samoa\" \"Andorra\" \"Antigua and Barbuda\" \"Br Virigin Is\" ...\n\n\n\nIdentify the predictor and the response.\n\nPredictor: gross national product per person (ppgdp) Response: fertility\n\nDraw the scatterplot of fertility on the vertical axis versus ppgdp on the horizontal axis and summarize the information in this graph. Does a straight-line mean function seem to be plausible for a summary of this graph?\n\n\n\nCode\nggplot(UN11, aes(x = ppgdp, y = fertility)) +\n  geom_point()\n\n\n\n\n\nSeems that the fertility rate varies a lot when the gross domestic product per person is between 0 and 12,500, but above that level the fertility rate seems more homogeneous. Doesn’t seem possible to have a straight line mean between these to variables, at least without making changes to the two distributions. The relationship seems non-linear.\n\nDraw the scatterplot of log(fertility) versus log(ppgdp) using natural logarithms. Does the simple linear regression model seem plausible for a summary of this graph? If you use a different base of logarithms, the shape of the graph won’t change, but the values on the axes will change.\n\n\n\nCode\nplot(UN11$ppgdp, UN11$fertility, log = \"xy\", pch = 16, col = \"blue\", main = \"Scatterplot with Log Axes for Ferlitity and GDP per person\", \n     xlab = \"GDP-PP (log scale)\", ylab = \"Fertility  (log scale)\")\n\n\n\n\n\nNow there is a visible negative relationship between Fertility and GDP per person. Fertility rate seems to decrease when the GDP is higher.\n\nAnnual income, in dollars, is an explanatory variable in a regression analysis. For a British version of the report on the analysis, all responses are converted to British pounds sterling (1 pound equals about 1.33 dollars, as of 2016).\n\n\nHow, if at all, does the slope of the prediction equation change?\n\nYes, the slope of the prediction equation will change. This is because the units of measurement for the explanatory variable have changed, which affects the scale and interpretation of the slope.\n\nHow, if at all, does the correlation change?\n\nThe correlation should not change. Converting the unit of measurement does not change the strength and direction of the relation between two variables.\n\nWater runoff in the Sierras (Data file: water in alr4) Can Southern California’s water supply in future years be predicted from past data? One factor affecting water availability is stream runoff.\n\nIf runoff could be predicted, engineers, planners, and policy makers could do their jobs more efficiently. The data file contains 43 years’ worth of precipitation measurements taken at six sites in the Sierra Nevada mountains (labeled APMAM, APSAB, APSLAKE, OPBPC, OPRC, and OPSLAKE) and stream runoff volume at a site near Bishop, California, labeled BSAAM. Draw the scatterplot matrix for these data and summarize the information available from these plots. (Hint: Use the pairs() function.)\n\n\nCode\npairs(water[,1:8])\n\n\n\n\n\nThe water stream runoff in Bishop seems to have a positive correlation with the volume of precipitation in certain sites of the sierra nevada, specifically with OPSLAKE, OPRC and OPBPC. This means that while more precipitation in those sites, the water stream runoff in Bishop is higher.\n\nProfessor ratings (Data file: Rateprof in alr4) In the website and online forum RateMyProfessors.com, students rate and comment on their instructors. Launched in 1999, the site includes millions of ratings on thousands of instructors. The data file includes the summaries of the ratings of 364 instructors at a large campus in the Midwest (Bleske-Rechek and Fritsch, 2011).\n\nEach instructor included in the data had at least 10 ratings over a several year period. Students provided ratings of 1–5 on quality, helpfulness, clarity, easiness of instructor’s courses, and raterInterest in the subject matter covered in the instructor’s courses. The data file provides the averages of these five ratings. Create a scatterplot matrix of these five variables. Provide a brief description of the relationships between the five ratings.\n\n\nCode\nstr(Rateprof)\n\n\n'data.frame':   366 obs. of  17 variables:\n $ gender         : Factor w/ 2 levels \"female\",\"male\": 2 2 2 2 2 2 2 2 2 2 ...\n $ numYears       : int  7 6 10 11 11 10 7 11 11 7 ...\n $ numRaters      : int  11 11 43 24 19 15 17 16 12 18 ...\n $ numCourses     : int  5 5 2 5 7 9 3 3 4 4 ...\n $ pepper         : Factor w/ 2 levels \"no\",\"yes\": 1 1 1 1 1 1 1 1 1 1 ...\n $ discipline     : Factor w/ 4 levels \"Hum\",\"SocSci\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ dept           : Factor w/ 48 levels \"Accounting\",\"Anthropology\",..: 17 42 3 17 45 45 45 17 34 17 ...\n $ quality        : num  4.64 4.32 4.79 4.25 4.68 ...\n $ helpfulness    : num  4.64 4.55 4.72 4.46 4.68 ...\n $ clarity        : num  4.64 4.09 4.86 4.04 4.68 ...\n $ easiness       : num  4.82 4.36 4.6 2.79 4.47 ...\n $ raterInterest  : num  3.55 4 3.43 3.18 4.21 ...\n $ sdQuality      : num  0.552 0.902 0.453 0.933 0.65 ...\n $ sdHelpfulness  : num  0.674 0.934 0.666 0.932 0.82 ...\n $ sdClarity      : num  0.505 0.944 0.413 0.999 0.582 ...\n $ sdEasiness     : num  0.405 0.505 0.541 0.588 0.612 ...\n $ sdRaterInterest: num  1.128 1.074 1.237 1.332 0.975 ...\n\n\nCode\nsubset_rate <- Rateprof[, c(\"quality\", \"helpfulness\", \"clarity\", \"easiness\", \"raterInterest\")]\n\npairs(subset_rate)\n\n\n\n\n\nSeems an strong positive relationship among clarity, helpfulness and quality, then easiness seems to have a moderate positive relationship with clarity, helpfulness and quality, and if we see raterInterest as a dependant variable, it shows some patter of low relationship with those 4 qualities.\n\nFor the student.survey data file in the smss package, conduct regression analyses relating (by convention, y denotes the outcome variable, x denotes the explanatory variable)\n\n\ny = political ideology and x = religiosity, (ii) y = high school GPA and x = hours of TV watching. (You can use student.survey in the R console, after loading the package, to see what each variable means.)**\n\n\n\nCode\nsuppressPackageStartupMessages(library(smss))\ndata(\"student.survey\")\nstudent <- student.survey\nstr(student)\n\n\n'data.frame':   60 obs. of  18 variables:\n $ subj: int  1 2 3 4 5 6 7 8 9 10 ...\n $ ge  : Factor w/ 2 levels \"f\",\"m\": 2 1 1 1 2 2 2 1 2 2 ...\n $ ag  : int  32 23 27 35 23 39 24 31 34 28 ...\n $ hi  : num  2.2 2.1 3.3 3.5 3.1 3.5 3.6 3 3 4 ...\n $ co  : num  3.5 3.5 3 3.2 3.5 3.5 3.7 3 3 3.1 ...\n $ dh  : int  0 1200 1300 1500 1600 350 0 5000 5000 900 ...\n $ dr  : num  5 0.3 1.5 8 10 3 0.2 1.5 2 2 ...\n $ tv  : num  3 15 0 5 6 4 5 5 7 1 ...\n $ sp  : int  5 7 4 5 6 5 12 3 5 1 ...\n $ ne  : int  0 5 3 6 3 7 4 3 3 2 ...\n $ ah  : int  0 6 0 3 0 0 2 1 0 1 ...\n $ ve  : logi  FALSE FALSE FALSE FALSE FALSE FALSE ...\n $ pa  : Factor w/ 3 levels \"d\",\"i\",\"r\": 3 1 1 2 2 1 2 2 2 2 ...\n $ pi  : Ord.factor w/ 7 levels \"very liberal\"<..: 6 2 2 4 1 2 2 2 1 3 ...\n $ re  : Ord.factor w/ 4 levels \"never\"<\"occasionally\"<..: 3 2 3 2 1 2 2 2 2 1 ...\n $ ab  : logi  FALSE FALSE FALSE FALSE FALSE FALSE ...\n $ aa  : logi  FALSE FALSE FALSE FALSE FALSE FALSE ...\n $ ld  : logi  FALSE NA NA FALSE FALSE NA ...\n\n\n\nGraphically portray how the explanatory variable relates to the outcome variable in each of the two cases\n\n\n\nCode\nggplot(student, aes(x=re, y=pi)) + geom_point() +\n  labs(title=\"Scatter Plot Religion and Political Ideology\",\n        x =\"Frequency attending religious services\", y = \"Political ideology\")\n\n\n\n\n\nSeems that there a correlation between frequency of attending relegious services and political ideology. Specifically, seems that being conservative is related to attending more services.\n\n\nCode\nggplot(student, aes(x=tv, y=hi)) + geom_point() +\n  labs(title=\"Scatter Plot Hours watching TV and High School GPA\",\n        x =\"Average hours watching TV / week\", y = \"HS GPA Score\")\n\n\n\n\n\nIn the case of GPA score in High school and average hours watching TV per week, it is difficult to see a clear relationshp, but we can say that seems that when the studnet watch TV 10 hours or less per week, there is no correlation with the GPA, but we can say that there is an light trend to get lower GPA when the student watch TV more than 10 hours per week.\n\nSummarize and interpret results of inferential analyses.\n\n\n\nCode\nstudent <- student.survey %>%\n mutate(pi_num = case_when(\n         pi == \"very liberal\" ~ 1,\n         pi == \"liberal\" ~ 2,\n         pi == \"slightly liberal\" ~ 3,\n         pi == \"moderate\" ~ 4,\n         pi == \"slightly conservative\" ~ 5,\n         pi == \"conservative\" ~ 6,\n         pi == \"very conservative\" ~ 7,\n         ))\n\nstudent$pi_num <- as.numeric(student$pi_num)\n\n\nmodel1 <- lm(pi_num ~ re, data=student)\nsummary(model1)\n\n\n\nCall:\nlm(formula = pi_num ~ re, data = student)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.8889 -0.5172 -0.2667  1.2040  2.7333 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   3.5253     0.1958  18.000  < 2e-16 ***\nre.L          2.1864     0.3919   5.579 7.27e-07 ***\nre.Q          0.1049     0.3917   0.268    0.790    \nre.C         -0.6958     0.3915  -1.777    0.081 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.315 on 56 degrees of freedom\nMultiple R-squared:  0.3872,    Adjusted R-squared:  0.3544 \nF-statistic:  11.8 on 3 and 56 DF,  p-value: 4.282e-06\n\n\nWhen we see the rregression output, we can see that seems there is a significant incidence from frequency to attend religious services and political ideology, for instance, seems that attending to religious services more frequently is associated to being more conservative.\n\n\nCode\nmodel2 <- lm(hi ~ tv, data=student)\nsummary(model2)\n\n\n\nCall:\nlm(formula = hi ~ tv, data = student)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.2583 -0.2456  0.0417  0.3368  0.7051 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  3.441353   0.085345  40.323   <2e-16 ***\ntv          -0.018305   0.008658  -2.114   0.0388 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4467 on 58 degrees of freedom\nMultiple R-squared:  0.07156,   Adjusted R-squared:  0.05555 \nF-statistic: 4.471 on 1 and 58 DF,  p-value: 0.03879\n\n\nSeems that there is a negative relationship between average TV hours per week incidence in the GPA score, where as more TV hours watching TV is related to lower GPA score, specifically for every additional hour of TV per week the score can get lower in 3 points. And even thought the relationship is significant at 0.05, still that incidence from TV hours to score seems weak or low. I would elminate 2-3 outliners (the cases with more than 20 hours per week) and look again at the regression output, but I can anticipate non-significant incidence from hours watching TV.\nAlso, important to note that R squared is very low, so TV hours explain very little of the GPA score."
  },
  {
    "objectID": "posts/Final_Project_Check-In-Akhilesh.html",
    "href": "posts/Final_Project_Check-In-Akhilesh.html",
    "title": "603_Project_Check_In_Akhilesh",
    "section": "",
    "text": "The blood transfusion dataset contain 748 samples with 5 input features: Input Features: • Recency (number of months since the last donation) • Frequency (total number of donations) • Monetary (total blood donated in c.c.) • Time (number of months since the first donation) • Age (age of the donor)\nSource: https://www.kaggle.com/datasets/ninalabiba/blood-transfusion-dataset\n\n\nCode\nBD <- read_csv(\"_data/transfusion.csv\")\n\n\nRows: 748 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (5): Recency (months), Frequency (times), Monetary (c.c. blood), Time (m...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nCode\nBD\n\n\n# A tibble: 748 × 5\n   `Recency (months)` `Frequency (times)` Monetary (c.c. blood…¹ Time …² wheth…³\n                <dbl>               <dbl>                  <dbl>   <dbl>   <dbl>\n 1                  2                  50                  12500      98       1\n 2                  0                  13                   3250      28       1\n 3                  1                  16                   4000      35       1\n 4                  2                  20                   5000      45       1\n 5                  1                  24                   6000      77       0\n 6                  4                   4                   1000       4       0\n 7                  2                   7                   1750      14       1\n 8                  1                  12                   3000      35       0\n 9                  2                   9                   2250      22       1\n10                  5                  46                  11500      98       1\n# … with 738 more rows, and abbreviated variable names\n#   ¹​`Monetary (c.c. blood)`, ²​`Time (months)`,\n#   ³​`whether he/she donated blood in March 2007`"
  },
  {
    "objectID": "posts/Final_Project_Check-In-Akhilesh.html#research-questions",
    "href": "posts/Final_Project_Check-In-Akhilesh.html#research-questions",
    "title": "603_Project_Check_In_Akhilesh",
    "section": "Research Questions:",
    "text": "Research Questions:\nBlood Donation Prediction Frequency:\nThe aim of this study is to develop linear regression, logistic regression machine learning model, to accurately predict whether a blood donor is likely to donate in the future. The results of this study could be useful in developing targeted strategies for donor recruitment and retention, ultimately improving the availability and accessibility of blood donations.\nIdentify Factors that Affect Blood Donation:\nHow do donation behaviors vary across different regions, and what factors may contribute to these variations? Using hypothesis testing, this research project aims to compare the donation patterns of donors from different regions based on demographic and donation-related variables, such as age, gender, donation frequency, and time since last donation. The findings can provide insights into the regional variations in blood donation behaviors and inform targeted strategies to address these differences, potentially leading to increased donation rates and more efficient allocation of resources for blood donation organizations. The study will visualize the results using plots and charts to identify any significant patterns or trends that could help healthcare and blood donation organizations to develop effective strategies to increase blood donation rates.\nSegmentation of Blood Donors:\nUse clustering techniques to segment blood donors based on their demographic and donation history characteristics. Explore methods such as K-means clustering or hierarchical clustering to identify groups of donors with similar characteristics. Examine the differences between these groups and explore their donation patterns over time.\nBuilding a Donor Retention Strategy:\nUse the insights gained from the previous projects to develop a donor retention strategy for the blood donation center. Identify factors that are associated with donor churn (i.e., donors who stop donating blood), and develop a plan to mitigate these factors. This can help healthcare and blood donation organizations to develop strategies to retain donors over the long term."
  },
  {
    "objectID": "posts/Final_Project_Check-In-Akhilesh.html#summary-of-the-data",
    "href": "posts/Final_Project_Check-In-Akhilesh.html#summary-of-the-data",
    "title": "603_Project_Check_In_Akhilesh",
    "section": "Summary of the data",
    "text": "Summary of the data\n\n\nCode\nsummary(BD)\n\n\n Recency (months) Frequency (times) Monetary (c.c. blood) Time (months)  \n Min.   : 0.000   Min.   : 1.000    Min.   :  250         Min.   : 2.00  \n 1st Qu.: 2.750   1st Qu.: 2.000    1st Qu.:  500         1st Qu.:16.00  \n Median : 7.000   Median : 4.000    Median : 1000         Median :28.00  \n Mean   : 9.507   Mean   : 5.515    Mean   : 1379         Mean   :34.28  \n 3rd Qu.:14.000   3rd Qu.: 7.000    3rd Qu.: 1750         3rd Qu.:50.00  \n Max.   :74.000   Max.   :50.000    Max.   :12500         Max.   :98.00  \n whether he/she donated blood in March 2007\n Min.   :0.000                             \n 1st Qu.:0.000                             \n Median :0.000                             \n Mean   :0.238                             \n 3rd Qu.:0.000                             \n Max.   :1.000"
  },
  {
    "objectID": "posts/Final_Project_Check-In-Akhilesh.html#descrpition-of-the-variables",
    "href": "posts/Final_Project_Check-In-Akhilesh.html#descrpition-of-the-variables",
    "title": "603_Project_Check_In_Akhilesh",
    "section": "Descrpition of the Variables:",
    "text": "Descrpition of the Variables:\nThis summary function presents a statistical description of a dataset related to blood donation, with five variables: Recency, Frequency, Monetary, Time, and whether the individual donated blood in March 2007. Here’s a breakdown of each variable:\nRecency (months): This variable represents the number of months since the last blood donation. The minimum value is 0 months, indicating that some individuals donated blood very recently. The mean is 9.507 months, suggesting that, on average, people donated blood around 9.5 months ago. The maximum value is 74 months, which means the longest gap between donations is 74 months.\nFrequency (times): This variable shows the total number of times an individual has donated blood. The minimum value is 1, meaning that at least one person has only donated blood once. The mean is 5.515 times, indicating that people, on average, have donated blood about 5.5 times. The maximum value is 50 times, showing that some individuals have donated blood quite frequently.\nMonetary (c.c. blood): This variable represents the total volume of blood donated by an individual, measured in cubic centimeters (c.c.). The minimum value is 250 c.c., which corresponds to the minimum single donation volume. The mean is 1379 c.c., suggesting that, on average, individuals have donated around 1.379 liters of blood. The maximum value is 12,500 c.c., indicating that the highest total volume donated by a person is 12.5 liters.\nTime (months): This variable measures the length of time an individual has been donating blood. The minimum value is 2 months, suggesting that some individuals are relatively new to blood donation. The mean is 34.28 months, indicating that, on average, people have been donating blood for about 34.3 months. The maximum value is 98 months, showing that some individuals have been donating blood for a long time.\nWhether he/she donated blood in March 2007: This is a binary variable that indicates if an individual donated blood in March 2007. The mean is 0.238, which means that about 23.8% of the individuals in the dataset donated blood in that specific month.\nThe summary function provides an overview of the dataset’s key statistics, such as minimum, 1st quartile, median, mean, 3rd quartile, and maximum values for each variable. This information helps to understand the distribution, central tendency, and spread of the data."
  },
  {
    "objectID": "posts/Final_Project_Check-In-Akhilesh.html#expected-contribution",
    "href": "posts/Final_Project_Check-In-Akhilesh.html#expected-contribution",
    "title": "603_Project_Check_In_Akhilesh",
    "section": "Expected Contribution:",
    "text": "Expected Contribution:\n\nAkhilesh Kumar: Will work on the Segmentation of Blood Donors and Building a Donor Retention Strategy\nSai Srinivas: Will work on the Blood Donation Prediction Frequency and Identify Factors that Affect Blood Donation"
  },
  {
    "objectID": "posts/Final_Project_Check-In-Akhilesh.html#references",
    "href": "posts/Final_Project_Check-In-Akhilesh.html#references",
    "title": "603_Project_Check_In_Akhilesh",
    "section": "References:",
    "text": "References:\nKaggle: https://www.kaggle.com/datasets/ninalabiba/blood-transfusion-dataset\nOriginal Owner and Donor: Prof. I-Cheng Yeh, Department of Information Management, Chung-Hua University, Hsin Chu, Taiwan 30067, R.O.C., e-mail:icyeh ‘@’ chu.edu.tw, TEL:886-3-5186511, Date Donated: October 3, 2008"
  },
  {
    "objectID": "posts/HW1_young.html",
    "href": "posts/HW1_young.html",
    "title": "Homework 1",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.1     ✔ purrr   1.0.1\n✔ tibble  3.1.8     ✔ dplyr   1.1.0\n✔ tidyr   1.3.0     ✔ stringr 1.5.0\n✔ readr   2.1.4     ✔ forcats 1.0.0\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\nCode\nlibrary(dplyr)\nlibrary(readxl)\ndf <- read_excel(\"C:/Users/rotte/Documents/R/603_Spring_2023/posts/_data/LungCapData.xls\")"
  },
  {
    "objectID": "posts/HW1_young.html#a",
    "href": "posts/HW1_young.html#a",
    "title": "Homework 1",
    "section": "a",
    "text": "a\n\n\nCode\n# descriptive statistics\nsummary(df$LungCap)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  0.507   6.150   8.000   7.863   9.800  14.675 \n\n\nCode\nsd(df$LungCap)\n\n\n[1] 2.662008\n\n\n\n\nCode\n# making histogram\nhist(df$LungCap)\n\n\n\n\n\nRange is 0.507~14.675. Median is 8.00. And it’s distribution is looks like normal distribution that mean is 7.863 and sd is 2.662."
  },
  {
    "objectID": "posts/HW1_young.html#b",
    "href": "posts/HW1_young.html#b",
    "title": "Homework 1",
    "section": "b",
    "text": "b\n\n\nCode\n# descriptive statistics\ndf %>%\n  group_by(Gender) %>%\n  summarise(mean(LungCap), sd(LungCap))\n\n\n# A tibble: 2 × 3\n  Gender `mean(LungCap)` `sd(LungCap)`\n  <chr>            <dbl>         <dbl>\n1 female            7.41          2.56\n2 male              8.31          2.68\n\n\n\n\nCode\n# making boxplot\nboxplot(LungCap~Gender, df)\n\n\n\n\n\nMean and sd of female’s LungCap are 7.406 and 2.564, respectively. And Mean and sd of male’s LungCap are 8.309 and 2.683, respectively. Male’s LungCap is bigger than female’s. We can also check this through boxplot."
  },
  {
    "objectID": "posts/HW1_young.html#c",
    "href": "posts/HW1_young.html#c",
    "title": "Homework 1",
    "section": "c",
    "text": "c\n\n\nCode\n# finding lungcap for smokers and non-smokers\ndf %>% group_by(Smoke) %>%\n  summarise(mean(LungCap))\n\n\n# A tibble: 2 × 2\n  Smoke `mean(LungCap)`\n  <chr>           <dbl>\n1 no               7.77\n2 yes              8.65\n\n\nSmokers have bigger Lung cap. It’s a different result from common sense. Through the t-test, I will find out whether this result is statistically significant.\n\n\nCode\nt.test(LungCap~Smoke, df, alternative=\"less\")\n\n\n\n    Welch Two Sample t-test\n\ndata:  LungCap by Smoke\nt = -3.6498, df = 117.72, p-value = 0.0001964\nalternative hypothesis: true difference in means between group no and group yes is less than 0\n95 percent confidence interval:\n       -Inf -0.4776762\nsample estimates:\n mean in group no mean in group yes \n         7.770188          8.645455 \n\n\nAs a result of the one-sided t-test, it was found to be statistically significant at the 95% level of significance.\n##d\nFirst, a new variable cAge is created and a new value is given for each age. For those under the age of 13, “Child”, 14, 15 years of age “Middle”, 16, 17 years of age “High”, and 18 years of age or older, “Adult” will be assigned.\n\n\nCode\ndf<-mutate(df, cAge = ifelse(Age<=13, \"Child\", ifelse(Age %in% 14:15, \"Middle\", ifelse(Age %in% 16:17, \"High\", \"Adult\"))))\n\n\n\n\nCode\ndf %>% group_by(cAge) %>%\n  summarise(mean(LungCap))\n\n\n# A tibble: 4 × 2\n  cAge   `mean(LungCap)`\n  <chr>            <dbl>\n1 Adult            11.0 \n2 Child             6.41\n3 High             10.2 \n4 Middle            9.05\n\n\n\n\nCode\nggplot(df, aes(x=Age, y=LungCap)) +\n  geom_point()\n\n\n\n\n\nLooking at each group’s lung caps, child is 6.41, middle is 9.05, high is 10.25, and adult is 10.96. That is, the lung caps grow with age. Here, it is possible to infer why the lung caps of smokers and non-smokers presented in this data are different from our common sense. The more adults there are, the more smokers there will be, and that may have led to a larger lung cap for smokers.\n##e\nFirst, let’s look at the children’s group.\n\n\nCode\nchilddf<-filter(df, cAge==\"Child\")\ntable(childdf$Smoke)\n\n\n\n no yes \n401  27 \n\n\nCode\nchilddf%>%group_by(Smoke) %>%\n  summarise(mean(LungCap))\n\n\n# A tibble: 2 × 2\n  Smoke `mean(LungCap)`\n  <chr>           <dbl>\n1 no               6.36\n2 yes              7.20\n\n\nSmokers have bigger lung cap. Let’s look at the picture in more detail.\n\n\nCode\nggplot(childdf, aes(x=Age, y=LungCap)) +\n  geom_point(aes(col=factor(Smoke)))\n\n\n\n\n\nFrom the plot, the older the age, the larger the lung cap for non-smokers. In other words, when looking at the entire child group, the growth of natural lung caps with growth is not well revealed, so smokers’ lung caps seem to be larger.\nNext, let’s look at the middle group.\n\n\nCode\nmiddf<-filter(df, cAge==\"Middle\")\ntable(middf$Smoke)\n\n\n\n no yes \n105  15 \n\n\nCode\nmiddf%>%group_by(Smoke) %>%\n  summarise(mean(LungCap))\n\n\n# A tibble: 2 × 2\n  Smoke `mean(LungCap)`\n  <chr>           <dbl>\n1 no               9.14\n2 yes              8.39\n\n\n\n\nCode\nboxplot(LungCap~Smoke, middf)\n\n\n\n\n\nNon-smokers of middle group seem to have bigger lung caps.\nThen, let’s look at the high group.\n\n\nCode\nhighdf<-filter(df, cAge==\"High\")\ntable(highdf$Smoke)\n\n\n\n no yes \n 77  20 \n\n\nCode\nhighdf%>%group_by(Smoke) %>%\n  summarise(mean(LungCap))\n\n\n# A tibble: 2 × 2\n  Smoke `mean(LungCap)`\n  <chr>           <dbl>\n1 no              10.5 \n2 yes              9.38\n\n\n\n\nCode\nboxplot(LungCap~Smoke, highdf)\n\n\n\n\n\nNon-smokers of high group also have bigger lung caps.\nLastly, let me check the adult group.\n\n\nCode\nadultdf<-filter(df, cAge==\"Adult\")\ntable(adultdf$Smoke)\n\n\n\n no yes \n 65  15 \n\n\nCode\nadultdf%>%group_by(Smoke) %>%\n  summarise(mean(LungCap))\n\n\n# A tibble: 2 × 2\n  Smoke `mean(LungCap)`\n  <chr>           <dbl>\n1 no               11.1\n2 yes              10.5\n\n\n\n\nCode\nboxplot(LungCap~Smoke, adultdf)\n\n\n\n\n\nEven in the adult group, non-smokers have a bigger lung cap.\nFinally, I took a look at the overall plot.\n\n\nCode\nggplot(df, aes(x=Age, y=LungCap)) +\n  geom_point(aes(col=factor(Smoke)))\n\n\n\n\n\nOverall, it seems that the lung cap of non-smokers (red) is higher than that of smokers (blue)."
  },
  {
    "objectID": "posts/HW1_young.html#a-1",
    "href": "posts/HW1_young.html#a-1",
    "title": "Homework 1",
    "section": "a",
    "text": "a\nThe probability of selecting inmate has exact 2 priority convictions is the number of inmates with 2 priority convictions divided by the total number of inmates.\n\n\nCode\n160/sum(p_df$freq)\n\n\n[1] 0.1975309\n\n\nThe answer is 0.1975."
  },
  {
    "objectID": "posts/HW1_young.html#b-1",
    "href": "posts/HW1_young.html#b-1",
    "title": "Homework 1",
    "section": "b",
    "text": "b\nIn the same way, the frequency of 0 priority convictions and 1 priority convictions can be combined and divided into the total number of prisoners.\n\n\nCode\n(128+434)/sum(p_df$freq)\n\n\n[1] 0.6938272\n\n\nThe answer is 0.6938."
  },
  {
    "objectID": "posts/HW1_young.html#c-1",
    "href": "posts/HW1_young.html#c-1",
    "title": "Homework 1",
    "section": "c",
    "text": "c\nAdd the probabilities obtained from problems a and b to get the answer.\n\n\nCode\n(160/sum(p_df$freq))+((128+434)/sum(p_df$freq))\n\n\n[1] 0.891358\n\n\nThe answer is 0.8914."
  },
  {
    "objectID": "posts/HW1_young.html#d",
    "href": "posts/HW1_young.html#d",
    "title": "Homework 1",
    "section": "d",
    "text": "d\nThis time, we can get the answer using the probability obtained from c. The total sum of the probabilities is 1, so you can get the answer by subtracting the value obtained from 1 and c.\n\n\nCode\n1-((160/sum(p_df$freq))+((128+434)/sum(p_df$freq)))\n\n\n[1] 0.108642\n\n\nThe answer is 0.1086."
  },
  {
    "objectID": "posts/HW1_young.html#e",
    "href": "posts/HW1_young.html#e",
    "title": "Homework 1",
    "section": "e",
    "text": "e\nTo obtain the expected value, we divide the sum of priority convictions(x) times frequency (freq) by the total number of prisoners.\n\n\nCode\nsum(p_df$x*p_df$freq)/810\n\n\n[1] 1.28642\n\n\nIn another way, the probability of priority conversions can be obtained and the expected value can be obtained by summing each frequency multiplied by this value.\n\n\nCode\np_df<-mutate(p_df, pro=freq/810)\np_df\n\n\n  x freq        pro\n1 0  128 0.15802469\n2 1  434 0.53580247\n3 2  160 0.19753086\n4 3   64 0.07901235\n5 4   24 0.02962963\n\n\n\n\nCode\nsum(p_df$x*p_df$pro)\n\n\n[1] 1.28642\n\n\nThe answer is the same."
  },
  {
    "objectID": "posts/HW1_young.html#f",
    "href": "posts/HW1_young.html#f",
    "title": "Homework 1",
    "section": "f",
    "text": "f\n\n\nCode\nmean<-sum(p_df$x*p_df$pro)\n\n\nFirst, to obtain the variance, get the sum of the squared difference between the x-value and the average value (expected value) and divided by the total number of prisoners.\nThe standard deviation is the square root of the variance.\n\n\nCode\nsum((x-mean)^2*p_df$freq)/810\n\n\n[1] 0.8562353\n\n\nCode\nsqrt(sum((x-mean)^2*p_df$freq)/810)\n\n\n[1] 0.9253298\n\n\nVariance is 0.8562 and standard deviation is 0.925.\nAlternatively, the variance can be obtained by multiplying the square of the difference between the x-value and the average value by each probability.\n\n\nCode\nsum((x-mean)^2*p_df$pro)\n\n\n[1] 0.8562353\n\n\nCode\nsqrt(sum((x-mean)^2*p_df$pro))\n\n\n[1] 0.9253298\n\n\nAs expected, the answer is the same."
  },
  {
    "objectID": "posts/dacss603hw3_LauraCollazo.html",
    "href": "posts/dacss603hw3_LauraCollazo.html",
    "title": "DACSS 603 Homework 3",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(alr4)\nlibrary(smss)"
  },
  {
    "objectID": "posts/dacss603hw3_LauraCollazo.html#a",
    "href": "posts/dacss603hw3_LauraCollazo.html#a",
    "title": "DACSS 603 Homework 3",
    "section": "a",
    "text": "a\nThe predictor variable is ppgdp and the response variable is fertility."
  },
  {
    "objectID": "posts/dacss603hw3_LauraCollazo.html#b",
    "href": "posts/dacss603hw3_LauraCollazo.html#b",
    "title": "DACSS 603 Homework 3",
    "section": "b",
    "text": "b\nA scatterplot of these variables is shown below. It revels that woman tend to have less children as GDP increases.\nA straight line function does not seem plausible for this graph as points are clustered primarily in one area. A linear regression line would therefore not be the best fit for the data.\n\n\nCode\nggplot(UN11, aes(x = ppgdp, y = fertility)) + \n  geom_point() +\nlabs(x = \"Per capita gross domestic product in US dollars\",\n     y = \"Number of children per woman\")"
  },
  {
    "objectID": "posts/dacss603hw3_LauraCollazo.html#c",
    "href": "posts/dacss603hw3_LauraCollazo.html#c",
    "title": "DACSS 603 Homework 3",
    "section": "c",
    "text": "c\nWhen log-log is used, a simple linear regression model does seem plausible to summarize this graph.\n\n\nCode\nggplot(UN11, aes(x = log(ppgdp), y = log(fertility))) + \n  geom_point() +\nlabs(x = \"Log of per capita gross domestic product in US dollars\",\n     y = \"Log of number of children per woman\")"
  },
  {
    "objectID": "posts/dacss603hw3_LauraCollazo.html#a-1",
    "href": "posts/dacss603hw3_LauraCollazo.html#a-1",
    "title": "DACSS 603 Homework 3",
    "section": "a",
    "text": "a\nThe slope of the equation will change because the explanatory variable is responsible for determining the slope."
  },
  {
    "objectID": "posts/dacss603hw3_LauraCollazo.html#b-1",
    "href": "posts/dacss603hw3_LauraCollazo.html#b-1",
    "title": "DACSS 603 Homework 3",
    "section": "b",
    "text": "b\nThe correlation will not change as correlation is not based on units."
  },
  {
    "objectID": "posts/dacss603hw3_LauraCollazo.html#a-2",
    "href": "posts/dacss603hw3_LauraCollazo.html#a-2",
    "title": "DACSS 603 Homework 3",
    "section": "1a",
    "text": "1a\nThe below graph shows how the explanatory variable (religiosity) relates to the outcome variable (political ideology).\n\n\nCode\ndata(\"student.survey\")\n\nggplot(student.survey, aes(x = re, y = pi)) + \n  geom_jitter() +\nlabs(title = \"Political idology & church attendance\",\n     x = \"religious service attendance\",\n     y = \"political ideology\")"
  },
  {
    "objectID": "posts/dacss603hw3_LauraCollazo.html#b-2",
    "href": "posts/dacss603hw3_LauraCollazo.html#b-2",
    "title": "DACSS 603 Homework 3",
    "section": "1b",
    "text": "1b\nThe plot reveals there is a weak positive correlation between how often a students attend religious services and how conservative they are."
  },
  {
    "objectID": "posts/dacss603hw3_LauraCollazo.html#a-3",
    "href": "posts/dacss603hw3_LauraCollazo.html#a-3",
    "title": "DACSS 603 Homework 3",
    "section": "2a",
    "text": "2a\nThe below graph shows how the explanatory variable (hours of TV watching) relates to the outcome variable (high school GPA).\n\n\nCode\nggplot(student.survey, aes(x = tv, y = hi)) + \n  geom_jitter() +\nlabs( title = \"High school GPA & average weekly hours of TV\",\n      x = \"average hours of TV per week\",\n      y = \"high school GPA\")"
  },
  {
    "objectID": "posts/dacss603hw3_LauraCollazo.html#b-3",
    "href": "posts/dacss603hw3_LauraCollazo.html#b-3",
    "title": "DACSS 603 Homework 3",
    "section": "2b",
    "text": "2b\nGlancing at the plot there doesn’t appear to be much of a correlation. However, when looking at the summary of the regression below, p < .05 which indicates statistical significance. R-squared is very low, though, meaning the regression is not a strong prediction model. The plot below includes the regression line to visualize this.\n\n\nCode\n summary(lm(hi~tv, data = student.survey))\n\n\n\nCall:\nlm(formula = hi ~ tv, data = student.survey)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.2583 -0.2456  0.0417  0.3368  0.7051 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  3.441353   0.085345  40.323   <2e-16 ***\ntv          -0.018305   0.008658  -2.114   0.0388 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4467 on 58 degrees of freedom\nMultiple R-squared:  0.07156,   Adjusted R-squared:  0.05555 \nF-statistic: 4.471 on 1 and 58 DF,  p-value: 0.03879\n\n\n\n\nCode\nggplot(student.survey, aes(x = tv, y = hi)) + \n  geom_jitter() +\n  geom_smooth(method = \"lm\") +\nlabs( title = \"High school GPA & average weekly hours of TV\",\n      x = \"average hours of TV per week\",\n      y = \"high school GPA\")\n\n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "posts/Rowley_Homework_2.html",
    "href": "posts/Rowley_Homework_2.html",
    "title": "Homework 2",
    "section": "",
    "text": "Code\n# load libraries\n\nlibrary(dplyr)\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nCode\nlibrary(magrittr)\nlibrary(ggplot2)\nlibrary(markdown)\nlibrary(ggtext)\n\n\nWarning: package 'ggtext' was built under R version 4.2.2\n\n\nCode\nlibrary(readxl)\n\n\nWarning: package 'readxl' was built under R version 4.2.2\n\n\n\nQuestion 1\nThe time between the date a patient was recommended for heart surgery and the surgery date\nfor cardiac patients in Ontario was collected by the Cardiac Care Network (“Wait Times Data\nGuide,” Ministry of Health and Long-Term Care, Ontario, Canada, 2006). The sample mean\nand sample standard deviation for wait times (in days) of patients for two cardiac procedures\nare given in the accompanying table. Assume that the sample is representative of the Ontario\npopulation. Construct the 90% confidence interval to estimate the actual mean wait time for each of the two procedures. Is the confidence interval narrower for angiography or bypass surgery?\n\n\nCode\n# bypass: sample size = 539, mean wait time = 19, SD = 10\n\nb_size=539\nb_mean=19\nb_sd=10\nb_ci=0.9\n\n# calculate standard error:\nbypass_se = b_sd/sqrt(b_size)\n\n# specify confidence interval:\nbypass_tail <- (1-b_ci)/2\nprint(bypass_tail)\n\n\n[1] 0.05\n\n\nCode\n# calculate t-value:\nb_t_value <-  qt(p=1-bypass_tail, df=b_size-1)\nprint(b_t_value)\n\n\n[1] 1.647691\n\n\nCode\n# calculate confidence intervals:\nb_conf_int <- c(b_mean-b_t_value*bypass_se, b_mean+b_t_value*bypass_se)\nprint(b_conf_int)\n\n\n[1] 18.29029 19.70971\n\n\nThe 90% confidence interval for wait-time for bypass surgery is 18.3–19.7 days.\n\n\nCode\n# angiography: sample size = 847, mean wait time = 18, SD = 9\n\na_size=847\na_mean=18\na_sd=9\na_ci=0.9\n\n# calculate standard error:\nangio_se = a_sd/sqrt(a_size)\n\n# specify confidence interval:\nangio_tail <- (1-a_ci)/2\nprint(angio_tail)\n\n\n[1] 0.05\n\n\nCode\n# calculate t-value:\na_t_value <-  qt(p=1-angio_tail, df=a_size-1)\nprint(a_t_value)\n\n\n[1] 1.646657\n\n\nCode\n# calculate confidence intervals:\na_conf_int <- c(a_mean-a_t_value*angio_se, a_mean+a_t_value*angio_se)\nprint(a_conf_int)\n\n\n[1] 17.49078 18.50922\n\n\nThe 90% confidence interval for wait-time for angiography surgery is 17.5–18.5 days.\n\n\nCode\n# calculate size of confidence interval for bypass:\n\nbypass_ci_size=19.70971-18.29029\nprint(bypass_ci_size)\n\n\n[1] 1.41942\n\n\n\n\nCode\n# calculate size of confidence interval for angiography:\n\nangio_ci_size=18.50922-17.49078\nprint(angio_ci_size)\n\n\n[1] 1.01844\n\n\nThe confidence level is narrower for angiography surgery (1.01844) than it is for bypass surgery (1.41942).\n\n\nQuestion 2\nA survey of 1031 adult Americans was carried out by the National Center for Public\nPolicy. Assume that the sample is representative of adult Americans. Among those surveyed, 567\nbelieved that college education is essential for success. Find the point estimate, p, of the\nproportion of all adult Americans who believe that a college education is essential for success.\nConstruct and interpret a 95% confidence interval for p.\n\n\nCode\n# binomial test - compares a sample proportion to a hypothesized proportion\n\ntotal_pop = 1031\nsurvey_pop = 567\n\nbinom.test(survey_pop, total_pop)\n\n\n\n    Exact binomial test\n\ndata:  survey_pop and total_pop\nnumber of successes = 567, number of trials = 1031, p-value = 0.001478\nalternative hypothesis: true probability of success is not equal to 0.5\n95 percent confidence interval:\n 0.5189927 0.5806243\nsample estimates:\nprobability of success \n             0.5499515 \n\n\nThe proportion of American adults who believe that a college education is essential for success, or p, is 0.55%, which falls between the 95% confidence interval of 52%-58%.\n\n\nQuestion 3\nSuppose that the financial aid office of UMass Amherst seeks to estimate the mean cost\nof textbooks per semester for students. The estimate will be useful if it is within $5 of the true\npopulation mean (i.e. they want the confidence interval to have a length of $10 or less). The\nfinancial aid office is pretty sure that the amount spent on books varies widely, with most values\nbetween $30 and $200. They think that the population standard deviation is about a quarter of\nthis range (in other words, you can assume they know the population standard deviation). Assuming the significance level to be 5%, what should be the size of the sample?\n\n\nCode\n# find sample size using [error=z*SD/sqrt of n]\n\n# range in cost of books:\nrange=200-30\n\n# population SD is 1/4 of range:\npopulation_sd = range/4\nprint(population_sd)\n\n\n[1] 42.5\n\n\nCode\n# If 95% of the area lies between −z and z, then 5% of the area must lie outside of this range. Since normal curves are symmetric, half of this amount (2.5%) must lie before −z. Then the area under the curve before z must be: 0.025+0.95=0.975. The number z is the 97.5th percentile of the standard normal distribution:\nz=qnorm(.975)\n\n# estimate is within $5 of true population mean:\nn=((z*population_sd)/5)^2\nprint(n)\n\n\n[1] 277.5454\n\n\nThe sample size should be 278.\n\n\nQuestion 4\nAccording to a union agreement, the mean income for all senior-level workers in a large service company equals $500 per week. A representative of a women’s group decides to analyze whether the mean income ‘u’ for female employees matches this norm. For a random sample of nine female employees, y=$410 and s=90.\nA. Test whether the mean income of female employees differs from $500 per week. Include assumptions, hypotheses, test statistics, and P-value. Interpret the result.\nB. Report the P-value for Ha: u<500. Interpret.\nC. Report and interpret the P-value for Ha: u>500. (Hint: the P-values for the two possible one-sided tests must sum to 1).\n\nA)\n\n\nCode\n# mean income = $500/week\n# mean income female: y=$410, s=90\n\n# A)\n# test statistics: describes how far your observed data is from the null hypothesis of no relationship between variables or no difference among sample groups (support or reject null)\n# formula: x_bar is sample mean, mu is hypothesized population mean, sd is sample standard deviation, and n is sample size.\n# formula: (sample mean-population mean)/(standard deviation/sqrt(sample size))\n\ntest_stat <- function(x_bar, mu, sd, n){return((x_bar-mu)/(sd/sqrt(n)))}\n\nsample_mean=410\npop_mean=500\nsd=90\nsample_size=9\n\n# find t-value:\nt_stat <- test_stat(sample_mean, pop_mean, sd, sample_size)\nprint(t_stat)\n\n\n[1] -3\n\n\n\n\nCode\n# t-value is negative, so find lower tail\n# degree of freedom = n-1\n# find p-value:\nlow_p_value <- pt(q=t_stat, df=8, lower.tail=TRUE)\nprint(low_p_value)\n\n\n[1] 0.008535841\n\n\nCode\n# find p-value for two-tailed t-test:\nlow_p_value <- 2*pt(t_stat, 8)\nprint(low_p_value)\n\n\n[1] 0.01707168\n\n\nMy assumption is that the mean income of female employees will differ from the mean of all senior-level workers ($500/week). After running test statistics, we see that the t-stat is -3, meaning that females’ average pay is 3 standard deviations from the population mean. We also see that the p-value is 0.02, which means that we can reject the null hypothesis, which assumes that there is no significant difference in pay across genders (i.e., females make $500/week on average).\n\n\nB)\n\n\nCode\n# B\n# calculate p-value for LT alternative hypothesis (Ha: u<500):\n\np_Ha = pt(q=t_stat, df=8, lower.tail=TRUE)\np_Ha\n\n\n[1] 0.008535841\n\n\nThe p-value for the lower-tail alternative hypothesis is 0.009, which supports our previous assertion that we can reject the null hypothesis and accept the alternative. This indicates that u≠500. In other words, females do not make $500/week.\n\n\nC)\n\n\nCode\n# C\n# calculate p-value for UT alternative hypothesis 2 (Ha: u>500):\n\np_Ha = pt(q=t_stat, df=8, lower.tail=FALSE)\np_Ha\n\n\n[1] 0.9914642\n\n\nThe p-value for the upper-tail alternative hypothesis is 0.991, which indicates that ‘u’ is not greater than 500. In other words, we now know that, on average, females make less than $500/week.\n\n\n\nQuestion 5\nJones and Smith separately conduct studies to test H0: μ = 500 against Ha: μ ≠ 500, each with n = 1000. Jones gets ȳ = 519.5, with se = 10.0. Smith gets ȳ = 519.7, with se = 10.0.\nA. Show that t = 1.95 and P-value = 0.051 for Jones. Show that t = 1.97 and P-value = 0.049 for Smith.\nB. Using α = 0.05, for each study indicate whether the result is “statistically significant.”\nC. Using this example, explain the misleading aspects of reporting the result of a test as “P ≤0.05” versus “P > 0.05,” or as “reject H0” versus “Do not reject H0,” without reporting the actual P-value.\n\nA)\n\n\nCode\n# H0: u=500\n# Ha: u≠500\n# n=1000\n# Jones: y=519.5, se=10.0\n# Smith : y=519.7, se=10.0\n\n# Jones: t=1.95\n\nt_jones = ((519.5 - 500)/ 10)\ncat(\"t value for Jones:\", t_jones, '\\n')\n\n\nt value for Jones: 1.95 \n\n\n\n\nCode\n# Jones: p-value=0.0515\n\ncat('p value for Jones:', round(2*pt(t_jones, df = 999, lower.tail=FALSE), 4), '\\n')\n\n\np value for Jones: 0.0515 \n\n\n\n\nCode\n# Smith: t=1.97\n\nt_smith = ((519.7 - 500)/ 10)\ncat(\"t value for Smith:\", t_smith, '\\n')\n\n\nt value for Smith: 1.97 \n\n\n\n\nCode\n# Smith: p-value=0.049\n\ncat('p value for Smith:', round(2*pt(t_smith, df = 999, lower.tail=FALSE), 4), '\\n')\n\n\np value for Smith: 0.0491 \n\n\n\n\nB)\nAssuming that 0.05 indicates statistical significance, we can see that while Smith’s study shows statistical significance with a p-value of 0.0491 (<0.05), Jones’s does not (p-value=0.0515, p-value>0.05).\n\n\nC)\nBased on this example, we can see why it is misleading to report a p-value as significant solely using “p ≤0.05,” “reject H0,” or “cannot reject H0” as indicators. After calculating each study’s respective p-value, we can see that the two values are very close, and, if we were to round each to the nearest hundredth, both would be equal to 0.05. So, it is important to provide the p-values themselves so as to make the distinction between degrees of significance. In other words, although Jones’s study showed statistical significance and Smith’s did not, the differences in the two p-values was marginal, so it is important to indicate by how slim a margin both were greater/less than the significance level (0.05) so as to not overestimate statistical significance.\n\n\n\nQuestion 6\nA school nurse wants to determine whether age is a factor in whether children choose a\nhealthy snack after school. She conducts a survey of 300 middle school students, with the results\nbelow. Test at α = 0.05 the claim that the proportion who choose a healthy snack differs by grade\nlevel. What is the null hypothesis? Which test should we use? What is the conclusion?\n\n\nCode\n# 6th: healthy = 31, unhealthy = 69\n# 7th: healthy = 43, unhealthy = 57\n# 8th: healthy = 51, unhealthy = 49\n# n=300, each grade has 100 survey participants\n\n# create dataframe:\ngrade <- c(rep(\"6th\", 100), rep(\"7th\", 100), rep(\"8th\", 100))\nsnack <- c(rep(\"healthy\", 31), rep(\"unhealthy\", 69), rep(\"healthy\", 43),\n           rep(\"unhealthy\", 57), rep(\"healthy\", 51), rep(\"unhealthy\", 49))\n\nsurvey_data <- data.frame(grade, snack)\nhead(survey_data)\n\n\n  grade   snack\n1   6th healthy\n2   6th healthy\n3   6th healthy\n4   6th healthy\n5   6th healthy\n6   6th healthy\n\n\nCode\n# transform dataframe into table:\ntable(survey_data$snack,survey_data$grade)\n\n\n           \n            6th 7th 8th\n  healthy    31  43  51\n  unhealthy  69  57  49\n\n\nCode\n# conduct chi-squared test:\nchisq.test(survey_data$snack,survey_data$grade,correct = FALSE)\n\n\n\n    Pearson's Chi-squared test\n\ndata:  survey_data$snack and survey_data$grade\nX-squared = 8.3383, df = 2, p-value = 0.01547\n\n\nThe null hypothesis is that age is not a factor in whether children choose a healthy snack after school. To test this hypothesis (two categorical variables), we should use Pearson’s Chi-squared test. After running this test, we can see that the p-value is 0.015, which indicates statistical significance. Thus, we can reject the null hypothesis and conclude that age is a factor in whether children choose a healthy snack after school.\n\n\nQuestion 7\nPer-pupil costs (in thousands of dollars) for cyber charter school tuition for school\ndistricts in three areas are shown. Test the claim that there is a difference in means for the three\nareas, using an appropriate test. What is the null hypothesis? Which test should we use? What is\nthe conclusion?\n\n\nCode\n# area 1: 6.2, 9.3, 6.8, 6.1, 6.7, 7.5\n# area 2: 7.5, 8.2, 8.5, 8.2, 7.0, 9.3\n# area 3: 5.8, 6.4, 5.6, 7.1, 3.0, 3.5\n\n# create dataframe:\narea <- c(rep(\"area_1\", 6), rep(\"area_2\", 6), rep(\"area_3\", 6))\ncost <- c(6.2, 9.3, 6.8, 6.1, 6.7, 7.5, 7.5, 8.2, 8.5, 8.2, 7.0, 9.3,\n          5.8, 6.4, 5.6, 7.1, 3.0, 3.5)\narea_cost <- data.frame(area,cost)\nhead(area_cost)\n\n\n    area cost\n1 area_1  6.2\n2 area_1  9.3\n3 area_1  6.8\n4 area_1  6.1\n5 area_1  6.7\n6 area_1  7.5\n\n\nCode\n# one-way ANOVA test:\none.way <- aov(cost ~ area, data = area_cost)\nsummary(one.way)\n\n\n            Df Sum Sq Mean Sq F value  Pr(>F)   \narea         2  25.66  12.832   8.176 0.00397 **\nResiduals   15  23.54   1.569                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe null hypothesis is that area has an effect on cost per pupil. Because we are testing the means of more than one group with one independent variable, we should use the one-way ANOVA test. After running the ANOVA test, we can see that the p-value (0.00397) is statistically significant. Thus, we can reject the null hypothesis and conclude that area does have an effect on cost per pupil."
  },
  {
    "objectID": "posts/HW2_JustineShakespeare.html",
    "href": "posts/HW2_JustineShakespeare.html",
    "title": "Homework 2",
    "section": "",
    "text": "The time between the date a patient was recommended for heart surgery and the surgery date for cardiac patients in Ontario was collected by the Cardiac Care Network (“Wait Times Data Guide,” Ministry of Health and Long-Term Care, Ontario, Canada, 2006). The sample mean and sample standard deviation for wait times (in days) of patients for two cardiac procedures are given in the accompanying table. Assume that the sample is representative of the Ontario population.\nConstruct the 90% confidence interval to estimate the actual mean wait time for each of the two procedures. Is the confidence interval narrower for angiography or bypass surgery?\nFirst we’ll calculate the confidence interval manually with the formula: CI = (X bar) ± (t × s/sqrt(n))\nWe’ll create objects in R to plug into this formula. We’ll start with the Bypass data.\n\n\nCode\nlibrary(tidyverse)\n\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.0      ✔ purrr   1.0.1 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.3.0      ✔ stringr 1.5.0 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\nCode\n# Bypass data\nBypassSample <- 539\nBypassMean <- 19\nBypassSD <- 10\n\n# standard error\nBypassSEM <- BypassSD/sqrt(BypassSample)\n\n# t-value\nConfidenceLevel <- .90\nBYPtail <- (1-ConfidenceLevel)/2\nBypass_t_score <- qt(1-BYPtail, df=BypassSample-1)\n\n# plug everything back into the CI formula:\nCIBypass <- c(BypassMean-Bypass_t_score*BypassSEM, \n              BypassMean+Bypass_t_score*BypassSEM)\n\nprint(CIBypass)\n\n\n[1] 18.29029 19.70971\n\n\nNow let’s do the same for the Angiography data.\n\n\nCode\nAngioSample <- 847\nAngioMean <- 18\nAngioSD <- 9\n\n# standard error\nAngioSEM <- AngioSD/sqrt(AngioSample)\n\n# t-value\nConfidenceLevel <- .90\nAngtail <- (1-ConfidenceLevel)/2\nAngio_t_score <- qt(1-Angtail, df=AngioSample-1)\n\n# plug everything back in\n\nCIAngio <- c(AngioMean-Angio_t_score*AngioSEM, \n             AngioMean+Angio_t_score*AngioSEM)\n\nprint(CIAngio)\n\n\n[1] 17.49078 18.50922\n\n\nNow we can compare the confidence intervals.\n\n\nCode\nabs(diff(CIAngio)) \n\n\n[1] 1.018436\n\n\nCode\nabs(diff(CIBypass))\n\n\n[1] 1.419421\n\n\nThe confidence interval for Angiography is narrower than for Bypass."
  },
  {
    "objectID": "posts/HW2_JustineShakespeare.html#question-2",
    "href": "posts/HW2_JustineShakespeare.html#question-2",
    "title": "Homework 2",
    "section": "Question 2",
    "text": "Question 2\nA survey of 1031 adult Americans was carried out by the National Center for Public Policy. Assume that the sample is representative of adult Americans. Among those surveyed, 567 believed that college education is essential for success. Find the point estimate, p, of the proportion of all adult Americans who believe that a college education is essential for success. Construct and interpret a 95% confidence interval for p.\nWe can use the prop.test() function to find the point estimate and the confidence interval:\n\n\nCode\nprop.test(x=567, n=1031, conf.level = .95)\n\n\n\n    1-sample proportions test with continuity correction\n\ndata:  567 out of 1031, null probability 0.5\nX-squared = 10.091, df = 1, p-value = 0.00149\nalternative hypothesis: true p is not equal to 0.5\n95 percent confidence interval:\n 0.5189682 0.5805580\nsample estimates:\n        p \n0.5499515"
  },
  {
    "objectID": "posts/HW2_JustineShakespeare.html#question-3",
    "href": "posts/HW2_JustineShakespeare.html#question-3",
    "title": "Homework 2",
    "section": "Question 3",
    "text": "Question 3\nSuppose that the financial aid office of UMass Amherst seeks to estimate the mean cost of textbooks per semester for students. The estimate will be useful if it is within $5 of the true population mean (i.e. they want the confidence interval to have a length of $10 or less). The financial aid office is pretty sure that the amount spent on books varies widely, with most values between $30 and $200. They think that the population standard deviation is about a quarter of this range (in other words, you can assume they know the population standard deviation). Assuming the significance level to be 5%, what should be the size of the sample.\nWe have to solve for n here. Because we have the standard deviation, we’ll use the CI formula that uses the z-score: CI = (X bar) ± (z × s/sqrt(n))\nThis can be rewritten: n=((s*z)/5)^2\nUsing the data we have:\n\n\nCode\nQ3_s = (200-30)/4\nQ3_z = qnorm(.975)\n\n((Q3_s*Q3_z)/5)^2\n\n\n[1] 277.5454"
  },
  {
    "objectID": "posts/HW2_JustineShakespeare.html#question-4",
    "href": "posts/HW2_JustineShakespeare.html#question-4",
    "title": "Homework 2",
    "section": "Question 4",
    "text": "Question 4\nAccording to a union agreement, the mean income for all senior-level workers in a large service company equals 500 per week. A representative of a women’s group decides to analyze whether the mean income μ for female employees matches this norm. For a random sample of nine female employees, ȳ = $410 and s = 90\n\n4.A\nTest whether the mean income of female employees differs from $500 per week. Include assumptions, hypotheses, test statistic, and P-value. Interpret the result.\nTo solve this problem I followed the steps of hypothesis testing:\n1. Specify the null hypothesis and alternative hypothesis\n2. Set the significance level (alpha)\n3. Calculate the test statistic\n4. Compare test statistics to the critical value determined by alpha, or check if p-value is smaller than alpha\n5. Draw a conclusion\n\nOur null hypothesis is that the population mean is 500 dollars, our alternative hypothesis is that the population mean is NOT $500\nThe significance level (alpha) is .05, or 5%\nCalculate the test statistic:\n\n\n\nCode\nQ4_SD <- 90\nQ4_ymean <- 410\nQ4_umean <- 500\nQ4_s_size <- 9\nQ4_SEM <- Q4_SD/sqrt(Q4_s_size)\n\nQ4_T_Statistic <- (Q4_ymean - Q4_umean)/(Q4_SEM)\nQ4_T_Statistic\n\n\n[1] -3\n\n\n\nCalculate the critical value so that we can compare it with the test statistic.\n\n\n\nCode\nQ4_Crit_Value <- qt(0.025, df=(Q4_s_size-1))\nQ4_Crit_Value\n\n\n[1] -2.306004\n\n\nThe critical value (on the lower tail end) is -2.3060. Because the test statistic is smaller than the critical value (and larger in absolute terms), we can conclude that we should reject the null hypothesis.\n\n(continued) calculate the p-value to check to see if it is smaller than alpha.\n\n\n\nCode\nQ4_p_value <- pt(Q4_T_Statistic, df = (Q4_s_size-1), lower.tail = TRUE)*2\nQ4_p_value\n\n\n[1] 0.01707168\n\n\nThe p-value is 0.01707, which is smaller than our alpha (.05). 0.01707 < 0.05 This again means that we can reject the null hypothesis.\n\nDraw a conclusion\n\nBecause the test statistic (-3) is in absolute terms larger than the critical values (2.30) AND because the p-value (0.01707) is less than the alpha levelof .05, we can reject the null hypothesis\nNote that when we calculate the confidence interval of these values we can also see that the estimated population mean here is outside of a 95% CI.\nTo calculate the confidence level we’ll first create some objects we’ll need in our formulas:\n\n\nCode\nConfLevel <- 0.95\nQ4_tail_area <- (1-ConfLevel)/2\nQ4_tscore <- qt(p=1-Q4_tail_area, df = Q4_s_size-1)\n\n# Then, using this and some of the objects we created earlier we'll plug everything in: \nQ4_CI <- c(Q4_ymean - (Q4_tscore * Q4_SEM), Q4_ymean + (Q4_tscore * Q4_SEM))\n\nprint(Q4_CI)\n\n\n[1] 340.8199 479.1801\n\n\nWe can see here that $500 is not within the 95% confidence interval, which is another reason we can reject the null hypothesis.\n\n\n4.B\nReport the P-value for Ha: μ < 500. Interpret.\nTo calculate a one sided P-value you do not need to multiply the formula by 2 and you need to specify that we’re only looking at one side. An alternative hypothesis of μ < 500 means the null hypothesis is μ >= 500. We need to specify that we are looking at the lower tail.\n\n\nCode\nQ4_p_value1 <- pt(Q4_T_Statistic, df = (Q4_s_size-1), lower.tail = TRUE)\nQ4_p_value1\n\n\n[1] 0.008535841\n\n\nThe p-value here is even lower than it was for the two sided test, it is 0.0085. We can definitely reject the null hypothesis that the true population mean is over $500.\n\n\n4.C\nReport and interpret the P-value for Ha: μ > 500.\nTo calculate the P-value for Ha: μ > 500 we run the same formula as before but we specify that we’re not interested in the lower tail.\n\n\nCode\nQ4_p_value2 <- pt(Q4_T_Statistic, df = (Q4_s_size-1), lower.tail = FALSE)\nQ4_p_value2\n\n\n[1] 0.9914642\n\n\nThe p-value for this one sided test is quite large. It means that we should not reject the null hypothesis that the true population mean is less than $500.\nTo test these p-values we can add them up.\n\n\nCode\nQ4_p_value1 + Q4_p_value2\n\n\n[1] 1\n\n\nWe find that they add up to 1."
  },
  {
    "objectID": "posts/HW2_JustineShakespeare.html#question-5",
    "href": "posts/HW2_JustineShakespeare.html#question-5",
    "title": "Homework 2",
    "section": "Question 5",
    "text": "Question 5\nJones and Smith separately conduct studies to test H0: μ = 500 against Ha: μ ≠ 500, each with n = 1000. Jones gets ȳ = 519.5, with se = 10.0. Smith gets ȳ = 519.7, with se = 10.0.\n\n5.A\nShow that t = 1.95 and P-value = 0.051 for Jones. Show that t = 1.97 and P-value = 0.049 for Smith.\nFirst we can create some objects to represent the values for this question. Then we can use those to get the test statistic and the p-value for Jones and Smith.\n\n\nCode\nQ5_SEM <- 10 # they have the same standard error\nJones_ymean <- 519.5\nSmith_ymean <- 519.7\nQ5_s_size <- 1000 # they have the same sample size\n\nQ5_umean <- 500\n\n# calculate the test statistic and p-value for Jones:\nJones_T_Statistic <- (Jones_ymean - Q5_umean)/(Q5_SEM)\nJones_T_Statistic\n\n\n[1] 1.95\n\n\nCode\n# we find that Jones' test statistic is 1.95. We can use that to find the p-value:\nJones_p_value <- pt(Jones_T_Statistic, df = (Q5_s_size-1), lower.tail = FALSE)*2\nJones_p_value\n\n\n[1] 0.05145555\n\n\nCode\n# Calculate test statistic and p-value for Smith:\nSmith_T_Statistic <- (Smith_ymean - Q5_umean)/(Q5_SEM)\nSmith_T_Statistic\n\n\n[1] 1.97\n\n\nCode\n# we find that Smith's test statistic is 1.97. We can use that to find the p-value:\nSmith_p_value <- pt(Smith_T_Statistic, df = (Q5_s_size-1), lower.tail = FALSE)*2\nSmith_p_value\n\n\n[1] 0.04911426\n\n\n\n\n5.B\nUsing α = 0.05, for each study indicate whether the result is “statistically significant.”\nAbove we found the p-value for both Jones and Smith, we can use that to show how those values compare to alpha set at 0.05.\n\n\nCode\nalpha <- 0.05\n\nJones_p_value < alpha\n\n\n[1] FALSE\n\n\nCode\nSmith_p_value < alpha\n\n\n[1] TRUE\n\n\nThis shows that technically Jones’ p-value is not statistically significant because it is more than the alpha of 0.05, but Smith’s p-value is technically statistically significant because it is less than the alpha.\n\n\n5.C\nUsing this example, explain the misleading aspects of reporting the result of a test as “P ≤ 0.05” versus “P > 0.05,” or as “reject H0” versus “Do not reject H0,” without reporting the actual P-value.\nThis question demonstrates how close two results can be and still technically not have the same significance level. In these cases it is important to include the p-value so that readers can put the results into context."
  },
  {
    "objectID": "posts/HW2_JustineShakespeare.html#question-6",
    "href": "posts/HW2_JustineShakespeare.html#question-6",
    "title": "Homework 2",
    "section": "Question 6",
    "text": "Question 6\nA school nurse wants to determine whether age is a factor in whether children choose a healthy snack after school. She conducts a survey of 300 middle school students, with the results below. Test at α = 0.05 the claim that the proportion who choose a healthy snack differs by grade level. What is the null hypothesis? Which test should we use? What is the conclusion?\nSince we’re dealing with categorical variables (grade and the choice of a healthy or unhealthy snack) we should use a chi-square test. First we’ll create the dataframe we’ll need to run the test in r.\n\n\nCode\nobs <- matrix(c(31, 69, 43, 57, 51, 49), nrow=3, byrow = TRUE)\ndimnames(obs) <- list(Grade = c(\"Grade_6\", \"Grade_7\", \"Grade_8\"),\n                      Snack = c(\"healthy\", \"unhealthy\"))\n\nobs\n\n\n         Snack\nGrade     healthy unhealthy\n  Grade_6      31        69\n  Grade_7      43        57\n  Grade_8      51        49\n\n\nThen we can run the chi-square test and print the result:\n\n\nCode\nChiResult <- chisq.test(obs)\n\nprint(ChiResult)\n\n\n\n    Pearson's Chi-squared test\n\ndata:  obs\nX-squared = 8.3383, df = 2, p-value = 0.01547\n\n\nGiven the p-value < 0.05, we can reject the null hypothesis that grade level does not have an effect on the choice of healthy snack. The relatively high X-squared value also indicates that there is a meaningful difference in the choices made by kids in different grade levels."
  },
  {
    "objectID": "posts/HW2_JustineShakespeare.html#question-7",
    "href": "posts/HW2_JustineShakespeare.html#question-7",
    "title": "Homework 2",
    "section": "Question 7",
    "text": "Question 7\nPer-pupil costs (in thousands of dollars) for cyber charter school tuition for school districts in three areas are shown. Test the claim that there is a difference in means for the three areas, using an appropriate test. What is the null hypothesis? Which test should we use? What is the conclusion?\nThe null hypothesis is that all three districts have equal average tuition. We can use the ANOVA test since we are comparing three means.\nFirst we’ll create the dataframe in r:\n\n\nCode\nArea1 <- c(6.2, 9.3, 6.8, 6.1, 6.7, 7.5)  \nArea2 <- c(7.5, 8.2, 8.5, 8.2, 7.0, 9.3)\nArea3 <- c(5.8, 6.4, 5.6, 7.1, 3.0, 3.5)\n\nQ7df <- data.frame(Area1, Area2, Area3)\n\nQ7df_PL <- pivot_longer(Q7df, cols = c(Area1, Area2, Area3), names_to = \"district\", values_to = \"tuition\")\nQ7df_PL\n\n\n# A tibble: 18 × 2\n   district tuition\n   <chr>      <dbl>\n 1 Area1        6.2\n 2 Area2        7.5\n 3 Area3        5.8\n 4 Area1        9.3\n 5 Area2        8.2\n 6 Area3        6.4\n 7 Area1        6.8\n 8 Area2        8.5\n 9 Area3        5.6\n10 Area1        6.1\n11 Area2        8.2\n12 Area3        7.1\n13 Area1        6.7\n14 Area2        7  \n15 Area3        3  \n16 Area1        7.5\n17 Area2        9.3\n18 Area3        3.5\n\n\nNow that we have a dataframe with one continuous variable and one categorical variable, we can use the anova test function in r aov() to test our hypothesis. This type of hypothesis test is appropriate for this scenario because we want to compare the mean of three groups.\n\n\nCode\nQ7o <- aov(tuition ~ district, data = Q7df_PL)\nsummary(Q7o)\n\n\n            Df Sum Sq Mean Sq F value  Pr(>F)   \ndistrict     2  25.66  12.832   8.176 0.00397 **\nResiduals   15  23.54   1.569                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nGiven that the p value here is less than 0.01, we can reject the null hypothesis that the mean tuition is the same at all three schools."
  },
  {
    "objectID": "posts/Homework2_AlexaPotter.html",
    "href": "posts/Homework2_AlexaPotter.html",
    "title": "Homework 2",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\n\n-- Attaching packages --------------------------------------- tidyverse 1.3.2 --\nv ggplot2 3.4.0      v purrr   0.3.5 \nv tibble  3.1.8      v dplyr   1.0.10\nv tidyr   1.2.1      v stringr 1.5.0 \nv readr   2.1.3      v forcats 0.5.2 \n-- Conflicts ------------------------------------------ tidyverse_conflicts() --\nx dplyr::filter() masks stats::filter()\nx dplyr::lag()    masks stats::lag()"
  },
  {
    "objectID": "posts/Homework2_AlexaPotter.html#a.-test-whether-the-mean-income-of-female-employees-differs-from-500-per-week.-include-assumptions-hypotheses-test-statistic-and-p-value.-interpret-the-result.",
    "href": "posts/Homework2_AlexaPotter.html#a.-test-whether-the-mean-income-of-female-employees-differs-from-500-per-week.-include-assumptions-hypotheses-test-statistic-and-p-value.-interpret-the-result.",
    "title": "Homework 2",
    "section": "A. Test whether the mean income of female employees differs from $500 per week. Include assumptions, hypotheses, test statistic, and P-value. Interpret the result.",
    "text": "A. Test whether the mean income of female employees differs from $500 per week. Include assumptions, hypotheses, test statistic, and P-value. Interpret the result.\nHypothesis: H0: μ = 500 HA: μ ≠ 500\nAssumptions: - The data is collected from a representative & randomly selected portion of the total population. - The data is a normal distribution - The two groups have the same population variance (homoskedasticity)\nFormula for t-test: t = (X‾ - μ0) / (s / √n) X‾ = 410 μ0 = 500 s = 90 n = 9\n\n\nCode\nt_stat <- (410 - 500) / (90 / sqrt(9))\nt_stat\n\n\n[1] -3\n\n\np-value:\n\n\nCode\n2*pt(q=t_stat, df=8)\n\n\n[1] 0.01707168\n\n\nSince the p-value is less than 0.05 we can reject the null hypothesis at significance level 0.05."
  },
  {
    "objectID": "posts/Homework2_AlexaPotter.html#b.-report-the-p-value-for-ha-μ-500.-interpret.",
    "href": "posts/Homework2_AlexaPotter.html#b.-report-the-p-value-for-ha-μ-500.-interpret.",
    "title": "Homework 2",
    "section": "B. Report the P-value for Ha: μ < 500. Interpret.",
    "text": "B. Report the P-value for Ha: μ < 500. Interpret.\n\n\nCode\npt(q= t_stat, df=8, lower.tail=TRUE)\n\n\n[1] 0.008535841\n\n\nSince the p value is less than 0.05 we can reject the null hypothesis at 0.05 significance level."
  },
  {
    "objectID": "posts/Homework2_AlexaPotter.html#c.-report-and-interpret-the-p-value-for-ha-μ-500.",
    "href": "posts/Homework2_AlexaPotter.html#c.-report-and-interpret-the-p-value-for-ha-μ-500.",
    "title": "Homework 2",
    "section": "C. Report and interpret the P-value for Ha: μ > 500.",
    "text": "C. Report and interpret the P-value for Ha: μ > 500.\n(Hint: The P-values for the two possible one-sided tests must sum to 1.)\n\n\nCode\npt(q= t_stat, df=8, lower.tail=FALSE)\n\n\n[1] 0.9914642\n\n\nSince the p value is greater than 0.05 we fail to reject the null hypothesis at 0.05 significance level."
  },
  {
    "objectID": "posts/Homework2_AlexaPotter.html#a.-show-that-t-1.95-and-p-value-0.051-for-jones.-show-that-t-1.97-and-p-value-0.049-for-smith.",
    "href": "posts/Homework2_AlexaPotter.html#a.-show-that-t-1.95-and-p-value-0.051-for-jones.-show-that-t-1.97-and-p-value-0.049-for-smith.",
    "title": "Homework 2",
    "section": "A. Show that t = 1.95 and P-value = 0.051 for Jones. Show that t = 1.97 and P-value = 0.049 for Smith.",
    "text": "A. Show that t = 1.95 and P-value = 0.051 for Jones. Show that t = 1.97 and P-value = 0.049 for Smith.\nJones\nt = (ȳ - μ)/se\n\n\nCode\njones_t <- (519.5-500)/10\nprint(jones_t)\n\n\n[1] 1.95\n\n\nCode\n#p-value:\n2*pt(-abs(jones_t),df=1000-1)\n\n\n[1] 0.05145555\n\n\nSmith\nt = (ȳ - μ)/se\n\n\nCode\nsmith_t <- (519.7-500)/10\nprint(smith_t)\n\n\n[1] 1.97\n\n\nCode\n#p-value:\n2*pt(-abs(smith_t),df=1000-1)\n\n\n[1] 0.04911426"
  },
  {
    "objectID": "posts/Homework2_AlexaPotter.html#b.-using-α-0.05-for-each-study-indicate-whether-the-result-is-statistically-significant.",
    "href": "posts/Homework2_AlexaPotter.html#b.-using-α-0.05-for-each-study-indicate-whether-the-result-is-statistically-significant.",
    "title": "Homework 2",
    "section": "B. Using α = 0.05, for each study indicate whether the result is “statistically significant.”",
    "text": "B. Using α = 0.05, for each study indicate whether the result is “statistically significant.”\nThe result is statistically significant when the p-value is less than or equal to the alpha level.\nAt α = 0.05, Jones’ p-value of 0.051 is not statistically significant. At α = 0.05, Smith’s p-value of 0.049 is statistically significant."
  },
  {
    "objectID": "posts/Homework2_AlexaPotter.html#c.-using-this-example-explain-the-misleading-aspects-of-reporting-the-result-of-a-test-as-p-0.05-versus-p-0.05-or-as-reject-h0-versus-do-not-reject-h0-without-reporting-the-actual-p-value.",
    "href": "posts/Homework2_AlexaPotter.html#c.-using-this-example-explain-the-misleading-aspects-of-reporting-the-result-of-a-test-as-p-0.05-versus-p-0.05-or-as-reject-h0-versus-do-not-reject-h0-without-reporting-the-actual-p-value.",
    "title": "Homework 2",
    "section": "C. Using this example, explain the misleading aspects of reporting the result of a test as “P ≤ 0.05” versus “P > 0.05,” or as “reject H0” versus “Do not reject H0,” without reporting the actual P-value.",
    "text": "C. Using this example, explain the misleading aspects of reporting the result of a test as “P ≤ 0.05” versus “P > 0.05,” or as “reject H0” versus “Do not reject H0,” without reporting the actual P-value.\nUsing “P ≤ 0.05” versus “P > 0.05,” can leave a gap in the understanding of the full analysis. While one value is statistically significant, these two are extremely close. It’s important to state at what value you reject or fail to reject the null hypothesis, because both of these would not be statistically significant at α = 0.01."
  },
  {
    "objectID": "posts/Homework3_AlexaPotter.html",
    "href": "posts/Homework3_AlexaPotter.html",
    "title": "Homework 3",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\n\n-- Attaching packages --------------------------------------- tidyverse 1.3.2 --\nv ggplot2 3.4.0      v purrr   0.3.5 \nv tibble  3.1.8      v dplyr   1.0.10\nv tidyr   1.2.1      v stringr 1.5.0 \nv readr   2.1.3      v forcats 0.5.2 \n-- Conflicts ------------------------------------------ tidyverse_conflicts() --\nx dplyr::filter() masks stats::filter()\nx dplyr::lag()    masks stats::lag()\n\n\nCode\nlibrary(car)\n\n\nLoading required package: carData\n\nAttaching package: 'car'\n\nThe following object is masked from 'package:dplyr':\n\n    recode\n\nThe following object is masked from 'package:purrr':\n\n    some\n\n\nCode\nlibrary(smss)\ndata(\"student.survey\", package = \"smss\")\ndf <- student.survey\nlibrary(alr4)\n\n\nLoading required package: effects\nlattice theme set by effectsTheme()\nSee ?effectsTheme for details.\n\n\nCode\ndata(\"UN11\", package = \"alr4\")\ndf <- UN11\ndata(\"Rateprof\", package = \"alr4\")\ndf <- Rateprof\ndata(\"water\", package = \"alr4\")\ndf <- water"
  },
  {
    "objectID": "posts/Homework3_AlexaPotter.html#a",
    "href": "posts/Homework3_AlexaPotter.html#a",
    "title": "Homework 3",
    "section": "A",
    "text": "A\nIdentify the predictor and the response.\nPredictor: x = ppgdp Response: y = fertility"
  },
  {
    "objectID": "posts/Homework3_AlexaPotter.html#b",
    "href": "posts/Homework3_AlexaPotter.html#b",
    "title": "Homework 3",
    "section": "B",
    "text": "B\nDraw the scatterplot of fertility on the vertical axis versus ppgdp on the horizontal axis and summarize the information in this graph. Does a straight-line mean function seem to be plausible for a summary of this graph?\n\n\nCode\nggplot(UN11, aes(y = fertility, x = ppgdp)) +\n  geom_point()+\n  stat_smooth(method = \"lm\", se = FALSE)\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nThis distribution has more of a curve to it rather than a straight line. The fitted line does not seem accurately to reflect the distribution."
  },
  {
    "objectID": "posts/Homework3_AlexaPotter.html#c",
    "href": "posts/Homework3_AlexaPotter.html#c",
    "title": "Homework 3",
    "section": "C",
    "text": "C\nDraw the scatterplot of log(fertility) versus log(ppgdp) using natural logarithms. Does the simple linear regression model seem plausible for a summary of this graph? If you use a different base of logarithms, the shape of the graph won’t change, but the values on the axes will change.\n\n\nCode\nggplot(UN11, aes(y = log(fertility), x = log(ppgdp))) +\n  geom_point()+\n  stat_smooth(method = \"lm\", se = FALSE)\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nYes the simpler linear regression does seem more plausible to fit and describe this distribution."
  },
  {
    "objectID": "posts/Homework3_AlexaPotter.html#a-1",
    "href": "posts/Homework3_AlexaPotter.html#a-1",
    "title": "Homework 3",
    "section": "A",
    "text": "A\nHow, if at all, does the slope of the prediction equation change?\nThe slope would not change in this conversion but the y intercept would."
  },
  {
    "objectID": "posts/Homework3_AlexaPotter.html#b-1",
    "href": "posts/Homework3_AlexaPotter.html#b-1",
    "title": "Homework 3",
    "section": "B",
    "text": "B\nHow, if at all, does the correlation change?\nCorrelation would not change as its standardized, its value does not depend on the unit of measurement."
  },
  {
    "objectID": "posts/Homework3_AlexaPotter.html#a-2",
    "href": "posts/Homework3_AlexaPotter.html#a-2",
    "title": "Homework 3",
    "section": "A",
    "text": "A\nGraphically portray how the explanatory variable relates to the outcome variable in each of the two cases\n\ni\ny = political ideology and x = religiosity\n\n\nCode\n#head(student.survey)\n#unique(student.survey$pi)\n#unique(student.survey$re)\n\n#class(student.survey$pi) <- \"character\"\n#class(student.survey$re) <- \"character\"\n\nstudent.survey %>%recode(pi, c(\"very conservative\" = \"7\", \n                            \"conservative\"= \"6\", \n                            \"slightly conservative\" = \"5\", \n                            \"moderate\" = \"4\", \n                            \"slightly liberal\" = \"3\", \n                            \"liberal\"= \"2\", \n                            \"very liberal\"= \"1\",\n                            .default = \"0\"))\n\n\nWarning in if (as.factor) {: the condition has length > 1 and only the first\nelement will be used\n\n\nError in if (as.factor) {: argument is not interpretable as logical\n\n\nCode\nstudent.survey %>%recode(re, c(\"every week\" = \"4\", \n                            \"most weeks\" = \"3\", \n                            \"occasionally \"= \"2\", \n                           \"never\"= \"1\",\n                           .default = \"0\"))\n\n\nError in is.factor(x): object 're' not found\n\n\nCode\nclass(student.survey$pi) <- \"numeric\"\nclass(student.survey$re) <- \"numeric\"\n\n\n\n\nCode\nggplot(data = student.survey, aes(x=re, y=pi))+\n  geom_point()+\n  geom_smooth(method = \"lm\", se = FALSE)+\n  scale_x_continuous(labels= c(\"every week\", \n                            \"most weeks\", \n                            \"occasionally\", \n                            \"never\"),\n                     breaks = c(4,3,2,1))+\n  scale_y_continuous(labels = c(\"very conservative\", \n                            \"conservative\", \n                            \"slightly conservative\", \n                            \"moderate\", \n                            \"slightly liberal\", \n                            \"liberal\", \n                            \"very liberal\"),\n                     breaks = c(7,6,5,4,3,2,1)) +\n  labs(x = \"Religiosity\", y = \"Political Ideology\")\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\nCode\nsummary(lm(pi~re, data=student.survey))\n\n\n\nCall:\nlm(formula = pi ~ re, data = student.survey)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.81243 -0.87160  0.09882  1.12840  3.09882 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   0.9308     0.4252   2.189   0.0327 *  \nre            0.9704     0.1792   5.416 1.22e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.345 on 58 degrees of freedom\nMultiple R-squared:  0.3359,    Adjusted R-squared:  0.3244 \nF-statistic: 29.34 on 1 and 58 DF,  p-value: 1.221e-06\n\n\n\n\nii\ny = high school GPA and x = hours of TV watching\n\n\nCode\nggplot(data = student.survey, aes(x=tv, y=hi))+\n  geom_point()+\n  geom_smooth(method = \"lm\", se = FALSE)+\n  labs(x = \"Hours of TV Watching\", y = \"High School GPA\")\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\nCode\nsummary(lm(hi~tv, data=student.survey))\n\n\n\nCall:\nlm(formula = hi ~ tv, data = student.survey)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.2583 -0.2456  0.0417  0.3368  0.7051 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  3.441353   0.085345  40.323   <2e-16 ***\ntv          -0.018305   0.008658  -2.114   0.0388 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4467 on 58 degrees of freedom\nMultiple R-squared:  0.07156,   Adjusted R-squared:  0.05555 \nF-statistic: 4.471 on 1 and 58 DF,  p-value: 0.03879"
  },
  {
    "objectID": "posts/Homework3_AlexaPotter.html#b-2",
    "href": "posts/Homework3_AlexaPotter.html#b-2",
    "title": "Homework 3",
    "section": "B",
    "text": "B\nSummarize and interpret results of inferential analyses.\nThe results in both regressions show they are statistically significant, however, the first graph displays the results as a linear regression better than the second example. A different form of regression would be better to capture the relationship rather than a linear regression."
  },
  {
    "objectID": "posts/HW1_ChristineBrydges.html",
    "href": "posts/HW1_ChristineBrydges.html",
    "title": "Homework 1",
    "section": "",
    "text": "First, let’s read in the data from the Excel file:\n\n\nCode\nlibrary(readxl)\ndf <- read_excel(\"_data/LungCapData.xls\")\n\n\nThe distribution of LungCap looks as follows:\n\n\nCode\nhist(df$LungCap)\n\n\n\n\n\nThe histogram suggests that the distribution is close to a normal distribution. Most of the observations are close to the mean. Very few observations are close to the margins (0 and 15).\n\n\n\nThe distribution and mean of the probability density of Lung Capacity can be shown in the box plots below, separated by genders ‘Male’ and ‘Female’\n\n\nCode\n# Create a Box Plot with Lung Capacity on y-axis, grouped by gender\nboxplot(LungCap~ Gender, data = df)\n\n# Add a title\ntitle(\"Lung Capacity: Males vs. Females\")\n\n\n\n\n\nThe boxplot suggests that there is no significant difference between lung capacity of females and males, as the error bars of each boxplot significantly overlap. Additionally, the mean of the probability density of lung capacity appear to be close between female and male constituents.\n\n\n\n\n\nCode\n# Create a Box Plot with Lung Capacity on y-axis, grouped by populations that smoke or do not smoke \nboxplot(LungCap~ Smoke, data = df)\n\n# Add a title\ntitle(\"Lung Capacity: Smokers vs. Non-Smokers\")\n\n\n\n\n\nThe boxplot suggests that smokers have a higher lung capacity than non-smokers, which is counter-intuitive.\n#c Next, we will explore the differences of lung capacity of non-smokers and smokers, broken down by age group, as shown in the boxplot below. The green color is used to call out groups that do smoke while the blue color is used to call out groups that do not smoke.\n\n\nCode\n#Group respondents into specific Age Groups \nAgeGroups <- cut(df$Age, breaks=c(0,13,15,17,19), labels=c('<13','14-15','16-17','>=18'))\nlevels(AgeGroups)\n\n\n[1] \"<13\"   \"14-15\" \"16-17\" \">=18\" \n\n\nCode\n#Create a stratified box plot based on two factors: Smoking vs. Nonsmoking and Age groups\nboxplot(df$LungCap~df$Smoke*AgeGroups, ylab=\"LungCap\", main=\"LungCap vs. Smoke, by AgeGroup\", las = 2, col=c(4,3))\n\n\n\n\n\nFrom these boxplots, you can see that when you separate the smoking and nonsmoking groups by age, the smokers have a higher lung capacity than non-smokers. This is because age is a confounding variable, with young people gaining lung capacity as they have bigger bodies, and with older peope generally smoking more than younger people . Once you take out age from consideration and compare “apples to apples” people of the same age but differences in whether they smoke or not, you can see that smoking DOES have a negative effect on lung capacity."
  },
  {
    "objectID": "posts/HW1_ChristineBrydges.html#a.-here-we-will-explore-the-probability-that-a-randomly-selected-inmate-has-exactly-2-prior-convictions-based-on-a-dataset-given-to-us.",
    "href": "posts/HW1_ChristineBrydges.html#a.-here-we-will-explore-the-probability-that-a-randomly-selected-inmate-has-exactly-2-prior-convictions-based-on-a-dataset-given-to-us.",
    "title": "Homework 1",
    "section": "a. Here, we will explore the probability that a randomly selected inmate has exactly 2 prior convictions, based on a dataset given to us.",
    "text": "a. Here, we will explore the probability that a randomly selected inmate has exactly 2 prior convictions, based on a dataset given to us.\nSince the data set is not continuous or binomial, we will use basic probability functions to find probabilities.\n\n\nCode\n#Calculate the probability of a prisoner having exactly 2 convictions (events/total possible events)\nprobability = (150/810) * 100\nprint(probability)\n\n\n[1] 18.51852\n\n\nWe can see that the probability of a prisoner having exactly 2 convictions is 18.5%."
  },
  {
    "objectID": "posts/HW1_ChristineBrydges.html#b.",
    "href": "posts/HW1_ChristineBrydges.html#b.",
    "title": "Homework 1",
    "section": "b.",
    "text": "b.\nNext, we’ll look at the probability that a randomly selected inmate has fewer than 2 prior convictions.\n\n\nCode\n# Calculate the probability of a prisoner having 0, or 1 convictions (fewer than 2 prior convictions) (events/total possible events)\nlessthan2convictions <- 128 + 434 \nprobabilitylessthan2 <- (lessthan2convictions/810) * 100\nprint(probabilitylessthan2)\n\n\n[1] 69.38272\n\n\nWe can see that the probability of a prisoner having less than 2 convictions is 69.4%."
  },
  {
    "objectID": "posts/HW1_ChristineBrydges.html#c.",
    "href": "posts/HW1_ChristineBrydges.html#c.",
    "title": "Homework 1",
    "section": "c. ",
    "text": "c. \nNext, we’ll look at the probability that a randomly selected inmate has 2 or less prior convictions.\n\n\nCode\n# Calculate the probability of a prisoner having 0, 1, or 2 convictions ( 2 or fewer prior convictions) (events/total possible events)\ntwoorlessconvictions <- 128 + 434 + 160\nprobability2orless <- (twoorlessconvictions/810) * 100\nprint(probability2orless)\n\n\n[1] 89.1358\n\n\nWe can see that the probability of a prisoner having 2 or less convictions is 89.1%."
  },
  {
    "objectID": "posts/HW1_ChristineBrydges.html#d.",
    "href": "posts/HW1_ChristineBrydges.html#d.",
    "title": "Homework 1",
    "section": "d. ",
    "text": "d. \nNext, we’ll look at the probability that a randomly selected inmate has more than 2 prior convictions.\n\n\nCode\n#Calculate the probability of a prisoner having more than 2 convictions ( 3 or 4 prior convictions) (events/total possible events)\nmorethan2convictions <- 64 + 24\nprobabilitymorethan2 <- (morethan2convictions/810) * 100\nprint(probabilitymorethan2)\n\n\n[1] 10.8642\n\n\nWe can see that the probability of a prisoner having more than 2 convictions is 10.9%."
  },
  {
    "objectID": "posts/HW1_ChristineBrydges.html#d.-1",
    "href": "posts/HW1_ChristineBrydges.html#d.-1",
    "title": "Homework 1",
    "section": "d. ",
    "text": "d. \nHere, we’ll calculate the expected value for the number of prior convictions.\n\n\nCode\n#define values\nx <- c(0,1,2,3,4)\n\n#define probabilities\nfrequency <- c(128/810, 434/810, 160/810, 64/810, 24/810)\n\n#calculate expected value\nsum(x * frequency)\n\n\n[1] 1.28642\n\n\nThe expected value is 1.29 prior convictions."
  },
  {
    "objectID": "posts/HW1_ChristineBrydges.html#f.",
    "href": "posts/HW1_ChristineBrydges.html#f.",
    "title": "Homework 1",
    "section": "f. ",
    "text": "f. \nIn this final section, we’ll calculate the variance and standard deviation for the prior convictions.\n\n\nCode\n# calculate variance of frequencies\nfrequency <- c(128, 434, 160, 64, 24)\nvar(frequency)\n\n\n[1] 25948\n\n\nThe variance is 5948.\n\n\nCode\n# calculate standard deviation of prior convictions \nfrequency <- c(128, 434, 160, 64, 24)\nsd(frequency)\n\n\n[1] 161.0838\n\n\nThe standard deviation is 161.1."
  },
  {
    "objectID": "posts/hw1_asch_harwood.html",
    "href": "posts/hw1_asch_harwood.html",
    "title": "Homework 1",
    "section": "",
    "text": "Code\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(knitr)\nlibrary(kableExtra)"
  },
  {
    "objectID": "posts/hw1_asch_harwood.html#d",
    "href": "posts/hw1_asch_harwood.html#d",
    "title": "Homework 1",
    "section": "d",
    "text": "d\n\n\nCode\ndf$AgeGroup <- cut(df$Age, breaks = c(0, 13, 15, 17, Inf),\n                    labels = c(\"<=13\", \"14 to 15\", \"16 to 17\", \">=18\"))\n\n\n\n\nCode\nage_group_mean <- df %>%\n  group_by(AgeGroup) %>%\n  summarise(mean(LungCap))\nkable(age_group_mean)\n\n\n\n\n \n  \n    AgeGroup \n    mean(LungCap) \n  \n \n\n  \n    <=13 \n    6.411932 \n  \n  \n    14 to 15 \n    9.045417 \n  \n  \n    16 to 17 \n    10.245876 \n  \n  \n    >=18 \n    10.964688 \n  \n\n\n\n\n\n\n\nCode\nggplot(df, aes(x = AgeGroup, y = LungCap)) +\n  geom_boxplot()\n\n\n\n\n\nThere appears to be a relationship between age and lung capacity, where as age group increases, lung capacity increases as well. This is understandable given that developmentally a younger child would have a lower lung capacity than a young adult."
  },
  {
    "objectID": "posts/hw1_asch_harwood.html#e",
    "href": "posts/hw1_asch_harwood.html#e",
    "title": "Homework 1",
    "section": "e",
    "text": "e\n\n\nCode\nggplot(df, aes(x = AgeGroup, y = LungCap, fill=Smoke)) +\n  geom_boxplot() \n\n\n\n\n\n\n\nCode\nggplot(df, aes(x = LungCap, fill = Smoke)) +\n  geom_histogram(alpha = 0.5, bins = 10, position = \"identity\") +\n  facet_wrap(~AgeGroup, nrow = 2, scales = \"free\") +\n  labs(x = \"Lung Capacity\", y = \"Frequency\") +\n  ggtitle(\"Distribution of Lung Capacity by Age Group, by Smoking Status\")\n\n\n\n\n\nWhen controlling for age, average smoker lung capacity is lower across the board for all age groups except “13 <=”. In the later group, there are far fewer smokers, as one would (hope to) expect, which explains the slightly higher mean lung capacity for smokers."
  },
  {
    "objectID": "posts/HW2_LTucksmith.html",
    "href": "posts/HW2_LTucksmith.html",
    "title": "HW2",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tidyr)\nknitr::opts_chunk$set(echo = TRUE)\n\n\n\n\nCode\ndf <- read_excel(\"_data/LungCapData.xls\")\n\n\n\nThe time between the date a patient was recommended for heart surgery and the surgery date for cardiac patients in Ontario was collected by the Cardiac Care Network (“Wait Times Data Guide,” Ministry of Health and Long-Term Care, Ontario, Canada, 2006). The sample mean and sample standard deviation for wait times (in days) of patients for two cardiac procedures are given in the accompanying table. Assume that the sample is representative of the Ontario population\n\nConstruct the 90% confidence interval to estimate the actual mean wait time for each of the two procedures. Is the confidence interval narrower for angiography or bypass surgery?\n\n\nCode\nbp_num <- 539\nbp_mean <- 19\nbp_sd <- 10\nbp_se <- sqrt(bp_sd)\nag_num <- 847\nag_mean <- 18\nag_sd <- 9\nag_se <- sqrt(ag_sd)\n\nalpha = 0.1\ndegrees.freedom = bp_num - 1\nt.score = qt(p=alpha/2, df=degrees.freedom,lower.tail=F)\n\nmargin.error <- t.score * bp_se\n\nlower.bound <- bp_mean - margin.error\nupper.bound <- bp_mean + margin.error\nbypass_confidenceInterval <- print(c(lower.bound,upper.bound))\n\n\n[1] 13.78954 24.21046\n\n\nCode\ndegrees.freedom = ag_num - 1\nt.score = qt(p=alpha/2, df=degrees.freedom,lower.tail=F)\n\nmargin.error <- t.score * ag_se\n\nlower.bound <- ag_mean - margin.error\nupper.bound <- ag_mean + margin.error\nangiography_confidenceInterval <- print(c(lower.bound,upper.bound))\n\n\n[1] 13.06003 22.93997\n\n\nThe confidence interval is narrower for angiography surgery than it is for bypass surgery. The bypass surgery confidence interval spans from 13.78954 to 24.21046 days while the angiography surgery spans from 13.06003 to 22.93997 days. Since the bypass surgery confidence interval spans 10.42 days and the angiography surgery spans 9.88 days, the angiography surgery has a narrower confidence interval than the bypass surgery.\n\nA survey of 1031 adult Americans was carried out by the National Center for Public Policy. Assume that the sample is representative of adult Americans. Among those surveyed, 567 believed that college education is essential for success. Find the point estimate, p, of the proportion of all adult Americans who believe that a college education is essential for success. Construct and interpret a 95% confidence interval for p.\n\n\n\nCode\nn = 1031 \ncollege_y = 567\np = college_y/n\nprint(p)\n\n\n[1] 0.5499515\n\n\nCode\nmargin_error <- qnorm(0.95)*sqrt(p*(1-p)/n)\nlowerbound <- p - margin_error\nupperbound <- p + margin_error\n\np_confidenceInterval <- print(c(lowerbound,upperbound))\n\n\n[1] 0.5244662 0.5754368\n\n\n\nSuppose that the financial aid office of UMass Amherst seeks to estimate the mean cost of textbooks per semester for students. The estimate will be useful if it is within $5 of the true population mean (i.e. they want the confidence interval to have a length of $10 or less). The financial aid office is pretty sure that the amount spent on books varies widely, with most values between $30 and $200. They think that the population standard deviation is about a quarter of this range (in other words, you can assume they know the population standard deviation). Assuming the significance level to be 5%, what should be the size of the sample\n\n\n\nCode\nmargin_error_max = 5\nconfidence_interval_length = 10\nlower_bound_avg = 30\nupper_bound_avg = 200\nrange = upper_bound_avg - lower_bound_avg\n\nz = .95 +(1-.95)/2\nn = ((z - (range/4))/confidence_interval_length)^2\n\nprint(n)\n\n\n[1] 17.24326\n\n\n\nAccording to a union agreement, the mean income for all senior-level workers in a large service company equals $500 per week. A representative of a women’s group decides to analyze whether the mean income μ for female employees matches this norm. For a random sample of nine female employees, ȳ = $410 and s = 90\n\n\n\nCode\nincome_n <- c(rnorm(9, mean = 410, sd = 90))\n\n\nA. Test whether the mean income of female employees differs from $500 per week. Include assumptions, hypotheses, test statistic, and P-value. Interpret the result.\nassumption: The mean income of female employees is $500 per week. null hypothesis: The mean income of female employees is $500 per week. alternative hypothesis: The mean income of female employees differs from $500 per week.\n\n\nCode\nincome_female <- t.test(income_n, mu=500)\nprint(income_female)\n\n\n\n    One Sample t-test\n\ndata:  income_n\nt = -2.7722, df = 8, p-value = 0.02422\nalternative hypothesis: true mean is not equal to 500\n95 percent confidence interval:\n 346.1252 485.8750\nsample estimates:\nmean of x \n 416.0001 \n\n\nThe result is that with 95% certainty the mean female income falls between $322.6557 and $441.1327. Since the range falls below $500, we reject the null hypothesis that the mean female income is $500, and accept the alternative hypothesis that the true mean female income is not equal to $500.\nB. Report the P-value for Ha: μ < 500. Interpret.\n\n\nCode\nincome_female <- t.test(income_n, mu=500, alternative = 'less')\nprint(income_female)\n\n\n\n    One Sample t-test\n\ndata:  income_n\nt = -2.7722, df = 8, p-value = 0.01211\nalternative hypothesis: true mean is less than 500\n95 percent confidence interval:\n     -Inf 472.3468\nsample estimates:\nmean of x \n 416.0001 \n\n\nThe p_value is 0.0081. It is less than the 0.5 alpha so we reject the null hypothesis that that the mean female income is $500, and accept the alternative hypothesis that the true mean female income is less than $500.\nC. Report and interpret the P-value for Ha: μ > 500.\n\n\nCode\nincome_female <- t.test(income_n, mu=500, alternative = 'greater')\nprint(income_female)\n\n\n\n    One Sample t-test\n\ndata:  income_n\nt = -2.7722, df = 8, p-value = 0.9879\nalternative hypothesis: true mean is greater than 500\n95 percent confidence interval:\n 359.6534      Inf\nsample estimates:\nmean of x \n 416.0001 \n\n\nThe p_value is 0.9919. It is not less than the 0.5 alpha so we fail to reject the null hypothesis that that the mean female income is $500.\n\nJones and Smith separately conduct studies to test H0: μ = 500 against Ha: μ ≠ 500, each with n = 1000. Jones gets ȳ = 519.5, with se = 10.0. Smith gets ȳ = 519.7, with se = 10.0.\n\n\n\nCode\nsd.js = 10*(sqrt(1000))\n\nincome_jones <- c(rnorm(1000, mean = 519.5, sd = sd.js))\nincomes_smith <- c(rnorm(1000, mean = 519.7, sd = sd.js))\n\n\nA. Show that t = 1.95 and P-value = 0.051 for Jones. Show that t = 1.97 and P-value = 0.049 for Smith.\n\n\nCode\njones_t <- (519.5 - 500)/(sd.js/sqrt(1000))\njones_p <- 2*(pt(-jones_t, 999))\n\nprint(jones_t)\n\n\n[1] 1.95\n\n\nCode\nprint(jones_p)\n\n\n[1] 0.05145555\n\n\nCode\nsmith_t <- (519.7 - 500)/(sd.js/sqrt(1000))\nsmith_p <- 2*(pt(-smith_t, 999))\n\nprint(smith_t)\n\n\n[1] 1.97\n\n\nCode\nprint(smith_p)\n\n\n[1] 0.04911426\n\n\nB. Using α = 0.05, for each study indicate whether the result is “statistically significant.”\nThe Jones study result is not statistically significant because their p-value is above the alpha. The Smith study result is statistically significant because the p-value is below the alpha.\nC. Using this example, explain the misleading aspects of reporting the result of a test as “P ≤ 0.05” versus “P > 0.05,” or as “reject H0” versus “Do not reject H0,” without reporting the actual P-value.\nBecause in this case the p-value is so close to the alpha, it’s misleading to not report the actual p-value. If someone only read that a null hypothesis was not rejected and not that statistical significance criteria was almost met, they may falsely attribute the strength of confidence of which the null hypothesis was not rejected with. It’s important to report the p-value so that the strength of confidence is known.\n\nA school nurse wants to determine whether age is a factor in whether children choose a healthy snack after school. She conducts a survey of 300 middle school students, with the results below. Test at α = 0.05 the claim that the proportion who choose a healthy snack differs by grade level. What is the null hypothesis? Which test should we use? What is the conclusion?\n\nx = proportion of students who choose a healthy snack null hypothesis: X is not affected by grade level. alternative hypothesis: X is affected by grade level.\n\n\nCode\ngrade <- c(6, 7, 8) \nhealthy_prop <- c(31, 43, 51)\nunhealthy_prop <- c(69, 57, 49)\nstudent_n <- c(100, 100, 100)\n\nstudent_data <- data.frame(grade, healthy_prop, unhealthy_prop)\n\nstudent_t <- prop.test(student_data$healthy_prop, n=student_n)\nprint(student_t)\n\n\n\n    3-sample test for equality of proportions without continuity\n    correction\n\ndata:  student_data$healthy_prop out of student_n\nX-squared = 8.3383, df = 2, p-value = 0.01547\nalternative hypothesis: two.sided\nsample estimates:\nprop 1 prop 2 prop 3 \n  0.31   0.43   0.51 \n\n\nThe conclusion is that since the p-value of 0.01547 is below our 0.05 value we reject the null hypothesis that the proportion of students who choose a healthy snack is not affected by grade level in favor of the alternative hypothesis that the proportion of students who choose a healthy snack is affected by grade level.\n\nPer-pupil costs (in thousands of dollars) for cyber charter school tuition for school districts in three areas are shown. Test the claim that there is a difference in means for the three areas, using an appropriate test. What is the null hypothesis? Which test should we use? What is the conclusion?\n\nnull hypothesis: There is not a difference in mean per-pupil cost (in thousands of dollars) for cyber charter school tuition for school districts for the three areas. alternative hypothesis: There is a difference in mean per-pupil cost (in thousands of dollars) for cyber charter school tuition for school districts for the three areas.\n\n\nCode\nvalues <- c(6.2,9.3,6.8,6.1,6.7,7.5,\n           7.5,8.2,8.5,8.2,7.0,9.3,\n           5.8,6.4,5.6,7.1,3.0,3.5)\narea <- c(1,1,1,1,1,1,\n          2,2,2,2,2,2,\n          3,3,3,3,3,3)\n\ncost_data <- data.frame(values, area)\n\nanova <- aov(values~area, data = cost_data)\nsummary(anova)\n\n\n            Df Sum Sq Mean Sq F value Pr(>F)  \narea         1  10.45  10.453   4.316 0.0542 .\nResiduals   16  38.75   2.422                 \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe conclusion is that since the p-value of 0.0542 is above our 0.05 value we fail reject the null hypothesis that there is not a difference in mean per-pupil cost (in thousands of dollars) for cyber charter school tuition for school districts for the three areas in favor of the alternative hypothesis that there is a difference in mean per-pupil cost (in thousands of dollars) for cyber charter school tuition for school districts for the three areas."
  },
  {
    "objectID": "posts/abigailbalint_hw2.html",
    "href": "posts/abigailbalint_hw2.html",
    "title": "Homework 2",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(readxl)\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/abigailbalint_hw2.html#question-1",
    "href": "posts/abigailbalint_hw2.html#question-1",
    "title": "Homework 2",
    "section": "Question 1",
    "text": "Question 1\nGenerating a table here to calculate standard deviation\n\n\nCode\nheart <- matrix(c(539, 19, 10, 847, 18, 9), ncol=3, byrow=TRUE)\ncolnames(heart) <- c('Sample Size','Mean Wait Time','Standard Deviation')\nrownames(heart) <- c('Bypass','Angiography')\nheart <- as.table(heart)\nhead(heart)\n\n\n            Sample Size Mean Wait Time Standard Deviation\nBypass              539             19                 10\nAngiography         847             18                  9\n\n\nCalculating confidence interval for Bypass:\n\n\nCode\nstander1 <- 10/sqrt(539)\nconfidence_level <- 0.90 \ntail_area <- (1-confidence_level)/2\nt_score <- qt(p = 1-tail_area, df = 539-1)\nt_score\n\n\n[1] 1.647691\n\n\nCode\nCI <- c(19 - t_score * stander1,\n        19 + t_score * stander1)\nprint(CI)\n\n\n[1] 18.29029 19.70971\n\n\nCalculating confidence interval for Angiography:\n\n\nCode\nstander2 <- 9/sqrt(847)\nconfidence_level <- 0.90 \ntail_area <- (1-confidence_level)/2\nt_score <- qt(p = 1-tail_area, df = 847-1)\nt_score\n\n\n[1] 1.646657\n\n\nCode\nCI <- c(18 - t_score * stander2,\n        18 + t_score * stander2)\nprint(CI)\n\n\n[1] 17.49078 18.50922\n\n\n\n\nCode\nBypassInt <- 19.70971 - 18.29029\nAngiographyInt <- 18.50922-17.49078 \nprint(BypassInt)  \n\n\n[1] 1.41942\n\n\nCode\nprint(AngiographyInt) \n\n\n[1] 1.01844\n\n\nQuestion: Is the confidence interval narrower for angiography or bypass surgery? Answer: The interval for angiography surgery is shorter at an interval of 1.01."
  },
  {
    "objectID": "posts/abigailbalint_hw2.html#question-2",
    "href": "posts/abigailbalint_hw2.html#question-2",
    "title": "Homework 2",
    "section": "Question 2",
    "text": "Question 2\nHere I am using a prop test function to find the p value -\n\n\nCode\nprop.test(567, 1031, conf.level = 0.95)\n\n\n\n    1-sample proportions test with continuity correction\n\ndata:  567 out of 1031, null probability 0.5\nX-squared = 10.091, df = 1, p-value = 0.00149\nalternative hypothesis: true p is not equal to 0.5\n95 percent confidence interval:\n 0.5189682 0.5805580\nsample estimates:\n        p \n0.5499515 \n\n\nAnswer: To interpret this, I am looking at my confidence interval and seeing that the true population mean of those who believe college is needed for success is somewhere between 52-58% which makes sense because the reported percentage from this random sample who believe college is needed for success is 54%, inside of that range."
  },
  {
    "objectID": "posts/abigailbalint_hw2.html#question-3",
    "href": "posts/abigailbalint_hw2.html#question-3",
    "title": "Homework 2",
    "section": "Question 3",
    "text": "Question 3\nMy calculations to find the sample size are step by step in the below code block:\n\n\nCode\n#Starting by trying to find margin of error:\n#Formula is error= z * SD/sqrt of n\n#By squaring the whole thing, can transform this formular to to n=z^2*SD^2/error^2\n# Z score for 95% confidence is 1.96\n1.96^2\n\n\n[1] 3.8416\n\n\nCode\n#n=3.8416*SD^2/error^2\n#Finding the range and dividing it by 4 since we know SD is quarter of the range\n(200-30)/4\n\n\n[1] 42.5\n\n\nCode\n42.5^2\n\n\n[1] 1806.25\n\n\nCode\n#Now formula looks like this and just need to find standard error n=3.8416*1806.25/error^2\n#We know that margin of error is 5 (within $5)\n5^2\n\n\n[1] 25\n\n\nCode\n#Now we can fill in the final formula n=3.8416*1806.25/25\n(3.8416*1806.25)/25\n\n\n[1] 277.5556\n\n\nCode\n#n=277.56\n\n\nAnswer: The sample size should be about 278 (rounded)"
  },
  {
    "objectID": "posts/abigailbalint_hw2.html#question-4",
    "href": "posts/abigailbalint_hw2.html#question-4",
    "title": "Homework 2",
    "section": "Question 4",
    "text": "Question 4\nA. Test whether the mean income of female employees differs from $500 per week. Include assumptions, hypotheses, test statistic, and P-value. Interpret the result.\nAssumptions - -Randomly generated sample -From the same general population -Distribution is normal\nNull hypothesis - females earn $500 a week Alternative hypothesis - females earn more or less than $500 a week\nCalculating the t value below using standard formula -\nt= (sample mean-population mean)/(standard deviation/sample size^2)\n\n\nCode\ntvalue =  (410 - 500) / (90/sqrt(9))\ntvalue\n\n\n[1] -3\n\n\nAnswer: t=-3\nThis tells us that the mean of the female group is three standard deviations away from the mean of the overall group’s pay.\nB. Report the P-value for alternative hypothesis: μ < 500. Interpret.\nCalculating p-value -\nFormula pt(q = t, df =standardeviation-1, lower.tail = TRUE)\n\n\nCode\npt(q = tvalue, df =8, lower.tail = TRUE)\n\n\n[1] 0.008535841\n\n\nAnswer: .01 (rounded)\nA p-value of .01 means we can reject the alternative hypothesis (mean for females < 500 a week)\nC. Report and interpret the P-value for alternative hypothesis: μ > 500.\n\n\nCode\npt(q = tvalue, df =8, lower.tail = FALSE)\n\n\n[1] 0.9914642\n\n\nAnswer: .99 (rounded) This is the same but flipped, can reject the alternative hypothesis (mean for females > 500 a week)"
  },
  {
    "objectID": "posts/abigailbalint_hw2.html#question-5",
    "href": "posts/abigailbalint_hw2.html#question-5",
    "title": "Homework 2",
    "section": "Question 5",
    "text": "Question 5\nUsing formula t= sample mean-population mean/standard error\n\n\nCode\nt=(519.5-500)/10\nprint(t)\n\n\n[1] 1.95\n\n\nCode\npt(q = t, df = 999, lower.tail = FALSE)*2\n\n\n[1] 0.05145555\n\n\nAnswer: Jones t=1.95, p=.051\n\n\nCode\nt=(519.7-500)/10\nprint(t)\n\n\n[1] 1.97\n\n\nCode\npt(q = t, df = 999, lower.tail = FALSE)*2\n\n\n[1] 0.04911426\n\n\nAnswer: Smith t=1.97, p=.049\nB. This makes Smith statistically significant because .049 falls below .05 but .051 does not.\nC. This shows that results presented this way can be misleading because even though the p-values are extremely close here, one would report rejecting the null hypothesis and one wouldn’t even though the differences in results are marginal."
  },
  {
    "objectID": "posts/abigailbalint_hw2.html#question-6",
    "href": "posts/abigailbalint_hw2.html#question-6",
    "title": "Homework 2",
    "section": "Question 6",
    "text": "Question 6\nNull hypothesis: Proportion of those who choose healthy snacks is not equal by grade level.\nGenerating table -\n\n\nCode\nsnack <- matrix(c(31, 43, 51, 69, 57, 49), ncol=3, byrow=TRUE)\ncolnames(snack) <- c('6th','7th','8th')\nrownames(snack) <- c('healthy','unhealthy')\nsnack <- as.table(snack)\nhead(snack)\n\n\n          6th 7th 8th\nhealthy    31  43  51\nunhealthy  69  57  49\n\n\nPerforming chi squared test -\n\n\nCode\nchisq.test(snack, .05, correct = FALSE)\n\n\n\n    Pearson's Chi-squared test\n\ndata:  snack\nX-squared = 8.3383, df = 2, p-value = 0.01547\n\n\nSince the p value is .01 we can assume that there is a difference by grade level in those who choose unhealthy vs healthy snacks, rejecting null hypothesis."
  },
  {
    "objectID": "posts/abigailbalint_hw2.html#question-7",
    "href": "posts/abigailbalint_hw2.html#question-7",
    "title": "Homework 2",
    "section": "Question 7",
    "text": "Question 7\nNull hypothesis: There is no difference in per-pupil costs between areas.\nGenerating data frame -\n\n\nCode\narea <- c(rep(\"Area1\", 6), rep(\"Area2\", 6), rep(\"Area3\", 6))\ncost <- c(6.2, 9.3, 6.8, 6.1, 6.7, 7.5, 7.5, 8.2, 8.5, 8.2, 7.0, 9.3,\n          5.8, 6.4, 5.6, 7.1, 3.0, 3.5)\ntuition <- data.frame(area,cost)\nhead(tuition)\n\n\n   area cost\n1 Area1  6.2\n2 Area1  9.3\n3 Area1  6.8\n4 Area1  6.1\n5 Area1  6.7\n6 Area1  7.5\n\n\n\n\nCode\nanova <- aov(cost ~ area, data = tuition)\nsummary(anova)\n\n\n            Df Sum Sq Mean Sq F value  Pr(>F)   \narea         2  25.66  12.832   8.176 0.00397 **\nResiduals   15  23.54   1.569                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nInterpretation: The p value is very low indicating there is a difference and we can reject the null hypothesis. :"
  },
  {
    "objectID": "posts/Final_Project_Check1_GHTan.html",
    "href": "posts/Final_Project_Check1_GHTan.html",
    "title": "Final Project Check 1",
    "section": "",
    "text": "My final project will be a further investigation on digital devices in schools that I have submitted as the final project for DACSS 601. I still explore the data from the survey “Programme for International Student Assessment” in 2018. In this assignment, I will propose my hypothesis, and present the descriptive statistics with minor changes base on my last project.\n\n\nCode\nlibrary(tidyverse)\n\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.0      ✔ purrr   0.3.5 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.4.1 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\nCode\nlibrary(ggplot2)\nlibrary(dbplyr)\n\n\n\nAttaching package: 'dbplyr'\n\nThe following objects are masked from 'package:dplyr':\n\n    ident, sql\n\n\nCode\npisa <- read_csv('_data/CY07_MSU_SCH_QQQ.csv')\n\n\nNew names:\nRows: 21903 Columns: 198\n── Column specification\n──────────────────────────────────────────────────────── Delimiter: \",\" chr\n(8): CNT, CYC, NatCen, STRATUM, SUBNATIO, SC053D11TA, PRIVATESCH, VER_DAT dbl\n(189): ...1, CNTRYID, CNTSCHID, Region, OECD, ADMINMODE, LANGTEST, SC001... lgl\n(1): BOOKID\nℹ Use `spec()` to retrieve the full column specification for this data. ℹ\nSpecify the column types or set `show_col_types = FALSE` to quiet this message.\n• `` -> `...1`\n\n\n\nResearch Questions\nMy final project will probe into what factors contribute to the accessibility to and human resources’ support for digital devices in schools. Additionally, I will explore if there is a correlations between career guidance and digital devices? I will conduct this research based on the data “Programme for International Student Assessment” (PISA) collected by the The Organization for Economic Co-operation and Development (OECD) in 2018.\n\n\nHpyotheis\nI propose that the size of urban population primarily contributes to the conditions of digital device. “OECD or Non-OECD” and “public or private schools” may be two cofounders, which is suppose to be incorporated into the regression analysis. Also, I hypothesize that the higher score a school report regarding career guidance, the higher score a school reports in terms of digital divices.\n\n\nCode\n# create a data frame\n#view(pisa)\n# select related variable\npisa_selected <- select(pisa,starts_with(c(\"SC001\", \"SC013\", \"SC016\", \"SC161\",\"SC155\")))\npisa2018_joint <-cbind(pisa[, 1:12], pisa_selected)\n# pisa_SC155\npisa2018_joint$Accessibility=rowMeans(pisa2018_joint[,c(\"SC155Q01HA\",\"SC155Q02HA\",                                                  \"SC155Q03HA\",\"SC155Q04HA\")])\npisa2018_joint$Human_Resource_Support=rowMeans(pisa2018_joint[\n  ,c(\"SC155Q05HA\",\"SC155Q06HA\", \"SC155Q07HA\",\"SC155Q08HA\",\"SC155Q09HA\", \"SC155Q10HA\", \"SC155Q11HA\")])\npisa2018_joint$Career_Guidance=rowSums(pisa2018_joint[, c(\"SC161Q02SA\",\"SC161Q03SA\",\"SC161Q04SA\",\"SC161Q04SA\")])\npisa_SC155 <- pisa2018_joint %>%\n  select(CNT, STRATUM, OECD, Career_Guidance,Accessibility, Human_Resource_Support, SC001Q01TA, SC013Q01TA) %>%\n  mutate(Urban=SC001Q01TA, Public_or_Private=SC013Q01TA) %>%\n  select(-c(SC001Q01TA, SC013Q01TA)) %>%\n  select(c(CNT,STRATUM,OECD,Urban, Public_or_Private,Career_Guidance,Accessibility,Human_Resource_Support))\npisa_SC155\n\n\n\n\nDescriptive Statistics\nThis original OECD PISA 2018 School Questionnaire Dataset is one part of PISA 2018 dataset with a focus on schools. It covers 80 countries and regions all over the world. The dataset documents 21,903 schools’ responses regarding 187 questions.After cleaning the data, the dataset includes 8 variables: CNT identifies countries. STRATUM identifies schools. OECD indicates if a school locates in a OECD country or not. Urban describes different conditions of urban communities where a school locates. Public_or_Private presents if a school is public or private. Career_Guidance demonstrates the score a school reports in terms of career guidance. Accessibility demonstrates the score a school reports in terms of accessibility to digital devices. Human_Resource_Support suggests the score a school reports in terms of human ressource support for digital devices.\nAfter using the summary function and visualization, I have already show the descriptive statistics. A large number of NA stands out. I will figure out how to deal with them properly.\n\n\nCode\nsummary(pisa_SC155)\n\n\n     CNT              STRATUM               OECD            Urban      \n Length:21903       Length:21903       Min.   :0.0000   Min.   :1.000  \n Class :character   Class :character   1st Qu.:0.0000   1st Qu.:2.000  \n Mode  :character   Mode  :character   Median :1.0000   Median :3.000  \n                                       Mean   :0.5171   Mean   :3.007  \n                                       3rd Qu.:1.0000   3rd Qu.:4.000  \n                                       Max.   :1.0000   Max.   :5.000  \n                                                        NA's   :1363   \n Public_or_Private Career_Guidance Accessibility   Human_Resource_Support\n Min.   :1.00      Min.   :0.000   Min.   :1.000   Min.   :1.000         \n 1st Qu.:1.00      1st Qu.:1.000   1st Qu.:2.000   1st Qu.:2.286         \n Median :1.00      Median :1.000   Median :2.750   Median :2.714         \n Mean   :1.19      Mean   :1.518   Mean   :2.674   Mean   :2.658         \n 3rd Qu.:1.00      3rd Qu.:2.000   3rd Qu.:3.250   3rd Qu.:3.000         \n Max.   :2.00      Max.   :4.000   Max.   :4.000   Max.   :4.000         \n NA's   :2092      NA's   :1499    NA's   :1185    NA's   :1236          \n\n\nCode\npisa_SC155_boxplot<-pisa_SC155 %>%\n  select(STRATUM, Career_Guidance, Accessibility, Human_Resource_Support) %>% \n  pivot_longer(cols=c(Career_Guidance, Accessibility, Human_Resource_Support), \n               names_to = \"Group\", values_to = \"Evaluation\")\n\nggplot(pisa_SC155_boxplot,aes(Evaluation, fill=Group))+\n  stat_boxplot(geom = \"errorbar\", # Error bars\n               width = 0.2)+\n  geom_boxplot()+\n  facet_wrap(~Group)+\n  labs(title=\"Pisa2018 Evaluation\")+\n  coord_flip()\n\n\nWarning: Removed 3920 rows containing non-finite values (`stat_boxplot()`).\nRemoved 3920 rows containing non-finite values (`stat_boxplot()`)."
  },
  {
    "objectID": "posts/HW3_young.html",
    "href": "posts/HW3_young.html",
    "title": "Homework 3",
    "section": "",
    "text": "Code\n# data loading\nlibrary(alr4)\n\n\nWarning: package 'alr4' was built under R version 4.2.3\n\n\nLoading required package: car\n\n\nLoading required package: carData\n\n\nLoading required package: effects\n\n\nWarning: package 'effects' was built under R version 4.2.3\n\n\nlattice theme set by effectsTheme()\nSee ?effectsTheme for details.\n\n\nCode\ndata(UN11)\n\n# check data\nhead(UN11)\n\n\n                region  group fertility   ppgdp lifeExpF pctUrban\nAfghanistan       Asia  other     5.968   499.0    49.49       23\nAlbania         Europe  other     1.525  3677.2    80.40       53\nAlgeria         Africa africa     2.142  4473.0    75.00       67\nAngola          Africa africa     5.135  4321.9    53.17       59\nAnguilla     Caribbean  other     2.000 13750.1    81.10      100\nArgentina   Latin Amer  other     2.172  9162.1    79.89       93\n\n\nCode\ndim(UN11)\n\n\n[1] 199   6\n\n\n\n\nThrough the given problem, it can be seen that the research topic is the effect of ppgdp on the birth rate. In this research topic, the predictor is ‘ppgdp’ and the response is ‘fertility’.\n\n\n\nFirst of all, a regression model between the ppgdp and fertility of countries was derived.\n\n\nCode\nsummary(lm(UN11$fertility~UN11$ppgdp))\n\n\n\nCall:\nlm(formula = UN11$fertility ~ UN11$ppgdp)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.9006 -0.8801 -0.3547  0.6749  3.7585 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  3.178e+00  1.048e-01  30.331  < 2e-16 ***\nUN11$ppgdp  -3.201e-05  4.655e-06  -6.877  7.9e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.206 on 197 degrees of freedom\nMultiple R-squared:  0.1936,    Adjusted R-squared:  0.1895 \nF-statistic: 47.29 on 1 and 197 DF,  p-value: 7.903e-11\n\n\nA regression model such as Fertility = 3.178-0.00003*ppGDP was derived. In other words, a single unit increase in ppgdp reduces the fertility rate by 0.00003.\nNow, expressing this as a scatterplot is as follows.\n\n\nCode\nlibrary(tidyverse)\n\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.1     ✔ purrr   1.0.1\n✔ tibble  3.1.8     ✔ dplyr   1.1.0\n✔ tidyr   1.3.0     ✔ stringr 1.5.0\n✔ readr   2.1.4     ✔ forcats 1.0.0\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n✖ dplyr::recode() masks car::recode()\n✖ purrr::some()   masks car::some()\n\n\nCode\noptions(scipen=999)\nplot(UN11$ppgdp, UN11$fertility, \n     xlab=\"ppGDP\",\n     ylab=\"Fertility\",\n     col=\"cornflowerblue\")\nabline(lm(UN11$fertility~UN11$ppgdp), col=\"blue\")\ntext(mean(UN11$ppgdp)+30000, mean(UN11$fertility)+0.5, \"Fertility = 3.178-0.00003*ppGDP\", col = \"blue\")\n\n\n\n\n\nOverall, the regression equation derived earlier seems reasonable at first glance as it shows a downward trend to the right, but it can also be seen that the distribution of ppgdp is quite biased to the right.\n\n\nCode\nmean(UN11$ppgdp)\n\n\n[1] 13011.95\n\n\nCode\nmedian(UN11$ppgdp)\n\n\n[1] 4684.5\n\n\nThis can also be seen from the comparison of the mean and the median, and the mean of ppgdp is 13011.95 and the median is 4684.5, indicating that the distribution of ppgdp is biased to the right. Therefore, a simple linear regression model that does not properly convert variables will have many limitations in explaining the variability of fertility.\n\n\n\nFirst, a new objects with a value obtained by logarithms each variable is generated.\n\n\nCode\nUN11$log_ppgdp<-log(UN11$ppgdp)\nUN11$log_fertility<-log(UN11$fertility)\n\n\nUsing these objects, the regression model was obtained in the same way as the problem (b) and a scatterplot was drawn.\n\n\nCode\nsummary(lm(UN11$log_fertility~UN11$log_ppgdp))\n\n\n\nCall:\nlm(formula = UN11$log_fertility ~ UN11$log_ppgdp)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.79828 -0.21639  0.02669  0.23424  0.95596 \n\nCoefficients:\n               Estimate Std. Error t value            Pr(>|t|)    \n(Intercept)     2.66551    0.12057   22.11 <0.0000000000000002 ***\nUN11$log_ppgdp -0.20715    0.01401  -14.79 <0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3071 on 197 degrees of freedom\nMultiple R-squared:  0.526, Adjusted R-squared:  0.5236 \nF-statistic: 218.6 on 1 and 197 DF,  p-value: < 0.00000000000000022\n\n\n\n\nCode\nplot(UN11$log_ppgdp, UN11$log_fertility, \n     xlab=\"Log(ppGDP)\",\n     ylab=\"Log(Fertility)\",\n     col=\"chartreuse4\")\nabline(lm(UN11$log_fertility~UN11$log_ppgdp), col=\"darkgreen\")\ntext(mean(UN11$log_ppgdp+1.2), mean(UN11$log_fertility+0.8), \"Log(Fertility) = 2.666-0.2072*Log(ppGDP)\", col = \"darkgreen\")\n\n\n\n\n\nA regression line that reflects the data much better than when a regression line was derived without transforming variables was derived. The R-squared value is also higher than before transform.(0.19 -> 0.53) In other words, the performance of the regression model improved by logarithms each variable.\n\n\nCode\n# change base of logarithms\nUN11$log10_ppgdp<-log(UN11$ppgdp, base=exp(10))\nUN11$log10_fertility<-log(UN11$fertility, base=exp(10))\n\nsummary(lm(UN11$log10_fertility~UN11$log10_ppgdp))\n\n\n\nCall:\nlm(formula = UN11$log10_fertility ~ UN11$log10_ppgdp)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.079828 -0.021639  0.002669  0.023424  0.095596 \n\nCoefficients:\n                 Estimate Std. Error t value            Pr(>|t|)    \n(Intercept)       0.26655    0.01206   22.11 <0.0000000000000002 ***\nUN11$log10_ppgdp -0.20715    0.01401  -14.79 <0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.03071 on 197 degrees of freedom\nMultiple R-squared:  0.526, Adjusted R-squared:  0.5236 \nF-statistic: 218.6 on 1 and 197 DF,  p-value: < 0.00000000000000022\n\n\nCode\nplot(UN11$log10_ppgdp, UN11$log10_fertility, \n     xlab=\"Log10(ppGDP)\",\n     ylab=\"Log10(Fertility)\",\n     col=\"gray\")\nabline(lm(UN11$log10_fertility~UN11$log10_ppgdp), col=\"black\")\ntext(mean(UN11$log10_ppgdp), mean(UN11$log10_fertility+0.07), \"Log10(Fertility) = 0.2666-0.2072*Log10(ppGDP)\", col = \"black\")\n\n\n\n\n\nWhen base of logarithms changed to 10, there are no changes in distribution of data and shape of line. But scales of each axes are changed."
  },
  {
    "objectID": "posts/HW3_young.html#a-1",
    "href": "posts/HW3_young.html#a-1",
    "title": "Homework 3",
    "section": "(a)",
    "text": "(a)\n\n\nCode\nsummary(lm(UN11$fertility~UN11$pound_ppgdp))\n\n\n\nCall:\nlm(formula = UN11$fertility ~ UN11$pound_ppgdp)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.9006 -0.8801 -0.3547  0.6749  3.7585 \n\nCoefficients:\n                     Estimate   Std. Error t value             Pr(>|t|)    \n(Intercept)       3.177911642  0.104772778  30.331 < 0.0000000000000002 ***\nUN11$pound_ppgdp -0.000042575  0.000006191  -6.877       0.000000000079 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.206 on 197 degrees of freedom\nMultiple R-squared:  0.1936,    Adjusted R-squared:  0.1895 \nF-statistic: 47.29 on 1 and 197 DF,  p-value: 0.00000000007903\n\n\n\n\nCode\nplot(UN11$pound_ppgdp, UN11$fertility, \n     xlab=\"ppGDP(£)\",\n     ylab=\"Fertility\",\n     col=\"cornflowerblue\")\nabline(lm(UN11$fertility~UN11$pound_ppgdp), col=\"blue\")\ntext(mean(UN11$pound_ppgdp)+30000, mean(UN11$fertility)+0.5, \"Fertility = 3.178-0.00004*ppGDP(£)\", col = \"blue\")\n\n\n\n\n\nWhen dollars are converted into pounds, the slope of the simple regression line changes.(from approx. 0.00003 to approx. 0.00004). But the intercept does not change."
  },
  {
    "objectID": "posts/HW3_young.html#b-1",
    "href": "posts/HW3_young.html#b-1",
    "title": "Homework 3",
    "section": "(b)",
    "text": "(b)\nEven if the monetary unit is changed, the r-squared value remains unchanged (equivalent to 0.1936). That is, there is no change in the ratio at which the change in ppgdp explains the change in the facility. In the problem (a), the change in slope is simply caused by the unit conversion of the x variable. In fact, applying the exchange rate of 1.33, which is the changed slope, shows that it is the same as the slope calculated in dollars.\n\n\nCode\n0.000042575/1.33\n\n\n[1] 0.00003201128"
  },
  {
    "objectID": "posts/HW3_young.html#i-a",
    "href": "posts/HW3_young.html#i-a",
    "title": "Homework 3",
    "section": "(i-a)",
    "text": "(i-a)\nLooking at the data first, it is composed of nominal variables.\n\n\nCode\ntable(student.survey$pi)\n\n\n\n         very liberal               liberal      slightly liberal \n                    8                    24                     6 \n             moderate slightly conservative          conservative \n                   10                     6                     4 \n    very conservative \n                    2 \n\n\nCode\ntable(student.survey$re)\n\n\n\n       never occasionally   most weeks   every week \n          15           29            7            9 \n\n\nRegression analysis such as logistic regression can also be performed for nominal variables. Here, regression analysis will be performed simply by assigning a number corresponding to each variable value. The level of the variable is a orderal variable, and in the case of religion, the higher the number, the more participation in religious activities, and in the case of political ideology, the higher the number, the more conservative it was.\n\n\nCode\n# transform variables\nstudent.survey$pol_id<-\n  ifelse(student.survey[,\"pi\"]==\"very liberal\", 1,\n       ifelse(student.survey[,\"pi\"]==\"liberal\", 2,\n              ifelse(student.survey[,\"pi\"]==\"slightly liberal\", 3,\n                     ifelse(student.survey[,\"pi\"]==\"moderate\", 4,\n                            ifelse(student.survey[,\"pi\"]==\"slightly conservative\", 5,\n                                   ifelse(student.survey[,\"pi\"]==\"conservative\", 6, 7))))))\n\n\nstudent.survey$rel_fre<-\n  ifelse(student.survey[,\"re\"]==\"never\",1,\n         ifelse(student.survey[,\"re\"]==\"occasionally\", 2,\n                ifelse(student.survey[,\"re\"]==\"most weeks\", 3, 4)))\n\n\n\n\nCode\n# regression model\nsummary(lm(student.survey$pol_id~student.survey$rel_fre))\n\n\n\nCall:\nlm(formula = student.survey$pol_id ~ student.survey$rel_fre)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.81243 -0.87160  0.09882  1.12840  3.09882 \n\nCoefficients:\n                       Estimate Std. Error t value   Pr(>|t|)    \n(Intercept)              0.9308     0.4252   2.189     0.0327 *  \nstudent.survey$rel_fre   0.9704     0.1792   5.416 0.00000122 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.345 on 58 degrees of freedom\nMultiple R-squared:  0.3359,    Adjusted R-squared:  0.3244 \nF-statistic: 29.34 on 1 and 58 DF,  p-value: 0.000001221\n\n\nCode\n# plot\nplot(student.survey$rel_fre, student.survey$pol_id, xaxt = 'n', yaxt='n',\n     xlab=\"Religiosity\",\n     ylab=\"Political Ideology\",\n     col=\"cornflowerblue\")\naxis(1, at = seq(1, 4, by = 1), labels = c(\"never\", \"occasionally\", \"most weeks\", \"every weeks\"))\naxis(2, at = seq(1,7, by=1), labels = c(\"very liberal\", \"liberal\", \"slightly liberal\", \"moderate\", \"slightly conservative\", \"conservative\", \"very conservative\"))\nabline(lm(student.survey$pol_id~student.survey$rel_fre), col=\"blue\")"
  },
  {
    "objectID": "posts/HW3_young.html#i-b",
    "href": "posts/HW3_young.html#i-b",
    "title": "Homework 3",
    "section": "(i-b)",
    "text": "(i-b)\nThe effect of religiosity on political ideology shows a positive correlation. In other words, the more often you participate in religious activities, the more conservative your political ideology becomes, and the more you do not participate in religious activities, the more liberal your political ideology tends to become."
  },
  {
    "objectID": "posts/HW3_young.html#ii-a",
    "href": "posts/HW3_young.html#ii-a",
    "title": "Homework 3",
    "section": "(ii-a)",
    "text": "(ii-a)\n\n\nCode\n# regression model\nsummary(lm(student.survey$hi~student.survey$tv))\n\n\n\nCall:\nlm(formula = student.survey$hi ~ student.survey$tv)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.2583 -0.2456  0.0417  0.3368  0.7051 \n\nCoefficients:\n                   Estimate Std. Error t value            Pr(>|t|)    \n(Intercept)        3.441353   0.085345  40.323 <0.0000000000000002 ***\nstudent.survey$tv -0.018305   0.008658  -2.114              0.0388 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4467 on 58 degrees of freedom\nMultiple R-squared:  0.07156,   Adjusted R-squared:  0.05555 \nF-statistic: 4.471 on 1 and 58 DF,  p-value: 0.03879\n\n\nCode\n# plot\nplot(student.survey$tv, student.survey$hi,\n     xlab=\"hours of TV watching\",\n     ylab=\"high school GPA\",\n     col=\"cornflowerblue\")\nabline(lm(student.survey$hi~student.survey$tv), col=\"blue\")"
  },
  {
    "objectID": "posts/HW3_young.html#ii-b",
    "href": "posts/HW3_young.html#ii-b",
    "title": "Homework 3",
    "section": "(ii-b)",
    "text": "(ii-b)\nAs shown in the figure, there is a weak negative correlation, However, size of effect is small(R-squared is 0.0715). In this case, there could be a variable that distorts the size or direction of the effect between the variable of watching TV and performance. Therefore, it is necessary to further analyze the relationship with other variables."
  },
  {
    "objectID": "posts/HW1_JustineShakespeare.html",
    "href": "posts/HW1_JustineShakespeare.html",
    "title": "Homework 1",
    "section": "",
    "text": "We start by loading the appropriate packages and reading in the data from the Excel file:\n\n\nCode\nlibrary(readxl)\nlibrary(tidyverse)\n\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.0      ✔ purrr   1.0.1 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.3.0      ✔ stringr 1.5.0 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\nCode\ndf <- read_excel(\"_data/LungCapData.xls\")\n\n\nWe can use the glimpse() function to take a look at the data:\n\n\nCode\nglimpse(df)\n\n\nRows: 725\nColumns: 6\n$ LungCap   <dbl> 6.475, 10.125, 9.550, 11.125, 4.800, 6.225, 4.950, 7.325, 8.…\n$ Age       <dbl> 6, 18, 16, 14, 5, 11, 8, 11, 15, 11, 19, 17, 12, 10, 10, 13,…\n$ Height    <dbl> 62.1, 74.7, 69.7, 71.0, 56.9, 58.7, 63.3, 70.4, 70.5, 59.2, …\n$ Smoke     <chr> \"no\", \"yes\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\",…\n$ Gender    <chr> \"male\", \"female\", \"female\", \"male\", \"male\", \"female\", \"male\"…\n$ Caesarean <chr> \"no\", \"no\", \"yes\", \"no\", \"no\", \"no\", \"yes\", \"no\", \"no\", \"no\"…\n\n\nThis shows that this dataset has 725 observations (rows) and 6 variables (columns). The first three columns are labeled: LungCap, Age, and Height and they are all classified as double, which is a type of numeric variable. The last three columns are Smoke, Gender, and Caesarean, which are all classified as character variables. LungCap refers to the lung capacity of the person, Age, Height, and Gender all describe the relevant characteristics of that person. Smoke is a dichotomous variable which records whether a person smokes or not. Caesarean is also a dichotomous variable that reflects whether a person was born by a caesaren or not.\n\n\nThe distribution of the LungCap variable looks as follows:\n\n\nCode\nhist(df$LungCap)\n\n\n\n\n\nThe histogram suggests that the distribution is close to a normal distribution. Most of the observations are close to the mean.\n\n\n\nWe can use the boxplot function to compare the distribution of the LungCap variable with respect to males and females.\n\n\nCode\nboxplot(LungCap ~ Gender, data=df)\n\n\n\n\n\n\n\n\nBy using pipes, the group_by() and the summarize() functions we can take a look at the mean lung capacity of smokers and non-smokers. The results are counter-intuitive: it looks as though the mean lung capacity for those who smoke is larger than those who do not.\n\n\nCode\ndf %>% \n  group_by(Smoke) %>% \n  summarize(\"Mean_LungCap\"=mean(LungCap))\n\n\n# A tibble: 2 × 2\n  Smoke Mean_LungCap\n  <chr>        <dbl>\n1 no            7.77\n2 yes           8.65\n\n\n\n\n\nTo examine the relationship between smoking and lung capacity within age groups we can use the mutate() and case_when() functions and piping to create an ordinal variable that captures different age groups:\n\n\nCode\ndf <- df %>% \n  mutate(AgeGroups = case_when(\n    Age<=13 ~ \"a. Less than or equal to 13\",\n    Age==14 | Age==15 ~ \"b. 14 or 15\", \n    Age==16 | Age==17 ~ \"c. 16 or 17\",\n    Age>=18 ~ \"d. Greater than or equal to 18\"))\n\n\nWith this new variable we can more easily compare the lung capacities for smokers and non-smokers within each age group. The following tables show the mean lung capacity for study subjects arranged by age group, first for those who reported not smoking and second for those who did report smoking:\n\n\n\n\nCode\nfilter(df, Smoke==\"no\") %>% \n  group_by(AgeGroups) %>% \n  summarize(\"Mean_LungCap\"=mean(LungCap)) %>% \n  arrange(AgeGroups)\n\n\n# A tibble: 4 × 2\n  AgeGroups                      Mean_LungCap\n  <chr>                                 <dbl>\n1 a. Less than or equal to 13            6.36\n2 b. 14 or 15                            9.14\n3 c. 16 or 17                           10.5 \n4 d. Greater than or equal to 18        11.1 \n\n\n\n\n\n\n\nCode\nfilter(df, Smoke==\"yes\") %>% \n  group_by(AgeGroups) %>% \n  summarize(\"Mean_LungCap\"=mean(LungCap)) %>% \n  arrange(AgeGroups)\n\n\n# A tibble: 4 × 2\n  AgeGroups                      Mean_LungCap\n  <chr>                                 <dbl>\n1 a. Less than or equal to 13            7.20\n2 b. 14 or 15                            8.39\n3 c. 16 or 17                            9.38\n4 d. Greater than or equal to 18        10.5 \n\n\nThis data shows that lung capacity generally increases with age.\nIt is interesting to note that on average all age groups that did not smoke had a higher lung capacity except for those who were 13 or under. The group found to have the lowest lung capacity in the whole dataset were children 13 or under who did not smoke.\nSince the finding related to this age group (study subjects aged 13 or under) was unexpected, I used the filter() and table() functions to examine the sample size of this group.\n\n\nCode\nThirteen_and_under <- filter(df, AgeGroups == \"a. Less than or equal to 13\")\ntable(Thirteen_and_under$Smoke)\n\n\n\n no yes \n401  27 \n\n\nAccording to this data, there were only 27 children in this study 13 years old or under who reported smoking. In order to better understand the relationship between age, smoking status and lung capacity we could run another study with a larger sample size."
  },
  {
    "objectID": "posts/HW1_JustineShakespeare.html#a.-1",
    "href": "posts/HW1_JustineShakespeare.html#a.-1",
    "title": "Homework 1",
    "section": "2a.",
    "text": "2a.\nWhat is the probability that a randomly selected inmate has exactly 2 prior convictions?\nIn the table above we can see that the probability that a randomly selected inmate has 2 prior convictions is .1975, or .2 rounded."
  },
  {
    "objectID": "posts/HW1_JustineShakespeare.html#b.-1",
    "href": "posts/HW1_JustineShakespeare.html#b.-1",
    "title": "Homework 1",
    "section": "2b.",
    "text": "2b.\nWhat is the probability that a randomly selected inmate has fewer than 2 prior convictions?\nThis question is looking for the cumulative probability that an inmate has less than 2 prior convictions. We can find cumulative probability by adding the relevant probabilities: The probability that an inmate has 0 convictions is .16 and the probability that an inmate has 1 conviction is .54.\n\n\nCode\n# We can see these numbers in the table above, but as a reminder and a way to \n# double check, we can calculate those probabilities again before adding them:\nzero_convictions <- 128/810\none_conviction <- 434/810\nProb_fewer_than_2 <- zero_convictions + one_conviction\nProb_fewer_than_2\n\n\n[1] 0.6938272"
  },
  {
    "objectID": "posts/HW1_JustineShakespeare.html#c.-1",
    "href": "posts/HW1_JustineShakespeare.html#c.-1",
    "title": "Homework 1",
    "section": "2c.",
    "text": "2c.\nWhat is the probability that a randomly selected inmate has 2 or fewer prior convictions?\nTo find the probability that a randomly selected inmate has 2 or fewer prior convictions we use the same method as the in the previous question: add together the probabilities for X = 0, X = 1, and X = 2:\n\n\nCode\n# We can see these numbers in the table above, but as a reminder and a way to \n# double check, we can calculate those probabilities again before adding them:\ntwo_convictions <- 160/810\nProb_2_or_fewer <- zero_convictions + one_conviction + two_convictions\nProb_2_or_fewer \n\n\n[1] 0.891358"
  },
  {
    "objectID": "posts/HW1_JustineShakespeare.html#d.",
    "href": "posts/HW1_JustineShakespeare.html#d.",
    "title": "Homework 1",
    "section": "2d.",
    "text": "2d.\nWhat is the probability that a randomly selected inmate has more than 2 prior convictions?\nTo find the probability that a randomly selected inmate has more than 2 prior convictions we use the same method as in the previous questions, except that we add the probabilities of X = 3 and X = 4:\n\n\nCode\n# We can see these numbers in the table above, but as a reminder and a way to \n# double check, we can calculate those probabilities again before adding them:\nthree_convictions <- 64/810\nfour_convictions <- 24/810\nProb_2_or_more <- three_convictions + four_convictions\nProb_2_or_more \n\n\n[1] 0.108642"
  },
  {
    "objectID": "posts/HW1_JustineShakespeare.html#e.",
    "href": "posts/HW1_JustineShakespeare.html#e.",
    "title": "Homework 1",
    "section": "2e.",
    "text": "2e.\nWhat is the expected value for the number of prior convictions?\nTo find the expected value for the number of prior convictions we multiply each possible value of X by its probability of occurring and add that up over all X. To do this in R I used some of the same variables I created above to calculate the expected value:\n\n\nCode\nExpVal <- sum(probability*X_prior_convictions)\nExpVal\n\n\n[1] 1.28642"
  },
  {
    "objectID": "posts/HW1_JustineShakespeare.html#f.",
    "href": "posts/HW1_JustineShakespeare.html#f.",
    "title": "Homework 1",
    "section": "2f.",
    "text": "2f.\nCalculate the variance and the standard deviation for the Prior Convictions.\nTo calculate the variance I used the expected value we just calculated and the following equation for variance: E[(X-u)^2]. Once I found the variance, I was able to find the standard deviation, since it is the square root of the variance:\n\n\nCode\nVarX <- sum((X_prior_convictions-ExpVal)^2*probability)\nVarX\n\n\n[1] 0.8562353\n\n\nCode\nsdX <- sqrt(VarX)\nsdX\n\n\n[1] 0.9253298"
  },
  {
    "objectID": "posts/EmmaNarkewicz_FinalProject_CheckIn1.html",
    "href": "posts/EmmaNarkewicz_FinalProject_CheckIn1.html",
    "title": "Final Project Proposal",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/EmmaNarkewicz_FinalProject_CheckIn1.html#research-question",
    "href": "posts/EmmaNarkewicz_FinalProject_CheckIn1.html#research-question",
    "title": "Final Project Proposal",
    "section": "Research Question",
    "text": "Research Question\nFor my final project I want to expand on research on the mental health, empathy, and burnout of medical school students using a data set of 886 medical students in Switzerland. The COVID-19 pandemic heightened the mental health challenges of health care workers around the world (Teisman et al., 2021). Numerous studies show that health care workers are prone to compassion fatigue due to working long hours in stressful work environments with continuous exposure to trauma (Jennings, 2009; Rodriguez & Carlotta, 2017; Peters, 2018; Yayha et al., 2021; Carrard et al., 2022; Shin et al., 2022).\nThe Association of American Medical Colleges (AAMC) found that 30% of surveyed medical students and residents met the criteria for depression and 10% reported having suicidal thoughts (Pasturel, 2020). Previous studies conducted on samples of health care workers in Switzerland, Iraq, and South Korea examined the impact of gender on burnout, finding that female medical students had higher rates of empathy and burnout than male coworkers (Carrard et al., 2022; Yahya et al., 2021; Shin et al., 2022). A 2009 multi-site study of medical students in the U.S. found statistically significant differences in depression by gender but not by ethnicity (Goebert et al., 2009). In contrast, the same study found statistically significant differences in suicidal ideation by ethnicity, but not by gender, with Black students reporting the highest rates of suicidal ideation & Caucasian students reporting the lowest rates of suicidal ideation (Goebert et al., 2009).\nResearch Question: Why are some medical students more likely to experience burnout than others?"
  },
  {
    "objectID": "posts/EmmaNarkewicz_FinalProject_CheckIn1.html#hypothesis-testing",
    "href": "posts/EmmaNarkewicz_FinalProject_CheckIn1.html#hypothesis-testing",
    "title": "Final Project Proposal",
    "section": "Hypothesis Testing",
    "text": "Hypothesis Testing\nI want to explore further how ethnic identity might serve as a protective or risk factor for the burnout of medical students, specifically for international medical students. A 2022 study of medical school students in Croatia found that international medical students experience higher rates of burnout mediated by social and familial loneliness (Gradiski et al., 2022). For my final project I will test whether or not a student’s first language being a national language of Switzerland – where the sample was taken – impacts their burnout. The commonly spoken national languages of Switzerland are French, German, and Italian (Kużelewska, 2016).\nHypothesis: Medical students whose native language is a national language of the country where they are studying will experience lower rates of burnout than medical students with other native languages.\nThe reasoning behind my hypothesis is if a medical student’s native language is one of the national language of Switzerland, they will have benefit from potential protective factors of social, cultural, and familial connections. In contrast, I expect medical students whose native language is not German, French, or Italian to be at higher risk for burnout mediated through increased stress from coping with different culture, language, and physical separation from their family."
  },
  {
    "objectID": "posts/EmmaNarkewicz_FinalProject_CheckIn1.html#descriptive-statistics",
    "href": "posts/EmmaNarkewicz_FinalProject_CheckIn1.html#descriptive-statistics",
    "title": "Final Project Proposal",
    "section": "Descriptive Statistics",
    "text": "Descriptive Statistics\nThe data set I will be analyzing contains demographic information on 886 medical students in Switzerland. Students answered demographic information about their age, gender, their year in school and well as the results of self-reported empathy, depression, anxiety, and burnout. The data set was downloaded from Kaggle at https://www.kaggle.com/datasets/thedevastator/medical-student-mental-health?select=Codebook+Carrard+et+al.+2022+MedTeach.csv but originally sourced for a 2022 publication in the Medical Teacher Journe by Carrard et al.\nImportant variables I want to explore in my data set as potential risk and protective factors:\n\nNative Language\nAge\nGender\nHaving a romantic partner\nSeeing a psychotherapist\nHours worked\nJefferson Scale Empathy (JSPE) total empathy score\nQuestionnaire of Cognitive and Affective Empathy (QCAE) Cognitive empathy score\nQuestionnaire of Cognitive and Affective Empathy QCAE Affective empathy score\nCenter for Epidemiologic Studies Depression Scale (CES-D) total score\nState & Trait Anxiety (STAI) score\nMaslach Burnout Inventory (MBI) Emotional Exhaustion\nMaslach Burnout Inventory (MBI) Cynicism\nMaslach Burnout Inventory (MBI) Academic Efficacy\n\nEach of the various empathy, mental health, and burnout scales are scored differently, so care needs to be taken in interpreting these findings.For example, a higher score on the emotional exhaustion and cynicism scales of the MBI indicate higher burn out, while a higher score on the MBI personal achievement indicates lower levels of burnout (Maslach et al., 1996).\n\n\nCode\n#Readin Final data set\n\nFinalDataSet <- read_csv(\"_data/med_student_burnout.csv\")\n\n\nRows: 886 Columns: 20\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (20): id, age, year, sex, glang, part, job, stud_h, health, psyt, jspe, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nCode\nFinalDataSet\n\n\n# A tibble: 886 × 20\n      id   age  year   sex glang  part   job stud_h health  psyt  jspe qcae_cog\n   <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>  <dbl>  <dbl> <dbl> <dbl>    <dbl>\n 1     2    18     1     1   120     1     0     56      3     0    88       62\n 2     4    26     4     1     1     1     0     20      4     0   109       55\n 3     9    21     3     2     1     0     0     36      3     0   106       64\n 4    10    21     2     2     1     0     1     51      5     0   101       52\n 5    13    21     3     1     1     1     0     22      4     0   102       58\n 6    14    26     5     2     1     1     1     10      2     0   102       48\n 7    17    23     5     2     1     1     0     15      3     0   117       58\n 8    21    23     4     1     1     1     1      8      4     0   118       65\n 9    23    23     4     2     1     1     1     20      2     0   118       69\n10    24    22     2     2     1     1     0     20      5     0   108       56\n# … with 876 more rows, and 8 more variables: qcae_aff <dbl>, amsp <dbl>,\n#   erec_mean <dbl>, cesd <dbl>, stai_t <dbl>, mbi_ex <dbl>, mbi_cy <dbl>,\n#   mbi_ea <dbl>\n\n\nPrior to examining the descriptive statistics from the med school data set I recoded qualitative variables stored as numeric values, using the Carrard et al., 2022 code book, replacing 0, 1 with clear demographic information about age, gender, having a partner etc. The explanatory variable NatLang which collapses down into if medical students native language is German, French, or Italian (NatSpeaker) or not (NotNatSpeaker).\n\n\nCode\n#Recoding categorical variables based on code book\n\nFinalRecoded <- FinalDataSet %>%\n  mutate(NatLang = case_when(\n    glang == 1 | glang == 15 | glang == 90 ~ \"NatSpeaker\",\n    glang > 19  & glang < 90 | glang > 90 ~ \"NonNatSpeaker\")\n  ) %>% \n  relocate(`NatLang`, .before = `age`)%>%\n  select(!contains(\"glang\")) %>%\nmutate(gender = case_when(\n         sex == 1  ~ \"Male\",\n         sex == 2 ~ \"Female\", \n         sex == 3 ~ \"Non-Binary\")\n        ) %>%\n  relocate(`gender`, .before = `age`)%>%\n  select(!contains(\"sex\")) %>%\nmutate(partner = case_when(\n         part == 0  ~ \"single\",\n         part == 1 ~ \"partnered\")\n        ) %>%\n  relocate(`partner`, .before = `age`)%>%\n  select(!\"part\") %>%\nmutate(paid_job = case_when(\n         job == 0  ~ \"no_job\",\n         job == 1 ~ \"yes_job\")\n        ) %>%\n  relocate(`paid_job`, .before = `age`)%>%\n  select(!\"job\") %>%\nmutate(health_sat = case_when(\n         health == 1  ~ \"very_dis\",\n         health == 2 ~ \"dis\",\n         health == 3 ~ \"neutral\",\n         health == 4 ~ \"sat\",\n         health  == 5 ~ \"dis_sat\")\n) %>%\n  relocate(`health_sat`, .before = `age`)%>%\n  select(!\"health\") %>%\nmutate(MHcare = case_when(\n         psyt == 0  ~ \"no_ther\",\n         psyt == 1 ~ \"yes_ther\")\n        ) %>%\nrelocate(`MHcare`, .before = `age`)%>%\n  select(!\"psyt\") \n\nFinalRecoded\n\n\n# A tibble: 886 × 20\n      id NatLang  gender partner paid_…¹ healt…² MHcare   age  year stud_h  jspe\n   <dbl> <chr>    <chr>  <chr>   <chr>   <chr>   <chr>  <dbl> <dbl>  <dbl> <dbl>\n 1     2 NonNatS… Male   partne… no_job  neutral no_th…    18     1     56    88\n 2     4 NatSpea… Male   partne… no_job  sat     no_th…    26     4     20   109\n 3     9 NatSpea… Female single  no_job  neutral no_th…    21     3     36   106\n 4    10 NatSpea… Female single  yes_job dis_sat no_th…    21     2     51   101\n 5    13 NatSpea… Male   partne… no_job  sat     no_th…    21     3     22   102\n 6    14 NatSpea… Female partne… yes_job dis     no_th…    26     5     10   102\n 7    17 NatSpea… Female partne… no_job  neutral no_th…    23     5     15   117\n 8    21 NatSpea… Male   partne… yes_job sat     no_th…    23     4      8   118\n 9    23 NatSpea… Female partne… yes_job dis     no_th…    23     4     20   118\n10    24 NatSpea… Female partne… no_job  dis_sat no_th…    22     2     20   108\n# … with 876 more rows, 9 more variables: qcae_cog <dbl>, qcae_aff <dbl>,\n#   amsp <dbl>, erec_mean <dbl>, cesd <dbl>, stai_t <dbl>, mbi_ex <dbl>,\n#   mbi_cy <dbl>, mbi_ea <dbl>, and abbreviated variable names ¹​paid_job,\n#   ²​health_sat\n\n\n\n\nCode\n#Descriptive statistics for quantitative variables\n\nsummary(FinalRecoded)\n\n\n       id           NatLang             gender            partner         \n Min.   :   2.0   Length:886         Length:886         Length:886        \n 1st Qu.: 447.5   Class :character   Class :character   Class :character  \n Median : 876.0   Mode  :character   Mode  :character   Mode  :character  \n Mean   : 889.7                                                           \n 3rd Qu.:1341.8                                                           \n Max.   :1790.0                                                           \n   paid_job          health_sat           MHcare               age       \n Length:886         Length:886         Length:886         Min.   :17.00  \n Class :character   Class :character   Class :character   1st Qu.:20.00  \n Mode  :character   Mode  :character   Mode  :character   Median :22.00  \n                                                          Mean   :22.38  \n                                                          3rd Qu.:24.00  \n                                                          Max.   :49.00  \n      year           stud_h           jspe          qcae_cog    \n Min.   :1.000   Min.   : 0.00   Min.   : 67.0   Min.   :37.00  \n 1st Qu.:1.000   1st Qu.:12.00   1st Qu.:101.0   1st Qu.:54.00  \n Median :3.000   Median :25.00   Median :107.0   Median :58.00  \n Mean   :3.103   Mean   :25.29   Mean   :106.4   Mean   :58.53  \n 3rd Qu.:5.000   3rd Qu.:36.00   3rd Qu.:113.0   3rd Qu.:63.00  \n Max.   :6.000   Max.   :70.00   Max.   :125.0   Max.   :76.00  \n    qcae_aff          amsp         erec_mean           cesd      \n Min.   :18.00   Min.   : 6.00   Min.   :0.3571   Min.   : 0.00  \n 1st Qu.:31.00   1st Qu.:20.00   1st Qu.:0.6667   1st Qu.: 9.00  \n Median :35.00   Median :23.00   Median :0.7262   Median :16.00  \n Mean   :34.78   Mean   :23.15   Mean   :0.7201   Mean   :18.05  \n 3rd Qu.:39.00   3rd Qu.:26.75   3rd Qu.:0.7857   3rd Qu.:25.00  \n Max.   :48.00   Max.   :35.00   Max.   :0.9524   Max.   :56.00  \n     stai_t         mbi_ex          mbi_cy          mbi_ea     \n Min.   :20.0   Min.   : 5.00   Min.   : 4.00   Min.   :10.00  \n 1st Qu.:34.0   1st Qu.:13.00   1st Qu.: 6.00   1st Qu.:21.00  \n Median :43.0   Median :17.00   Median : 9.00   Median :24.00  \n Mean   :42.9   Mean   :16.88   Mean   :10.08   Mean   :24.21  \n 3rd Qu.:51.0   3rd Qu.:20.00   3rd Qu.:13.00   3rd Qu.:28.00  \n Max.   :77.0   Max.   :30.00   Max.   :24.00   Max.   :36.00  \n\n\nNote that id is not a true numeric variable and therefor the descriptive statistics for it should be disregarded.\nMedical students in the sample studied for an average of 25 hours a week, with a maximum of 70 hours.\nScores on the Jefferson Scale of Physician Empathy (JSPE) range from 20-140 with a higher score indicating higher empathy. The mean JSPE score in the sample was was 106.2 and the median JSPE score was 107.0 indicating relatively high empathy. There was a broad range from as low to 67-125, with the IQR indicating most medical students scored in the low 100s range.\nThere were are scores for all 3 components of MBI burnout: emotional exhaustion, cynicism, and personal achievement.\n\nOn the mbi-ex, medical school students’ scores ranged from 5-30, with a median score of 17 and a mean score of 16.88. According to the MBI score guide, half of medical students in the sample exhibit low-level burnout (scoring 17 or less), and the other half exhibiting moderate burnout in terms of emotional exhaustion.\nOn the mbi-cyn, medical students’ scores ranged from 4-24, with a median of 9 and mean of 10.08. According to the scoring guide, the majority of the sample exhibit moderate burnout (6-11) with some exhibiting high level burnout (12+) in the dimension of cynicism.\nOn the mbi-ea, medical students scores ranged from 10-36, with a median score of 24 and a mean score of 24.01. A score of 33 or less indicates high level of burnout and a score between 24-39 indicates moderate level burnout, with medical students falling in the high and moderate burnout range for personal achievement.\n\n\n\nCode\n#Frequency of categorical & ordinal variables\n\n prop.table(table(select(FinalRecoded, NatLang)))\n\n\nNatLang\n   NatSpeaker NonNatSpeaker \n    0.8950339     0.1049661 \n\n\nCode\n prop.table(table(select(FinalRecoded, gender)))\n\n\ngender\n     Female        Male  Non-Binary \n0.683972912 0.310383747 0.005643341 \n\n\nCode\n prop.table(table(select(FinalRecoded, partner)))\n\n\npartner\npartnered    single \n0.5632054 0.4367946 \n\n\nCode\n prop.table(table(select(FinalRecoded, paid_job)))\n\n\npaid_job\n   no_job   yes_job \n0.6512415 0.3487585 \n\n\nCode\n prop.table(table(select(FinalRecoded, health_sat)))\n\n\nhealth_sat\n       dis    dis_sat    neutral        sat   very_dis \n0.09819413 0.25282167 0.15349887 0.45372460 0.04176072 \n\n\nCode\n prop.table(table(select(FinalRecoded, MHcare)))\n\n\nMHcare\n no_ther yes_ther \n0.775395 0.224605 \n\n\nFrom the proportion tables above it can be seen that majority (90%) of the sample speaks one of the national languages of Switzerland, while only 10% are non native speakers. The sample is also mostly female (68%), with less than 1% identifying as non-binary. Over half (56%) of the medical students reported having partners, but only about a third of medical students had a paid job (34.9%). The most common (45%) report from medical students was that they were satisfied with their health and less than one quarter (22.5%) of medical student reported seeing a therapist in the last 12 months.\nFinally, I created a several box plots comparing the empathy, mental health, and burnout scores of medical students whose native language is a national language of Switzerland vs. students whose native language is not.\n\n\nCode\n# Empathy Score\n\nggplot(data = FinalRecoded, aes(x= NatLang, y = jspe, fill = NatLang)) +\n  geom_boxplot() +\n  labs(title = \"Box Plot - JSPE measure of Empathy\", x = \"Native Language Spoken\", y = \"JSPE\") \n\n\n\n\n\nFrom the box plot of JSPE scores, native speakers of national languages have slightly higher median empathy than non-native students, though they are very close and there appear to be several low outliers for native speakers.\n\n\nCode\n#Depression Score\nggplot(data = FinalRecoded, aes(x= NatLang, y = cesd, fill = NatLang)) + geom_boxplot() +\n  labs(title = \"Box Plot - cesd measure of Depression\", x = \"Native Language Spoken\", y = \"cesd\") \n\n\n\n\n\nNon-native national language speakers have a higher median on CESD than native national language speakers, suggesting higher depression.\n\n\nCode\n#Anxiety Score\nggplot(data = FinalRecoded, aes(x= NatLang, y = stai_t, fill = NatLang)) + geom_boxplot() +\n  labs(title = \"Box Plot - stai_t measure of Anxiety\", x = \"Native Language Spoken\", y = \"stai_t\") \n\n\n\n\n\nNon-native national languages speakers also seem to score higher for anxiety as measured by the stai_t.\n\n\nCode\n#Emotional Exhaustion Burnout Score\nggplot(data = FinalRecoded, aes(x= NatLang, y = mbi_ex, fill = NatLang)) + geom_boxplot() +\n  labs(title = \"Box Plot - mbi_ex Emotional Burnout\", x = \"Native Language Spoken\", y = \"mbi_ex\") \n\n\n\n\n\nNative language speaking and non-native language speaking medical students seemed to have nearly identical median scores for Emotional Exhaustion (mbi_ex).\n\n\nCode\n#Cynicism Score\nggplot(data = FinalRecoded, aes(x= NatLang, y = mbi_cy, fill = NatLang)) + geom_boxplot() +\n  labs(title = \"Box Plot - mbi_cy Cynicism Burnout\", x = \"Native Language Spoken\", y = \"mbi_cy\") \n\n\n\n\n\nNon-native speaking medical students appeared to score slightly higher on average than non-native speaking medical students on Cynicism as measured by the mbi-cy, with a higher median score.\n\n\nCode\n#Academic Efficacy Score\nggplot(data = FinalRecoded, aes(x= NatLang, y = mbi_ea, fill = NatLang)) + geom_boxplot() +\n  labs(title = \"Box Plot - mbi-ea Academic Efficacy Burnout\", x = \"Native Language Spoken\", y = \"mbi_ea\") \n\n\n\n\n\nLastly, Native language speaking and non-native language speaking medical students seemed to have nearly identical median scores for Personal Achievement burnout (mbi_ea).\nI look forward to analyzing the data set and testing my hypothesis in the coming weeks, as well as getting feedback on this proposal."
  },
  {
    "objectID": "posts/EmmaNarkewicz_FinalProject_CheckIn1.html#works-cited",
    "href": "posts/EmmaNarkewicz_FinalProject_CheckIn1.html#works-cited",
    "title": "Final Project Proposal",
    "section": "Works Cited",
    "text": "Works Cited\nCarrard, V., Bourquin, C., Berney, S, Schlegel, K., Gaume, J., Bart, P-A., Preisig M., Mast, M. A., & Berney, A. (2022) The relationship between medical students’ empathy, mental health, and burnout: A cross-sectional study, Medical Teacher, 44:12, 1392-1399, DOI: 10.1080/0142159X.2022.2098708\nGradiski, I. P., Borovecki, A., Ćurković, M., San-Martín, M., Delgado Bolton, R. C., & Vivanco, L. (2022). Burnout in International Medical Students: Characterization of Professionalism and Loneliness as Predictive Factors of Burnout. International journal of environmental research and public health, 19(3), 1385. https://doi.org/10.3390/ijerph19031385\nGoebert., D., Thompson., D., Takeshita., J., Beach, C., Bryson, P., Ephgrave, K., Kent. A., Kunkel., M., Schechter., J., Tate., J. (2009). Depressive Symptoms in Medical Students and Residents: A Multischool Study. Academic Medicine 84(2):p 236-241, DOI: 10.1097/ACM.0b013e31819391bb\nJennings, M.L. Medical Student Burnout: Interdisciplinary Exploration and Analysis. J Med Humanit 30, 253–269 (2009). https://doi.org/10.1007/s10912-009-9093-5\nKużelewska,E. (2016).Language Policy in Switzerland. Studies in Logic, Grammar and Rhetoric,45(1) 125-140. https://doi.org/10.1515/slgr-2016-0020\nMaslach, C., Jackson, S.E., & Jackson, Leiter, M. P. (Eds.) (1996). Maslach Burnout Inventory manual (3rd ed.).\nPaturel, A. (2020). Healing the very youngest healers. American Association of Medical Colleges (AAMC). https://www.aamc.org/news-insights/healing-very-youngest-healers#:~:text=In%20a%20recent%20study%20%2C%209.4,as%20their%20same%2Dage%20peers.\nPeters E. (2018). Compassion fatigue in nursing: A concept analysis. Nursing forum, 53(4), 466–480. https://doi.org/10.1111/nuf.12274\nRadloff, L.S. (1977). The CES-D Scale: a self-report depression scale for research in the general population. Applied Psychological Measurement, 1:385-401.\nRodriguez, S. Y. S., Carlotta, M. S.. (2017). Predictors of Burnout Syndrome in psychologists. Estudos De Psicologia (campinas), 34(Estud. psicol. (Campinas), 2017 34(1)), 141–150. https://doi.org/10.1590/1982-02752017000100014\nShin, H. S., Park, H., & Lee, Y. M. (2022). The relationship between medical students’ empathy and burnout levels by gender and study years. Patient education and counseling, 105(2), 432–439. https://doi.org/10.1016/j.pec.2021.05.036\nTiesman, H., Weissman, D., Stone., D., Quinlan, K., & Chosewood, L. (2021). Suicide Prevention for Healthcare Workers. CDC. https://blogs.cdc.gov/niosh-science-blog/2021/09/17/suicide-prevention-hcw/\nWilliams, B., Beovich, B. Psychometric properties of the Jefferson Scale of Empathy: a COSMIN systematic review protocol. Syst Rev 8, 319 (2019). https://doi.org/10.1186/s13643-019-1240-0\nYahya, M. S., Abutiheen, A. A., & Al- Haidary, A. F. (2021). Burnout among medical students of the University of Kerbala and its correlates. Middle East Current Psychiatry, Ain Shams University, 28(1), 78. https://doi.org/10.1186/s43045-021-00152-2"
  },
  {
    "objectID": "posts/Kristin_Abijaoude_FinalCheckIn1.html",
    "href": "posts/Kristin_Abijaoude_FinalCheckIn1.html",
    "title": "Final Project Check in 1",
    "section": "",
    "text": "Code\n# load packages\npackages <- c(\"readr\", \"readxl\", \"summarytools\", \"tidyverse\", \"dplyr\")\nlapply(packages, require, character.only = TRUE)\n\n\nLoading required package: readr\n\n\nLoading required package: readxl\n\n\nLoading required package: summarytools\n\n\nLoading required package: tidyverse\n\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.0     ✔ dplyr   1.1.0\n✔ tibble  3.1.8     ✔ stringr 1.5.0\n✔ tidyr   1.2.1     ✔ forcats 0.5.2\n✔ purrr   1.0.0     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n✖ tibble::view()  masks summarytools::view()\n\n\n[[1]]\n[1] TRUE\n\n[[2]]\n[1] TRUE\n\n[[3]]\n[1] TRUE\n\n[[4]]\n[1] TRUE\n\n[[5]]\n[1] TRUE\n\n\nCode\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/Kristin_Abijaoude_FinalCheckIn1.html#overview",
    "href": "posts/Kristin_Abijaoude_FinalCheckIn1.html#overview",
    "title": "Final Project Check in 1",
    "section": "Overview",
    "text": "Overview\nBullying continues to be a persistent problem in schools.\nTypes of bullying faced by those affected include physical fights, exclusion, rumors, snarky “jokes”, and name-calling. Every bullied student dreads going to school because they have to face their bullies, who would find any reason, or no reason at all, to target them. Bullying can happen outside of school, especially with today’s advanced technology and near-universal access to the Internet. While students are always encouraged to tell a trusted adult, such as a teacher, trusted adults in authority have a spotty record when it comes to tackling this epidemic.\nIn the US alone, one of every five students report being bullied on school grounds, including name-calling (13% among those who reported bullying), being pushed or shoved (5%), or have property destroyed on purpose (1%). 15% of students who reported bullying were cyberbullied 1. Globally, one in three students report bullying, from as low as 7% in the Central Asian country of Tajikistan to as high as 74% in Samoa.2\nThe negative effects on bullying include low self-esteem, feeling angry or isolated, and distress, as well as physical effects like loss of sleep, headaches, and disordered eating. Bullying can be so detrimental to the victim that they take their own life to escape the pain.3\nWhen discussing ways to combat bullying, it’s too simplistic to say that “kids are just cruel”. My purpose is to find why some students are more vulnerable to being targets of bullying, and how we can use those parameters to create solutions to end bullying once and for all."
  },
  {
    "objectID": "posts/Kristin_Abijaoude_FinalCheckIn1.html#hypotheses-and-proposed-model",
    "href": "posts/Kristin_Abijaoude_FinalCheckIn1.html#hypotheses-and-proposed-model",
    "title": "Final Project Check in 1",
    "section": "Hypotheses and Proposed Model",
    "text": "Hypotheses and Proposed Model\nI will use multiple regression to test out my hypotheses, given the multiple independent variables such as body size, age, and gender. Multiple regression is a good model to measure the effects that multiple factors have on an given outcome. In this project, I will use these variables to explore a relationship between those variables and bullying.\n\nHa: Students who report loneliness and fewer friends are more vulnerable of being targets or bullying.\nHa: Male students are more likely to face physical abuse by bullies, while female students are more likely to face verbal abuse.\nHa: More female students who report bullying are targeted for being underweight, while male students who report bullying are targeted for being overweight."
  },
  {
    "objectID": "posts/Kristin_Abijaoude_FinalCheckIn1.html#data-summary",
    "href": "posts/Kristin_Abijaoude_FinalCheckIn1.html#data-summary",
    "title": "Final Project Check in 1",
    "section": "Data Summary",
    "text": "Data Summary\n\n\nCode\nbully <- read_xlsx(\"_data/Bullying.xlsx\",\n                   range = cell_rows(2:56982))\nbully\n\n\n# A tibble: 56,980 × 18\n   record Bullie…¹ Bulli…² Cyber…³ Custo…⁴ Sex   Physi…⁵ Physi…⁶ Felt_…⁷ Close…⁸\n    <dbl> <chr>    <chr>   <chr>   <chr>   <chr> <chr>   <chr>   <chr>   <chr>  \n 1      1 Yes      Yes     <NA>    13 yea… Fema… 0 times 0 times Always  2      \n 2      2 No       No      No      13 yea… Fema… 0 times 0 times Never   3 or m…\n 3      3 No       No      No      14 yea… Male  0 times 0 times Never   3 or m…\n 4      4 No       No      No      16 yea… Male  0 times 2 or 3… Never   3 or m…\n 5      5 No       No      No      13 yea… Fema… 0 times 0 times Rarely  3 or m…\n 6      6 No       No      No      13 yea… Male  0 times 1 time  Never   3 or m…\n 7      7 No       No      No      14 yea… Fema… 1 time  0 times Someti… 3 or m…\n 8      8 No       No      No      12 yea… Fema… 0 times 0 times Rarely  3 or m…\n 9      9 No       No      No      13 yea… Male  1 time  2 or 3… Never   3 or m…\n10     10 Yes      No      No      14 yea… Fema… 0 times 0 times Always  0      \n# … with 56,970 more rows, 8 more variables: Miss_school_no_permission <chr>,\n#   Other_students_kind_and_helpful <chr>, Parents_understand_problems <chr>,\n#   Most_of_the_time_or_always_felt_lonely <chr>,\n#   Missed_classes_or_school_without_permission <chr>, Were_underweight <chr>,\n#   Were_overweight <chr>, Were_obese <chr>, and abbreviated variable names\n#   ¹​Bullied_on_school_property_in_past_12_months,\n#   ²​Bullied_not_on_school_property_in_past_12_months, …\n\n\nThis 2018 study was conducted by Global School-Based Student Health Survey (GSHS), where 56,981 students from Argentina participated by filling out the questionnaire in regards to their mental health and behavior.4\n\n\nCode\ncolnames(bully)\n\n\n [1] \"record\"                                          \n [2] \"Bullied_on_school_property_in_past_12_months\"    \n [3] \"Bullied_not_on_school_property_in_past_12_months\"\n [4] \"Cyber_bullied_in_past_12_months\"                 \n [5] \"Custom_Age\"                                      \n [6] \"Sex\"                                             \n [7] \"Physically_attacked\"                             \n [8] \"Physical_fighting\"                               \n [9] \"Felt_lonely\"                                     \n[10] \"Close_friends\"                                   \n[11] \"Miss_school_no_permission\"                       \n[12] \"Other_students_kind_and_helpful\"                 \n[13] \"Parents_understand_problems\"                     \n[14] \"Most_of_the_time_or_always_felt_lonely\"          \n[15] \"Missed_classes_or_school_without_permission\"     \n[16] \"Were_underweight\"                                \n[17] \"Were_overweight\"                                 \n[18] \"Were_obese\"                                      \n\n\nCode\ndim(bully) # 56980 rows and 18 columns\n\n\n[1] 56980    18\n\n\n\n\nCode\nprint(dfSummary(bully,\n                        varnumbers = FALSE,\n                        plain.ascii  = FALSE, \n                        style        = \"grid\", \n                        graph.magnif = 0.70, \n                        valid.col    = FALSE),\n      method = 'render',\n      table.classes = 'table-condensed')\n\n\n\n\nData Frame Summary\nbully\nDimensions: 56980 x 18\n  Duplicates: 0\n\n\n  \n    \n      Variable\n      Stats / Values\n      Freqs (% of Valid)\n      Graph\n      Missing\n    \n  \n  \n    \n      record\n[numeric]\n      Mean (sd) : 28534.9 (16479.7)min ≤ med ≤ max:1 ≤ 28521.5 ≤ 57094IQR (CV) : 28540.5 (0.6)\n      56980 distinct values\n      \n      0\n(0.0%)\n    \n    \n      Bullied_on_school_property_in_past_12_months\n[character]\n      1. No2. Yes\n      43838(78.6%)11903(21.4%)\n      \n      1239\n(2.2%)\n    \n    \n      Bullied_not_on_school_property_in_past_12_months\n[character]\n      1. No2. Yes\n      44263(78.4%)12228(21.6%)\n      \n      489\n(0.9%)\n    \n    \n      Cyber_bullied_in_past_12_months\n[character]\n      1. No2. Yes\n      44213(78.4%)12196(21.6%)\n      \n      571\n(1.0%)\n    \n    \n      Custom_Age\n[character]\n      1. 11 years old or younger2. 12 years old3. 13 years old4. 14 years old5. 15 years old6. 16 years old7. 17 years old8. 18 years old or older\n      48(0.1%)145(0.3%)10574(18.6%)12946(22.8%)12812(22.5%)11737(20.6%)8227(14.5%)383(0.7%)\n      \n      108\n(0.2%)\n    \n    \n      Sex\n[character]\n      1. Female2. Male\n      29361(52.0%)27083(48.0%)\n      \n      536\n(0.9%)\n    \n    \n      Physically_attacked\n[character]\n      1. 0 times2. 1 time3. 10 or 11 times4. 12 or more times5. 2 or 3 times6. 4 or 5 times7. 6 or 7 times8. 8 or 9 times\n      46996(82.8%)5248(9.2%)115(0.2%)790(1.4%)2405(4.2%)695(1.2%)302(0.5%)189(0.3%)\n      \n      240\n(0.4%)\n    \n    \n      Physical_fighting\n[character]\n      1. 0 times2. 1 time3. 10 or 11 times4. 12 or more times5. 2 or 3 times6. 4 or 5 times7. 6 or 7 times8. 8 or 9 times\n      43245(76.3%)6932(12.2%)165(0.3%)939(1.7%)3650(6.4%)1028(1.8%)489(0.9%)264(0.5%)\n      \n      268\n(0.5%)\n    \n    \n      Felt_lonely\n[character]\n      1. Always2. Most of the time3. Never4. Rarely5. Sometimes\n      3120(5.5%)6422(11.3%)17931(31.7%)14427(25.5%)14714(26.0%)\n      \n      366\n(0.6%)\n    \n    \n      Close_friends\n[character]\n      1. 02. 13. 24. 3 or more\n      3331(6.0%)4732(8.5%)9110(16.3%)38731(69.3%)\n      \n      1076\n(1.9%)\n    \n    \n      Miss_school_no_permission\n[character]\n      1. 0 days2. 1 or 2 days3. 10 or more days4. 3 to 5 days5. 6 to 9 days\n      38654(70.1%)9738(17.7%)1468(2.7%)3925(7.1%)1331(2.4%)\n      \n      1864\n(3.3%)\n    \n    \n      Other_students_kind_and_helpful\n[character]\n      1. Always2. Most of the time3. Never4. Rarely5. Sometimes\n      9710(17.5%)15820(28.5%)4775(8.6%)10966(19.8%)14150(25.5%)\n      \n      1559\n(2.7%)\n    \n    \n      Parents_understand_problems\n[character]\n      1. Always2. Most of the time3. Never4. Rarely5. Sometimes\n      13072(23.9%)9570(17.5%)11964(21.9%)10459(19.2%)9542(17.5%)\n      \n      2373\n(4.2%)\n    \n    \n      Most_of_the_time_or_always_felt_lonely\n[character]\n      1. No2. Yes\n      47072(83.1%)9542(16.9%)\n      \n      366\n(0.6%)\n    \n    \n      Missed_classes_or_school_without_permission\n[character]\n      1. No2. Yes\n      38654(70.1%)16462(29.9%)\n      \n      1864\n(3.3%)\n    \n    \n      Were_underweight\n[character]\n      1. No2. Yes\n      35318(98.0%)733(2.0%)\n      \n      20929\n(36.7%)\n    \n    \n      Were_overweight\n[character]\n      1. No2. Yes\n      25376(70.4%)10675(29.6%)\n      \n      20929\n(36.7%)\n    \n    \n      Were_obese\n[character]\n      1. No2. Yes\n      33396(92.6%)2655(7.4%)\n      \n      20929\n(36.7%)\n    \n  \n\nGenerated by summarytools 1.0.1 (R version 4.2.2)2023-03-25"
  },
  {
    "objectID": "posts/FinalPart1_LTucksmith.html",
    "href": "posts/FinalPart1_LTucksmith.html",
    "title": "FinalPart1",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(janitor)\nknitr::opts_chunk$set(echo = TRUE)\n\n\nThe most profitable sports league in the world, the National Football League, generated $18 billion in revenue in 2021 according to sportico.com. This total represents national media rights, league sponsorships with gambling companies, news outlets, and other companies, and shared revenue and royalties from the league’s various affiliates and subsidiaries, such as NFL Enterprises, NFL Properties, and NFL International. While the 32 teams which make up the NFL have separate revenue streams stemming from merchandise and ticketing sales and various other endeavors, each team also receives a slice of shared revenue from the NFL from the games’ national and local broadcasts and sponsorships. As money is what keeps the NFL afloat and allows teams to competitively pay for top players and coaches, NFL teams benefit from working with media outlets and appearing in media and news. What I plan to analyze is how the relationship between the NFL and media plays out in games, if it does at all. Can the relationship between media and game outcome be measured? Does a team’s weekly media attention share affect their weekly game outcome? My analysis will attempt to measure this relationship by comparing Google Trends generated News Interest Over Time scores for the 2020-21 and 2021-22 for each match up that occurred in those two seasons.\nMy hypothesis is that the highest scored weeks for each team will generate the highest probability of winning that same or next given week. In addition, I plan to analyze how the relationship between the media and game scores changes given the media score of the opposition team, and if the relationship between the media scores and the sports betting data differs from that of the media scores and game outcome. I hypothesize that the team with the higher media score will be favored and that the spread will widen if the individual media score is above a to be determined .\nThe first dataset is a collection of all the matchups and scores that occurred in the NFL since 1966. The matchup information includes the names of the two teams competing, home/away team status, the match site, the weather on game day, and the final score for each team. Sports betting data for each game since 1977 is also included, and includes who was favored, the point spread, and the over/under line. The dataset is from https://www.kaggle.com/datasets/tobycrabtree/nfl-scores-and-betting-data and was created from a variety of sources including games and scores from public websites such as ESPN, NFL.com, and Pro Football Reference. Weather information is from NOAA data, cross-referenced with NFLweather.com. Betting data reflects lines available at sportsline.com and aussportsbetting.com. For the analysis, I will limit the data to the data collected from the 2020-21 and 2021-22 seasons.\n\n\nCode\ngame_scores <- read.csv(\"~/Documents/GitHub/603_Spring_2023/posts/_data/spreadspoke_scores.csv\")\ncolnames(game_scores)\n\n\n [1] \"schedule_date\"       \"schedule_season\"     \"schedule_week\"      \n [4] \"schedule_playoff\"    \"team_home\"           \"score_home\"         \n [7] \"score_away\"          \"team_away\"           \"team_favorite_id\"   \n[10] \"spread_favorite\"     \"over_under_line\"     \"stadium\"            \n[13] \"stadium_neutral\"     \"weather_temperature\" \"weather_wind_mph\"   \n[16] \"weather_humidity\"    \"weather_detail\"     \n\n\nCode\ngame_scores <- game_scores[game_scores$schedule_season == 2021 | game_scores$schedule_season == 2022, ]\nsummary(game_scores)\n\n\n schedule_date      schedule_season schedule_week      schedule_playoff\n Length:569         Min.   :2021    Length:569         Mode :logical   \n Class :character   1st Qu.:2021    Class :character   FALSE:543       \n Mode  :character   Median :2021    Mode  :character   TRUE :26        \n                    Mean   :2021                                       \n                    3rd Qu.:2022                                       \n                    Max.   :2022                                       \n                                                                       \n  team_home           score_home      score_away     team_away        \n Length:569         Min.   : 0.00   Min.   : 0.00   Length:569        \n Class :character   1st Qu.:17.00   1st Qu.:15.00   Class :character  \n Mode  :character   Median :23.00   Median :21.00   Mode  :character  \n                    Mean   :23.53   Mean   :21.52                     \n                    3rd Qu.:30.00   3rd Qu.:28.00                     \n                    Max.   :56.00   Max.   :51.00                     \n                                                                      \n team_favorite_id   spread_favorite   over_under_line   stadium         \n Length:569         Min.   :-20.000   Min.   :32.00   Length:569        \n Class :character   1st Qu.: -7.000   1st Qu.:42.00   Class :character  \n Mode  :character   Median : -4.000   Median :45.00   Mode  :character  \n                    Mean   : -5.452   Mean   :45.33                     \n                    3rd Qu.: -3.000   3rd Qu.:48.00                     \n                    Max.   : -1.000   Max.   :58.50                     \n                                                                        \n stadium_neutral weather_temperature weather_wind_mph weather_humidity\n Mode :logical   Min.   : 7.00       Min.   : 0.000   Min.   :  8.00  \n FALSE:558       1st Qu.:72.00       1st Qu.: 0.000   1st Qu.: 52.50  \n TRUE :11        Median :72.00       Median : 0.000   Median : 64.00  \n                 Mean   :64.57       Mean   : 2.411   Mean   : 66.11  \n                 3rd Qu.:72.00       3rd Qu.: 0.000   3rd Qu.: 81.00  \n                 Max.   :82.00       Max.   :33.000   Max.   :100.00  \n                 NA's   :334         NA's   :333      NA's   :506     \n weather_detail    \n Length:569        \n Class :character  \n Mode  :character  \n                   \n                   \n                   \n                   \n\n\nCode\nglimpse(game_scores)\n\n\nRows: 569\nColumns: 17\n$ schedule_date       <chr> \"9/9/2021\", \"9/12/2021\", \"9/12/2021\", \"9/12/2021\",…\n$ schedule_season     <int> 2021, 2021, 2021, 2021, 2021, 2021, 2021, 2021, 20…\n$ schedule_week       <chr> \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", …\n$ schedule_playoff    <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, F…\n$ team_home           <chr> \"Tampa Bay Buccaneers\", \"Atlanta Falcons\", \"Buffal…\n$ score_home          <int> 31, 6, 16, 19, 27, 33, 37, 16, 33, 34, 16, 38, 13,…\n$ score_away          <int> 29, 32, 23, 14, 24, 41, 21, 28, 29, 14, 17, 3, 27,…\n$ team_away           <chr> \"Dallas Cowboys\", \"Philadelphia Eagles\", \"Pittsbur…\n$ team_favorite_id    <chr> \"TB\", \"ATL\", \"BUF\", \"CAR\", \"MIN\", \"SF\", \"JAX\", \"SE…\n$ spread_favorite     <dbl> -7.5, -3.5, -6.5, -5.0, -3.0, -7.5, -3.0, -2.5, -6…\n$ over_under_line     <dbl> 51.5, 48.0, 48.5, 44.5, 48.0, 46.0, 44.5, 48.5, 53…\n$ stadium             <chr> \"Raymond James Stadium\", \"Mercedes-Benz Stadium\", …\n$ stadium_neutral     <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, F…\n$ weather_temperature <int> NA, 72, NA, NA, NA, 72, 72, 72, NA, 72, NA, NA, NA…\n$ weather_wind_mph    <int> NA, 0, NA, NA, NA, 0, 0, 0, NA, 0, NA, NA, NA, NA,…\n$ weather_humidity    <int> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ weather_detail      <chr> \"\", \"indoor\", \"\", \"\", \"\", \"indoor\", \"indoor\", \"ind…\n\n\nThe second dataset I will use is a combination of data from Google Trends. To collect this data I pulled weekly “News Search” scores for each of the 32 NFL teams over the 2020-21 and 2021-22 season. To populate these scores, Google determines the week with the highest volume of News Searches within the time frame for each team and assigns it a score of 100. From there, the other weeks of the time frame get assigned their score to be the percentage of news search volume in proportion to the week scored as 100. For example, if week 7 was scored at 100 and week 3 had 30% of the news search volume that week 7 did, week 3’s score would be 30.\n\n\nCode\n#import csv for each NFL team\ndf1 <- read.csv(\"~/Documents/GitHub/603_Spring_2023/posts/_data/multiTimeline.csv\")\ndf2 <- read.csv(\"~/Documents/GitHub/603_Spring_2023/posts/_data/multiTimeline (1).csv\")\ndf3 <- read.csv(\"~/Documents/GitHub/603_Spring_2023/posts/_data/multiTimeline (2).csv\")\ndf4 <- read.csv(\"~/Documents/GitHub/603_Spring_2023/posts/_data/multiTimeline (3).csv\")\ndf5 <- read.csv(\"~/Documents/GitHub/603_Spring_2023/posts/_data/multiTimeline (4).csv\")\ndf6 <- read.csv(\"~/Documents/GitHub/603_Spring_2023/posts/_data/multiTimeline (5).csv\")\ndf7 <- read.csv(\"~/Documents/GitHub/603_Spring_2023/posts/_data/multiTimeline (6).csv\")\ndf8 <- read.csv(\"~/Documents/GitHub/603_Spring_2023/posts/_data/multiTimeline (7).csv\")\ndf9 <- read.csv(\"~/Documents/GitHub/603_Spring_2023/posts/_data/multiTimeline (8).csv\")\ndf10 <- read.csv(\"~/Documents/GitHub/603_Spring_2023/posts/_data/multiTimeline (9).csv\")\ndf11 <- read.csv(\"~/Documents/GitHub/603_Spring_2023/posts/_data/multiTimeline (10).csv\")\ndf12 <- read.csv(\"~/Documents/GitHub/603_Spring_2023/posts/_data/multiTimeline (11).csv\")\ndf13 <- read.csv(\"~/Documents/GitHub/603_Spring_2023/posts/_data/multiTimeline (12).csv\")\ndf14 <- read.csv(\"~/Documents/GitHub/603_Spring_2023/posts/_data/multiTimeline (13).csv\")\ndf15 <- read.csv(\"~/Documents/GitHub/603_Spring_2023/posts/_data/multiTimeline (14).csv\")\ndf16 <- read.csv(\"~/Documents/GitHub/603_Spring_2023/posts/_data/multiTimeline (15).csv\")\ndf17 <- read.csv(\"~/Documents/GitHub/603_Spring_2023/posts/_data/multiTimeline (16).csv\")\ndf18 <- read.csv(\"~/Documents/GitHub/603_Spring_2023/posts/_data/multiTimeline (17).csv\")\ndf19 <- read.csv(\"~/Documents/GitHub/603_Spring_2023/posts/_data/multiTimeline (18).csv\")\ndf20 <- read.csv(\"~/Documents/GitHub/603_Spring_2023/posts/_data/multiTimeline (19).csv\")\ndf21 <- read.csv(\"~/Documents/GitHub/603_Spring_2023/posts/_data/multiTimeline (20).csv\")\ndf22 <- read.csv(\"~/Documents/GitHub/603_Spring_2023/posts/_data/multiTimeline (21).csv\")\ndf23 <- read.csv(\"~/Documents/GitHub/603_Spring_2023/posts/_data/multiTimeline (22).csv\")\ndf24 <- read.csv(\"~/Documents/GitHub/603_Spring_2023/posts/_data/multiTimeline (23).csv\")\ndf25 <- read.csv(\"~/Documents/GitHub/603_Spring_2023/posts/_data/multiTimeline (24).csv\")\ndf26 <- read.csv(\"~/Documents/GitHub/603_Spring_2023/posts/_data/multiTimeline (25).csv\")\ndf27 <- read.csv(\"~/Documents/GitHub/603_Spring_2023/posts/_data/multiTimeline (26).csv\")\ndf28 <- read.csv(\"~/Documents/GitHub/603_Spring_2023/posts/_data/multiTimeline (27).csv\")\ndf29 <- read.csv(\"~/Documents/GitHub/603_Spring_2023/posts/_data/multiTimeline (28).csv\")\ndf30 <- read.csv(\"~/Documents/GitHub/603_Spring_2023/posts/_data/multiTimeline (29).csv\")\ndf31 <- read.csv(\"~/Documents/GitHub/603_Spring_2023/posts/_data/multiTimeline (30).csv\")\ndf32 <- read.csv(\"~/Documents/GitHub/603_Spring_2023/posts/_data/multiTimeline (31).csv\")\n\n#bind columns to create one data frame that holds all the scores for each team\ntrend_scores <- bind_cols(df1,df2,df3,df4,df5,df6,df7,df8,df9,df10,df11,df12,df13,df14,df15,\n                          df16,df17,df18,df19,df20,df21,df22,df23,df24,df25,df26,df27,df28,df29,df30,df31,df32)\n\n\nNew names:\n* Category..All.categories -> Category..All.categories...1\n* Category..All.categories -> Category..All.categories...2\n* Category..All.categories -> Category..All.categories...3\n* Category..All.categories -> Category..All.categories...4\n* Category..All.categories -> Category..All.categories...5\n* ...\n\n\nCode\n#create new column that holds the dates for each week, which R stored as the row names\ntrend_scores <- rownames_to_column(trend_scores, \"Week\")\n#rename column names to be row one values so that each column is the team name\ntrend_scores <- trend_scores %>% \n  row_to_names(row_number = 1)\nsummary(trend_scores)\n\n\n     Week           Tennessee Titans: (United States)\n Length:83          Length:83                        \n Class :character   Class :character                 \n Mode  :character   Mode  :character                 \n Jacksonville Jaguars: (United States) Philadelphia Eagles: (United States)\n Length:83                             Length:83                           \n Class :character                      Class :character                    \n Mode  :character                      Mode  :character                    \n Kansas City Chiefs: (United States) Chicago Bears: (United States)\n Length:83                           Length:83                     \n Class :character                    Class :character              \n Mode  :character                    Mode  :character              \n Green Bay Packers: (United States) San Francisco 49ers: (United States)\n Length:83                          Length:83                           \n Class :character                   Class :character                    \n Mode  :character                   Mode  :character                    \n Minnesota Vikings: (United States) New York Giants: (United States)\n Length:83                          Length:83                       \n Class :character                   Class :character                \n Mode  :character                   Mode  :character                \n Las Vegas Raiders: (United States) Buffalo Bills: (United States)\n Length:83                          Length:83                     \n Class :character                   Class :character              \n Mode  :character                   Mode  :character              \n New England Patriots: (United States) Carolina Panthers: (United States)\n Length:83                             Length:83                         \n Class :character                      Class :character                  \n Mode  :character                      Mode  :character                  \n Miami Dolphins: (United States) Seattle Seahawks: (United States)\n Length:83                       Length:83                        \n Class :character                Class :character                 \n Mode  :character                Mode  :character                 \n Detroit Lions: (United States) Cincinnati Bengals: (United States)\n Length:83                      Length:83                          \n Class :character               Class :character                   \n Mode  :character               Mode  :character                   \n New Orleans Saints: (United States) Denver Broncos: (United States)\n Length:83                           Length:83                      \n Class :character                    Class :character               \n Mode  :character                    Mode  :character               \n Baltimore Ravens: (United States) New York Jets: (United States)\n Length:83                         Length:83                     \n Class :character                  Class :character              \n Mode  :character                  Mode  :character              \n Washington Commanders: (United States) Houston Texans: (United States)\n Length:83                              Length:83                      \n Class :character                       Class :character               \n Mode  :character                       Mode  :character               \n Tampa Bay Buccaneers: (United States) Indianapolis Colts: (United States)\n Length:83                             Length:83                          \n Class :character                      Class :character                   \n Mode  :character                      Mode  :character                   \n Cleveland Browns: (United States) Arizona Cardinals: (United States)\n Length:83                         Length:83                         \n Class :character                  Class :character                  \n Mode  :character                  Mode  :character                  \n Los Angeles Chargers: (United States) Los Angeles Rams: (United States)\n Length:83                             Length:83                        \n Class :character                      Class :character                 \n Mode  :character                      Mode  :character                 \n Atlanta Falcons: (United States) Dallas Cowboys: (United States)\n Length:83                        Length:83                      \n Class :character                 Class :character               \n Mode  :character                 Mode  :character               \n Pittsburgh Steelers: (United States)\n Length:83                           \n Class :character                    \n Mode  :character                    \n\n\nCode\nglimpse(trend_scores)\n\n\nRows: 83\nColumns: 33\n$ Week                                     <chr> \"2021-08-01\", \"2021-08-08\", \"…\n$ `Tennessee Titans: (United States)`      <chr> \"12\", \"29\", \"32\", \"41\", \"37\",…\n$ `Jacksonville Jaguars: (United States)`  <chr> \"10\", \"14\", \"17\", \"24\", \"16\",…\n$ `Philadelphia Eagles: (United States)`   <chr> \"17\", \"19\", \"15\", \"18\", \"16\",…\n$ `Kansas City Chiefs: (United States)`    <chr> \"6\", \"4\", \"14\", \"7\", \"10\", \"1…\n$ `Chicago Bears: (United States)`         <chr> \"15\", \"26\", \"30\", \"21\", \"46\",…\n$ `Green Bay Packers: (United States)`     <chr> \"21\", \"18\", \"22\", \"17\", \"33\",…\n$ `San Francisco 49ers: (United States)`   <chr> \"11\", \"9\", \"18\", \"26\", \"31\", …\n$ `Minnesota Vikings: (United States)`     <chr> \"30\", \"29\", \"38\", \"36\", \"60\",…\n$ `New York Giants: (United States)`       <chr> \"18\", \"42\", \"31\", \"30\", \"54\",…\n$ `Las Vegas Raiders: (United States)`     <chr> \"15\", \"22\", \"39\", \"15\", \"46\",…\n$ `Buffalo Bills: (United States)`         <chr> \"2\", \"1\", \"1\", \"2\", \"3\", \"3\",…\n$ `New England Patriots: (United States)`  <chr> \"30\", \"26\", \"38\", \"32\", \"92\",…\n$ `Carolina Panthers: (United States)`     <chr> \"9\", \"20\", \"57\", \"38\", \"48\", …\n$ `Miami Dolphins: (United States)`        <chr> \"13\", \"22\", \"27\", \"19\", \"45\",…\n$ `Seattle Seahawks: (United States)`      <chr> \"22\", \"19\", \"19\", \"22\", \"28\",…\n$ `Detroit Lions: (United States)`         <chr> \"33\", \"32\", \"42\", \"39\", \"51\",…\n$ `Cincinnati Bengals: (United States)`    <chr> \"0\", \"1\", \"3\", \"4\", \"0\", \"2\",…\n$ `New Orleans Saints: (United States)`    <chr> \"26\", \"24\", \"23\", \"59\", \"46\",…\n$ `Denver Broncos: (United States)`        <chr> \"27\", \"34\", \"23\", \"33\", \"40\",…\n$ `Baltimore Ravens: (United States)`      <chr> \"6\", \"25\", \"47\", \"18\", \"70\", …\n$ `New York Jets: (United States)`         <chr> \"18\", \"29\", \"57\", \"29\", \"52\",…\n$ `Washington Commanders: (United States)` <chr> \"12\", \"31\", \"20\", \"40\", \"42\",…\n$ `Houston Texans: (United States)`        <chr> \"41\", \"48\", \"90\", \"12\", \"70\",…\n$ `Tampa Bay Buccaneers: (United States)`  <chr> \"7\", \"8\", \"18\", \"8\", \"29\", \"5…\n$ `Indianapolis Colts: (United States)`    <chr> \"14\", \"45\", \"37\", \"16\", \"42\",…\n$ `Cleveland Browns: (United States)`      <chr> \"18\", \"14\", \"18\", \"20\", \"24\",…\n$ `Arizona Cardinals: (United States)`     <chr> \"19\", \"11\", \"0\", \"26\", \"21\", …\n$ `Los Angeles Chargers: (United States)`  <chr> \"3\", \"14\", \"4\", \"32\", \"8\", \"1…\n$ `Los Angeles Rams: (United States)`      <chr> \"4\", \"8\", \"11\", \"12\", \"10\", \"…\n$ `Atlanta Falcons: (United States)`       <chr> \"10\", \"47\", \"19\", \"33\", \"56\",…\n$ `Dallas Cowboys: (United States)`        <chr> \"26\", \"20\", \"24\", \"20\", \"42\",…\n$ `Pittsburgh Steelers: (United States)`   <chr> \"26\", \"23\", \"25\", \"31\", \"45\",…"
  },
  {
    "objectID": "posts/FelixBetancourt_Finalpart1 v2.html",
    "href": "posts/FelixBetancourt_Finalpart1 v2.html",
    "title": "Final Project - Check point 1",
    "section": "",
    "text": "Code\nknitr::opts_chunk$set(echo = TRUE, warning = FALSE)\n\n\n\nBurnout in the workplace.\nDACSS 603, Spring 2023\nBurnout is a pervasive issue in many professions, and its consequences can be significant for individuals and organizations alike. According to Maslach and Leiter (2016), burnout is characterized by emotional exhaustion, despersonalization, and reduced personal accomplishment. It is prevalent in a variety of fields, including healthcare (West et al., 2016), among other professions. Burnout can have serious consequences, including decreased job satisfaction, increased absenteeism, and turnover (West et al., 2016).\nThere is a growing body of research exploring the causes and consequences of burnout, as well as potential solutions. Some scholars have identified factors such as job demands, lack of control, and social support as contributing to burnout (Bakker & Demerouti, 2017).\nLee & Eissenstat (2017), for instance, affirm that psychological job demands and work-to-family conflict, as well as control over working hours/schedule, decision-making authority, and role clarity, have significant effects on burnout.\nThe aim of this research paper is to provide a comprehensive understanding of how type of business, and seniority level explain the level of burnout.\nNull hypothesis: The type of business, and seniority does not affect the burnout rate.\nAlternative hypotheses:\n\nWorking in Services (vs product type of business) predict significantly higher burnout rate, especially in Female workers.\nLess years of experience (lower seniority) is significantly related to higher burnout rate.\nThe numbers of work hours allocated affect the burnout rate significantly in WFH setup (vs on site).\n\n\n\nAbout the Data\n\n\nCode\n# Loading packages\n\nsuppressPackageStartupMessages(library(dplyr))\nsuppressPackageStartupMessages(library(tidyverse))\n\n# Reading the data.\n\nburn <- read.csv(\"_data/burnout.csv\")\n\n\nThe dataset was obtained from:\nhttps://www.kaggle.com/datasets/blurredmachine/are-your-employees-burning-out\nLet’s check the strucutre of the dataset:\n\n\nCode\n#Structure\nstr(burn)\n\n\n'data.frame':   22750 obs. of  9 variables:\n $ Employee.ID         : chr  \"fffe32003000360033003200\" \"fffe3700360033003500\" \"fffe31003300320037003900\" \"fffe32003400380032003900\" ...\n $ Date.of.Joining     : chr  \"2008-09-30\" \"2008-11-30\" \"2008-03-10\" \"2008-11-03\" ...\n $ Gender              : chr  \"Female\" \"Male\" \"Female\" \"Male\" ...\n $ Company.Type        : chr  \"Service\" \"Service\" \"Product\" \"Service\" ...\n $ WFH.Setup.Available : chr  \"No\" \"Yes\" \"Yes\" \"Yes\" ...\n $ Designation         : num  2 1 2 1 3 2 3 2 3 3 ...\n $ Resource.Allocation : num  3 2 NA 1 7 4 6 4 6 6 ...\n $ Mental.Fatigue.Score: num  3.8 5 5.8 2.6 6.9 3.6 7.9 4.4 NA NA ...\n $ Burn.Rate           : num  0.16 0.36 0.49 0.2 0.52 0.29 0.62 0.33 0.56 0.67 ...\n\n\nThe dataset contain 9 variables and 22750 observations.\nFour of the variables are categorical and five are numeric (including one as date)\nLet’s see a Summary for each variable.\n\n\nCode\nsummary(burn)\n\n\n Employee.ID        Date.of.Joining       Gender          Company.Type      \n Length:22750       Length:22750       Length:22750       Length:22750      \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n                                                                            \n WFH.Setup.Available  Designation    Resource.Allocation Mental.Fatigue.Score\n Length:22750        Min.   :0.000   Min.   : 1.000      Min.   : 0.000      \n Class :character    1st Qu.:1.000   1st Qu.: 3.000      1st Qu.: 4.600      \n Mode  :character    Median :2.000   Median : 4.000      Median : 5.900      \n                     Mean   :2.179   Mean   : 4.481      Mean   : 5.728      \n                     3rd Qu.:3.000   3rd Qu.: 6.000      3rd Qu.: 7.100      \n                     Max.   :5.000   Max.   :10.000      Max.   :10.000      \n                                     NA's   :1381        NA's   :2117        \n   Burn.Rate    \n Min.   :0.000  \n 1st Qu.:0.310  \n Median :0.450  \n Mean   :0.452  \n 3rd Qu.:0.590  \n Max.   :1.000  \n NA's   :1124   \n\n\nAccording to the source of the data, here is an explanation of each variable:\nEmployee ID: The unique ID allocated for each employee.\nDate of Joining: The date-time when the employee has joined the organization.\nGender: The gender of the employee\nCompany Type: The type of company where the employee is working\nWFH Setup Available: Is the work from home facility available for the employee\nDesignation: The designation of the employee of work in the organization. In the range of [0.0, 5.0] bigger is higher designation.\nResource Allocation: The amount of resource allocated to the employee to work, ie. number of working hours.In the range of [1.0, 10.0] (higher means more resource)\nMental Fatigue Score: The level of fatigue mentally the employee is facing.In the range of [0.0, 10.0] where 0.0 means no fatigue and 10.0 means completely fatigue.\nBurn Rate: The value we need to predict for each employee telling the rate of Bur out while working.In the range of [0.0, 1.0] where the higher the value is more is the burn out.\n\n\nReferences:\nBakker, A. B., & Demerouti, E. (2017). Job demands-resources theory: Taking stock and looking forward. Journal of Occupational Health Psychology, 22(3), 273–285.\nLee, Y., Eissenstat, S. (2017). A longitudinal examination of the causes and effects of burnout based on the job demands-resources model. International Journal for Educational and Vocational Guidance, 18(3), 337–354.\nMaslach, C., & Leiter, M. P. (2016). Understanding the burnout experience: Recent research and its implications for psychiatry. World Psychiatry, 15(2), 103–111.\nWest, C. P., Dyrbye, L. N., Erwin, P., Shanafelt, T. D., (2016). Interventions to promote physician well-being and mitigate burnout: A systematic review and meta-analysis. The Lancet, 388(10057), 2272–228"
  },
  {
    "objectID": "posts/Hw2_thrishul.html",
    "href": "posts/Hw2_thrishul.html",
    "title": "Homework - 2",
    "section": "",
    "text": "The time between the date a patient was recommended for heart surgery and the surgery date for cardiac patients in Ontario was collected by the Cardiac Care Network (“Wait Times Data Guide,” Ministry of Health and Long-Term Care, Ontario, Canada, 2006). The sample mean and sample standard deviation for wait times (in days) of patients for two cardiac procedures are given in the accompanying table. Assume that the sample is representative of the Ontario population\n Surgical Procedure Sample Size Mean Wait Time Standard Deviation\nBypass 539 19 10 Angiography 847 18 9\n\n\n\nCode\n# To construct a confidence interval for the mean wait time for each procedure, we will use the following formula:\n\n# CI = x̄ ± zα/2 * (σ/√n)\n# For bypass surgery:\n# x̄ = 19\n# σ = 10\n# n = 539\n# zα/2 = 1.645\n\n# CI = 19 ± 1.645 * (10/√539) = (17.8, 20.2)\n\n# For angiography:\n# x̄ = 18\n# σ = 9\n# n = 847\n# zα/2 = 1.645\n\n# CI = 18 ± 1.645 * (9/√847) = (17.2, 18.8)\n\n# The confidence interval for bypass surgery is (17.8, 20.2) and the confidence interval for angiography is (17.2, 18.8). We can see that the confidence interval for angiography is narrower, which means that we are more certain about the mean wait time for angiography than for bypass surgery."
  },
  {
    "objectID": "posts/Hw2_thrishul.html#question-2",
    "href": "posts/Hw2_thrishul.html#question-2",
    "title": "Homework - 2",
    "section": "Question 2",
    "text": "Question 2\nA survey of 1031 adult Americans was carried out by the National Center for Public Policy. Assume that the sample is representative of adult Americans. Among those surveyed, 567 believed that college education is essential for success. Find the point estimate, p, of the proportion of all adult Americans who believe that a college education is essential for success. Construct and interpret a 95% confidence interval for p.\n\n\nCode\n# The point estimate, p, of the proportion of all adult Americans who believe that a college education is essential for success is:\n\n# p = 567/1031 = 0.5498\n\n# To construct a 95% confidence interval for p, we will use the following formula:\n\n# CI = p ± zα/2 * √(p(1-p)/n)\n\n# 95% corresponds to a z-score of 1.96\n\n# CI = 0.5498 ± 1.96 * √(0.5498(1-0.5498)/1031) = (0.517, 0.582)\n# The 95% confidence interval for the proportion of all adult Americans who believe that a college education is essential for success is (0.517, 0.582). This means that we can be 95% confident that the true proportion of all adult Americans who believe that a college education is essential for success falls within this interval. \n\n# We can interpret this as saying that, based on the sample data, we estimate that between 51.7% and 58.2% of all adult Americans believe that a college education is essential for success."
  },
  {
    "objectID": "posts/Hw2_thrishul.html#question-3",
    "href": "posts/Hw2_thrishul.html#question-3",
    "title": "Homework - 2",
    "section": "Question 3",
    "text": "Question 3\nSuppose that the financial aid office of UMass Amherst seeks to estimate the mean cost of textbooks per semester for students. The estimate will be useful if it is within $5 of the true population mean (i.e. they want the confidence interval to have a length of $10 or less). The financial aid office is pretty sure that the amount spent on books varies widely, with most values between $30 and $200. They think that the population standard deviation is about a quarter of this range (in other words, you can assume they know the population standard deviation). Assuming the significance level to be 5%, what should be the size of the sample?\n\n\nCode\n# margin of error formula\n## ME = z * (sigma / sqrt(n))\n### solving for n\n#### n = (z * sigma/ME)^2\n##### replacing values\n###### z = significance level 5% = critical z value for 95% conf int = 1.96\n###### sigma = 1/4 of range of textbook costs = (200-30)/4 = 42.5\n###### ME = estimate within 5 of the true population mean = 5\n\n# calculate result\nn <- round((1.96*42.5/5)^2)\n\n# print result\ncat(\"Ideal sample size:\", n, \"\\n\")\n\n\nIdeal sample size: 278 \n\n\nUsing the margin of error formula for confidence intervals and solving for n (sample size), we see that the ideal sample size (rounded to the nearest integer) is 278."
  },
  {
    "objectID": "posts/Hw2_thrishul.html#question-4",
    "href": "posts/Hw2_thrishul.html#question-4",
    "title": "Homework - 2",
    "section": "Question 4",
    "text": "Question 4\nAccording to a union agreement, the mean income for all senior-level workers in a large service company equals $500 per week. A representative of a women’s group decides to analyze whether the mean income μ for female employees matches this norm. For a random sample of nine female employees, ȳ = $410 and s = 90.\nTest whether the mean income of female employees differs from $500 per week. Include assumptions, hypotheses, test statistic, and P-value. Interpret the result.\nAssumptions:\nThe data is normally distributed.\nThe sample is a simple random sample.\nThe standard deviation of the population is unknown.\nHypotheses:\nH0: μ = 500\nH1: Ha: μ ≠ 500\nTest statistic:\nt = (ȳ - μ) / (s / sqrt(n)) = (410 - 500) / (90 / sqrt(9)) = -3 P-value:\nThe P-value is 0.01707168, which is less than the level of significance of 0.05. Therefore, we reject the null hypothesis and conclude that there is sufficient evidence to suggest that the mean income of female employees differs from $500 per week. Report the P-value for Ha: μ < 500. Interpret.\nThe P-value is 0.008535841, which is less than the level of significance of 0.05. Therefore, we reject the null hypothesis and conclude that there is sufficient evidence to suggest that the mean income of female employees is less than $500 per week. Report and interpret the P-value for Ha: μ > 500.\nThe P-value is 0.9914642, which is more than the level of significance of 0.05. Therefore, we fail to reject the null hypothesis and conclude that there is sufficient evidence to suggest that the mean income of female employees is greater than $500 per week.\n\n\nCode\n# A.3.a. Test statistic\n(410 - 500) / (90 / sqrt(9))\n\n\n[1] -3\n\n\nCode\n# A.4.a. P-value two-sided\npt(-3, 8) * 2\n\n\n[1] 0.01707168\n\n\nCode\n# B.1. P-value μ < 500\npt(-3, 8)\n\n\n[1] 0.008535841\n\n\nCode\n# C.1. P-value μ > 500\npt(-3, 8, lower.tail=FALSE)\n\n\n[1] 0.9914642"
  },
  {
    "objectID": "posts/Hw2_thrishul.html#question-5",
    "href": "posts/Hw2_thrishul.html#question-5",
    "title": "Homework - 2",
    "section": "Question 5",
    "text": "Question 5\nJones and Smith separately conduct studies to test H0: μ = 500 against Ha: μ ≠ 500, each with n = 1000. Jones gets ȳ = 519.5, with se = 10.0. Smith gets ȳ = 519.7, with se = 10.0.\nShow that t = 1.95 and P-value = 0.051 for Jones. Show that t = 1.97 and P-value = 0.049 for Smith.\nJones:\nt = (ȳ - μ) / se = (519.5 - 500) / 10 = 19.5 / 10 = 1.95\nP-value = round(2 * pt(q=1.95, df=999, lower.tail=FALSE), 3) = 0.051\nSmith:\nt = (ȳ - μ) / se = (519.7 - 500) / 10 = 19.7 / 10 = 1.97\nP-value = round(2 * pt(q=1.97, df=999, lower.tail=FALSE), 3) = 0.049\nUsing α = 0.05, for each study indicate whether the result is “statistically significant.”\nSince Jones’ P-value is greater than 0.05, we fail to reject the null hypothesis, indicating that his results are not statistically significant. In contrast, Smith’s P-value is less than 0.05, therefore we reject the null hypothesis and find his results statistically significant. Using this example, explain the misleading aspects of reporting the result of a test as “P ≤ 0.05” versus “P > 0.05,” or as “reject H0” versus “Do not reject H0,” without reporting the actual P-value.\n“P ≤ 0.05” or “reject H0” without reporting the actual P-value can be misleading because it doesn’t provide information on how strong the evidence is against the null hypothesis. For example, both Jones and Smith barely pass the 0.05 threshold, having 0.049 and 0.051, respectively. Reporting this would help readers and analysts to take the strength of the evidence in consideration (in this case, “rejecting H0” or “failing to reject H0” should be taken with a grain of salt).\n\n\nCode\n# A.1.b. Proving Jones' p-value\nround(2 * pt(q=1.95, df=999, lower.tail=FALSE), 3)\n\n\n[1] 0.051\n\n\nCode\n# A.2.b. Proving Smith's p-value\nround(2 * pt(q=1.97, df=999, lower.tail=FALSE), 3)\n\n\n[1] 0.049"
  },
  {
    "objectID": "posts/Hw2_thrishul.html#question-6",
    "href": "posts/Hw2_thrishul.html#question-6",
    "title": "Homework - 2",
    "section": "Question 6",
    "text": "Question 6\nA school nurse wants to determine whether age is a factor in whether children choose a healthy snack after school. She conducts a survey of 300 middle school students, with the results below. Test at α = 0.05 the claim that the proportion who choose a healthy snack differs by grade level. What is the null hypothesis? Which test should we use? What is the conclusion?\nGrade level 6th grade 7th grade 8th grade Healthy snack 31 43 51 Unhealthy snack 69 57 49 Null hypothesis: There is no difference in the proportion of students who choose a healthy snack based on grade level.\nTest: Chi-squared test because we are assessing whether proportions of outcomes (choosing healthy versus unhealthy snacks) in each grade are equal or different.\nConclusion: Since the p-value is 0.01547, we reject the null hypothesis at the 0.05 level of significance and conclude that there is a significant difference in the proportion of healthy snack choices among the different grade levels.\n\n\nCode\n# create a matrix of the observed values\nobserved <- matrix(c(31, 43, 51, 69, 57, 49), nrow = 2, byrow = TRUE)\n\n# perform the chi-squared test\nresult <- chisq.test(observed)\n\n# print the results\nprint(observed)\n\n\n     [,1] [,2] [,3]\n[1,]   31   43   51\n[2,]   69   57   49\n\n\nCode\nprint(result)\n\n\n\n    Pearson's Chi-squared test\n\ndata:  observed\nX-squared = 8.3383, df = 2, p-value = 0.01547"
  },
  {
    "objectID": "posts/Hw2_thrishul.html#question-7",
    "href": "posts/Hw2_thrishul.html#question-7",
    "title": "Homework - 2",
    "section": "Question 7",
    "text": "Question 7\nPer-pupil costs (in thousands of dollars) for cyber charter school tuition for school districts in three areas are shown. Test the claim that there is a difference in means for the three areas, using an appropriate test. What is the null hypothesis? Which test should we use? What is the conclusion?\nArea 1 6.2 9.3 6.8 6.1 6.7 7.5 Area 2 7.5 8.2 8.5 8.2 7.0 9.3 Area 3 5.8 6.4 5.6 7.1 3.0 3.5 Null hypothesis: There is no difference in means for the three areas.\nTest: Analysis of Variance (ANOVA) because we are computing the difference between the means of three or more groups.\nConclusion: Given that the P-value associated to the F-statistic is 0.00397, we reject the null hypothesis and conclude that there is a significant difference in means for the three areas.\n\n\nCode\narea1 <- c(6.2, 9.3, 6.8, 6.1, 6.7, 7.5)\narea2 <- c(7.5, 8.2, 8.5, 8.2, 7.0, 9.3)\narea3 <- c(5.8, 6.4, 5.6, 7.1, 3.0, 3.5)\n\nanova_result <- aov(c(area1, area2, area3) ~ rep(c(\"Area 1\", \"Area 2\", \"Area 3\")\n                                                 , c(6, 6, 6)))\nprint(summary(anova_result))\n\n\n                                                 Df Sum Sq Mean Sq F value\nrep(c(\"Area 1\", \"Area 2\", \"Area 3\"), c(6, 6, 6))  2  25.66  12.832   8.176\nResiduals                                        15  23.54   1.569        \n                                                  Pr(>F)   \nrep(c(\"Area 1\", \"Area 2\", \"Area 3\"), c(6, 6, 6)) 0.00397 **\nResiduals                                                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "posts/Tyler_Tewksbury_Final1.html",
    "href": "posts/Tyler_Tewksbury_Final1.html",
    "title": "Final Project Part 1",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/Tyler_Tewksbury_Final1.html#background-and-research-question",
    "href": "posts/Tyler_Tewksbury_Final1.html#background-and-research-question",
    "title": "Final Project Part 1",
    "section": "Background and Research Question",
    "text": "Background and Research Question\nIn 2020, after the release of the Netflix series The Queen’s Gambit, interest in chess was at an all-time high. This led many fans of the show to use popular websites such as Chess.com and Lichess.org to begin learning the game. These websites use a rating system that mimics that of official in-person chess leagues, increasing your rating number as you win and decreasing as you lose. This can be used to measure one’s skill in chess, and determining if they can enter certain competitions.\nWhen playing chess online, as you face someone completely random that the website matches you against, there is no guarantee that you will play against someone with an identical rating. Thus, there will typically be a difference between the two players’ rating. Obviously the player with the higher rating would be more likely to win, right? That is where this study comes in. By quantifying the effect of rating difference on win chance, players may be able to understand more about the match they are currently in. Knowing how likely they are to win, how likely their opponent is to win, and this could lead to further interesting research about making the most fair chess matches possible. As there are not any academic studies on the topic, there is no proven indicator that a slight discrepancy in rating has is an indicator to a player’s win chance. This poses the research question:\nHow strong of a predictor is the difference between players chess rating in determining the victor?"
  },
  {
    "objectID": "posts/Tyler_Tewksbury_Final1.html#dataset",
    "href": "posts/Tyler_Tewksbury_Final1.html#dataset",
    "title": "Final Project Part 1",
    "section": "Dataset",
    "text": "Dataset\nThe dataset being used is sourced from Kaggle: https://www.kaggle.com/datasets/datasnaek/chess\nGathered in 2016, the dataset contains information from over 20,000 matches on Lichess.org via the Lichess API. Information on the players’, their opening moves, the results of the match, and more are all columns within the dataset.\n\n\nCode\n#reading in the dataset\nchess <- read.csv(\"_data/chess_games.csv\")"
  },
  {
    "objectID": "posts/Tyler_Tewksbury_Final1.html#descriptive-statistics",
    "href": "posts/Tyler_Tewksbury_Final1.html#descriptive-statistics",
    "title": "Final Project Part 1",
    "section": "Descriptive Statistics",
    "text": "Descriptive Statistics\n\n\nCode\nstr(chess)\n\n\n'data.frame':   20058 obs. of  16 variables:\n $ id            : chr  \"TZJHLljE\" \"l1NXvwaE\" \"mIICvQHh\" \"kWKvrqYL\" ...\n $ rated         : chr  \"FALSE\" \"TRUE\" \"TRUE\" \"TRUE\" ...\n $ created_at    : num  1.5e+12 1.5e+12 1.5e+12 1.5e+12 1.5e+12 ...\n $ last_move_at  : num  1.5e+12 1.5e+12 1.5e+12 1.5e+12 1.5e+12 ...\n $ turns         : int  13 16 61 61 95 5 33 9 66 119 ...\n $ victory_status: chr  \"outoftime\" \"resign\" \"mate\" \"mate\" ...\n $ winner        : chr  \"white\" \"black\" \"white\" \"white\" ...\n $ increment_code: chr  \"15+2\" \"5+10\" \"5+10\" \"20+0\" ...\n $ white_id      : chr  \"bourgris\" \"a-00\" \"ischia\" \"daniamurashov\" ...\n $ white_rating  : int  1500 1322 1496 1439 1523 1250 1520 1413 1439 1381 ...\n $ black_id      : chr  \"a-00\" \"skinnerua\" \"a-00\" \"adivanov2009\" ...\n $ black_rating  : int  1191 1261 1500 1454 1469 1002 1423 2108 1392 1209 ...\n $ moves         : chr  \"d4 d5 c4 c6 cxd5 e6 dxe6 fxe6 Nf3 Bb4+ Nc3 Ba5 Bf4\" \"d4 Nc6 e4 e5 f4 f6 dxe5 fxe5 fxe5 Nxe5 Qd4 Nc6 Qe5+ Nxe5 c4 Bb4+\" \"e4 e5 d3 d6 Be3 c6 Be2 b5 Nd2 a5 a4 c5 axb5 Nc6 bxc6 Ra6 Nc4 a4 c3 a3 Nxa3 Rxa3 Rxa3 c4 dxc4 d5 cxd5 Qxd5 exd5 \"| __truncated__ \"d4 d5 Nf3 Bf5 Nc3 Nf6 Bf4 Ng4 e3 Nc6 Be2 Qd7 O-O O-O-O Nb5 Nb4 Rc1 Nxa2 Ra1 Nb4 Nxa7+ Kb8 Nb5 Bxc2 Bxc7+ Kc8 Qd\"| __truncated__ ...\n $ opening_eco   : chr  \"D10\" \"B00\" \"C20\" \"D02\" ...\n $ opening_name  : chr  \"Slav Defense: Exchange Variation\" \"Nimzowitsch Defense: Kennedy Variation\" \"King's Pawn Game: Leonardis Variation\" \"Queen's Pawn Game: Zukertort Variation\" ...\n $ opening_ply   : int  5 4 3 3 5 4 10 5 6 4 ...\n\n\nThe dataset contains 20058 observations across 16 variables.\n\n\nCode\nsummary(chess)\n\n\n      id               rated             created_at         last_move_at      \n Length:20058       Length:20058       Min.   :1.377e+12   Min.   :1.377e+12  \n Class :character   Class :character   1st Qu.:1.478e+12   1st Qu.:1.478e+12  \n Mode  :character   Mode  :character   Median :1.496e+12   Median :1.496e+12  \n                                       Mean   :1.484e+12   Mean   :1.484e+12  \n                                       3rd Qu.:1.503e+12   3rd Qu.:1.503e+12  \n                                       Max.   :1.504e+12   Max.   :1.504e+12  \n     turns        victory_status        winner          increment_code    \n Min.   :  1.00   Length:20058       Length:20058       Length:20058      \n 1st Qu.: 37.00   Class :character   Class :character   Class :character  \n Median : 55.00   Mode  :character   Mode  :character   Mode  :character  \n Mean   : 60.47                                                           \n 3rd Qu.: 79.00                                                           \n Max.   :349.00                                                           \n   white_id          white_rating    black_id          black_rating \n Length:20058       Min.   : 784   Length:20058       Min.   : 789  \n Class :character   1st Qu.:1398   Class :character   1st Qu.:1391  \n Mode  :character   Median :1567   Mode  :character   Median :1562  \n                    Mean   :1597                      Mean   :1589  \n                    3rd Qu.:1793                      3rd Qu.:1784  \n                    Max.   :2700                      Max.   :2723  \n    moves           opening_eco        opening_name        opening_ply    \n Length:20058       Length:20058       Length:20058       Min.   : 1.000  \n Class :character   Class :character   Class :character   1st Qu.: 3.000  \n Mode  :character   Mode  :character   Mode  :character   Median : 4.000  \n                                                          Mean   : 4.817  \n                                                          3rd Qu.: 6.000  \n                                                          Max.   :28.000  \n\n\nLooking at the summary, it is clear what variables will be used and if any new columns will be added. The following will prove relevance to the research question:\n\n`rated``\n`victory_status``\nwinner\nwhite_id\nwhite_rating\nblack_id\nblack_rating\n\nA new column containing the difference between the rating will be added in the next iteration for analysis. Having this added column will make the functions necessary for analysis easier, as calculating the difference will not need to be repeated for each observation.\n\nwhite_rating and black_rating\n\n\nCode\nrange(chess$white_rating)\n\n\n[1]  784 2700\n\n\nCode\nrange(chess$black_rating)\n\n\n[1]  789 2723\n\n\nThe ranges of the two sides are nearly identical, and are quite large nearing 2000. This could be both good and bad for the study, as the large range could prove significant, but it may be necessary to break the models into smaller ranges. This could also be interesting, perhaps seeing if the rating differences at a lower level matter more than that of a higher level, or vice versa."
  },
  {
    "objectID": "posts/Tyler_Tewksbury_Final1.html#proposed-models",
    "href": "posts/Tyler_Tewksbury_Final1.html#proposed-models",
    "title": "Final Project Part 1",
    "section": "Proposed Models",
    "text": "Proposed Models\nThe obvious model for this question will be a a linear probability regression, as the victory status is a binary variable. Proposed models initially are:\nLinear probability including unranked Linear probability excluding unranked\nThere will be more models, potentially differentiating between the different ranges as stated earlier. More possibilities include looking at exclusively drawn game data, analyzing favored openings depending on rank, or possibly finding other predictors if the rank difference is not significant."
  },
  {
    "objectID": "posts/HW1_RahulSomu.html",
    "href": "posts/HW1_RahulSomu.html",
    "title": "Homework1",
    "section": "",
    "text": "First, let’s read in the data from the Excel file:\n\n\nCode\nlibrary(readxl)\nlibrary(dplyr)\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nCode\nlibrary(ggplot2)\ngetwd()\n\n\n[1] \"/Users/rahulsomu/Documents/DACSS_601/603_repo/posts\"\n\n\nCode\ndf <- read_excel(\"_data/LungCapData.xls\")\n\n\nThe distribution of LungCap looks as follows:\n1a) The histogram suggests that the distribution is close to a normal distribution. Most of the observations are close to the mean. Very few observations are close to the margins (0 and 15).\n\n\nCode\nhist(df$LungCap)\n\n\n\n\n\n1b) Median lung capacity of male is greater than that of female.\n\n\nCode\nboxplot(LungCap ~ Gender, data = df, xlab = \"Gender\", ylab = \"Lung Capacity\",\n        main = \"Distribution of Lung Capacity by Gender\")\n\n\n\n\n\n1c) Logically mean lung capacity of non-smokers should be more than of smokers but with the data, it’s other way round\n\n\nCode\n# Calculate the mean lung capacity for smokers and non-smokers\nmean_lungcap_smokers <- mean(df$LungCap[df$Smoke == \"yes\"])\nmean_lungcap_non_smokers <- mean(df$LungCap[df$Smoke == \"no\"])\n\n# Print the mean lung capacities\ncat(\"Mean lung capacity for smokers:\", round(mean_lungcap_smokers, 2), \"\\n\")\n\n\nMean lung capacity for smokers: 8.65 \n\n\nCode\ncat(\"Mean lung capacity for non-smokers:\", round(mean_lungcap_non_smokers, 2), \"\\n\")\n\n\nMean lung capacity for non-smokers: 7.77 \n\n\n1d) The average lung capacity growth for the non-smokers is more that of the smokers. The lung capacity has been gradually increasing with age 1e) The discrepancy in 1c is due the data for the 13 or younger age where the average lung capacity for the non-smokers is less that of the smokers. Also there have been more data points for 13 or younger age group non-smokers which is affecting the mean of entire distribution.\n\n\nCode\n# Define the age groups\ndf <- df %>%\n  mutate(age_groups = cut(Age, c(0, 13, 15, 17, Inf), labels = c(\"<= 13\", \"14-15\", \"16-17\", \">= 18\")))\n\n# Compare the probability distribution of lung capacity by gender\ndf %>%\n  ggplot(aes(x = Gender, y = LungCap)) +\n  geom_boxplot() +\n  labs(x = \"Gender\", y = \"Lung Capacity\", \n       title = \"Lung Capacity by Gender\")\n\n\n\n\n\nCode\n# Compare the mean lung capacities for smokers and non-smokers\ndf %>%\n  group_by(Smoke) %>%\n  summarize(mean_lungcap = mean(LungCap)) %>%\n  print()\n\n\n# A tibble: 2 × 2\n  Smoke mean_lungcap\n  <chr>        <dbl>\n1 no            7.77\n2 yes           8.65\n\n\nCode\n# Examine the relationship between smoking and lung capacity within age groups\ndf %>%\n  filter(Smoke %in% c(\"yes\", \"no\")) %>%\n  group_by(age_groups, Smoke) %>%\n  summarize(mean_lungcap = mean(LungCap)) %>%\n  print()\n\n\n`summarise()` has grouped output by 'age_groups'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 8 × 3\n# Groups:   age_groups [4]\n  age_groups Smoke mean_lungcap\n  <fct>      <chr>        <dbl>\n1 <= 13      no            6.36\n2 <= 13      yes           7.20\n3 14-15      no            9.14\n4 14-15      yes           8.39\n5 16-17      no           10.5 \n6 16-17      yes           9.38\n7 >= 18      no           11.1 \n8 >= 18      yes          10.5 \n\n\nCode\n# Compare the lung capacities for smokers and non-smokers within each age group\ndf %>%\n  filter(Smoke %in% c(\"yes\", \"no\")) %>%\n  ggplot(aes(x = age_groups, y = LungCap, fill = Smoke)) +\n  geom_boxplot() +\n  labs(x = \"Age Group\", y = \"Lung Capacity\", \n       title = \"Lung Capacity by Smoking Status and Age Group\")\n\n\n\n\n\n#Challange2"
  },
  {
    "objectID": "posts/Jerin_Jacob_HW1.html",
    "href": "posts/Jerin_Jacob_HW1.html",
    "title": "Homework 1",
    "section": "",
    "text": "Code\nlibrary(readxl)\nlibrary(dplyr)\nlibrary(magrittr)\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/Jerin_Jacob_HW1.html#loading-the-data",
    "href": "posts/Jerin_Jacob_HW1.html#loading-the-data",
    "title": "Homework 1",
    "section": "Loading the Data",
    "text": "Loading the Data\n\n\nCode\ndf <- read_excel(\"_data/LungCapData.xls\")\ndf\n\n\n# A tibble: 725 × 6\n   LungCap   Age Height Smoke Gender Caesarean\n     <dbl> <dbl>  <dbl> <chr> <chr>  <chr>    \n 1    6.48     6   62.1 no    male   no       \n 2   10.1     18   74.7 yes   female no       \n 3    9.55    16   69.7 no    female yes      \n 4   11.1     14   71   no    male   no       \n 5    4.8      5   56.9 no    male   no       \n 6    6.22    11   58.7 no    female no       \n 7    4.95     8   63.3 no    male   yes      \n 8    7.32    11   70.4 no    male   no       \n 9    8.88    15   70.5 no    male   no       \n10    6.8     11   59.2 no    male   no       \n# … with 715 more rows"
  },
  {
    "objectID": "posts/Jerin_Jacob_HW1.html#a",
    "href": "posts/Jerin_Jacob_HW1.html#a",
    "title": "Homework 1",
    "section": "A",
    "text": "A\n\n\nCode\nsummary(df)\n\n\n    LungCap            Age            Height         Smoke          \n Min.   : 0.507   Min.   : 3.00   Min.   :45.30   Length:725        \n 1st Qu.: 6.150   1st Qu.: 9.00   1st Qu.:59.90   Class :character  \n Median : 8.000   Median :13.00   Median :65.40   Mode  :character  \n Mean   : 7.863   Mean   :12.33   Mean   :64.84                     \n 3rd Qu.: 9.800   3rd Qu.:15.00   3rd Qu.:70.30                     \n Max.   :14.675   Max.   :19.00   Max.   :81.80                     \n    Gender           Caesarean        \n Length:725         Length:725        \n Class :character   Class :character  \n Mode  :character   Mode  :character  \n                                      \n                                      \n                                      \n\n\nCode\nhist(df$LungCap, xlab = \"Lung Capacity\", main = \"\", freq = F)\n\n\n\n\n\nThe histogram shows that the distribution is almost a normal distribution with most of the values close to the mean. ## B\n\n\nCode\nboxplot(LungCap ~ Gender, df)\n\n\n\n\n\nThe minimum, first quartile, median, third quartile and maximum, all of them appear to be slightly higher for males than females."
  },
  {
    "objectID": "posts/Jerin_Jacob_HW1.html#c",
    "href": "posts/Jerin_Jacob_HW1.html#c",
    "title": "Homework 1",
    "section": "C",
    "text": "C\n\n\nCode\ndff<- df |>\n  group_by(Smoke) |>\n  summarise(LungCap = mean(LungCap))\n\n\nLung capacity of non-smokers is higher than that of smokers which is against the expectation!"
  },
  {
    "objectID": "posts/Jerin_Jacob_HW1.html#d",
    "href": "posts/Jerin_Jacob_HW1.html#d",
    "title": "Homework 1",
    "section": "D",
    "text": "D\n\nAge less than or equal to 13\n\n\nCode\ndf1 <- df |>\n  filter(Age <= 13) |>\n  group_by(Smoke)|>\n  summarise(LungCap = mean(LungCap)) |>\n  mutate(Age = \"<=13\")\ndf1\n\n\n# A tibble: 2 × 3\n  Smoke LungCap Age  \n  <chr>   <dbl> <chr>\n1 no       6.36 <=13 \n2 yes      7.20 <=13 \n\n\n\n\nAge 14 & 15\n\n\nCode\ndf2 <- df |>\n  filter(Age >= 14 & Age <= 15) |>\n  group_by(Smoke)|>\n  summarise(LungCap = mean(LungCap))|>\n  mutate(Age = \"14&15\")\nclass(df2)\n\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n\n\n\nAge 16 to 17\n\n\nCode\ndf3 <- df |>\n  filter(Age >= 16 & Age <= 17) |>\n  group_by(Smoke)|>\n  summarise(LungCap = mean(LungCap))|>\n  mutate(Age = \"16&17\")\n\n\n\n\nAge greater than or equal to 18\n\n\nCode\ndf4 <- df |>\n  filter(Age >= 18) |>\n  group_by(Smoke)|>\n  summarise(LungCap = mean(LungCap))|>\n  mutate(Age = \">=18\")"
  },
  {
    "objectID": "posts/Jerin_Jacob_HW1.html#e",
    "href": "posts/Jerin_Jacob_HW1.html#e",
    "title": "Homework 1",
    "section": "E",
    "text": "E\n\n\nCode\nnew_df <- rbind(df1,df2, df3,df4)\nnew_df\n\n\n# A tibble: 8 × 3\n  Smoke LungCap Age  \n  <chr>   <dbl> <chr>\n1 no       6.36 <=13 \n2 yes      7.20 <=13 \n3 no       9.14 14&15\n4 yes      8.39 14&15\n5 no      10.5  16&17\n6 yes      9.38 16&17\n7 no      11.1  >=18 \n8 yes     10.5  >=18 \n\n\nCode\nggplot(new_df, aes(fill=Smoke, y=LungCap, x=Age)) +\n  geom_bar(position='dodge', stat='identity')\n\n\nError in ggplot(new_df, aes(fill = Smoke, y = LungCap, x = Age)): could not find function \"ggplot\"\n\n\nCode\nggplot(dff, aes(y=LungCap, x=Smoke, fill = Smoke)) +\n  geom_bar(position='dodge', stat='identity')  \n\n\nError in ggplot(dff, aes(y = LungCap, x = Smoke, fill = Smoke)): could not find function \"ggplot\"\n\n\nOnly age group that showed similar result that of step C is <=13. The analysis could go wrong if the lung capacity of smokers and non smokers are studied without considering the age."
  },
  {
    "objectID": "posts/DerianToth_M_HW1.html",
    "href": "posts/DerianToth_M_HW1.html",
    "title": "Homework_One",
    "section": "",
    "text": "Question 1\n\n(1a) What does the distribution of LungCap look like?\nFirst, let’s read in the data from the Excel file:\n\n\nCode\nlibrary(\"quarto\")\nlibrary(\"tidyverse\")\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.0     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.1     ✔ tibble    3.1.8\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the \u001b]8;;http://conflicted.r-lib.org/\u0007conflicted package\u001b]8;;\u0007 to force all conflicts to become errors\n\n\nCode\nlibrary(\"palmerpenguins\")\nlibrary(readxl)\nlibrary(dplyr)\nlibrary(ggplot2)\ndf <- read_excel(\"_data/LungCapData.xls\")\n#View(df)\n\n\nThe distribution of LungCap looks as follows:\n\n\nCode\nhist(df$LungCap)\n\n\n\n\n\nThe histogram suggests that the distribution is close to a normal distribution. Most of the observations are close to the mean. Very few observations are close to the margins (0 and 15).\n\n\n(1b) The probability distribution of the LungCap with respect to gender is as follows:\n\n\nCode\nboxplot(df$LungCap ~ df$Gender)\n\n\n\n\n\n\n\n(1c) The mean lung capacities for smokers and non-smokers can be found in the table below:\n\n\nCode\ndf %>%\n  group_by(Smoke) %>%\n  summarise_at(vars(LungCap), list(name = mean))\n\n\n# A tibble: 2 × 2\n  Smoke  name\n  <chr> <dbl>\n1 no     7.77\n2 yes    8.65\n\n\nThese means are not what I would expect. It looks like those who smoke (“yes”) have a higher long capacity (8.65) than those who do not smoke (7.77).\n\n\n(1d) The relationship between Smoking and Lung Capacity within age groups\n\n\nCode\n#Age groups defined by:\n#“less than or\n#equal to 13”, \n#“14 to 15”, \n#“16 to 17”, \n#“greater than or equal to 18”.\n\n# Create variable\ndf <- df %>% \n  mutate(age_group = case_when(\n      Age <= 13 ~ \"0-13\",\n      Age > 13 & Age < 16 ~ \"14-15\",\n      Age > 15 & Age < 18 ~ \"16-18\",\n      Age >= 18 ~ \">= 18\"),\n    # Convert to factor\n    age_group = factor(\n      age_group,\n      level = c(\"0-13\", \"14-15\",\"16-18\", \">= 18\")))\n\nView(df)\n\ndf %>%\n  group_by(age_group,Smoke) %>%\n  summarise_at(vars(LungCap), list(name = mean))\n\n\n# A tibble: 8 × 3\n# Groups:   age_group [4]\n  age_group Smoke  name\n  <fct>     <chr> <dbl>\n1 0-13      no     6.36\n2 0-13      yes    7.20\n3 14-15     no     9.14\n4 14-15     yes    8.39\n5 16-18     no    10.5 \n6 16-18     yes    9.38\n7 >= 18     no    11.1 \n8 >= 18     yes   10.5 \n\n\nCode\ndbinom(x=8,size=8,prob=.5)\n\n\n[1] 0.00390625\n\n\nCode\ndbinom(x=6,size=8,prob=.5)\n\n\n[1] 0.109375\n\n\n\n\n(1e) Compare the lung capacities for smokers and non-smokers within each age group.\n\n\nCode\nggplot(df, aes(x=age_group, y=LungCap, color = Smoke)) +\n  geom_boxplot()\n\n\n\n\n\nThis data visualization makes more sense for what we expect from lung capacity when comparing smokers to non smokers. It looks like lunch capacity increases as the participants get older. The data could have more participants who are smokers and who are older. This unbalance in participants could be skewing the overall average lunch capacity.\n\n\n\nQuestion 2:Setting up the Dataframe\n\n\nCode\nStatePrison <- data.frame(number_convictions = 0:4, InMateCount = c(128, 434, 160, 64, 24)) %>%\n                            mutate(Probability = InMateCount/810)\n\nView(StatePrison)\n\n\n\n(2a) What is the probability that a randomly selected inmate has exactly 2 prior convictions?\n\n\nCode\ndbinom(x = 1, size = 1, p = 160/810)\n\n\n[1] 0.1975309\n\n\n\n\n(2b) What is the probability that a randomly selected inmate has fewer than 2 prior convictions?\n\n\nCode\ndbinom(x = 1, size = 1, p = sum(128+434)/810)\n\n\n[1] 0.6938272\n\n\n\n\n(2c) What is the probability that a randomly selected inmate has 2 or fewer prior convictions?\n\n\nCode\ndbinom(x = 1, size = 1, p = sum(128+434+160)/810)\n\n\n[1] 0.891358\n\n\n\n\n(2d) What is the probability that a randomly selected inmate has more than 2 prior convictions?\n\n\nCode\ndbinom(x = 1, size = 1, p = sum(64+24)/810)\n\n\n[1] 0.108642\n\n\n\n\n(2e) What is the expected value for the number of prior convictions?\n\n\nCode\nEV <- sum(StatePrison$number_convictions *StatePrison$Probability)\nprint(EV)\n\n\n[1] 1.28642\n\n\n\n\n(2f) Calculate the variance and the standard deviation for the Prior Convictions.\n\n\nCode\nVar <- sum((StatePrison$number_convictions - EV) ^ 2 * StatePrison$Probability)\n\nprint(Var)\n\n\n[1] 0.8562353\n\n\n\n\nCode\nSD <- sqrt(Var)\n\nprint(SD)\n\n\n[1] 0.9253298"
  },
  {
    "objectID": "posts/FinalPart1_MiguelCuriel.html",
    "href": "posts/FinalPart1_MiguelCuriel.html",
    "title": "The Pandemic’s Toll on Mental Health: A Look at the Tech Sector (DRAFT)",
    "section": "",
    "text": "Research Question\nThe technology industry - aka tech - has been one of the highest employers in the twenty-first century. Not only that, but it has been praised for having some of the happiest workers1 2.\nBut, what has made this industry so appealing? A case study on Google published in the International Journal of Corporate Social Responsibility in 20173 points to several elements that make high-tech unique, such as having a distinct culture proposition, aligning individual behaviors to company-wide goals, having managers be coaches rather than bosses, and being able to interact with people from other cultures.\nEvidently, Google is one-in-a-million high-tech company, but there are certainly commonalities shared with smaller new tech (startup) companies. Culture Amp, a company focused on surveying employees in startups, elaborated an analysis based on their results from 2015-2020 surveys4 and mention that elements such as an open and honest two-way communication, workplace flexibility, and fair division of workload, are what make new tech companies valued.\nHowever, the previous data omits the downsides of such cultures. Besides the multiple blog posts and news articles one can find talking about burnout5, the darker side of tech also includes (but is not limited to) ageism6, gender inequality7 8, and even migration issues9 10.\nA survey lead by Blind in 202111 found that, out of 2400 workers in tech, 64% said their mental health is worse after the pandemic. Further, layoffs by the thousands, plummeting stock prices, and generalized revaluation of an entire industry’s value, are just few words that can describe what has happened to tech in 2022 and 2023. An industry that once employed over 5 million people in the US alone12 and had nearly 700 billion dollars in funding worldwide13 has now laid off over 300 thousand people14 and nearly halved in funding.\nIt is reasonable to hypothesize that the aforementioned events will take a toll on the workers of this industry, but what was the mental health state before this unfortunate series of events? More specifically, the proposed research question is:\n\nWhat is the trend in mental health issues among workers in the technology industry from 2017 to 2021, as measured by survey data, and what factors may contribute to these changes?\n\n\n\n\nHypothesis\nBased on the present research question and on previous studies, these are the hypotheses that come to mind:\n\nH1: There has been an increase in the prevalence of mental health issues among workers in the technology industry from 2017 to 2021.\nH2: There has been no significant change in the prevalence of mental health issues among workers in the technology industry from 2017 to 2021.\nH3: The increase in mental health issues among workers in the technology industry from 2017 to 2021 is related to factors such as age, gender, and company policies with regards to mental health care.\nH4: The increase in mental health issues among workers in the technology industry from 2017 to 2021 is not related to any specific factors but rather a general trend across all demographics.\n\n\n\n\nDescriptive Statistics\nTo analyze the state of mental health and the contributing factors, I will rely on data provided by the Open Sourcing Mental Health (OSMH), specifically using their Mental Health in Tech Survey.\nOSMH15 is a non-profit dedicated to raising awareness, educating, and providing resources to support mental wellness in the tech and open source communities. It began operations in 2013 and since 2014 it has conducted and published an annual or bi-annual survey analyzing several mental health indicators.\nAs of March 20, 2023, the 2022 survey has not yet been published. Therefore, this analysis employs historical data, specifically utilizing surveys conducted from 2017 through 2021. All datasets are publicly available on OSMH’s website or on Kaggle:\n\nhttps://osmhhelp.org/research.html\nhttps://www.kaggle.com/osmihelp/datasets\n\nGiven the size of the data, both files were appended using Microsoft Excel and were then exported to a CSV file. Also, there was not a 100% match between columns in both files, and some re-coding had to be implemented before importing into R. The steps taken to consolidate both files offline are noted in the section below.\n\n\n\n\n\n\nNote\n\n\n\n\n\n\nA “year” column was added to differentiate between both files.\nData pertaining to insurance information (e.g., “Does you company provide a mental health insurance plan?”) was removed because in the 2019 it was optional and was, therefore, almost entirely blank.\nBoth files contained columns pertaining to the mental disorder each respondent may or may not have. However, these columns were inconsistent and could not be interpreted without making assumptions which may lead to an incorrect interpretation of the data. For that reason, these columns were removed.\nThe raw data files followed a title case naming convention (e.g., “Does your employer provide mental health resources?”). All column names were changed to a snake_case format (e.g., “employer_provides_mental_health_resources”).\nThe rest of the columns and data therein contained is left as is.\n\n\n\n\nThe raw consolidated file contains over 70 columns. After reviewing findings from past research and taking the present research question into consideration, the following are the variables of interest:\n\nDEPENDENT VARIABLE\n\nhas_mental_disorder: Do you currently have a mental health disorder?\n\nINDEPENDENT VARIABLES\n\nage: What is your age?\ngender: What is your gender?\nrace: What is your race?\nfamily_history_mental_illness: Do you have a family history of mental illness?\nhad_mental_disorder: Have you had a mental health disorder in the past?\nsought_treatment: Have you ever sought treatment for a mental health disorder from a mental health professional?\nwillingness_to_share_with_friends: How willing would you be to share with friends and family that you have a mental illness?\ntech_industry_supports_mh: Overall, how well do you think the tech industry supports employees with mental health issues?\nnumber_of_employees: How many employees does your company or organization have?\ncompany_provides_mhcare: Does your employer provide mental health benefits as part of healthcare coverage?\nanonymity_protected_if_use_resources: Is your anonymity protected if you choose to take advantage of mental health or substance abuse treatment resources provided by your employer?\neasy_to_leave_for_mhcare: If a mental health issue prompted you to request a medical leave from work, how easy or difficult would it be to ask for that leave?\ncomfortable_talking_to_supervisor: Would you feel comfortable discussing a mental health issue with your direct supervisor(s)?\ncomfortable_talking_to_coworkers_about_mh: Would you feel comfortable discussing a mental health issue with your coworkers?\nemployer_mh_importance: Overall, how much importance does your employer place on mental health?\ncurrent_or_previous_employer_supportive: Have you observed or experienced a supportive or well handled response to a mental health issue in your current or previous workplace?\nyear: In what year was the survey conducted?\n\n\nAfter manually merging the files in R, some additional cleansing is still entailed before the dataset is fully operational. These changes are easier to implement in R, can be found in the code chunk below and include steps such as treating inconsistent categorical columns. Once these changes are made, a summary of the resulting dataframe is created:\n\n\nCode\n# load neccesary packages\nlibrary(tidyverse) # used for elementary data wrangling and visualization\nlibrary(naniar) # used for missing values visualization\nlibrary(summarytools) # used for table summarizing descriptive statistics\n\n# temporarily set working directory to read in data\nsetwd(\"/Users/macuriels/Documents/Umass/umass_dacss_quantitativeanalysis/posts/_data\")\n\n# read in consolidated file with 2019 and 2021 data\ndf <- read_csv(\"mhit.csv\")\n\n# treating inconsistent binary code in column(s) of interest\ndf$sought_treatment <- ifelse(df$sought_treatment == \"TRUE\", 1, 0)\n\n# treating inconsistent gender naming conventions\ndf$gender <- ifelse(grepl(\"female\", df$gender, ignore.case = TRUE), \"Female\" \n                    ,ifelse(grepl(\"male\", df$gender, ignore.case = TRUE), \"Male\"\n                           ,\"Other\"))\n\n# create new dataframe with columns of interest\ndf <- df |>\n  select(\n    year\n    ,has_mental_disorder\n    ,age\n    ,gender\n    ,race\n    ,family_history_mental_illness\n    ,had_mental_disorder\n    ,sought_treatment\n    ,willingness_to_share_with_friends\n    ,tech_industry_supports_mh\n    ,number_of_employees\n    ,company_provides_mhcare\n    ,anonymity_protected_if_use_resources\n    ,easy_to_leave_for_mhcare\n    ,comfortable_talking_to_supervisor\n    ,comfortable_talking_to_coworkers_about_mh\n    ,employer_mh_importance\n    ,current_or_previous_employer_supportive\n  )\n\n# remove rows with missing values\ndf <- df[complete.cases(df),]\n\n# remove columns with missing values\ndf <- df[,colSums(is.na(df)) == 0]\n\n# summarize resulting dataframe\ndfSummary(df)\n\n\nData Frame Summary  \ndf  \nDimensions: 988 x 18  \nDuplicates: 0  \n\n-----------------------------------------------------------------------------------------------------------------------------------------------\nNo   Variable                                    Stats / Values                 Freqs (% of Valid)   Graph                 Valid      Missing  \n---- ------------------------------------------- ------------------------------ -------------------- --------------------- ---------- ---------\n1    year                                        Mean (sd) : 2018 (1.1)         2017 : 432 (43.7%)   IIIIIIII              988        0        \n     [numeric]                                   min < med < max:               2018 : 272 (27.5%)   IIIII                 (100.0%)   (0.0%)   \n                                                 2017 < 2018 < 2021             2019 : 182 (18.4%)   III                                       \n                                                 IQR (CV) : 2 (0)               2020 :  56 ( 5.7%)   I                                         \n                                                                                2021 :  46 ( 4.7%)                                             \n\n2    has_mental_disorder                         1. Don't Know                   52 ( 5.3%)          I                     988        0        \n     [character]                                 2. No                          243 (24.6%)          IIII                  (100.0%)   (0.0%)   \n                                                 3. Possibly                    195 (19.7%)          III                                       \n                                                 4. Yes                         498 (50.4%)          IIIIIIIIII                                \n\n3    age                                         1. 30                           67 ( 6.8%)          I                     988        0        \n     [character]                                 2. 37                           54 ( 5.5%)          I                     (100.0%)   (0.0%)   \n                                                 3. 28                           53 ( 5.4%)          I                                         \n                                                 4. 27                           47 ( 4.8%)                                                    \n                                                 5. 32                           45 ( 4.6%)                                                    \n                                                 6. 35                           45 ( 4.6%)                                                    \n                                                 7. 34                           44 ( 4.5%)                                                    \n                                                 8. 36                           42 ( 4.3%)                                                    \n                                                 9. 31                           38 ( 3.8%)                                                    \n                                                 10. 33                          38 ( 3.8%)                                                    \n                                                 [ 38 others ]                  515 (52.1%)          IIIIIIIIII                                \n\n4    gender                                      1. Female                      285 (28.8%)          IIIII                 988        0        \n     [character]                                 2. Male                        525 (53.1%)          IIIIIIIIII            (100.0%)   (0.0%)   \n                                                 3. Other                       178 (18.0%)          III                                       \n\n5    race                                        1. American Indian or Alaska     1 ( 0.1%)                                988        0        \n     [character]                                 2. Asian                        48 ( 4.9%)                                (100.0%)   (0.0%)   \n                                                 3. Black or African American    15 ( 1.5%)                                                    \n                                                 4. Caucasian                     1 ( 0.1%)                                                    \n                                                 5. Hispanic                      1 ( 0.1%)                                                    \n                                                 6. I prefer not to answer       28 ( 2.8%)                                                    \n                                                 7. More than one of the abov    34 ( 3.4%)                                                    \n                                                 8. White                       859 (86.9%)          IIIIIIIIIIIIIIIII                         \n                                                 9. White Hispanic                1 ( 0.1%)                                                    \n\n6    family_history_mental_illness               1. I don't know                242 (24.5%)          IIII                  988        0        \n     [character]                                 2. No                          219 (22.2%)          IIII                  (100.0%)   (0.0%)   \n                                                 3. Yes                         527 (53.3%)          IIIIIIIIII                                \n\n7    had_mental_disorder                         1. Don't Know                   64 ( 6.5%)          I                     988        0        \n     [character]                                 2. No                          248 (25.1%)          IIIII                 (100.0%)   (0.0%)   \n                                                 3. Possibly                    167 (16.9%)          III                                       \n                                                 4. Yes                         509 (51.5%)          IIIIIIIIII                                \n\n8    sought_treatment                            Min  : 0                       0 : 857 (86.7%)      IIIIIIIIIIIIIIIII     988        0        \n     [numeric]                                   Mean : 0.1                     1 : 131 (13.3%)      II                    (100.0%)   (0.0%)   \n                                                 Max  : 1                                                                                      \n\n9    willingness_to_share_with_friends           1. 0                            30 ( 3.0%)                                988        0        \n     [character]                                 2. 1                            23 ( 2.3%)                                (100.0%)   (0.0%)   \n                                                 3. 10                          176 (17.8%)          III                                       \n                                                 4. 2                            48 ( 4.9%)                                                    \n                                                 5. 3                            47 ( 4.8%)                                                    \n                                                 6. 4                            42 ( 4.3%)                                                    \n                                                 7. 5                           117 (11.8%)          II                                        \n                                                 8. 6                            88 ( 8.9%)          I                                         \n                                                 9. 7                           146 (14.8%)          II                                        \n                                                 10. 8                          158 (16.0%)          III                                       \n                                                 11. 9                          113 (11.4%)          II                                        \n\n10   tech_industry_supports_mh                   Mean (sd) : 2.6 (0.9)          1 : 121 (12.2%)      II                    988        0        \n     [numeric]                                   min < med < max:               2 : 301 (30.5%)      IIIIII                (100.0%)   (0.0%)   \n                                                 1 < 3 < 5                      3 : 406 (41.1%)      IIIIIIII                                  \n                                                 IQR (CV) : 1 (0.4)             4 : 150 (15.2%)      III                                       \n                                                                                5 :  10 ( 1.0%)                                                \n\n11   number_of_employees                         1. 1-5                          22 ( 2.2%)                                988        0        \n     [character]                                 2. 100-500                     269 (27.2%)          IIIII                 (100.0%)   (0.0%)   \n                                                 3. 26-100                      179 (18.1%)          III                                       \n                                                 4. 500-1000                     86 ( 8.7%)          I                                         \n                                                 5. 6-25                        118 (11.9%)          II                                        \n                                                 6. More than 1000              314 (31.8%)          IIIIII                                    \n\n12   company_provides_mhcare                     1. I don't know                239 (24.2%)          IIII                  988        0        \n     [character]                                 2. No                           46 ( 4.7%)                                (100.0%)   (0.0%)   \n                                                 3. Not eligible for coverage    29 ( 2.9%)                                                    \n                                                 4. Yes                         674 (68.2%)          IIIIIIIIIIIII                             \n\n13   anonymity_protected_if_use_resources        1. I don't know                585 (59.2%)          IIIIIIIIIII           988        0        \n     [character]                                 2. No                           29 ( 2.9%)                                (100.0%)   (0.0%)   \n                                                 3. Yes                         374 (37.9%)          IIIIIII                                   \n\n14   easy_to_leave_for_mhcare                    1. Difficult                    88 ( 8.9%)          I                     988        0        \n     [character]                                 2. I don't know                184 (18.6%)          III                   (100.0%)   (0.0%)   \n                                                 3. Neither easy nor difficul   114 (11.5%)          II                                        \n                                                 4. Somewhat difficult          125 (12.7%)          II                                        \n                                                 5. Somewhat easy               270 (27.3%)          IIIII                                     \n                                                 6. Very easy                   207 (21.0%)          IIII                                      \n\n15   comfortable_talking_to_supervisor           1. Maybe                       345 (34.9%)          IIIIII                988        0        \n     [character]                                 2. No                          256 (25.9%)          IIIII                 (100.0%)   (0.0%)   \n                                                 3. Yes                         387 (39.2%)          IIIIIII                                   \n\n16   comfortable_talking_to_coworkers_about_mh   1. Maybe                       448 (45.3%)          IIIIIIIII             988        0        \n     [character]                                 2. No                          242 (24.5%)          IIII                  (100.0%)   (0.0%)   \n                                                 3. Yes                         298 (30.2%)          IIIIII                                    \n\n17   employer_mh_importance                      Mean (sd) : 5.1 (2.4)          11 distinct values           :             988        0        \n     [numeric]                                   min < med < max:                                            :             (100.0%)   (0.0%)   \n                                                 0 < 5 < 10                                                  :   .                             \n                                                 IQR (CV) : 4 (0.5)                                  : . : . : : : :                           \n                                                                                                     : : : : : : : : . .                       \n\n18   current_or_previous_employer_supportive     1. Maybe/Not sure              246 (24.9%)          IIII                  988        0        \n     [character]                                 2. No                          333 (33.7%)          IIIIII                (100.0%)   (0.0%)   \n                                                 3. Yes, I experienced          214 (21.7%)          IIII                                      \n                                                 4. Yes, I observed             195 (19.7%)          III                                       \n-----------------------------------------------------------------------------------------------------------------------------------------------\n\n\nAt this point, the dataset has no null values and categorical variables have been harmonized, therefore the data should be ready for further analysis and visualizations.\n\n\n\n\n\n\n\nFootnotes\n\n\nFox, M. (2016, November 11). Why Are Tech Workers So Satisfied With Their Jobs? Retrieved March 17, 2023, from https://www.forbes.com/sites/meimeifox/2016/11/11/why-are-tech-workers-so-satisfied-with-their-jobs/?sh=4eac1918a059↩︎\nWronski, L., & Cohen, J. (2019, November 4). This is the industry sector that has some of the happiest workers in America. Retrieved March 17, 2023, from https://www.cnbc.com/2019/11/04/this-is-the-industry-that-has-some-of-the-happiness-workers-in-america.html↩︎\nKim, K. T. (2017). GOOGLE: A reflection of culture, leader, and management. International Journal of Corporate Social Responsibility, 2(10). https://doi.org/10.1186/s40991-017-0021-0↩︎\nMcPherson, J. (n.d.). Tech company cultures are not all the same. Culture Amp. Retrieved March 17, 2023, from https://www.cultureamp.com/blog/tech-company-culture↩︎\nGoncharov, A. (2023, March 13). How I burnt out in FAANG, but my job was not the problem. Blog.Goncharov.ai. Retrieved March 17, 2023, from https://blog.goncharov.ai/how-i-burnt-out-in-faang-but-my-job-was-not-the-problem↩︎\nRosales, A., & Jakob, S. (2021). Perceptions of age in contemporary tech. Sciendo, 42(1), 79-91. https://doi.org/10.2478/nor-2021-0021↩︎\nMickey, E. L. (2021). The Organization of Networking and Gender Inequality in the New Economy: Evidence from the Tech Industry. Work & Occupations, 49(4), 383-420. https://doi.org/10.1177/07308884221102134↩︎\nHardey, M. (2020). The Culture of Women in Tech : An Unsuitable Job for a Woman (1st ed.). Emerald Publishing.↩︎\nBanerjee, P., & Rincón, L. (2019). Trouble in Tech Paradise. Journal of Water Resources Planning & Management, 145(4), 24-29. https://doi.org/10.1177/1536504219854714↩︎\nMatloff, N. (2013). Immigration and the tech industry: As a labour shortage remedy, for innovation, or for cost savings? Migration Letters, 10(2), 210-227. ISSN: 1741-8984 Online ISSN: 1741-8992↩︎\nBlind (2021, January 29). Deteriorating Mental Health In The Workplace. Retrieved March 17, 2023, from https://www.teamblind.com/blog/index.php/2021/01/29/deteriorating-mental-health-in-the-workplace/↩︎\nThe United States Bureau of Labor and Statistics via CompTIA (2023, March 3). Cyberstates 2021: The Definitive Guide to the Tech Industry and Workforce. Retrieved March 17, 2023, from https://www.comptia.org/content/tech-jobs-report↩︎\nCrunchbase News. (2023, January 5). Global VC Funding on a Slide since Q4 2022. Retrieved March 17, 2023, from https://news.crunchbase.com/venture/global-vc-funding-slide-q4-2022↩︎\nLayoffs.fyi. (n.d.). Layoffs.fyi - Tracking all tech startup layoffs since COVID-19. https://layoffs.fyi↩︎\nOpen Sourcing Mental Health (n.d.). About OSMH. Retrieved March 18, 2023, from https://osmhhelp.org/about/about-osmi.html↩︎"
  },
  {
    "objectID": "posts/JustineShakespeare_FinalProject.html",
    "href": "posts/JustineShakespeare_FinalProject.html",
    "title": "Maternal Mortality and Women’s Empowerment: Exploring Socio-Political Factors Associated with Better Outcomes for Mothers Across the Globe",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(summarytools)\n\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/JustineShakespeare_FinalProject.html#research-question",
    "href": "posts/JustineShakespeare_FinalProject.html#research-question",
    "title": "Maternal Mortality and Women’s Empowerment: Exploring Socio-Political Factors Associated with Better Outcomes for Mothers Across the Globe",
    "section": "Research Question",
    "text": "Research Question\nDespite advancements in health and technology, maternal mortality remains a significant public health challenge around the world. According to the World Health Organization, approximately 287,000 women died from complications related to pregnancy or childbirth in 2020.1 Recently it was reported that global declines in maternal mortality have stalled,2 even increased in the United States,3 and data from 2020 has shown the pandemic brought a sharp uptick in maternal deaths.4 The World Health Organization has stated that the majority of maternal deaths are preventable.5\nPast research has investigated variables associated with higher rates of maternal mortality, both at the level of the individual and country. Many studies have found that factors associated with healthcare, such as skilled birth attendance or antenatal visits, are strongly associated with maternal mortality.6 7 8 Other studies considering social determinants of health have found that factors such as adult literacy, education, income levels, and access to water and sanitation are all associated with maternal mortality.9 10 The majority of maternal deaths occur in low-income countries.11\nThis research intends to explore the relationship between women’s empowerment and maternal mortality. In particular, this analysis will focus on variables related to women’s political empowerment, civil liberties, and participation in civil society and will control for confounding variables related to economic development.\nPast research has found a link between higher levels of gender equality and women’s empowerment and reduced maternal mortality, although analysis has primarily focused on individual countries or a subset of countries (typically lower-income).12 13 14 This research will investigate whether these trends hold true when looking at over 165 countries from all levels of economic development.\nIn short, this analysis seeks to answer the question: How do factors related to women’s empowerment impact maternal mortality?"
  },
  {
    "objectID": "posts/JustineShakespeare_FinalProject.html#hypothesis",
    "href": "posts/JustineShakespeare_FinalProject.html#hypothesis",
    "title": "Maternal Mortality and Women’s Empowerment: Exploring Socio-Political Factors Associated with Better Outcomes for Mothers Across the Globe",
    "section": "Hypothesis",
    "text": "Hypothesis\nHypothesis 1: Where women are politically empowered, have civil liberties, and are able to freely participate in civil society, health outcomes are better and the maternal mortality ratio is lower. This will hold true even when controlling for variables which are known to have an effect on both women’s empowerment and maternal mortality.\nNote for professor: Another way that this analysis could address the research question above is to explore the mechanisms through which women’s empowerment affects maternal mortality. Specifically, since much of the literature discusses immediate healthcare concerns this analysis could examine (in addition to the link between women’s empowerment and MMR) whether women’s empowerment is associated with the two variables: skilled birth attendance and antenatal visits. In this case, the second hypothesis would be:\nHypothesis 2: Women’s political empowerment and their participation in civil society has a positive effect on skilled birth attendance and antenatal visits, resulting in a reduced maternal mortality ratio.\nIn this way the analysis will both establish the link between women’s empowerment and reduced maternal mortality while also providing insight into how women’s empowerment affects maternal mortality."
  },
  {
    "objectID": "posts/JustineShakespeare_FinalProject.html#introduction-to-the-data",
    "href": "posts/JustineShakespeare_FinalProject.html#introduction-to-the-data",
    "title": "Maternal Mortality and Women’s Empowerment: Exploring Socio-Political Factors Associated with Better Outcomes for Mothers Across the Globe",
    "section": "Introduction to the Data",
    "text": "Introduction to the Data\nThis research will utilize several datasets in order to capture all of the relevant variables.\nThe dependent variable, Maternal Mortality Ratio, is drawn from the Maternal and newborn health table from UNICEF. This data is from 2017.15\n\n\n\n\n\n\nDefinitions\n\n\n\nAccording to the World Health Organization, the maternal mortality ratio (MMR) is defined as: “the number of maternal deaths during a given time period per 100,000 live births during the same time period.”\nMaternal deaths are defined as: “deaths from any cause related to or aggravated by pregnancy or its management (excluding accidental or incidental causes) during pregnancy and childbirth or within 42 days of termination of pregnancy…”16\n\n\nThe independent variables featured in this analysis will be drawn from the Varieties of Democracy V-Dem dataset.17 From there, the following three variables that relate to gender equality and women’s rights will be used:\n\nWomen civil society participation index18\nAs defined by the V-Dem codebook: “Women’s civil society participation is understood to include open discussion of political issues, participation in civil society organizations, and representation in the ranks of journalists.”19\nWomen civil liberties index20\nAs defined by the V-Dem codebook: “Women’s civil liberties are understood to include freedom of domestic movement, the right to private property, freedom from forced labor, and access to justice.”21\nWomen political empowerment index22\nAs defined by the V-Dem codebook: “Women’s political empowerment is defined as a process of increasing capacity for women, leading to greater choice, agency, and participation in societal decision-making. It is understood to incorporate three equally-weighted dimensions: fundamental civil liberties, women’s open discussion of political issues and participation in civil society organizations, and the descriptive representation of women in formal political positions.”23\n\nAll three of these variables are on an interval scale, from low to high (0-1). In order to match the timeframe of the dependent variable, the 2017 value for all of these variables will be used.\nIn addition to these variables, a confounding variable that seeks to account for a nation’s wealth and development will be considered in order to control for its effect on both maternal mortality and women’s empowerment.\nGDP per capita will be used as a way to control for a nation’s economic development. This variable is drawn from the International Monetary Fund (IMF)’s data.24 GDP per capita from 2017 will be used in order to be consistent with the data in the rest of the analysis dataset.\nThe resulting database will be a cross-sectional, cross-national sample of data from 169 countries (some observations were removed because of missing values)."
  },
  {
    "objectID": "posts/JustineShakespeare_FinalProject.html#loading-and-cleaning-the-data",
    "href": "posts/JustineShakespeare_FinalProject.html#loading-and-cleaning-the-data",
    "title": "Maternal Mortality and Women’s Empowerment: Exploring Socio-Political Factors Associated with Better Outcomes for Mothers Across the Globe",
    "section": "Loading and Cleaning the Data",
    "text": "Loading and Cleaning the Data\nFirst let’s load and set up the data. Because we are drawing from multiple datasets for this analysis, this will be a fairly involved process. First we’ll load and clean up the Maternal and Newborn Health dataset from UNICEF.\n\n\nCode\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(summarytools)\n\n# Load the dataset, skip the first five rows, rename all of the variables we \n# want and delete the ones we don't\nMaternalNewbornHealth <- read_excel(\"UNICEF_mat_newborn.xlsx\", skip = 6, \n                                    col_names= c(\"Countries_and_areas\", \n                                                 rep(\"delete\", 28), \n                                                 \"Maternal_Mortality_Ratio_2017\", \n                                                 \"delete\", \"delete\")) %>% \n  select(!contains(\"delete\"))\n\n\nNew names:\n• `delete` -> `delete...2`\n• `delete` -> `delete...3`\n• `delete` -> `delete...4`\n• `delete` -> `delete...5`\n• `delete` -> `delete...6`\n• `delete` -> `delete...7`\n• `delete` -> `delete...8`\n• `delete` -> `delete...9`\n• `delete` -> `delete...10`\n• `delete` -> `delete...11`\n• `delete` -> `delete...12`\n• `delete` -> `delete...13`\n• `delete` -> `delete...14`\n• `delete` -> `delete...15`\n• `delete` -> `delete...16`\n• `delete` -> `delete...17`\n• `delete` -> `delete...18`\n• `delete` -> `delete...19`\n• `delete` -> `delete...20`\n• `delete` -> `delete...21`\n• `delete` -> `delete...22`\n• `delete` -> `delete...23`\n• `delete` -> `delete...24`\n• `delete` -> `delete...25`\n• `delete` -> `delete...26`\n• `delete` -> `delete...27`\n• `delete` -> `delete...28`\n• `delete` -> `delete...29`\n• `delete` -> `delete...31`\n• `delete` -> `delete...32`\n\n\nCode\n# remove the last 57 rows, which include summaries and endnotes, not data\nMNH <- slice(MaternalNewbornHealth, 1:(nrow(MaternalNewbornHealth) - 57)) \n\n# select just the variables we need for this analysis\nMNH_trim <- MNH %>% \n  select(Countries_and_areas, Maternal_Mortality_Ratio_2017) \n\n# instead of NAs, this dataset has \" - \" where there are no values. We'll \n# remove those here.\nMNH_trim <- MNH_trim %>% \n  filter(!str_detect(`Maternal_Mortality_Ratio_2017`, \"-\"))\n\n# change the Maternal Mortality variable to numeric.\nMNH_trim$`Maternal_Mortality_Ratio_2017` <- as.numeric(MNH_trim$`Maternal_Mortality_Ratio_2017`)\n\n# take a quick look at our UNICEF data\nhead(MNH_trim)\n\n\n# A tibble: 6 × 2\n  Countries_and_areas Maternal_Mortality_Ratio_2017\n  <chr>                                       <dbl>\n1 Afghanistan                                   638\n2 Albania                                        15\n3 Algeria                                       112\n4 Angola                                        241\n5 Antigua and Barbuda                            42\n6 Argentina                                      39\n\n\nNext let’s add in the variables related to women’s empowerment. We’ll first need to load in the Varieties of Democracy dataset, then we’ll select the appropriate variables and clean it up a bit.\n\n\nCode\nV_DEM <- readRDS(\"V-Dem-CY-Full+Others-v13.rds\")\n\n# Select just the variables we need for this analysis and filter to year 2017.\nV_DEM_trim <- V_DEM %>% \n  filter(year == 2017) %>% \n  select(country_name, v2x_gender, v2x_gencl, \n       v2x_gencs)\n\n# rename the country_name variable so we can join it with the UNICEF data later. \nV_DEM_trim <- rename(V_DEM_trim, \"Countries_and_areas\" = country_name)\n\n# Change some of the country names where the V-Dem dataset differs from the \n# UNICEF data. \nV_DEM_trim$Countries_and_areas[V_DEM_trim$Countries_and_areas == \"Moldova\"] <- \"Republic of Moldova\"\nV_DEM_trim$Countries_and_areas[V_DEM_trim$Countries_and_areas == \"Laos\"] <- \"Lao People's Democratic Republic\"\nV_DEM_trim$Countries_and_areas[V_DEM_trim$Countries_and_areas == \"The Gambia\"] <- \"Gambia\"\nV_DEM_trim$Countries_and_areas[V_DEM_trim$Countries_and_areas == \"Republic of the Congo\"] <- \"Congo\"\nV_DEM_trim$Countries_and_areas[V_DEM_trim$Countries_and_areas == \"Syria\"] <- \"Syrian Arab Republic\"\nV_DEM_trim$Countries_and_areas[V_DEM_trim$Countries_and_areas == \"Iran\"] <- \"Iran (Islamic Republic of)\"\nV_DEM_trim$Countries_and_areas[V_DEM_trim$Countries_and_areas == \"Cape Verde\"] <- \"Cabo Verde\"\nV_DEM_trim$Countries_and_areas[V_DEM_trim$Countries_and_areas == \"Ivory Coast\"] <- \"Côte d'Ivoire\"\nV_DEM_trim$Countries_and_areas[V_DEM_trim$Countries_and_areas == \"Venezuela\"] <- \"Venezuela (Bolivarian Republic of)\"\nV_DEM_trim$Countries_and_areas[V_DEM_trim$Countries_and_areas == \"Tanzania\"] <- \"United Republic of Tanzania\"\nV_DEM_trim$Countries_and_areas[V_DEM_trim$Countries_and_areas == \"South Korea\"] <- \"Republic of Korea\"\nV_DEM_trim$Countries_and_areas[V_DEM_trim$Countries_and_areas == \"North Korea\"] <- \"Democratic People's Republic of Korea\"\nV_DEM_trim$Countries_and_areas[V_DEM_trim$Countries_and_areas == \"Vietnam\"] <- \"Viet Nam\"\nV_DEM_trim$Countries_and_areas[V_DEM_trim$Countries_and_areas == \"Bolivia\"] <- \"Bolivia (Plurinational State of)\"\nV_DEM_trim$Countries_and_areas[V_DEM_trim$Countries_and_areas == \"United States of America\"] <- \"Gambia\"\nV_DEM_trim$Countries_and_areas[V_DEM_trim$Countries_and_areas == \"Russia\"] <- \"Russian Federation\"\nV_DEM_trim$Countries_and_areas[V_DEM_trim$Countries_and_areas == \"Burma/Myanmar\"] <- \"Myanmar\"\n\n# take a quick look at our V-Dem data\nhead(V_DEM_trim)\n\n\n  Countries_and_areas v2x_gender v2x_gencl v2x_gencs\n1              Mexico      0.794     0.651     0.763\n2            Suriname      0.861     0.817     0.823\n3              Sweden      0.932     0.949     0.867\n4         Switzerland      0.939     0.962     0.877\n5               Ghana      0.818     0.827     0.767\n6        South Africa      0.862     0.764     0.838\n\n\nNext, we’ll load anc clean the IMF dataset with the data on GDP per capita.\n\n\nCode\nimf_GDP <- read_excel(\"imf-dm-export-20230322.xls\")\n\n# Select just the variables we need for this analysis.\nimf_GDP_2017 <- select(imf_GDP, `GDP per capita, current prices\\n (U.S. dollars per capita)`, `2017`)\n\n# rename the country_name variable so we can join it with the UNICEF data later.\nimf_GDP_2017 <- rename(imf_GDP_2017, \"Countries_and_areas\" = `GDP per capita, current prices\\n (U.S. dollars per capita)`)\n\n# change the GDP per capita variable to numeric.\nimf_GDP_2017$`2017` <- as.numeric(imf_GDP_2017$`2017`)\n\n\nWarning: NAs introduced by coercion\n\n\nCode\n# Change some of the country names where the IMF dataset differs from the \n# UNICEF data. \nimf_GDP_2017$Countries_and_areas[imf_GDP_2017$Countries_and_areas == \"Bahamas, The\"] <- \"Bahamas\"\nimf_GDP_2017$Countries_and_areas[imf_GDP_2017$Countries_and_areas == \"Bolivia\"] <- \"Bolivia (Plurinational State of)\"\nimf_GDP_2017$Countries_and_areas[imf_GDP_2017$Countries_and_areas == \"China, People's Republic of\"] <- \"China\"\nimf_GDP_2017$Countries_and_areas[imf_GDP_2017$Countries_and_areas == \"Congo, Dem. Rep. of the\"] <- \"Democratic Republic of the Congo\"\nimf_GDP_2017$Countries_and_areas[imf_GDP_2017$Countries_and_areas == \"Congo, Republic of\"] <- \"Congo\"\nimf_GDP_2017$Countries_and_areas[imf_GDP_2017$Countries_and_areas == \"Czech Republic\"] <- \"Czechia\"\nimf_GDP_2017$Countries_and_areas[imf_GDP_2017$Countries_and_areas == \"Gambia, The\"] <- \"Gambia\"\nimf_GDP_2017$Countries_and_areas[imf_GDP_2017$Countries_and_areas == \"Iran\"] <- \"Iran (Islamic Republic of)\"\nimf_GDP_2017$Countries_and_areas[imf_GDP_2017$Countries_and_areas == \"Korea, Republic of\"] <- \"Republic of Korea\"\nimf_GDP_2017$Countries_and_areas[imf_GDP_2017$Countries_and_areas == \"Kyrgyz Republic\"] <- \"Kyrgyzstan\"\nimf_GDP_2017$Countries_and_areas[imf_GDP_2017$Countries_and_areas == \"Lao P.D.R.\"] <- \"Lao People's Democratic Republic\"\nimf_GDP_2017$Countries_and_areas[imf_GDP_2017$Countries_and_areas == \"Micronesia, Fed. States of\"] <- \"Micronesia (Federated States of)\"\nimf_GDP_2017$Countries_and_areas[imf_GDP_2017$Countries_and_areas == \"Moldova\"] <- \"Republic of Moldova\"\nimf_GDP_2017$Countries_and_areas[imf_GDP_2017$Countries_and_areas == \"Slovak Republic\"] <- \"Slovakia\"\nimf_GDP_2017$Countries_and_areas[imf_GDP_2017$Countries_and_areas == \"South Sudan, Republic of\"] <- \"South Sudan\"\nimf_GDP_2017$Countries_and_areas[imf_GDP_2017$Countries_and_areas == \"Syria\"] <- \"Syrian Arab Republic\"\nimf_GDP_2017$Countries_and_areas[imf_GDP_2017$Countries_and_areas == \"São Tomé and Príncipe\"] <- \"Sao Tome and Principe\"\nimf_GDP_2017$Countries_and_areas[imf_GDP_2017$Countries_and_areas == \"Tanzania\"] <- \"United Republic of Tanzania\"\nimf_GDP_2017$Countries_and_areas[imf_GDP_2017$Countries_and_areas == \"Türkiye, Republic of\"] <- \"Turkey\"\nimf_GDP_2017$Countries_and_areas[imf_GDP_2017$Countries_and_areas == \"Venezuela\"] <- \"Venezuela (Bolivarian Republic of)\"\nimf_GDP_2017$Countries_and_areas[imf_GDP_2017$Countries_and_areas == \"Vietnam\"] <- \"Viet Nam\"\n\n# take a quick look at our IMF data\nhead(imf_GDP_2017)\n\n\n# A tibble: 6 × 2\n  Countries_and_areas `2017`\n  <chr>                <dbl>\n1 <NA>                   NA \n2 Afghanistan           636.\n3 Albania              4526.\n4 Algeria              4080.\n5 Andorra             40018.\n6 Angola               4300.\n\n\nFinally, let’s join together all three datasets!\n\n\nCode\n# first we'll join the UNICEF and V-DEM data.\nJOIN_UNICEF_VDEM <- full_join(MNH_trim, V_DEM_trim, by = \"Countries_and_areas\") \n\n# then we'll add in the IMF data.\nJOIN_UNICEF_VDEM_IMF <- full_join(JOIN_UNICEF_VDEM, imf_GDP_2017, by = \"Countries_and_areas\") \n\n# remove the last 56 rows, which include notes, countries, and regions not in \n# the original UN dataset\nMMR_Combined <- slice(JOIN_UNICEF_VDEM_IMF, 1:(nrow(JOIN_UNICEF_VDEM_IMF) - 56)) \n\n# rename our variables for ease of use\nMMR_Analysis <-rename(MMR_Combined, \"Country\" = Countries_and_areas, \n                      \"PolEmp\" = v2x_gender, \"CivLib\" = v2x_gencl, \n                      \"CivSoc\" = v2x_gencs, \"GDP_2017\" = `2017`)\n\n# remove NAs for anlaysis\nMMR_Analysis <- na.omit(MMR_Analysis)\n\n# Let's take a look!\nhead(MMR_Analysis, limit = 10)\n\n\n# A tibble: 6 × 6\n  Country     Maternal_Mortality_Ratio_2017 PolEmp CivLib CivSoc GDP_2017\n  <chr>                               <dbl>  <dbl>  <dbl>  <dbl>    <dbl>\n1 Afghanistan                           638  0.52   0.221  0.424     636.\n2 Albania                                15  0.831  0.786  0.76     4526.\n3 Algeria                               112  0.748  0.719  0.604    4080.\n4 Angola                                241  0.708  0.549  0.575    4300.\n5 Argentina                              39  0.86   0.823  0.788   14618.\n6 Armenia                                26  0.824  0.866  0.912    3869.\n\n\nCode\nglimpse(MMR_Analysis)\n\n\nRows: 169\nColumns: 6\n$ Country                       <chr> \"Afghanistan\", \"Albania\", \"Algeria\", \"An…\n$ Maternal_Mortality_Ratio_2017 <dbl> 638, 15, 112, 241, 39, 26, 6, 5, 26, 14,…\n$ PolEmp                        <dbl> 0.520, 0.831, 0.748, 0.708, 0.860, 0.824…\n$ CivLib                        <dbl> 0.221, 0.786, 0.719, 0.549, 0.823, 0.866…\n$ CivSoc                        <dbl> 0.424, 0.760, 0.604, 0.575, 0.788, 0.912…\n$ GDP_2017                      <dbl> 635.789, 4525.887, 4079.653, 4300.097, 1…"
  },
  {
    "objectID": "posts/JustineShakespeare_FinalProject.html#descriptive-statistics",
    "href": "posts/JustineShakespeare_FinalProject.html#descriptive-statistics",
    "title": "Maternal Mortality and Women’s Empowerment: Exploring Socio-Political Factors Associated with Better Outcomes for Mothers Across the Globe",
    "section": "Descriptive Statistics",
    "text": "Descriptive Statistics\nLet’s take a closer look at the dependent variable, the Maternal Mortality Ratio. We can check out the mean, standard deviation, the full range (the lowest and highest values), values in the 25th, and 75th quantiles, as well as the median. This will give us a sense of the full spread of this variable.\n\n\nCode\nMMR_spread <- MMR_Analysis %>% \n  summarize(\"Mean\" = mean(Maternal_Mortality_Ratio_2017, na.rm = TRUE),\n            \"Standard_Deviation\" = sd(Maternal_Mortality_Ratio_2017, na.rm = TRUE),\n            \"Lowest\" = min(Maternal_Mortality_Ratio_2017, na.rm = TRUE), \n            \"25th quantile\" = quantile(x = Maternal_Mortality_Ratio_2017, probs = .25),\n            \"Median\" = median(Maternal_Mortality_Ratio_2017, na.rm = TRUE),\n            \"75th quantile\" = quantile(x = Maternal_Mortality_Ratio_2017, probs = .75),\n            \"Highest\" = max(Maternal_Mortality_Ratio_2017, na.rm = TRUE))\nMMR_spread\n\n\n# A tibble: 1 × 7\n   Mean Standard_Deviation Lowest `25th quantile` Median `75th quantile` Highest\n  <dbl>              <dbl>  <dbl>           <dbl>  <dbl>           <dbl>   <dbl>\n1  173.               243.      2              11     59             248    1150\n\n\nWe can see that there is quite a range, from 2 to 1,150. The relatively large standard deviation indicates there is a fair amount of variability as well. We can see from the median and 75th quantile numbers that most of the countries in this data have MMRs lower than about 250. But the highest value here shows us that some countries have an MMR of well over 4 times that amount.\nLet’s take a look at which countries have the highest MMRs (the most deaths per 100,000 live births).\n\n\nCode\nMMR_high <- MMR_Analysis %>% \n  select(\"Country\", \"Maternal_Mortality_Ratio_2017\") %>% \n  arrange(desc(as.numeric(Maternal_Mortality_Ratio_2017)))\n\nprint(MMR_high, limit = 10)\n\n\n# A tibble: 169 × 2\n   Country                  Maternal_Mortality_Ratio_2017\n   <chr>                                            <dbl>\n 1 South Sudan                                       1150\n 2 Chad                                              1140\n 3 Sierra Leone                                      1120\n 4 Nigeria                                            917\n 5 Central African Republic                           829\n 6 Somalia                                            829\n 7 Mauritania                                         766\n 8 Guinea-Bissau                                      667\n 9 Liberia                                            661\n10 Afghanistan                                        638\n# … with 159 more rows\n\n\nLet’s see which countries have the lowest MMRs.\n\n\nCode\nMMR_low <- MMR_Analysis %>% \n  select(\"Country\", \"Maternal_Mortality_Ratio_2017\") %>% \n  arrange(as.numeric(Maternal_Mortality_Ratio_2017))\n\nprint(MMR_low, limit = 10)\n\n\n# A tibble: 169 × 2\n   Country              Maternal_Mortality_Ratio_2017\n   <chr>                                        <dbl>\n 1 Belarus                                          2\n 2 Italy                                            2\n 3 Norway                                           2\n 4 Poland                                           2\n 5 Czechia                                          3\n 6 Finland                                          3\n 7 Greece                                           3\n 8 Israel                                           3\n 9 United Arab Emirates                             3\n10 Denmark                                          4\n# … with 159 more rows\n\n\nFinally, we’ll take a look at some of the explanatory variables we’re interested in exploring.\n\n\nCode\nsummary(MMR_Analysis[, c(\"PolEmp\", \"CivLib\", \"CivSoc\", \"GDP_2017\")])\n\n\n     PolEmp           CivLib           CivSoc          GDP_2017       \n Min.   :0.1400   Min.   :0.0240   Min.   :0.1440   Min.   :   277.7  \n 1st Qu.:0.6440   1st Qu.:0.5550   1st Qu.:0.5840   1st Qu.:  1758.5  \n Median :0.7900   Median :0.7430   Median :0.7600   Median :  5178.9  \n Mean   :0.7369   Mean   :0.6834   Mean   :0.6919   Mean   : 13169.2  \n 3rd Qu.:0.8670   3rd Qu.:0.8630   3rd Qu.:0.8460   3rd Qu.: 15626.3  \n Max.   :0.9650   Max.   :0.9830   Max.   :0.9390   Max.   :111211.6  \n\n\nCode\ndfSummary(MMR_Analysis[, c(\"PolEmp\", \"CivLib\", \"CivSoc\", \"GDP_2017\")])\n\n\nData Frame Summary  \nMMR_Analysis  \nDimensions: 169 x 4  \nDuplicates: 0  \n\n-----------------------------------------------------------------------------------------------------------------\nNo   Variable    Stats / Values                  Freqs (% of Valid)    Graph                 Valid      Missing  \n---- ----------- ------------------------------- --------------------- --------------------- ---------- ---------\n1    PolEmp      Mean (sd) : 0.7 (0.2)           145 distinct values                 :       169        0        \n     [numeric]   min < med < max:                                                  : :       (100.0%)   (0.0%)   \n                 0.1 < 0.8 < 1                                                     : : :                         \n                 IQR (CV) : 0.2 (0.2)                                          . . : : :                         \n                                                                           : : : : : : :                         \n\n2    CivLib      Mean (sd) : 0.7 (0.2)           150 distinct values                   : :   169        0        \n     [numeric]   min < med < max:                                                    . : :   (100.0%)   (0.0%)   \n                 0 < 0.7 < 1                                                       : : : :                       \n                 IQR (CV) : 0.3 (0.3)                                        . . : : : : :                       \n                                                                       . : : : : : : : : :                       \n\n3    CivSoc      Mean (sd) : 0.7 (0.2)           138 distinct values                 :       169        0        \n     [numeric]   min < med < max:                                                  . :       (100.0%)   (0.0%)   \n                 0.1 < 0.8 < 0.9                                                   : :                           \n                 IQR (CV) : 0.3 (0.3)                                            : : : .                         \n                                                                       . : : : : : : : :                         \n\n4    GDP_2017    Mean (sd) : 13169.2 (18895.1)   168 distinct values   :                     169        0        \n     [numeric]   min < med < max:                                      :                     (100.0%)   (0.0%)   \n                 277.7 < 5178.9 < 111211.6                             :                                         \n                 IQR (CV) : 13867.7 (1.4)                              :                                         \n                                                                       : : . .                                   \n-----------------------------------------------------------------------------------------------------------------"
  },
  {
    "objectID": "posts/HW_3_Diana_Rinker.html",
    "href": "posts/HW_3_Diana_Rinker.html",
    "title": "Homework 3",
    "section": "",
    "text": "DACSS 603, spring 2023\n\n\nHomework 3, Diana Rinker.\nLoading necessary libraries:\n\n\nCode\nlibrary(dplyr)\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nCode\nlibrary(ggplot2)\nlibrary(alr4)\n\n\nLoading required package: car\n\n\nLoading required package: carData\n\n\n\nAttaching package: 'car'\n\n\nThe following object is masked from 'package:dplyr':\n\n    recode\n\n\nLoading required package: effects\n\n\nlattice theme set by effectsTheme()\nSee ?effectsTheme for details.\n\n\nCode\nlibrary(smss)\n\n\n\n\nQuestion 1\nUnited Nations (Data file: UN11 in alr4) The data in the file UN11 contains several variables, including ppgdp, the gross national product per person in U.S. dollars, and fertility, the birth rate per 1000 females, both from the year 2009. The data are for 199 localities, mostly UN member countries, but also other areas such as Hong Kong that are not independent countries. The data were collected from the United Nations (2011). We will study the dependence of fertility on ppgdp.\n\nIdentify the predictor and the response.\nDV/response: fertility\nIV/predictor: ppgdp\nDraw the scatterplot of fertility on the vertical axis versus ppgdp on the horizontal axis and summarize the information in this graph. Does a straight-line mean function seem to be plausible for a summary of this graph?\n\n\n\nCode\ndata(UN11)\nstr(UN11)\n\n\n'data.frame':   199 obs. of  6 variables:\n $ region   : Factor w/ 8 levels \"Africa\",\"Asia\",..: 2 4 1 1 3 5 2 3 8 4 ...\n $ group    : Factor w/ 3 levels \"oecd\",\"other\",..: 2 2 3 3 2 2 2 2 1 1 ...\n $ fertility: num  5.97 1.52 2.14 5.13 2 ...\n $ ppgdp    : num  499 3677 4473 4322 13750 ...\n $ lifeExpF : num  49.5 80.4 75 53.2 81.1 ...\n $ pctUrban : num  23 53 67 59 100 93 64 47 89 68 ...\n - attr(*, \"na.action\")= 'omit' Named int [1:34] 4 5 8 28 41 67 68 72 79 83 ...\n  ..- attr(*, \"names\")= chr [1:34] \"Am Samoa\" \"Andorra\" \"Antigua and Barbuda\" \"Br Virigin Is\" ...\n\n\nCode\n    ggplot(data=UN11, mapping= aes(x=ppgdp, y =fertility ))+\n      geom_point()+\n      stat_smooth(method = \"lm\", se = T)+\n      geom_hline(yintercept = mean(UN11$fertility), color = \"red\")+\n      labs(title=\"Scatterplot of fertility by ppgdp\")+\n      stat_smooth(method = \"lm\", se = T)\n\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nFrom the graph above it is obvious, that the straight line mean is not representative of the data. Even if be draw a linear model with “stat_smooth(method =”lm”, se = T)“, the line is still far from most datapoints ( which indicate high value of error in the model)\n\nDraw the scatterplot of log(fertility) versus log(ppgdp) using natural logarithms. Does the simple linear regression model seem plausible for a summary of this graph? If you use a different base of logarithms, the shape of the graph won’t change, but the values on the axes will change.\n\n\n\nCode\n   ggplot(data=UN11, mapping= aes(x=log(ppgdp), y =log(fertility) ))+\n      geom_point()+\n      stat_smooth(method = \"lm\", se = T)+\n      geom_hline(yintercept = mean(log(UN11$fertility)), color = \"red\")+\n      labs(title=\"Scatterplot of log(fertility) by log(ppgdp) \")\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nSimple linear regression seems much more plausible on this logged scatter plot.\nBelow, I am using different bases (5 and 7) of the logarithms. The shape of a line doesn’t change, only x-scale limits change:\n\n\nCode\nggplot(data=UN11, mapping= aes(x=log(ppgdp, base =5), y =log(fertility), base =5 ))+\n      geom_point()+\n      stat_smooth(method = \"lm\", se = T)+\n      geom_hline(yintercept = mean(log(UN11$fertility),  base =5), color = \"red\")+\n      labs(title=\"Scatterplot of log(fertility) by log(ppgdp) base =5 \")\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nCode\nggplot(data=UN11, mapping= aes(x=log(ppgdp, base =7), y =log(fertility), base =7))+\n      geom_point()+\n      stat_smooth(method = \"lm\", se = T)+\n      geom_hline(yintercept = mean(log(UN11$fertility),  base =7), color = \"red\")+\n      labs(title=\"Scatterplot of log(fertility) by log(ppgdp), base =7 \")\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\nQuestion 2\nAnnual income, in dollars, is an explanatory variable in a regression analysis. For a British version of the report on the analysis, all responses are converted to British pounds sterling (1 pound equals about 1.33 dollars, as of 2016).\n(a) How, if at all, does the slope of the prediction equation change?\n(b) How, if at all, does the correlation change?\nTo answer these questions, I will run linear regressions for the ppgpd in dollars and pounds and compare results.\n\n\nCode\nUN11$ppgdp.pound<-UN11$ppgdp*1.33\nfit1<- lm(lifeExpF ~ ppgdp, data = UN11)\nsummary(fit1)\n\n\n\nCall:\nlm(formula = lifeExpF ~ ppgdp, data = UN11)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-20.825  -4.889   2.618   6.619  11.299 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 6.837e+01  7.370e-01  92.762   <2e-16 ***\nppgdp       3.018e-04  3.274e-05   9.218   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.483 on 197 degrees of freedom\nMultiple R-squared:  0.3014,    Adjusted R-squared:  0.2978 \nF-statistic: 84.98 on 1 and 197 DF,  p-value: < 2.2e-16\n\n\nCode\nfit2<- lm(lifeExpF ~ ppgdp.pound, data = UN11)\nsummary(fit2)\n\n\n\nCall:\nlm(formula = lifeExpF ~ ppgdp.pound, data = UN11)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-20.825  -4.889   2.618   6.619  11.299 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 6.837e+01  7.370e-01  92.762   <2e-16 ***\nppgdp.pound 2.269e-04  2.462e-05   9.218   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.483 on 197 degrees of freedom\nMultiple R-squared:  0.3014,    Adjusted R-squared:  0.2978 \nF-statistic: 84.98 on 1 and 197 DF,  p-value: < 2.2e-16\n\n\nVisualizing the model with both variables (ppgdp and ppgdp.pound ) produces the same model:\n\n\nCode\nggplot( data=UN11, mapping=aes(x= ppgdp , y=lifeExpF))+\n  geom_point() +\n  geom_smooth(method = 'lm')+\n      labs(title=\"Scatterplot of Female life expectancy by annual income \", x=\"PPGDP in US dollars\", y=\"Life expectancy\")\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nCode\nggplot( data=UN11, mapping=aes(x= ppgdp.pound , y=lifeExpF))+\n  geom_point() +\n  geom_smooth(method = 'lm')+\n      labs(title=\"Scatterplot of Female life expectancy by annual income \", x=\"PPGDP in British pounds\", y=\"Life expectancy\")\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nVisualizing logged variables ( for the US dollars and British pounds) :\n\n\nCode\nggplot( data=UN11, mapping=aes(x= log(ppgdp) , y=lifeExpF   )     )+\n  geom_point() +\n  geom_smooth(method = 'lm')+\n      labs(title=\"Scatterplot of Female life expectancy by annual income \", x=\"Logged PPGDP in US dollars\", y=\"Life expectancy\")\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nCode\nggplot( data=UN11, mapping=aes(x= log(ppgdp.pound) , y=lifeExpF    )     )+\n  geom_point() +\n  geom_smooth(method = 'lm')+\n      labs(title=\"Scatterplot of Female life expectancy by annual income \", x=\"Logged PPGDP in British pounds\", y=\"Life expectancy\")\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nThere is no change in slope and correlation between variablle in US dollars and British pounds.\n\n\nCode\nfit_logged<- lm(lifeExpF~ log(ppgdp), data = UN11)\n\nsummary(fit_logged)\n\n\n\nCall:\nlm(formula = lifeExpF ~ log(ppgdp), data = UN11)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-25.749  -2.879   1.280   3.987  12.345 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  29.8148     2.5314   11.78   <2e-16 ***\nlog(ppgdp)    5.0188     0.2942   17.06   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.448 on 197 degrees of freedom\nMultiple R-squared:  0.5964,    Adjusted R-squared:  0.5943 \nF-statistic: 291.1 on 1 and 197 DF,  p-value: < 2.2e-16\n\n\nHowever, using log() function on x variable allowed us to build a better fitted model with R^2 increased from 0.30 to 0.60. I can conclude that 1% change in PPGDP correspons with 5 % increase in Life expectancy.\n\n\nQuestion 3\nWater runoff in the Sierras (Data file: water in alr4) Can Southern California’s water supply in future years be predicted from past data? One factor affecting water availability is stream runoff. If runoff could be predicted, engineers, planners, and policy makers could do their jobs\nmore efficiently. The data file contains 43 years’ worth of precipitation measurements taken at six sites in the Sierra Nevada mountains (labeled APMAM, APSAB, APSLAKE, OPBPC, OPRC, and OPSLAKE) and stream runoff volume at a site near Bishop, California, labeled BSAAM. Draw the scatterplot matrix for these data and summarize the information available from these plots. (Hint: Use the pairs() function.)\n\n\nCode\nlibrary(alr4)\ndata(water)\nstr(water)\n\n\n'data.frame':   43 obs. of  8 variables:\n $ Year   : int  1948 1949 1950 1951 1952 1953 1954 1955 1956 1957 ...\n $ APMAM  : num  9.13 5.28 4.2 4.6 7.15 9.7 5.02 6.7 10.5 9.1 ...\n $ APSAB  : num  3.58 4.82 3.77 4.46 4.99 5.65 1.45 7.44 5.85 6.13 ...\n $ APSLAKE: num  3.91 5.2 3.67 3.93 4.88 4.91 1.77 6.51 3.38 4.08 ...\n $ OPBPC  : num  4.1 7.55 9.52 11.14 16.34 ...\n $ OPRC   : num  7.43 11.11 12.2 15.15 20.05 ...\n $ OPSLAKE: num  6.47 10.26 11.35 11.13 22.81 ...\n $ BSAAM  : int  54235 67567 66161 68094 107080 67594 65356 67909 92715 70024 ...\n\n\nCode\nhead(water)\n\n\n  Year APMAM APSAB APSLAKE OPBPC  OPRC OPSLAKE  BSAAM\n1 1948  9.13  3.58    3.91  4.10  7.43    6.47  54235\n2 1949  5.28  4.82    5.20  7.55 11.11   10.26  67567\n3 1950  4.20  3.77    3.67  9.52 12.20   11.35  66161\n4 1951  4.60  4.46    3.93 11.14 15.15   11.13  68094\n5 1952  7.15  4.99    4.88 16.34 20.05   22.81 107080\n6 1953  9.70  5.65    4.91  8.88  8.15    7.41  67594\n\n\nCode\nwater$Year <-as.factor(water$Year)\nlevels(water$Year)\n\n\n [1] \"1948\" \"1949\" \"1950\" \"1951\" \"1952\" \"1953\" \"1954\" \"1955\" \"1956\" \"1957\"\n[11] \"1958\" \"1959\" \"1960\" \"1961\" \"1962\" \"1963\" \"1964\" \"1965\" \"1966\" \"1967\"\n[21] \"1968\" \"1969\" \"1970\" \"1971\" \"1972\" \"1973\" \"1974\" \"1975\" \"1976\" \"1977\"\n[31] \"1978\" \"1979\" \"1980\" \"1981\" \"1982\" \"1983\" \"1984\" \"1985\" \"1986\" \"1987\"\n[41] \"1988\" \"1989\" \"1990\"\n\n\nCode\npairs(subset (water, select = c(APMAM : BSAAM)))\n\n\n\n\n\nFrom the scatter plot matrix above I can see that some sources of water well correlated to each other, as their scatter plots are very close to a straight line. Moreover, there are two groups of water sources that demonstrate being inter-connected. The first group is APMAM, APSAB and APSLAKE. The second group is OPBPC, OPRC, OPLAKE and BSAAM. It is possible that the sites within a grouop share a water source or separated from each other (may be by mountains).\n\n\nCode\nhead(water)\n\n\n  Year APMAM APSAB APSLAKE OPBPC  OPRC OPSLAKE  BSAAM\n1 1948  9.13  3.58    3.91  4.10  7.43    6.47  54235\n2 1949  5.28  4.82    5.20  7.55 11.11   10.26  67567\n3 1950  4.20  3.77    3.67  9.52 12.20   11.35  66161\n4 1951  4.60  4.46    3.93 11.14 15.15   11.13  68094\n5 1952  7.15  4.99    4.88 16.34 20.05   22.81 107080\n6 1953  9.70  5.65    4.91  8.88  8.15    7.41  67594\n\n\nCode\npairs(subset (water, select = c(APMAM : APSLAKE)))\n\n\n\n\n\n\n\nCode\nhead(water)\n\n\n  Year APMAM APSAB APSLAKE OPBPC  OPRC OPSLAKE  BSAAM\n1 1948  9.13  3.58    3.91  4.10  7.43    6.47  54235\n2 1949  5.28  4.82    5.20  7.55 11.11   10.26  67567\n3 1950  4.20  3.77    3.67  9.52 12.20   11.35  66161\n4 1951  4.60  4.46    3.93 11.14 15.15   11.13  68094\n5 1952  7.15  4.99    4.88 16.34 20.05   22.81 107080\n6 1953  9.70  5.65    4.91  8.88  8.15    7.41  67594\n\n\nCode\npairs(subset (water, select = c(OPBPC : BSAAM)))\n\n\n\n\n\n\n\nQuestion 4\nProfessor ratings (Data file: Rateprof in alr4) In the website and online forum RateMyProfessors.com, students rate and comment on their instructors. Launched in 1999, the site includes millions of ratings on thousands of instructors. The data file includes the summaries of the ratings of 364 instructors at a large campus in the Midwest (Bleske-Rechek and Fritsch, 2011). Each instructor included in the data had at least 10 ratings over a several year period. Students provided ratings of 1–5 on quality, helpfulness, clarity, easiness of instructor’s courses, and rater Interest in the subject matter covered in the instructor’s courses. The data file provides the averages of these five ratings. Create a scatter plot matrix of these five variables. Provide a brief description of the relationships between the five ratings.\n\n\nCode\ndata(Rateprof)\nstr(Rateprof)\n\n\n'data.frame':   366 obs. of  17 variables:\n $ gender         : Factor w/ 2 levels \"female\",\"male\": 2 2 2 2 2 2 2 2 2 2 ...\n $ numYears       : int  7 6 10 11 11 10 7 11 11 7 ...\n $ numRaters      : int  11 11 43 24 19 15 17 16 12 18 ...\n $ numCourses     : int  5 5 2 5 7 9 3 3 4 4 ...\n $ pepper         : Factor w/ 2 levels \"no\",\"yes\": 1 1 1 1 1 1 1 1 1 1 ...\n $ discipline     : Factor w/ 4 levels \"Hum\",\"SocSci\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ dept           : Factor w/ 48 levels \"Accounting\",\"Anthropology\",..: 17 42 3 17 45 45 45 17 34 17 ...\n $ quality        : num  4.64 4.32 4.79 4.25 4.68 ...\n $ helpfulness    : num  4.64 4.55 4.72 4.46 4.68 ...\n $ clarity        : num  4.64 4.09 4.86 4.04 4.68 ...\n $ easiness       : num  4.82 4.36 4.6 2.79 4.47 ...\n $ raterInterest  : num  3.55 4 3.43 3.18 4.21 ...\n $ sdQuality      : num  0.552 0.902 0.453 0.933 0.65 ...\n $ sdHelpfulness  : num  0.674 0.934 0.666 0.932 0.82 ...\n $ sdClarity      : num  0.505 0.944 0.413 0.999 0.582 ...\n $ sdEasiness     : num  0.405 0.505 0.541 0.588 0.612 ...\n $ sdRaterInterest: num  1.128 1.074 1.237 1.332 0.975 ...\n\n\nCode\nhead(Rateprof)\n\n\n  gender numYears numRaters numCourses pepper discipline              dept\n1   male        7        11          5     no        Hum           English\n2   male        6        11          5     no        Hum Religious Studies\n3   male       10        43          2     no        Hum               Art\n4   male       11        24          5     no        Hum           English\n5   male       11        19          7     no        Hum           Spanish\n6   male       10        15          9     no        Hum           Spanish\n   quality helpfulness  clarity easiness raterInterest sdQuality sdHelpfulness\n1 4.636364    4.636364 4.636364 4.818182      3.545455 0.5518564     0.6741999\n2 4.318182    4.545455 4.090909 4.363636      4.000000 0.9020179     0.9341987\n3 4.790698    4.720930 4.860465 4.604651      3.432432 0.4529343     0.6663898\n4 4.250000    4.458333 4.041667 2.791667      3.181818 0.9325048     0.9315329\n5 4.684211    4.684211 4.684211 4.473684      4.214286 0.6500112     0.8200699\n6 4.233333    4.266667 4.200000 4.533333      3.916667 0.8632717     1.0327956\n  sdClarity sdEasiness sdRaterInterest\n1 0.5045250  0.4045199       1.1281521\n2 0.9438798  0.5045250       1.0744356\n3 0.4129681  0.5407021       1.2369438\n4 0.9990938  0.5882300       1.3322506\n5 0.5823927  0.6117753       0.9749613\n6 0.7745967  0.6399405       0.6685579\n\n\nCode\npairs(subset (Rateprof, select =c(quality : raterInterest)))\n\n\n\n\n\nAs we can see from the scatter plot above, three variables are clearly highly correlated: quality, helpfulness and clarity. Particularly, quality highly correlated with both helpfulness and clarity. I suspect, these variables might create multicollinearity problem if used together in multiple regression model.\nConnection between clarity and helpfulness is also very distinct, but the variability seems bigger.\nEasiness variable appears positively correlated with the first three (quality, helpfulness and clarity), however there is much higher variability.\nRater’s interest appears less connected with the other variables, and might demonstrate a smaller slope of a regression line when calculated.\n\n\n\nQuestion 5\nFor the student.survey data file in the smss package, conduct regression analyses relating (by convention, y denotes the outcome variable, x denotes the explanatory variable)\n(i) y = political ideology and x = religiosity,\n(ii) y = high school GPA and x = hours of TV watching. (You can use ?student.survey in the R console, after loading the package, to see what each variable means.)\n(a) Graphically portray how the explanatory variable relates to the outcome variable in each of the two cases\n(b) Summarize and interpret results of inferential analyses.\n\ni. y = political ideology and x = religiosity\nBy using ?student.survey function, I found out that : Political ideology is stored as PI variable, Religiosity doesn’t have its own variable, but can be represented as “how often you attend religious services” from this data set, variable RE\nTherefore , my model can be presented as\n\\[\nPI =\\beta_o +\\beta_1*RE\n\\]\n\n\nCode\ndata(student.survey)\n?student.survey\n\n\nstarting httpd help server ... done\n\n\nCode\nstr(student.survey)\n\n\n'data.frame':   60 obs. of  18 variables:\n $ subj: int  1 2 3 4 5 6 7 8 9 10 ...\n $ ge  : Factor w/ 2 levels \"f\",\"m\": 2 1 1 1 2 2 2 1 2 2 ...\n $ ag  : int  32 23 27 35 23 39 24 31 34 28 ...\n $ hi  : num  2.2 2.1 3.3 3.5 3.1 3.5 3.6 3 3 4 ...\n $ co  : num  3.5 3.5 3 3.2 3.5 3.5 3.7 3 3 3.1 ...\n $ dh  : int  0 1200 1300 1500 1600 350 0 5000 5000 900 ...\n $ dr  : num  5 0.3 1.5 8 10 3 0.2 1.5 2 2 ...\n $ tv  : num  3 15 0 5 6 4 5 5 7 1 ...\n $ sp  : int  5 7 4 5 6 5 12 3 5 1 ...\n $ ne  : int  0 5 3 6 3 7 4 3 3 2 ...\n $ ah  : int  0 6 0 3 0 0 2 1 0 1 ...\n $ ve  : logi  FALSE FALSE FALSE FALSE FALSE FALSE ...\n $ pa  : Factor w/ 3 levels \"d\",\"i\",\"r\": 3 1 1 2 2 1 2 2 2 2 ...\n $ pi  : Ord.factor w/ 7 levels \"very liberal\"<..: 6 2 2 4 1 2 2 2 1 3 ...\n $ re  : Ord.factor w/ 4 levels \"never\"<\"occasionally\"<..: 3 2 3 2 1 2 2 2 2 1 ...\n $ ab  : logi  FALSE FALSE FALSE FALSE FALSE FALSE ...\n $ aa  : logi  FALSE FALSE FALSE FALSE FALSE FALSE ...\n $ ld  : logi  FALSE NA NA FALSE FALSE NA ...\n\n\nCode\n# head(student.survey)\nclass(student.survey$pi)\n\n\n[1] \"ordered\" \"factor\" \n\n\nCode\nlevels(student.survey$pi)\n\n\n[1] \"very liberal\"          \"liberal\"               \"slightly liberal\"     \n[4] \"moderate\"              \"slightly conservative\" \"conservative\"         \n[7] \"very conservative\"    \n\n\nCode\nclass(student.survey$re)\n\n\n[1] \"ordered\" \"factor\" \n\n\nWe can see that both variables are ordered factor variables. We will need to convert factor variables into the numeric variables for regression, and then use them in linear model:\n\n\nCode\nstudent.survey$pi <-as.numeric (student.survey$pi)\nstudent.survey$re <-as.numeric (student.survey$re)\nsummary(lm(pi~re, data=student.survey))\n\n\n\nCall:\nlm(formula = pi ~ re, data = student.survey)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.81243 -0.87160  0.09882  1.12840  3.09882 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   0.9308     0.4252   2.189   0.0327 *  \nre            0.9704     0.1792   5.416 1.22e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.345 on 58 degrees of freedom\nMultiple R-squared:  0.3359,    Adjusted R-squared:  0.3244 \nF-statistic: 29.34 on 1 and 58 DF,  p-value: 1.221e-06\n\n\nCode\nggplot(data = student.survey, aes(x = re, y = pi)) +\n  geom_point() +\n  geom_smooth(method = 'lm', se=F)+\n  labs(title=\"Political ideology and religiosity\", x=\"Frequency of going to church\", y = \"level of consrvatism\")\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nBased on the summary above, we can see that religiosity and political ideology are positively related, but the religiosity only contributes about 30 % of political ideology (R^2 =0.34). Overal w can say that higher level of conservatism can be somewhat (30%) predicted by frequency of church attendance.\n\n\nii. y = high school GPA and x = hours of TV watching\nHigh school GPA is presented as HI variable, and hours of TV watching - by TV variable.\n\n\nCode\nclass(student.survey$hi)\n\n\n[1] \"numeric\"\n\n\nCode\nclass(student.survey$tv)\n\n\n[1] \"numeric\"\n\n\nBoth variables are numeric, which allows us to run a simple linear regression:\n\n\nCode\nsummary(lm(data=student.survey, hi ~tv))\n\n\n\nCall:\nlm(formula = hi ~ tv, data = student.survey)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.2583 -0.2456  0.0417  0.3368  0.7051 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  3.441353   0.085345  40.323   <2e-16 ***\ntv          -0.018305   0.008658  -2.114   0.0388 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4467 on 58 degrees of freedom\nMultiple R-squared:  0.07156,   Adjusted R-squared:  0.05555 \nF-statistic: 4.471 on 1 and 58 DF,  p-value: 0.03879\n\n\nWe can see that the the slope of Independent variable (tv) is very small and negative. It’s statistical significance is also very low and depends on alpha level that we select. R squared, which indicates the proportion of the variance that can be explained by x (tv watching time), is very low (0.072). Taking this into account, I conclude there is no contribution of TV watching to GPA level.\nTo visualize these relationship, we can build a scatter plot and a fitted linear line:\n\n\nCode\nggplot (data=student.survey, mapping=aes(x=tv, y=hi))+\n  geom_point()+\n  stat_smooth(method = \"lm\", se = T)+\n  labs(title=\"Sctterplot between hours of TV watching and high school GPA\", y=\"High school GPA\", x=\"hours of TV watching\")\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nFrom the graph above we can see, that the standard error band is getting wider towards the right side of the chart, due to low amount of observations in that part of the sample. It also adds uncertainty to a conclusion and calls for more data and further investigation. Since the overall slope does go down, I would suggest that there may be another factor that contributes to the relationship between TV watching and GPA."
  },
  {
    "objectID": "posts/FPchck_young.html",
    "href": "posts/FPchck_young.html",
    "title": "Final Project Check In",
    "section": "",
    "text": "Spring is the season when baseball starts. In Korea, baseball is also the most popular league that attracts the largest number of spectators among all professional sports, and I have been a big fan since I was young. As you know, baseball is a sport with countless numbers, and this data analysis is actually done a lot. I can’t think of an interesting way to use and learn various statistical techniques in the subject of quantity analysis than using the sport of baseball. Therefore, I chose baseball as the my final project topic."
  },
  {
    "objectID": "posts/FPchck_young.html#key-concepts-and-operational-definitions",
    "href": "posts/FPchck_young.html#key-concepts-and-operational-definitions",
    "title": "Final Project Check In",
    "section": "Key Concepts and Operational Definitions",
    "text": "Key Concepts and Operational Definitions\nA baseball team’s score is simply available. However, it is necessary to first determine how to define the manager’s intervention. There are various roles that managers can play in baseball’s offense. They include determining the batting order, using pinch-hitters, bunting, etc. Although it has not been confirmed yet, the director’s intervention in this project will be determined by the number of attempts to steal and bunt in consideration of the ease of obtaining data. In summary, the independent variable is the baseball manager’s intervention in the game(offense), and the dependent variable is the team’s score."
  },
  {
    "objectID": "posts/FPchck_young.html#how-to-test-hypothesis",
    "href": "posts/FPchck_young.html#how-to-test-hypothesis",
    "title": "Final Project Check In",
    "section": "How to test hypothesis",
    "text": "How to test hypothesis\nThere are many ways to do this. First of all, it is possible to simply compare the average score between teams with a lot of manager intervention and teams with less manager intervention. However, this simple comparison may not take into account the team’s differences in offensive power. In other words, if a team with strong offensive power (i.e., a team with the ability to score more points without the manager’s intervention) had more coach intervention, the conclusion could be distorted. Therefore, considering this, a method of obtaining the expected score level of each team using a regression model and comparing how much the actual score was can be used."
  },
  {
    "objectID": "posts/AdithyaParupudi_hw2.html",
    "href": "posts/AdithyaParupudi_hw2.html",
    "title": "Homework 2",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.1     ✔ purrr   0.3.4\n✔ tibble  3.1.8     ✔ dplyr   1.0.9\n✔ tidyr   1.2.1     ✔ stringr 1.4.1\n✔ readr   2.1.3     ✔ forcats 0.5.2\n\n\nWarning: package 'ggplot2' was built under R version 4.2.2\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\nCode\nlibrary(hrbrthemes)\n\n\nNOTE: Either Arial Narrow or Roboto Condensed fonts are required to use these themes.\n      Please use hrbrthemes::import_roboto_condensed() to install Roboto Condensed and\n      if Arial Narrow is not on your system, please see https://bit.ly/arialnarrow\n\n\nCode\nlibrary(viridis)\n\n\nWarning: package 'viridis' was built under R version 4.2.2\n\n\nLoading required package: viridisLite\n\n\nCode\nlibrary(readxl)"
  },
  {
    "objectID": "posts/AdithyaParupudi_hw2.html#a",
    "href": "posts/AdithyaParupudi_hw2.html#a",
    "title": "Homework 2",
    "section": "4A",
    "text": "4A\nWe assume that the sample is random and that the population has a normal distribution. Null hypothesis: H0: μ = 500 Alternative hypothesis: Ha: μ ≠ 500 We will reject the null hypothesis at a p-value, p <= 0.05\n\n\nCode\n# defining variables\ns_mean <- 410\nμ <- 500\ns_sd <- 90\ns_size <- 9\n\n\n\n\nCode\n# Calculating test-statistic\nt_score <- (s_mean-μ)/(s_sd/sqrt(s_size))\nt_score\n\n\n[1] -3\n\n\n\n\nCode\n# Calculating p-value\n\np <- 2*pt(t_score, s_size-1)\np\n\n\n[1] 0.01707168\n\n\nThe test-statistic is -3 and p-value is 0.01707168. As p-value is less than the 0.05, we reject the null hypothesis. Therefore, the mean income of female employees is not equal to $500."
  },
  {
    "objectID": "posts/AdithyaParupudi_hw2.html#b",
    "href": "posts/AdithyaParupudi_hw2.html#b",
    "title": "Homework 2",
    "section": "4B",
    "text": "4B\nWe assume that the sample is random and that the population has a normal distribution. Null hypothesis: H0: μ = 500 Alternative hypothesis: Ha: μ < 500 We will reject the null hypothesis at a p-value less than 0.05\n\n\nCode\np_val <- pt(t_score, s_size-1, lower.tail = TRUE)\np_val\n\n\n[1] 0.008535841\n\n\nThe p-value is 0.008535841. As p-value is less than the 0.05, we reject the null hypothesis. Therefore, the mean income of female employees is less than $500."
  },
  {
    "objectID": "posts/AdithyaParupudi_hw2.html#c",
    "href": "posts/AdithyaParupudi_hw2.html#c",
    "title": "Homework 2",
    "section": "4C",
    "text": "4C\nWe assume that the sample is random and that the population has a normal distribution. Null hypothesis: H0: μ = 500 Alternative hypothesis: Ha: μ > 500 We will reject the null hypothesis at a p-value less than 0.05\n\n\nCode\np_val_c <- pt(t_score, s_size-1, lower.tail = FALSE)\np_val_c\n\n\n[1] 0.9914642\n\n\nThe p-value is 0.9914642. As p-value is less than the 0.05, we reject the null hypothesis. Therefore, the mean income of female employees is greater than $500."
  },
  {
    "objectID": "posts/AdithyaParupudi_hw2.html#a-1",
    "href": "posts/AdithyaParupudi_hw2.html#a-1",
    "title": "Homework 2",
    "section": "5A",
    "text": "5A\nWe assume that the sample is random and that the population has a normal distribution. Null hypothesis: H0: μ = 500 Alternative hypothesis: Ha: μ ≠ 500 We will reject the null hypothesis at a p-value less than 0.05\n\n\nCode\n# Calculating t-statistic and p-value for Jones\ns_mean <- 519.5\nμ <- 500\nse <- 10\ns_size <- 1000\n\njames_t <- (s_mean-μ)/se\njames_t\n\n\n[1] 1.95\n\n\nCode\np <- 2*pt(james_t, s_size-1, lower.tail = FALSE)\np\n\n\n[1] 0.05145555\n\n\n\n\nCode\n# Calculating t-statistic and p-value for Smith\ns_mean <- 519.7\nμ <- 500\nse <- 10\ns_size <- 1000\n\nsmith_t <- (s_mean-μ)/se\nsmith_t\n\n\n[1] 1.97\n\n\nCode\np <- 2*pt(smith_t, s_size-1, lower.tail = FALSE)\np\n\n\n[1] 0.04911426\n\n\nThe test-statistic is 1.95, p-value is 0.05145555 for Jones and the test-statistic is 1.97, p-value is 0.05145555 for Smith."
  },
  {
    "objectID": "posts/AdithyaParupudi_hw2.html#b-1",
    "href": "posts/AdithyaParupudi_hw2.html#b-1",
    "title": "Homework 2",
    "section": "5B",
    "text": "5B\nThe p-value is 0.05145555 for Jones. As p-value is greater than the 0.05, we fail to reject the null hypothesis. The p-value is 0.04911426 for Jones. As p-value is less than the 0.05, we reject the null hypothesis. Therefore, the result is statistically significant for Smith, but not Jones."
  },
  {
    "objectID": "posts/AdithyaParupudi_hw2.html#c-1",
    "href": "posts/AdithyaParupudi_hw2.html#c-1",
    "title": "Homework 2",
    "section": "5C",
    "text": "5C\nIf we fail to report the P-value and simply state whether the P-value is less than/equal to or greater than the defined significance level of the test, one cannot determine the strength of the conclusion. In the Jones/Smith example, reporting the results only as P ≤ 0.05 versus P > 0.05 will lead to different conclusions about very similar results."
  },
  {
    "objectID": "posts/FelixBetancourt_HW1_v2.html",
    "href": "posts/FelixBetancourt_HW1_v2.html",
    "title": "Homework 1",
    "section": "",
    "text": "Code\nknitr::opts_chunk$set(echo = TRUE, warning = FALSE)"
  },
  {
    "objectID": "posts/FelixBetancourt_HW1_v2.html#homework-1",
    "href": "posts/FelixBetancourt_HW1_v2.html#homework-1",
    "title": "Homework 1",
    "section": "Homework 1",
    "text": "Homework 1\nDACSS 603, Spring 2023\n\n\nCode\n# Loading packages\n\nsuppressPackageStartupMessages(library(dplyr))\nsuppressPackageStartupMessages(library(tidyverse))\nlibrary(formattable)\nsuppressPackageStartupMessages(library(kableExtra))\nlibrary(ggplot2)\nlibrary(readxl)\n\n# Setting working directory and loading dataset.\n\n\n\nlung <- read_excel(\"_data/LungCapData.xls\")"
  },
  {
    "objectID": "posts/FelixBetancourt_HW1_v2.html#part-a---lung-capacity-dataset",
    "href": "posts/FelixBetancourt_HW1_v2.html#part-a---lung-capacity-dataset",
    "title": "Homework 1",
    "section": "Part A - Lung Capacity Dataset",
    "text": "Part A - Lung Capacity Dataset\nLet’s first explore the database\nStructure\n\n\nCode\nstr(lung)\n\n\ntibble [725 × 6] (S3: tbl_df/tbl/data.frame)\n $ LungCap  : num [1:725] 6.47 10.12 9.55 11.12 4.8 ...\n $ Age      : num [1:725] 6 18 16 14 5 11 8 11 15 11 ...\n $ Height   : num [1:725] 62.1 74.7 69.7 71 56.9 58.7 63.3 70.4 70.5 59.2 ...\n $ Smoke    : chr [1:725] \"no\" \"yes\" \"no\" \"no\" ...\n $ Gender   : chr [1:725] \"male\" \"female\" \"female\" \"male\" ...\n $ Caesarean: chr [1:725] \"no\" \"no\" \"yes\" \"no\" ...\n\n\nSummary\n\n\nCode\nLC_table1 <- summary(lung)\nprint(LC_table1)\n\n\n    LungCap            Age            Height         Smoke          \n Min.   : 0.507   Min.   : 3.00   Min.   :45.30   Length:725        \n 1st Qu.: 6.150   1st Qu.: 9.00   1st Qu.:59.90   Class :character  \n Median : 8.000   Median :13.00   Median :65.40   Mode  :character  \n Mean   : 7.863   Mean   :12.33   Mean   :64.84                     \n 3rd Qu.: 9.800   3rd Qu.:15.00   3rd Qu.:70.30                     \n Max.   :14.675   Max.   :19.00   Max.   :81.80                     \n    Gender           Caesarean        \n Length:725         Length:725        \n Class :character   Class :character  \n Mode  :character   Mode  :character  \n                                      \n                                      \n                                      \n\n\n\na) What does the distribution of LungCap look like?\n\n\nCode\n# Density histogram\nhist(lung$LungCap, freq = FALSE, col=\"white\", main = \"Histogram of Probability Density for Lung Capacity\", xlab = \"Values\", ylab = \"Density\")\nlines(density(lung$LungCap), col = \"blue\", lwd = 2)\n\n\n\n\n\nDistribution looks like a normal distribution, most of the observation close to the mean and a few cases close to the extrems (0.5 and 14.6).\n\n\nb) Compare the probability distribution of the LungCap with respect to Males and Females? (Hint: make boxplots separated by gender using the boxplot() function)\n\n\nCode\nbox_plot_crop<-ggplot(data=lung, aes(x=Gender, y=LungCap, fill=Gender)) \nbox_plot_crop+ geom_boxplot() +\n  theme(legend.position = \"right\") +\n  theme (axis.text.x=element_blank(),\n        axis.ticks.x=element_blank())+\n  coord_cartesian(ylim =  c(0, 15))+\n  labs(title=\"Box Plot - Lung Capacity\",\n        x =\"Gender\", y = \"Frequency\")\n\n\n\n\n\n\n\nCode\nsuppressPackageStartupMessages (library(hrbrthemes))\nsuppressPackageStartupMessages (library(viridis))\n\nGender_d <- ggplot(data=lung, aes(x=LungCap, group=Gender, fill=Gender)) +\n    geom_density(adjust=1.5, alpha=.4) +\n    theme_ipsum()\nGender_d\n\n\n\n\n\n\n\nCode\nLC_table <- lung %>%  \n group_by(Gender) %>%\n  summarise(N = n(), LC.Mean = mean(LungCap, na.rm=TRUE), LC.Median = median(LungCap, na.rm=TRUE), LC.SD = sd(LungCap, na.rm=TRUE))\n  \nLC_table_o <- LC_table[with (LC_table, order(-LC.Mean)),]\nformattable(LC_table_o) %>% \n  kable(\"html\", escape = F, caption = \"Summary of Lung Capacity Grouped by Gender\", align = c(\"l\", \"c\", \"c\", \"c\", \"c\", \"c\")) %>% \n  kable_classic(full_width = F, html_font = \"Cambria\")\n\n\n\n\nSummary of Lung Capacity Grouped by Gender\n \n  \n    Gender \n    N \n    LC.Mean \n    LC.Median \n    LC.SD \n  \n \n\n  \n    male \n    367 \n    8.309332 \n    8.35 \n    2.683238 \n  \n  \n    female \n    358 \n    7.405746 \n    7.75 \n    2.564242 \n  \n\n\n\n\n\nIt was not possible for me to plot a Box Plot with Density like I did for Histogram but I was able to plot separately the Box Plot and Density. It seems that the distribution for the two genders are similar while the Male distribution shows a bit higher average lung capacity.\n\n\nc. Compare the mean lung capacities for smokers and non-smokers. Does it make sense?\nLet’s visualize the differences\n\n\nCode\nbox_plot_crop <- ggplot(data=lung, aes(x=Smoke, y=LungCap, fill=Smoke)) \nbox_plot_crop+ geom_boxplot() +\n  theme(legend.position = \"right\") +\n  theme (axis.text.x=element_blank(),\n        axis.ticks.x=element_blank())+\n  coord_cartesian(ylim =  c(0, 15))+\n  labs(title=\"Box Plot - Lung Capacity\",\n        x =\"Smoke\", y = \"Density\")\n\n\n\n\n\nNow let’s see the specific numbers in this summary\n\n\nCode\nLC_table_s <- lung %>%  \n group_by(Smoke) %>%\n  summarise(N = n(), LC.Mean = mean(LungCap, na.rm=TRUE), LC.Median = median(LungCap, na.rm=TRUE), LC.SD = sd(LungCap, na.rm=TRUE))\n\nLC_table_s2 <- LC_table_s[with (LC_table_s, order(-LC.Mean)),]\nformattable(LC_table_s2) %>% \n  kable(\"html\", escape = F, caption = \"Summary of Lung Capacity Grouped by Smoker\", align = c(\"l\", \"c\", \"c\", \"c\", \"c\", \"c\")) %>% \n  kable_classic(full_width = F, html_font = \"Cambria\")\n\n\n\n\nSummary of Lung Capacity Grouped by Smoker\n \n  \n    Smoke \n    N \n    LC.Mean \n    LC.Median \n    LC.SD \n  \n \n\n  \n    yes \n    77 \n    8.645454 \n    8.65 \n    1.882894 \n  \n  \n    no \n    648 \n    7.770188 \n    7.90 \n    2.726113 \n  \n\n\n\n\n\nSeems that smokers has more lung capacity than non-smokers, and I would expect the opposite. It is interesting to note that the lung capacity in smokers is more homogeneous distribution (lower SD) compared to non-smokers, even though the number of smokers is significantly smaller than non-smokers.\n\n\nd. Examine the relationship between Smoking and Lung Capacity within age groups: “less than or equal to 13”, “14 to 15”, “16 to 17”, and “greater than or equal to 18”.\n\n\ne. Compare the lung capacities for smokers and non-smokers within each age group. Is your answer different from the one in part c. What could possibly be going on here?\n\n\nCode\n# First, let's create the categories for age\n\nlung2 <-lung %>%\n  mutate(Age_Cat = case_when(\n        Age >= 0 & Age <= 13 ~ \"13 or less\",\n        Age >= 14 & Age <= 15 ~ \"14 to 15\" ,\n        Age >= 16 & Age <= 17 ~ \"16 to 17\" ,\n        Age >= 18 ~ \"18 or more\" ,\n        ))\n\nbox_plot_crop2<-ggplot(data=lung2, aes(x=Smoke, y=LungCap, fill=Smoke)) \nbox_plot_crop2+ geom_boxplot() +\n  theme(legend.position = \"right\") +\n  theme (axis.text.x=element_blank(),\n        axis.ticks.x=element_blank())+\n  coord_cartesian(ylim =  c(0, 15))+\n  labs(title=\"Box Plot - Lung Capacity\",\n        x =\"Smoke\", y = \"Density\")+\n  facet_wrap(.~Age_Cat, scales= \"free\")\n\n\n\n\n\nFrom this graph I can note the following:\n\nExcept for the group of 13 or less, non-smokers shows higher lung capacity compared with smokers.\nOn the other hand, seems that the Lung Capacity increases with the years regardless the smoke condition.\n\nLet’s see more detailed numbers. I want to see the Lung Capacity Means and dispersion for each group. for this we will do a crosstab showing n, mean, median and sd.\nFirst let’s see the frequencies for both variables in a crosstab.\n\n\nCode\nxtabs(~Age_Cat+Smoke, data=lung2)\n\n\n            Smoke\nAge_Cat       no yes\n  13 or less 401  27\n  14 to 15   105  15\n  16 to 17    77  20\n  18 or more  65  15\n\n\nMean Lung Capacity by Age Group and Smoke condition\n\n\nCode\nwith(lung2, tapply(LungCap, list(Age_Group=Age_Cat,Smoker=Smoke), mean) )\n\n\n            Smoker\nAge_Group           no       yes\n  13 or less  6.358746  7.201852\n  14 to 15    9.138810  8.391667\n  16 to 17   10.469805  9.383750\n  18 or more 11.068846 10.513333\n\n\nMedian Lung Capacity by Age Group and Smoke condition\n\n\nCode\nwith(lung2, tapply(LungCap, list(Age_Group=Age_Cat,Smoker=Smoke), median) )\n\n\n            Smoker\nAge_Group        no    yes\n  13 or less  6.575  7.025\n  14 to 15    9.000  8.475\n  16 to 17   10.600  9.550\n  18 or more 10.850 10.475\n\n\nStandard Deviation Lung Capacity by Age Group and Smoke condition\n\n\nCode\nwith(lung2, tapply(LungCap, list(Age_Group=Age_Cat,Smoker=Smoke), sd) )\n\n\n            Smoker\nAge_Group          no      yes\n  13 or less 2.214412 1.577728\n  14 to 15   1.546130 1.437497\n  16 to 17   1.536745 1.326136\n  18 or more 1.555139 1.250959\n\n\nClearly the lung capacity is higher for non smokers within each age group except 13 or less years old group, where the smoker’s mean (and median) is higher than non-smokers.\nAs noted before it is also clear that the lung capacity increases with the age regardless the smoke condition.\nHowever it is interesting to note that:\n\nThe number of cases in the group 13 or less is aout the 50% of the whole sample. So it is explaining the findings in the question part “c” above (higher overall Lung Capacity average for smokers).\nThe lung capacity makes a bigger jump for non-smokers from 13 or less to the next bracket (14-15 years) compared to smokers. In other words, non-smokers close the gap and exceed smokers when they pass the 13 years.\n\nIt seems that smoking in early ages affects lungs capacity and making it falling behind consistently from non-smokers as getting older."
  },
  {
    "objectID": "posts/FelixBetancourt_HW1_v2.html#part-b---let-x-number-of-prior-convictions-for-prisoners-at-a-state-prison-at-which-there-are-810-prisoners.",
    "href": "posts/FelixBetancourt_HW1_v2.html#part-b---let-x-number-of-prior-convictions-for-prisoners-at-a-state-prison-at-which-there-are-810-prisoners.",
    "title": "Homework 1",
    "section": "Part B - Let X = number of prior convictions for prisoners at a state prison at which there are 810 prisoners.",
    "text": "Part B - Let X = number of prior convictions for prisoners at a state prison at which there are 810 prisoners.\n\n\nCode\n# create the frequency table\nconvicted <- data.frame(\n  p_convic = c(0, 1, 2, 3, 4),\n  freq = c(128, 434, 160, 64, 24))\n\n\nFrequency Table and Cumulative Frequency Table:\n\n\nCode\nconv.tbl <- xtabs(freq ~ p_convic, data=convicted)\nprint (conv.tbl)\n\n\np_convic\n  0   1   2   3   4 \n128 434 160  64  24 \n\n\nCode\ncumfreq_data <- cumsum(conv.tbl)\nprint (cumfreq_data)\n\n\n  0   1   2   3   4 \n128 562 722 786 810 \n\n\n\n\nCode\n# \nprob_data <- conv.tbl/810\n\nprob_data2 <- as.vector(prob_data)\nprob_data3 <- data.frame (\n  p_convic = c(convicted$p_convic),\n  prob = c(prob_data2))\n\n\n\na) What is the probability that a randomly selected inmate has exactly 2 prior convictions?\n\n\nCode\nsum(prob_data3[which(prob_data3$p_convic == 2), 2])\n\n\n[1] 0.1975309\n\n\nThe Probability is 19.7%\n\n\nb) What is the probability that a randomly selected inmate has fewer than 2 prior convictions?\n\n\nCode\nsum(prob_data3[which(prob_data3$p_convic < 2), 2])\n\n\n[1] 0.6938272\n\n\nThe Probability is 69.3%\n\n\nc) What is the probability that a randomly selected inmate has 2 or fewer prior convictions?\n\n\nCode\nsum(prob_data3[which(prob_data3$p_convic <= 2), 2])\n\n\n[1] 0.891358\n\n\nThe Probability is 89.1%\n\n\nd) What is the probability that a randomly selected inmate has more than 2 prior convictions?\n\n\nCode\nsum(prob_data3[which(prob_data3$p_convic > 2), 2])\n\n\n[1] 0.108642\n\n\nThe Probability is 10.8%\n\n\ne) What is the expected value for the number of prior convictions?\n\n\nCode\np_convic2 <- as.vector(convicted$p_convic)\nconvicted_prob <- as.vector(prob_data)\n\nExpected_mean <- sum(p_convic2*convicted_prob)\nExpected_mean\n\n\n[1] 1.28642\n\n\nThe long term mean for prior convictions is 1.29\n\n\nf) Calculate the variance and the standard deviation for the Prior Convictions.\nVariance\n\n\nCode\n#Variance\n\nVar_c <- var(rep(convicted$p_convic, convicted$freq))\nVar_c\n\n\n[1] 0.8572937\n\n\nStandard Deviation\n\n\nCode\nSD_c <- sd(rep(convicted$p_convic, convicted$freq))\nSD_c\n\n\n[1] 0.9259016"
  },
  {
    "objectID": "posts/AdithyaParupudi_hw3.html",
    "href": "posts/AdithyaParupudi_hw3.html",
    "title": "Homework 3",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(stats)\nlibrary(alr4)\nlibrary(smss)\n\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/AdithyaParupudi_hw3.html#a",
    "href": "posts/AdithyaParupudi_hw3.html#a",
    "title": "Homework 3",
    "section": "A",
    "text": "A\nThe predictor variable is ppgdp and the response variable is fertility."
  },
  {
    "objectID": "posts/AdithyaParupudi_hw3.html#b",
    "href": "posts/AdithyaParupudi_hw3.html#b",
    "title": "Homework 3",
    "section": "B",
    "text": "B\n\n\nCode\nUN11 %>%\n  select(c(ppgdp, fertility)) %>%\n  ggplot(aes(x = ppgdp, y = fertility)) +\n  geom_point() +\n  geom_smooth(method = lm)\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nThe graph shows an intense negative relationship between a country’s gross national product per person and fertility rate at the beginning, then appears to be little change in fertility in relationship to ppgdp moving beyond this point. A straight-line mean function does not seem to be an appropriate measure for summary of this graph."
  },
  {
    "objectID": "posts/AdithyaParupudi_hw3.html#c",
    "href": "posts/AdithyaParupudi_hw3.html#c",
    "title": "Homework 3",
    "section": "C",
    "text": "C\n\n\nCode\nUN11 %>%\n  select(c(ppgdp, fertility)) %>%\n  ggplot(aes(x = log(ppgdp), y = log(fertility))) +\n  geom_point() +\n  geom_smooth(method = lm)\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nThe relationship between the variables appears to be negative throughout the graph. The simple linear regression seems plausible to summarise this graph"
  },
  {
    "objectID": "posts/AdithyaParupudi_hw3.html#a-1",
    "href": "posts/AdithyaParupudi_hw3.html#a-1",
    "title": "Homework 3",
    "section": "A",
    "text": "A\n\n\nCode\nUN11$british <- 1.33 * UN11$ppgdp\nsummary(lm(fertility ~ british, UN11))\n\n\n\nCall:\nlm(formula = fertility ~ british, data = UN11)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.9006 -0.8801 -0.3547  0.6749  3.7585 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  3.178e+00  1.048e-01  30.331  < 2e-16 ***\nbritish     -2.407e-05  3.500e-06  -6.877  7.9e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.206 on 197 degrees of freedom\nMultiple R-squared:  0.1936,    Adjusted R-squared:  0.1895 \nF-statistic: 47.29 on 1 and 197 DF,  p-value: 7.903e-11\n\n\nCode\nsummary(lm(fertility ~ ppgdp, UN11))\n\n\n\nCall:\nlm(formula = fertility ~ ppgdp, data = UN11)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.9006 -0.8801 -0.3547  0.6749  3.7585 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  3.178e+00  1.048e-01  30.331  < 2e-16 ***\nppgdp       -3.201e-05  4.655e-06  -6.877  7.9e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.206 on 197 degrees of freedom\nMultiple R-squared:  0.1936,    Adjusted R-squared:  0.1895 \nF-statistic: 47.29 on 1 and 197 DF,  p-value: 7.903e-11\n\n\nThe magnitude of the slope has reduced very slightly, the slope of the prediction equation changed."
  },
  {
    "objectID": "posts/AdithyaParupudi_hw3.html#b-1",
    "href": "posts/AdithyaParupudi_hw3.html#b-1",
    "title": "Homework 3",
    "section": "B",
    "text": "B\n\n\nCode\ncor(UN11$ppgdp, UN11$fertility)\n\n\n[1] -0.4399891\n\n\nCode\ncor(UN11$british, UN11$fertility)\n\n\n[1] -0.4399891\n\n\nThe correlation does not change after the conversion."
  },
  {
    "objectID": "posts/AdithyaParupudi_hw3.html#a-2",
    "href": "posts/AdithyaParupudi_hw3.html#a-2",
    "title": "Homework 3",
    "section": "A",
    "text": "A\n\n\nCode\nstudent.survey %>%\n  select(c(pi, re)) %>%\n  ggplot() + \n  geom_bar(aes(x = re, fill = pi)) +\n  xlab(\"Religiosity\") +\n  ylab(\"Political ideology\") \n\n\n\n\n\nReligiosity and conservatism seem to have a positive relationship.\n\n\nCode\nstudent.survey %>%\n  select(c(tv, hi)) %>%\n  ggplot(aes(x = tv, y = hi)) + \n  geom_point() +\n  geom_smooth(method=lm) +\n  xlab(\"Average Hours of TV watched per Week\") +\n  ylab(\"High School GPA\") \n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nHigh school GPA and TV-watching seem to have a negative relationship."
  },
  {
    "objectID": "posts/AdithyaParupudi_hw3.html#b-2",
    "href": "posts/AdithyaParupudi_hw3.html#b-2",
    "title": "Homework 3",
    "section": "B",
    "text": "B\n\n\nCode\nsummary(lm(data = student.survey, formula = as.numeric(pi) ~ as.numeric(re)))\n\n\n\nCall:\nlm(formula = as.numeric(pi) ~ as.numeric(re), data = student.survey)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.81243 -0.87160  0.09882  1.12840  3.09882 \n\nCoefficients:\n               Estimate Std. Error t value Pr(>|t|)    \n(Intercept)      0.9308     0.4252   2.189   0.0327 *  \nas.numeric(re)   0.9704     0.1792   5.416 1.22e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.345 on 58 degrees of freedom\nMultiple R-squared:  0.3359,    Adjusted R-squared:  0.3244 \nF-statistic: 29.34 on 1 and 58 DF,  p-value: 1.221e-06\n\n\nAt a significance level of 0.01, there is a statistically significant association between religiosity and political ideology (as p-value < .01). The correlation is moderate and positive, suggesting that as weekly church attendance increases, political ideology becomes more conservative leaning.\n\n\nCode\nsummary(lm(data = student.survey, formula = hi ~ tv))\n\n\n\nCall:\nlm(formula = hi ~ tv, data = student.survey)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.2583 -0.2456  0.0417  0.3368  0.7051 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  3.441353   0.085345  40.323   <2e-16 ***\ntv          -0.018305   0.008658  -2.114   0.0388 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4467 on 58 degrees of freedom\nMultiple R-squared:  0.07156,   Adjusted R-squared:  0.05555 \nF-statistic: 4.471 on 1 and 58 DF,  p-value: 0.03879\n\n\nWith a slope of -0.018, there is a negative association between hours of tv watched per week and high school GPA, meaning that as hours of tv viewing increase, a student’s GPA tends to decrease. There is a statistically significant relationship between hours of tv viewed per week and GPA at a significance level of 0.05. However, the R-squared value is close to 0, which suggests that the regression model does not provide a strong prediction for the observed variables. This is not suprising after looking at the scatter plot with hours of tv watched and GPA, since there does not appear to be a linear trend in the data."
  },
  {
    "objectID": "posts/Hw1_thrishul.html",
    "href": "posts/Hw1_thrishul.html",
    "title": "Homework - 1",
    "section": "",
    "text": "First, let’s read in the data from the Excel file:\n\n\nCode\nlibrary(readxl)\nlibrary(tidyverse)\n\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.0     ✔ purrr   1.0.1\n✔ tibble  3.1.8     ✔ dplyr   1.1.0\n✔ tidyr   1.3.0     ✔ stringr 1.5.0\n✔ readr   2.1.4     ✔ forcats 1.0.0\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\nCode\ndf <- read_excel(\"_data/LungCapData.xls\")\nhead(df)\n\n\n# A tibble: 6 × 6\n  LungCap   Age Height Smoke Gender Caesarean\n    <dbl> <dbl>  <dbl> <chr> <chr>  <chr>    \n1    6.48     6   62.1 no    male   no       \n2   10.1     18   74.7 yes   female no       \n3    9.55    16   69.7 no    female yes      \n4   11.1     14   71   no    male   no       \n5    4.8      5   56.9 no    male   no       \n6    6.22    11   58.7 no    female no       \n\n\nThe distribution of LungCap looks as follows:\n\n\nCode\nhist(df$LungCap)\n\n\n\n\n\nThe histogram suggests that the distribution is close to a normal distribution. Most of the observations are close to the mean. Very few observations are close to the margins (0 and 15).\nupdate and comit check\n\n\nCode\n# Subset the data frame by gender\nmale_df <- df[df$Gender == \"male\", ]\nfemale_df <- df[df$Gender == \"female\", ]\n\n# Create separate boxplots for males and females\nboxplot(male_df$LungCap, female_df$LungCap, \n        names = c(\"Male\", \"Female\"),\n        xlab = \"Gender\", ylab = \"Lung Capacity\",\n        main = \"Lung Capacity by Gender\")\n\n\n\n\n\n\n\nCode\nno_smoke <- df[df$Smoke == \"no\",]\nyes_smoke <- df[df$Smoke == \"yes\",]\nmean(no_smoke$LungCap)\n\n\n[1] 7.770188\n\n\nCode\nmean(yes_smoke$LungCap)\n\n\n[1] 8.645455\n\n\n\n\n\n\n\nCode\nage_group_1 <- df[df$Age <= 13, ]\nage_group_2 <- df[df$Age >= 14 & df$Age <= 15, ]\nage_group_3 <- df[df$Age >= 16 & df$Age <= 17, ]\nage_group_4 <- df[df$Age >= 18, ]\npar(mfrow=c(2,2)) # Set up a 2x2 grid of plots\n\nboxplot(LungCap ~ Smoke, data = age_group_1, \n        names = c(\"Non-Smoker\", \"Smoker\"),\n        main = \"Age Group <= 13\")\n\nboxplot(LungCap ~ Smoke, data = age_group_2, \n        names = c(\"Non-Smoker\", \"Smoker\"),\n        main = \"Age Group 14-15\")\n\nboxplot(LungCap ~ Smoke, data = age_group_3, \n        names = c(\"Non-Smoker\", \"Smoker\"),\n        main = \"Age Group 16-17\")\n\nboxplot(LungCap ~ Smoke, data = age_group_4, \n        names = c(\"Non-Smoker\", \"Smoker\"),\n        main = \"Age Group >= 18\")\n\n\n\n\n\n\n\nCode\n# Calculate the mean and standard deviation of Lung Capacity for each age group and smoking status\nagg_data <- aggregate(LungCap ~ Age + Smoke, data = df, \n                      FUN = function(x) c(mean = mean(x), sd = sd(x)))\n\nagg_data\n\n\n   Age Smoke LungCap.mean LungCap.sd\n1    3    no    2.9466923  1.7725478\n2    4    no    2.9416667  1.1691521\n3    5    no    3.4987500  1.4349371\n4    6    no    4.4610000  1.4426914\n5    7    no    4.6202703  1.7044248\n6    8    no    5.2743902  1.5602933\n7    9    no    6.6743750  1.4851993\n8   10    no    6.5861702  1.3697906\n9   11    no    7.4325000  1.5284734\n10  12    no    7.7471311  1.5590134\n11  13    no    8.2700820  1.6003504\n12  14    no    8.7785000  1.4940444\n13  15    no    9.4663636  1.5326404\n14  16    no   10.0577778  1.4230731\n15  17    no   11.0492187  1.5239153\n16  18    no   10.8475000  1.4681731\n17  19    no   11.2585714  1.6228260\n18  10   yes    5.9812500  1.4073283\n19  11   yes    7.5156250  2.1181586\n20  12   yes    6.7464286  0.9939849\n21  13   yes    7.8968750  1.1576174\n22  14   yes    7.8291667  1.7020147\n23  15   yes    8.7666667  1.1875000\n24  16   yes    8.8972222  1.3884819\n25  17   yes    9.7818182  1.1881756\n26  18   yes   10.3903846  1.2926692\n27  19   yes   11.3125000  0.6187184\n\n\n#Question 2 # Let X = number of prior convictions for prisoners at a state prison at which there are 810 prisoners."
  },
  {
    "objectID": "posts/FinalProject.html",
    "href": "posts/FinalProject.html",
    "title": "Final Project Check-in 1",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(dplyr)\nlibrary(ggplot2)\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/FinalProject.html#overview",
    "href": "posts/FinalProject.html#overview",
    "title": "Final Project Check-in 1",
    "section": "Overview",
    "text": "Overview\nThe disastrous effects that a highly contagious disease can have on the world have been strongly illustrated by the COVID-19 pandemic. Millions of people have passed away as a consequence of the pandemic and also impacted the lives of billions of people around the world. Current state of affairs has brought to light the necessity for research on factor and tactics to effectively combat pandemics in the future.\nIt’s critical to comprehend the variables affecting COVID-19 mortality as the pandemic spreads further. The goal of this study is to look at the correlations between a nation’s COVID-19 mortality rate and its population density, median age, GDP per-capita, prevalence of diabetes, hospital beds per 1,000 people, and human development index.\nIn this project I’m aiming to research To what extent do these socioeconomic factors contribute to the variation in COVID-19 mortality rate across the world and derive the relationship of COVID-19 mortality rate with population density,median age, GDP per capita, diabetes prevalence, hospital beds per thousand people and human development index.\n#DataSet\nThe data set contains time series data of around 193 countries around the world. There are around 84,000 records of the countries over the period of time.\nDatasource: https://www.kaggle.com/datasets/fedesoriano/coronavirus-covid19-vaccinations-data\n\n\nCode\ndf <- read_excel(\"_data/COVID_Data.xlsx\")\ndf_selected <- df[,c(\"iso_code\",\"continent\",\"location\",\"date\",\"total_cases_per_million\",\"population_density\",\"median_age\",\n\"gdp_per_capita\",\"diabetes_prevalence\",\"hospital_beds_per_thousand\",\"human_development_index\")]\ndataset_dim <- (dim(df_selected))\ndataset_dim\n\n\n[1] 84772    11\n\n\nCode\ncountries_count <- (length(unique(df_selected$location)))\ncountries_count\n\n\n[1] 193\n\n\nCode\ncountries_list <- (unique(df_selected$location))\n\nhead(df_selected)\n\n\n# A tibble: 6 × 11\n  iso_code conti…¹ locat…² date  total…³ popul…⁴ media…⁵ gdp_p…⁶ diabe…⁷ hospi…⁸\n  <chr>    <chr>   <chr>   <chr>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n1 AFG      Asia    Afghan… 2020…   0.873    54.4    18.6   1804.    9.59     0.5\n2 AFG      Asia    Afghan… 2020…   1.05     54.4    18.6   1804.    9.59     0.5\n3 AFG      Asia    Afghan… 2020…   1.10     54.4    18.6   1804.    9.59     0.5\n4 AFG      Asia    Afghan… 2020…   1.95     54.4    18.6   1804.    9.59     0.5\n5 AFG      Asia    Afghan… 2020…   2.06     54.4    18.6   1804.    9.59     0.5\n6 AFG      Asia    Afghan… 2020…   2.34     54.4    18.6   1804.    9.59     0.5\n# … with 1 more variable: human_development_index <dbl>, and abbreviated\n#   variable names ¹​continent, ²​location, ³​total_cases_per_million,\n#   ⁴​population_density, ⁵​median_age, ⁶​gdp_per_capita, ⁷​diabetes_prevalence,\n#   ⁸​hospital_beds_per_thousand\n\n\nCode\nsummary(df_selected)\n\n\n   iso_code          continent           location             date          \n Length:84772       Length:84772       Length:84772       Length:84772      \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n                                                                            \n total_cases_per_million population_density   median_age    gdp_per_capita    \n Min.   :     0.02       Min.   :    1.98   Min.   :15.10   Min.   :   661.2  \n 1st Qu.:   454.56       1st Qu.:   36.25   1st Qu.:22.20   1st Qu.:  4466.5  \n Median :  2579.10       Median :   82.60   Median :30.60   Median : 13367.6  \n Mean   : 14350.40       Mean   :  361.01   Mean   :30.76   Mean   : 19633.0  \n 3rd Qu.: 16110.12       3rd Qu.:  205.86   3rd Qu.:39.60   3rd Qu.: 27936.9  \n Max.   :179667.38       Max.   :19347.50   Max.   :48.20   Max.   :116935.6  \n NA's   :1               NA's   :4819       NA's   :5783    NA's   :6696      \n diabetes_prevalence hospital_beds_per_thousand human_development_index\n Min.   : 0.990      Min.   : 0.100             Min.   :0.394          \n 1st Qu.: 5.290      1st Qu.: 1.300             1st Qu.:0.602          \n Median : 7.110      Median : 2.400             Median :0.756          \n Mean   : 7.651      Mean   : 3.047             Mean   :0.731          \n 3rd Qu.: 9.740      3rd Qu.: 4.200             3rd Qu.:0.852          \n Max.   :22.020      Max.   :13.800             Max.   :0.957          \n NA's   :4872        NA's   :12687              NA's   :5802           \n\n\nMethodology:\nMultiple linear regression models will be used to carry out the analysis. The socioeconomic determinants will be the independent variables, where as the COVID-19 mortality rate will be the dependent variable.\nExpected Results:\nThe findings of this investigation will aid in understanding the variables affecting the COVID-19 mortality rate. Population density, median age, diabetes prevalence, and hospital beds per thousand people are anticipated to have a positive correlation with the COVID-19 mortality rate, whereas GDP per capita and the human development index are anticipated to have a negative correlation. Planning public health policies and actions to lessen the effects of COVID-19 will benefit from the findings.\nConclusion:\nThe goal of this study is to understand the relationship between socioeconomic factors and the COVID-19 mortality rate. The results will be helpful in planning public health policy and initiatives and will shed light on the factors that affect the COVID-19 death rate."
  },
  {
    "objectID": "posts/HW2_young.html",
    "href": "posts/HW2_young.html",
    "title": "Homework 2",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.1     ✔ purrr   1.0.1\n✔ tibble  3.1.8     ✔ dplyr   1.1.0\n✔ tidyr   1.3.0     ✔ stringr 1.5.0\n✔ readr   2.1.4     ✔ forcats 1.0.0\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\nCode\nlibrary(dplyr)"
  },
  {
    "objectID": "posts/HW2_young.html#more",
    "href": "posts/HW2_young.html#more",
    "title": "Homework 2",
    "section": "More",
    "text": "More\nRandom data were generated using the given statistics and each confidence interval was also obtained.\n\n\nCode\nset.seed(1)\n\nbypass.s<-rnorm(539, mean=19, sd=10)\nangio.s<-rnorm(847, mean=18, sd=9)\n\nt.test(bypass.s, conf.level = .9)\n\n\n\n    One Sample t-test\n\ndata:  bypass.s\nt = 44.184, df = 538, p-value < 2.2e-16\nalternative hypothesis: true mean is not equal to 0\n90 percent confidence interval:\n 18.44913 19.87842\nsample estimates:\nmean of x \n 19.16377 \n\n\nCode\nt.test(angio.s, conf.level = .9)\n\n\n\n    One Sample t-test\n\ndata:  angio.s\nt = 54.954, df = 846, p-value < 2.2e-16\nalternative hypothesis: true mean is not equal to 0\n90 percent confidence interval:\n 17.15787 18.21788\nsample estimates:\nmean of x \n 17.68788 \n\n\nThe statistics of the generated data did not exactly match the statistics given by the problem, so the exact same confidence interval was not obtained, but they are similar."
  },
  {
    "objectID": "posts/HW2_young.html#a",
    "href": "posts/HW2_young.html#a",
    "title": "Homework 2",
    "section": "A",
    "text": "A\nFirst, assume that s in the population is equal to the sample in question. And the significance level is set at 5%.\nNull Hypothesis: μ = 500 Alternative Hypothesis: μ ≠ 500\nSince it is a test that estimates whether the parameter is a specific value, a two-side t-test should be performed. At this time, t is ȳ - μ / s/(n^1/2).\n\n\nCode\nt.a<-((410-500)/(90/3))\nt.a\n\n\n[1] -3\n\n\nt is -3. And since it is a two-side test, the p-value of this hypothesis can be obtained by multiplying the p-value at t=-3.\n\n\nCode\np.a<-(pt(t.a, df=8))*2\np.a\n\n\n[1] 0.01707168\n\n\np-value is 0.017. And this is smaller than our significant level(0.05). So, we can reject our null hypothesis. This means that At the 95% confidence level, μ is not 500."
  },
  {
    "objectID": "posts/HW2_young.html#b",
    "href": "posts/HW2_young.html#b",
    "title": "Homework 2",
    "section": "B",
    "text": "B\nHere, the null hypothesis is μ >= 500. Then, the alternative hypothesis is μ < 500. In this case, a one-sided test shall be performed.\n\n\nCode\nt.b<--3\np.b<-pt(t.b, df=8)\np.b\n\n\n[1] 0.008535841\n\n\nThe p-value can be obtained in the same way as A. Here, the value of p is 0.009. This means that at the 95% confidence level, μ is not greater than 500."
  },
  {
    "objectID": "posts/HW2_young.html#c",
    "href": "posts/HW2_young.html#c",
    "title": "Homework 2",
    "section": "C",
    "text": "C\nHere, contrary to B, the the null hypothesis is μ =< 500. Since the direction of the inequality has changed, the area of the p value in the t-distribution must be opposite to B.\n\n\nCode\nt.c<--3\np.c<-(1-pt(t.c, df=8))\np.c\n\n\n[1] 0.9914642\n\n\nUsing these facts, the p-value is 0.9914. Now, at the 5% significance level, the null hypothesis cannot be rejected. That is, at a 95% confidence level, μ is less than 500."
  },
  {
    "objectID": "posts/HW2_young.html#a-1",
    "href": "posts/HW2_young.html#a-1",
    "title": "Homework 2",
    "section": "A",
    "text": "A\nt is (ȳ-μ) / (sd/sqrt(n)). And sd/sqrt(n) is se. So we can compute t through this equation.\n\n\nCode\n# Jones's t and p-value\njones.t<-(519.5-500)/10\njones.p<-(1-pt(jones.t, df=999))*2\njones.t\n\n\n[1] 1.95\n\n\nCode\njones.p\n\n\n[1] 0.05145555\n\n\n\n\nCode\n# Smith's t and p-value\nsmith.t<-(519.7-500)/10\nsmith.p<-(1-pt(smith.t, df=999))*2\nsmith.t\n\n\n[1] 1.97\n\n\nCode\nsmith.p\n\n\n[1] 0.04911426"
  },
  {
    "objectID": "posts/HW2_young.html#b-1",
    "href": "posts/HW2_young.html#b-1",
    "title": "Homework 2",
    "section": "B",
    "text": "B\nα is a level arbitrarily set according to the researcher’s judgment, and does not affect the t or p-value of each hypothesis.\nThe p value means the probability that t will have such a value when t is a normal distribution, that is, even if ȳ=μ=500, and se is 10, the probability of ȳ=519.5 can be 5.14%. Therefore, p greater than α means that even if ȳ=μ is true, the probability of accidentally ȳ=519.5 is greater than the type 1 error set by the researcher (false positive), which means that the null hypothesis cannot be rejected.\nSo when α=0.05, Jones’s null hypothesis cannot be rejected, and Smith’s null hypothesis can be rejected. In other words, only Smith’s research results are statistically significant."
  },
  {
    "objectID": "posts/HW2_young.html#c-1",
    "href": "posts/HW2_young.html#c-1",
    "title": "Homework 2",
    "section": "C",
    "text": "C\nAs seen in B, the p-value itself means the probability of a specific t, and not revealing a specific p-value does not tell how statistically significant this study will be at the significance level (type 1 error), so information loss occurs."
  },
  {
    "objectID": "posts/Jerin_Jacob_Final_Project.html",
    "href": "posts/Jerin_Jacob_Final_Project.html",
    "title": "Final Project 603",
    "section": "",
    "text": "Now a days, movies are a well marketed entertainment product. Just like any other products in the market, movies are also having an allocated a marketing budget and promotional activities are done in scale. This often result in the opening weekend’s gross ticketing volume to rise. But are the pre-release promotional activities helping the movie to collect more or is it just creating a hype initially? Or does the movie’s gross collection is not at all dependant on pre release promotions? This dataset has 200 highest grossing movies of 2022. It has both the opening week’s gross as well as the total gross collection of the movies, along with other variables. Assuming that opening week’s collection is depending on the pre-release promotion, by looking on the relationship between opening week’s gross and total gross, I am trying to see how the pre-release activities help the producers earn more in boxoffice.\nResearch Question: Is the total collection of a movie depending on the pre-release promotional activities?\nNull hypothesis: The total gross collection of a movie is not dependant on the opening week’s collection\nAlternative hypotheses: Total gross collection is positively dependant on the opening week’s collection"
  },
  {
    "objectID": "posts/Jerin_Jacob_Final_Project.html#reading-the-data",
    "href": "posts/Jerin_Jacob_Final_Project.html#reading-the-data",
    "title": "Final Project 603",
    "section": "Reading the data",
    "text": "Reading the data\n\n\nCode\nlibrary(readxl)\nlibrary(tidyverse)\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.0     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.1     ✔ tibble    3.2.0\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the \u001b]8;;http://conflicted.r-lib.org/\u0007conflicted package\u001b]8;;\u0007 to force all conflicts to become errors\n\n\n\n\nCode\ndf <- read.csv(\"_data/project_data.csv\")\n#df\n\n\nThere are 10 variables with 200 rows.\n\nCOLUMN DESCRIPTION\n\n‘Rank’: rank of the movie\n‘Release’: release date of the movie\n‘Gross’: domestic gross of the movie\n‘max_th’: maximum number of theaters the movie was released in\n‘Opening’: gross on opening weekend\n‘perc_tot_gr’: domestic percentage of the total gross\n‘open_th’: number of theaters the movie opened in\n‘Open’: opening date\n‘Close’: closing date\n‘Distributor’: name of the distributor\n‘int_gross’: international gross\n‘world_gross’: worldwide gross"
  },
  {
    "objectID": "posts/Jerin_Jacob_Final_Project.html#summary-of-each-variables",
    "href": "posts/Jerin_Jacob_Final_Project.html#summary-of-each-variables",
    "title": "Final Project 603",
    "section": "Summary of each variables",
    "text": "Summary of each variables\n\n\nCode\nsummary(df)\n\n\n      Rank          Release              Gross               max_th      \n Min.   :  1.00   Length:200         Min.   :   304287   Min.   :   5.0  \n 1st Qu.: 50.75   Class :character   1st Qu.:  1028500   1st Qu.: 457.8  \n Median :100.50   Mode  :character   Median :  3995788   Median :1083.5  \n Mean   :100.50                      Mean   : 36977151   Mean   :1760.2  \n 3rd Qu.:150.25                      3rd Qu.: 21273142   3rd Qu.:3189.0  \n Max.   :200.00                      Max.   :718732821   Max.   :4751.0  \n                                                                         \n    Opening           perc_tot_gr       open_th           Open          \n Min.   :     3265   Min.   : 0.10   Min.   :   1.0   Length:200        \n 1st Qu.:   243919   1st Qu.:20.50   1st Qu.: 160.5   Class :character  \n Median :  1066404   Median :35.45   Median : 916.5   Mode  :character  \n Mean   : 11649361   Mean   :32.05   Mean   :1588.8                     \n 3rd Qu.:  8137392   3rd Qu.:43.25   3rd Qu.:3156.2                     \n Max.   :187420998   Max.   :77.80   Max.   :4735.0                     \n                                                                        \n    Close           Distributor          int_gross          world_gross       \n Length:200         Length:200         Min.   :4.201e+03   Min.   :5.100e+03  \n Class :character   Class :character   1st Qu.:7.076e+05   1st Qu.:2.099e+06  \n Mode  :character   Mode  :character   Median :6.311e+06   Median :1.287e+07  \n                                       Mean   :5.177e+07   Mean   :8.765e+07  \n                                       3rd Qu.:3.040e+07   3rd Qu.:6.049e+07  \n                                       Max.   :1.539e+09   Max.   :2.176e+09  \n                                       NA's   :3                              \n\n\nUsing the glimpse() function, let’s have a look at how our data would look like!\n\n\nCode\nglimpse(df)\n\n\nRows: 200\nColumns: 12\n$ Rank        <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17,…\n$ Release     <chr> \"Top Gun: Maverick\", \"Avatar: The Way of Water\", \"Black Pa…\n$ Gross       <int> 718732821, 636955746, 453474324, 411331607, 376851080, 369…\n$ max_th      <int> 4751, 4340, 4396, 4534, 4697, 4427, 4417, 4375, 4258, 4402…\n$ Opening     <int> 126707459, 134100226, 181339761, 187420998, 145075625, 107…\n$ perc_tot_gr <dbl> 17.6, 21.1, 40.0, 45.6, 38.5, 28.9, 36.3, 42.0, 37.8, 39.8…\n$ open_th     <int> 4735, 4202, 4396, 4534, 4676, 4391, 4417, 4375, 4234, 4402…\n$ Open        <chr> \"2022-05-27\", \"2022-12-16\", \"2022-11-11\", \"2022-05-06\", \"2…\n$ Close       <chr> \"2022-12-16\", \"\", \"\", \"\", \"2022-09-23\", \"\", \"\", \"2022-10-1…\n$ Distributor <chr> \"Paramount Pictures\", \"20th Century Studios\", \"Walt Disney…\n$ int_gross   <int> 770000000, 1539273359, 389276658, 544444197, 625127000, 56…\n$ world_gross <dbl> 1488732821, 2176229105, 842750982, 955775804, 1001978080, …"
  },
  {
    "objectID": "posts/Jerin_Jacob_Final_Project.html#references",
    "href": "posts/Jerin_Jacob_Final_Project.html#references",
    "title": "Final Project 603",
    "section": "References:",
    "text": "References:\n\nNasir, Suphan & Öcal, Figen. (2016). Film Marketing: The Impact of Publicity Activities on Demand Generation. 10.4018/978-1-5225-0143-5.ch019.\nElizabeth Cooper-Martin (1991) ,“Consumers and Movies: Some Findings on Experiential Products”, in NA - Advances in Consumer Research Volume 18, eds. Rebecca H. Holman and Michael R. Solomon, Provo, UT : Association for Consumer Research, Pages: 372-378."
  },
  {
    "objectID": "posts/HW1_DarronBunt.html#a",
    "href": "posts/HW1_DarronBunt.html#a",
    "title": "Homework - 1",
    "section": "A",
    "text": "A\nWhat does the distribution of LungCap look like? (Hint: Plot a histogram with probability density on the y axis)\nFirst, let’s read in the data from the Excel file:\n\n\nCode\nLungCapData <- read_excel(\"_data/LungCapData.xls\")\n\n\nThe distribution of LungCap looks as follows:\n\n\nCode\nhist(LungCapData$LungCap)\n\n\n\n\n\nThe histogram suggests that the distribution is close to a normal distribution. Most of the observations are close to the mean, while very few observations are close to the margins (0 and 15)."
  },
  {
    "objectID": "posts/HW1_DarronBunt.html#b",
    "href": "posts/HW1_DarronBunt.html#b",
    "title": "Homework - 1",
    "section": "B",
    "text": "B\nCompare the probability distribution of the LungCap with respect to Males and Females? (Hint: make boxplots separated by gender using the boxplot() function)\n\n\nCode\n# Create a boxplot comparing LungCap for males and females\nboxplot(LungCap~Gender, data=LungCapData)\n\n\n\n\n\nThe boxplot suggests that the median for lung capacity in males is slightly higher than that of females. The IQR for lung capacity in males is also slightly higher than that of females. The minimum value for females is lower than that of males, as is the maximum. All of this suggests that males are more likely to have a greater lung capacity than females."
  },
  {
    "objectID": "posts/HW1_DarronBunt.html#c",
    "href": "posts/HW1_DarronBunt.html#c",
    "title": "Homework - 1",
    "section": "C",
    "text": "C\nCompare the mean lung capacities for smokers and non-smokers. Does it make sense?\n\n\nCode\n# Calculate mean lung capacity for smokers and non-smokers\nLungCapData %>%\n  group_by(Smoke) %>%\n  summarise_at(vars(LungCap),\n               list(LCap = mean))\n\n\n# A tibble: 2 × 2\n  Smoke  LCap\n  <chr> <dbl>\n1 no     7.77\n2 yes    8.65\n\n\nThis suggests that the mean lung capacity for smokers is greater than that of non-smokers. This is not what I would have expected given the (negative) impact that smoking has on the lungs."
  },
  {
    "objectID": "posts/HW1_DarronBunt.html#d",
    "href": "posts/HW1_DarronBunt.html#d",
    "title": "Homework - 1",
    "section": "D",
    "text": "D\nExamine the relationship between Smoking and Lung Capacity within age groups: “less than or equal to 13”, “14 to 15”, “16 to 17”, and “greater than or equal to 18”.\n\n\nCode\n# Create a new variable, AgeGroup, using the parameters outlined above\nAgeSmokeLC <- LungCapData %>%\n mutate(\n   AgeGroup = case_when(\n     Age <= 13 ~ \"13 and Under\",\n     Age == 14 | Age == 15 ~ \"14-15\",\n     Age == 16 | Age == 17 ~ \"16-17\", \n     Age >= 18 ~ \"18 and Over\")) \n\n# Calculate the mean lung capacity for smokers and non-smokers in each age group  \n  AgeSmokeLCMean <- AgeSmokeLC %>%\n    group_by(AgeGroup, Smoke) %>%\n  summarise_at(vars(LungCap),\n               list(MeanLungCap = mean)) %>%\n  arrange(desc(MeanLungCap))\nAgeSmokeLCMean\n\n\n# A tibble: 8 × 3\n# Groups:   AgeGroup [4]\n  AgeGroup     Smoke MeanLungCap\n  <chr>        <chr>       <dbl>\n1 18 and Over  no          11.1 \n2 18 and Over  yes         10.5 \n3 16-17        no          10.5 \n4 16-17        yes          9.38\n5 14-15        no           9.14\n6 14-15        yes          8.39\n7 13 and Under yes          7.20\n8 13 and Under no           6.36\n\n\nThis suggests that lung capacity increases with age; the mean lung capacity for each subsequent age category is greater than the one before it. Individuals aged 13 and under who smoke have a greater mean lung capacity than those who do not; however, at ages 14-15, 16-17, and 18+, non-smokers have a greater lung capacity than their similarly aged smoking counterparts."
  },
  {
    "objectID": "posts/HW1_DarronBunt.html#e",
    "href": "posts/HW1_DarronBunt.html#e",
    "title": "Homework - 1",
    "section": "E",
    "text": "E\nCompare the lung capacities for smokers and non-smokers within each age group. Is your answer different from the one in part c. What could possibly be going on here?\nFrom the age group data above, we can ascertain that in the majority of age groups (three of the four), non-smokers have greater mean lung capacity than smokers, and yet when we calculated the mean lung capacity solely for smokers/non-smokers (ie. not accounting for age), smokers had greater mean lung capacity. The mean lung capacities for smokers/non-smokers (again, not accounting for age) are also lower than one might expect considering that when we do account for age, most of the represented groups have a higher mean lung capacity. This would suggest that something is skewing our results.\nOne way to gain insight into this is to examine how many respondents fell into each age/smoking status category.\n\n\nCode\n# Count number of responses in each age group who are smokers/non-smokers\nAgeSmokeLC %>%\n  count(AgeGroup, Smoke)\n\n\n# A tibble: 8 × 3\n  AgeGroup     Smoke     n\n  <chr>        <chr> <int>\n1 13 and Under no      401\n2 13 and Under yes      27\n3 14-15        no      105\n4 14-15        yes      15\n5 16-17        no       77\n6 16-17        yes      20\n7 18 and Over  no       65\n8 18 and Over  yes      15\n\n\nThere are more non-smoking individuals aged 13 and Under in this sample than all other categories combined. Accordingly, data from this group can (and has) skewed the overall non-smoking mean."
  },
  {
    "objectID": "posts/HW1_DarronBunt.html#a-1",
    "href": "posts/HW1_DarronBunt.html#a-1",
    "title": "Homework - 1",
    "section": "A",
    "text": "A\nWhat is the probability that a randomly selected inmate has exactly 2 prior convictions?\nTo answer this question, we are going to need to know the total number of inmates.\n\n\nCode\n#Find total number of inmates\nsum(PriorConvictions$Inmates)\n\n\n[1] 810\n\n\nTo calculate this probability we are going calculate the binomial distribution. For this, we require three main arguments: x - the number specifying the outcomes you’re trying to calculate (for this question, x=1) size - the size of the experiment (for this question, size=1) prob - the probability of success for any one trial in the experiment (for this question, prob = the total number of inmates with two convictions / the total number of inmates, or 160/810)\n\n\nCode\n# Calculate probability of randomly selecting an inmate with two prior convictions\ndbinom(x=1, size=1, prob=(160/810))\n\n\n[1] 0.1975309\n\n\nThe probability of randomly selecting an inmate with two prior convictions is roughly 19.8%."
  },
  {
    "objectID": "posts/HW1_DarronBunt.html#b-1",
    "href": "posts/HW1_DarronBunt.html#b-1",
    "title": "Homework - 1",
    "section": "B",
    "text": "B\nWhat is the probability that a randomly selected inmate has fewer than 2 prior convictions?\nHaving fewer than two prior convictions would mean that the inmate could either have zero or one prior convictions.\nx = 1 size = 1 prob = ((the total number of inmates with no prior convictions + inmates with 1 conviction) / the total number of inmates, or ((128 + 434)/810), or 562/810.\n\n\nCode\n# Calculate probability of randomly selecting an inmate with less than two prior convictions\ndbinom(x=1, size=1, prob=(562/810))\n\n\n[1] 0.6938272\n\n\nThe probability of randomly selecting an inmate with fewer than two prior convictions is roughly 69.4%."
  },
  {
    "objectID": "posts/HW1_DarronBunt.html#c-1",
    "href": "posts/HW1_DarronBunt.html#c-1",
    "title": "Homework - 1",
    "section": "C",
    "text": "C\nWhat is the probability that a randomly selected inmate has 2 or fewer prior convictions?\nHaving two or fewer prior convictions would mean that the inmate could either have zero, one, or two prior convictions.\nx = 1 size = 1 prob = ((the total number of inmates with no prior convictions + inmates with 1 conviction + inmates with 2 convictions) / the total number of inmates, or ((128 + 434 + 160)/810), or 722/810.\n\n\nCode\n# Calculate probability of randomly selecting an inmate with two or fewer prior convictions\ndbinom(x=1, size=1, prob=(722/810))\n\n\n[1] 0.891358\n\n\nThe probability of randomly selecting an inmate with two or fewer prior convictions is roughly 89.1%."
  },
  {
    "objectID": "posts/HW1_DarronBunt.html#d-1",
    "href": "posts/HW1_DarronBunt.html#d-1",
    "title": "Homework - 1",
    "section": "D",
    "text": "D\nWhat is the probability that a randomly selected inmate has more than 2 prior convictions?\nHaving more than two prior convictions would mean that the inmate could either have three or four prior convictions.\nx = 1 size = 1 prob = ((the total number of inmates with 3 convictions + inmates with 4 convictions) / the total number of inmates, or ((64+24)/810), or 88/810.\n\n\nCode\n# Calculate probability of randomly selecting an inmate with more than two prior convictions\ndbinom(x=1, size=1, prob=(88/810))\n\n\n[1] 0.108642\n\n\nThe probability of randomly selecting an inmate with more than two prior convictions is roughly 10.9%."
  },
  {
    "objectID": "posts/HW1_DarronBunt.html#e-1",
    "href": "posts/HW1_DarronBunt.html#e-1",
    "title": "Homework - 1",
    "section": "E",
    "text": "E\nWhat is the expected value for the number of prior convictions? (The expected value of a discrete random variable X, symbolized as E(X), is often referred to as the long-term average or mean)\nIn order to calculate the expected value for the number of prior convictions, we will need to use the proportional breakdown of the number of inmates with each number of convictions.\n\n\nCode\n# Calculate proportion of inmates with 0,1,2,3,4 convictions\nPriorConvictionsProp <- PriorConvictions %>%\n  mutate(Proportion = Inmates/810)\nPriorConvictionsProp\n\n\n  PriorConv Inmates Proportion\n1         0     128 0.15802469\n2         1     434 0.53580247\n3         2     160 0.19753086\n4         3      64 0.07901235\n5         4      24 0.02962963\n\n\n\n\nCode\n# Calculate the expected value for prior number of convictions\nExpValue <- sum(PriorConvictionsProp$PriorConv*PriorConvictionsProp$Proportion)\nExpValue\n\n\n[1] 1.28642\n\n\nThe expected value is 1.29 prior convictions."
  },
  {
    "objectID": "posts/HW1_DarronBunt.html#f",
    "href": "posts/HW1_DarronBunt.html#f",
    "title": "Homework - 1",
    "section": "F",
    "text": "F\nCalculate the variance and the standard deviation for the Prior Convictions.\n\n\nCode\n# Calculate the variance for prior convictions\nPriorConvVar <- sum((PriorConvictionsProp$PriorConv - ExpValue) ^ 2 * PriorConvictionsProp$Proportion)\nPriorConvVar\n\n\n[1] 0.8562353\n\n\n\n\nCode\n# Calculate the standard deviation for prior convictions\nPriorConvSTD <- sqrt(PriorConvVar)\nPriorConvSTD\n\n\n[1] 0.9253298"
  },
  {
    "objectID": "posts/Thrishul_Final_project_checkin_1.html#variable-descriptions",
    "href": "posts/Thrishul_Final_project_checkin_1.html#variable-descriptions",
    "title": "Final Project check in 1",
    "section": "VARIABLE DESCRIPTIONS:",
    "text": "VARIABLE DESCRIPTIONS:\n\ngender: specifies gender of the student(male/female)\nrace: specifies race of the student(group A,group B,group C)\nparental level of education: specifies highest educational qualification of any parent of each student\nlunch_type: standard/reduced,the type of lunch package selected for the student\ntest_prep: specifies if the test preparation course was completed by the student or not\nmath_score: specifies score in math(our target variable)\nreading_score: specifies score in reading\nwriting_score: specifies score in writing\n\nAll scores are taken out of 100.\n##Hypothesis\nThere is a significant correlation between the features available in the dataset, such as parental level of education, test preparation course, and lunch type, and the math score of students. By using various regression algorithms, it is possible to predict the math score of students with reasonable accuracy based on the available features. Furthermore, the use of regression algorithms can identify the most influential factors that contribute to student performance in Math, allowing educators and policymakers to take appropriate actions to improve student performance"
  },
  {
    "objectID": "posts/Thrishul_Final_project_checkin_1.html#importing-libraries",
    "href": "posts/Thrishul_Final_project_checkin_1.html#importing-libraries",
    "title": "Final Project check in 1",
    "section": "Importing Libraries",
    "text": "Importing Libraries\n\n\nCode\nset.seed(12345)\nlibrary(caret)\n\n\nWarning: package 'caret' was built under R version 4.2.3\n\n\nLoading required package: ggplot2\n\n\nLoading required package: lattice\n\n\nCode\nlibrary(Metrics)\n\n\nWarning: package 'Metrics' was built under R version 4.2.3\n\n\n\nAttaching package: 'Metrics'\n\n\nThe following objects are masked from 'package:caret':\n\n    precision, recall\n\n\nCode\n#The caret package provides a wide range of functions for training and evaluating machine learning models, while the Metrics package provides various metrics for evaluating model performance, including the R-squared score \n\nlibrary(glmnet)\n\n\nWarning: package 'glmnet' was built under R version 4.2.3\n\n\nLoading required package: Matrix\n\n\nLoaded glmnet 4.1-7\n\n\nCode\n#The glmnet package provides functions for fitting regularized regression models, including Ridge regression (glmnet function with alpha = 0) and Lasso regression (glmnet function with alpha = 1)\n\n#To perform cross-validation, the cv.glmnet function which performs k-fold cross-validation with a specified number of folds (nfolds)\n\n#Similarly, performing Lasso regression by setting alpha = 1 in the glmnet function\n\n\n\n\nCode\nlibrary(readr)\nStudentsPerformance <- read_csv(\"_data/StudentsPerformance.csv\", show_col_types = FALSE)\nstr(StudentsPerformance)\n\n\nspc_tbl_ [1,000 × 8] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ gender                     : chr [1:1000] \"female\" \"female\" \"female\" \"male\" ...\n $ race/ethnicity             : chr [1:1000] \"group B\" \"group C\" \"group B\" \"group A\" ...\n $ parental level of education: chr [1:1000] \"bachelor's degree\" \"some college\" \"master's degree\" \"associate's degree\" ...\n $ lunch                      : chr [1:1000] \"standard\" \"standard\" \"standard\" \"free/reduced\" ...\n $ test preparation course    : chr [1:1000] \"none\" \"completed\" \"none\" \"none\" ...\n $ math score                 : num [1:1000] 72 69 90 47 76 71 88 40 64 38 ...\n $ reading score              : num [1:1000] 72 90 95 57 78 83 95 43 64 60 ...\n $ writing score              : num [1:1000] 74 88 93 44 75 78 92 39 67 50 ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   gender = col_character(),\n  ..   `race/ethnicity` = col_character(),\n  ..   `parental level of education` = col_character(),\n  ..   lunch = col_character(),\n  ..   `test preparation course` = col_character(),\n  ..   `math score` = col_double(),\n  ..   `reading score` = col_double(),\n  ..   `writing score` = col_double()\n  .. )\n - attr(*, \"problems\")=<externalptr> \n\n\n\n\nCode\nsummary(StudentsPerformance)\n\n\n    gender          race/ethnicity     parental level of education\n Length:1000        Length:1000        Length:1000                \n Class :character   Class :character   Class :character           \n Mode  :character   Mode  :character   Mode  :character           \n                                                                  \n                                                                  \n                                                                  \n    lunch           test preparation course   math score     reading score   \n Length:1000        Length:1000             Min.   :  0.00   Min.   : 17.00  \n Class :character   Class :character        1st Qu.: 57.00   1st Qu.: 59.00  \n Mode  :character   Mode  :character        Median : 66.00   Median : 70.00  \n                                            Mean   : 66.09   Mean   : 69.17  \n                                            3rd Qu.: 77.00   3rd Qu.: 79.00  \n                                            Max.   :100.00   Max.   :100.00  \n writing score   \n Min.   : 10.00  \n 1st Qu.: 57.75  \n Median : 69.00  \n Mean   : 68.05  \n 3rd Qu.: 79.00  \n Max.   :100.00"
  },
  {
    "objectID": "posts/Thrishul_Final_project_checkin_1.html#proposed-models",
    "href": "posts/Thrishul_Final_project_checkin_1.html#proposed-models",
    "title": "Final Project check in 1",
    "section": "Proposed Models",
    "text": "Proposed Models\nvarious regression algorithms such as Linear Regression, Polynomial Regression, Ridge Regression, Lasso Regression, and ElasticNet Regression will be trained on the preprocessed dataset. The performance of each algorithm will be evaluated using metrics such as Mean Squared Error, Root Mean Squared Error, and R-squared."
  },
  {
    "objectID": "posts/HW2_solutions_Pang.html",
    "href": "posts/HW2_solutions_Pang.html",
    "title": "Homework 2",
    "section": "",
    "text": "Please check your answers against the solutions."
  },
  {
    "objectID": "posts/HW2_solutions_Pang.html#question-1",
    "href": "posts/HW2_solutions_Pang.html#question-1",
    "title": "Homework 2",
    "section": "Question 1",
    "text": "Question 1\n\nbypass_n = 539\nangio_n = 847\n\nbypass_sample_mean = 19\nangio_sample_mean = 18\n\nbypass_sample_sd = 10\nangio_sample_sd = 9\n\nbypass_se = bypass_sample_sd/sqrt(bypass_n)\nangio_se = angio_sample_sd/sqrt(angio_n)\n\nbypass_me = qt(0.95, df = bypass_n - 1)*bypass_se\nangio_me = qt(0.95, df = angio_n - 1)*angio_se\n\nThe confidence intervals:\n\nprint(bypass_sample_mean + c(-bypass_me, bypass_me))\n\n[1] 18.29029 19.70971\n\nprint(angio_sample_mean + c(-angio_me, angio_me))\n\n[1] 17.49078 18.50922\n\n\nThe size of the confidence intervals, which is twice the margin of error:\n\n2 * bypass_me\n\n[1] 1.419421\n\n2 * angio_me\n\n[1] 1.018436\n\n\nThe confidence interval for angiography is narrower."
  },
  {
    "objectID": "posts/HW2_solutions_Pang.html#question-2",
    "href": "posts/HW2_solutions_Pang.html#question-2",
    "title": "Homework 2",
    "section": "Question 2",
    "text": "Question 2\none-step solution:\n\nn = 1031\nk = 567\nprop.test(k, n)\n\n\n    1-sample proportions test with continuity correction\n\ndata:  k out of n, null probability 0.5\nX-squared = 10.091, df = 1, p-value = 0.00149\nalternative hypothesis: true p is not equal to 0.5\n95 percent confidence interval:\n 0.5189682 0.5805580\nsample estimates:\n        p \n0.5499515 \n\n\nAlternatively:\n\np_hat <- k/n # point estimate\nse = sqrt((p_hat*(1-p_hat))/n) # standard error\ne = qnorm(0.975)*se # margin of error\np_hat + c(-e, e) # confidence interval \n\n[1] 0.5195839 0.5803191\n\n\nAlternatively, we can use the exact binomial test. In large samples like the one we have, the results should essentially be the same as prop.test().\n\nbinom.test(k, n)\n\n\n    Exact binomial test\n\ndata:  k and n\nnumber of successes = 567, number of trials = 1031, p-value = 0.001478\nalternative hypothesis: true probability of success is not equal to 0.5\n95 percent confidence interval:\n 0.5189927 0.5806243\nsample estimates:\nprobability of success \n             0.5499515"
  },
  {
    "objectID": "posts/HW2_solutions_Pang.html#question-3",
    "href": "posts/HW2_solutions_Pang.html#question-3",
    "title": "Homework 2",
    "section": "Question 3",
    "text": "Question 3\n\nrange = 200-30\npopulation_sd = range/4\n\nRemember:\n\\[CI_{95} = \\bar x \\pm z \\frac{s}{\\sqrt n}\\] (We can use \\(z\\) because we assume population standard deviation is known.)\nWe want the number \\(n\\) that ensures:\n\\[ z \\frac{s}{\\sqrt n} = 5 \\] \\[ zs = 5 \\sqrt n\\] \\[ \\frac{zs}{5} = \\sqrt n\\] \\[  (\\frac{zs}{5})^2 = n\\]\nIn our case:\n\nz = qnorm(.975)\ns = population_sd\nn = ((z *s) / 5)^2\nprint(n)\n\n[1] 277.5454\n\n\nRounding up, we need a sample of 278."
  },
  {
    "objectID": "posts/HW2_solutions_Pang.html#question-4",
    "href": "posts/HW2_solutions_Pang.html#question-4",
    "title": "Homework 2",
    "section": "Question 4",
    "text": "Question 4\nWe can write a function to find the t-statistic, and then do all the tests in a, b, and c using that.\n\\[t = \\frac{\\bar x - \\mu}{s / \\sqrt n}\\]\nwhere \\(\\bar x\\) is them sample mean, \\(\\mu\\) is the hypothesizes population mean, \\(s\\) is the sample standard deviation, and \\(n\\) is the sample size.\nWriting this in R:\n\nget_t_stat <- function(x_bar, mu, sd, n){\n  return((x_bar - mu) / (sd / sqrt(n)))\n}\n\nFind the t-statistic:\n\nt_stat <- get_t_stat(x_bar = 410, mu = 500, sd = 90, n = 9)\n\n\nA\nTwo-tailed test\n\nn = 9\npval_two_tail = 2*pt(t_stat, df = n-1)\npval_two_tail\n\n[1] 0.01707168\n\n\nWe can reject the hypothesis that population mean is 500.\n\n\nB\n\npval_lower_tail = pt(t_stat, df = n-1)\npval_lower_tail\n\n[1] 0.008535841\n\n\nWe can reject the hypothesis that population mean is greater than 500.\n\n\nC\n\npval_upper_tail = pt(t_stat, df = n-1, lower.tail=FALSE)\npval_upper_tail\n\n[1] 0.9914642\n\n\nWe fail to reject the hypothesis that population mean is less than 500.\nAlternatively for C, we could just subtract the answer in B from 1:\n\n1 - pval_lower_tail\n\n[1] 0.9914642"
  },
  {
    "objectID": "posts/HW2_solutions_Pang.html#question-5",
    "href": "posts/HW2_solutions_Pang.html#question-5",
    "title": "Homework 2",
    "section": "Question 5",
    "text": "Question 5\n\nt_jones = ((519.5 - 500)/ 10)\nt_smith = ((519.7 - 500)/ 10)\ncat(\"t value for Jones:\", t_jones, '\\n')\n\nt value for Jones: 1.95 \n\ncat(\"t value for Smith:\", t_smith, '\\n')\n\nt value for Smith: 1.97 \n\ncat('p value for Jones:', round(2*pt(t_jones, df = 999, lower.tail=FALSE), 4), '\\n')\n\np value for Jones: 0.0515 \n\ncat('p value for Smith:', round(2*pt(t_smith, df = 999, lower.tail=FALSE), 4), '\\n')\n\np value for Smith: 0.0491 \n\n\nAt 0.05 level Smith’s result is statistically significant but Jones’s is not. The result show the arbitrariness of the 0.05 demarcation line and the importance of reporting actual p-values to better make sense of results."
  },
  {
    "objectID": "posts/HW2_solutions_Pang.html#question-6",
    "href": "posts/HW2_solutions_Pang.html#question-6",
    "title": "Homework 2",
    "section": "Question 6:",
    "text": "Question 6:\n\n# Creating the dataframe\ngrade_level <- c(rep(\"6th grade\", 100), rep(\"7th grade\", 100), rep(\"8th grade\", 100))\nsnack <- c(rep(\"healthy snack\", 31), rep(\"unhealthy snack\", 69), rep(\"healthy snack\", 43),\n           rep(\"unhealthy snack\", 57), rep(\"healthy snack\", 51), rep(\"unhealthy snack\", 49))\nsnack_data <- data.frame(grade_level, snack)\n\nWe are conducting a Chi-square test in this question since we are testing the association between two categorical variables.\n\ntable(snack_data$snack,snack_data$grade_level)\n\n                 \n                  6th grade 7th grade 8th grade\n  healthy snack          31        43        51\n  unhealthy snack        69        57        49\n\nchisq.test(snack_data$snack,snack_data$grade_level,correct = FALSE)\n\n\n    Pearson's Chi-squared test\n\ndata:  snack_data$snack and snack_data$grade_level\nX-squared = 8.3383, df = 2, p-value = 0.01547\n\n\nA p-value smaller than 0.05 indicates that there is a relationship between grade level and the choice of snack."
  },
  {
    "objectID": "posts/HW2_solutions_Pang.html#question-7",
    "href": "posts/HW2_solutions_Pang.html#question-7",
    "title": "Homework 2",
    "section": "Question 7:",
    "text": "Question 7:\n\n# Creating the dataframe\nArea <- c(rep(\"Area1\", 6), rep(\"Area2\", 6), rep(\"Area3\", 6))\ncost <- c(6.2, 9.3, 6.8, 6.1, 6.7, 7.5, 7.5, 8.2, 8.5, 8.2, 7.0, 9.3,\n          5.8, 6.4, 5.6, 7.1, 3.0, 3.5)\nArea_cost <- data.frame(Area,cost)\n\nSince we are comparing the means of more than two groups, we are using the ANOVA test in this question.\n\none.way <- aov(cost ~ Area, data = Area_cost)\nsummary(one.way)\n\n            Df Sum Sq Mean Sq F value  Pr(>F)   \nArea         2  25.66  12.832   8.176 0.00397 **\nResiduals   15  23.54   1.569                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe small p-value tells us that the three areas have a difference in means."
  },
  {
    "objectID": "posts/dacss603_final_LauraCollazo.html",
    "href": "posts/dacss603_final_LauraCollazo.html",
    "title": "Proposal for DACSS 603 Final Project",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(psych)\nlibrary(lattice)\nlibrary(FSA)\nlibrary(kableExtra)\n\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/dacss603_final_LauraCollazo.html#dataset",
    "href": "posts/dacss603_final_LauraCollazo.html#dataset",
    "title": "Proposal for DACSS 603 Final Project",
    "section": "Dataset",
    "text": "Dataset\nThe data for this study was collected during the last month of the Fall 2021 semester at a large public university in the mid-west of the United States (Jeng et al., 2023). Students were recruited through voluntary response sampling in an online introduction to statistics course and received extra credit for participating in the study. The instructor of this course was not a member of the research team. A total of 240 students completed the survey and responses from 17 students were removed due to either missing demographics, the same rating for every example, or identical response made to more than 50% of open-ended questions. In total, responses from 223 students are included.\nThe full dataset can be found here.\n\nVariables\nTo answer the research question for the present study, the following variables have been selected. Details on these variables are included in following sections.\n\ngender\nrace\nbel_1\nbel_2\nbel_2_r\nbel_3\nbel_4\nbel_4_r\nbel_5\nbel_5_r\nbel_6\n\nUsing the above variables, additional variables were created to aid in analysis. These included 6 belonging variables with the data un-coded to show the actual response options and 6 more belonging variables as order factors. The variable bel_sum was also created to create an overall belonging score for each participant.\n\nGender\nThe responses for gender include “Man” (2) and “Woman or non-binary” (1). The researchers who collected this data explain that their were so few students who identified as non-binary that the sample was too small for a separate analysis (Jeng et al., 2023). What they did do was run all analysis twice, once with non-binary students excluded and once with this group combine with respondents who identified as women. They found their findings to be the same in both instances and therefore choose to combine these two groups. For the purpose of this study, this variable will be coded as “Male” and “Female.”\n\n\nRace/Ethnicity\nThe response options for race include “Asian or Asian American” (1), “Black or African American” (2), “Hispanic or Latino” (3), “White” (4), and “Other” (5).\n\n\nBelonging\nA total of 6 Likert questions were asked to measure social belonging which were adapted from Goodenow’s (1993) Psychological Sense of School Membership (PSSM) scale. Five response options (“Not at all true”, “Slightly true”, “Moderately true”, “Mostly true”, and “Completely true”) were provided to students to answer the below questions. Questions 2, 4, and 5 were reversed scored which is why 2 variables exist for each of these questions.\n\nI feel like a real part of this class\nSometimes I feel as if I don’t belong in this class\nI am included in lots of activities in this class\nI feel very different from most other students in this class\nI wish I were in a different class\nI feel proud of belonging to this class\n\n\n\nExplore Data\n\nPrepare Data\n\n\nCode\n# read in data\n\nbelonging_data <- read_csv(\"_data/belonging_survey_2022-07-08.csv\", show_col_types = FALSE)\n\n# tidy data\n\ndata <- belonging_data %>%\n  \n# filter to include only 1 set of student belonging responses\n  \n  filter(example_num == 1) %>% \n  \n# select needed columns\n  \n  select(gender, race, bel_1, bel_2, bel_2_r, bel_3, bel_4, bel_4_r, bel_5, bel_5_r, bel_6) %>%\n\n# create variable bel_sum which sums all belonging responses\n  \n  mutate(bel_sum = rowSums(across(c(bel_1, bel_2_r, bel_3, bel_4_r, bel_5_r, bel_6))))\n\n### Create new variables with Likert scores as an ordered factor\n\ndata$f_bel_1 = factor(data$bel_1,\n                       ordered = TRUE,\n                       levels = c(\"1\", \"2\", \"3\", \"4\", \"5\")\n                       )\n\ndata$f_bel_2 = factor(data$bel_2,\n                       ordered = TRUE,\n                       levels = c(\"1\", \"2\", \"3\", \"4\", \"5\")\n                       )\n\ndata$f_bel_3 = factor(data$bel_3,\n                       ordered = TRUE,\n                       levels = c(\"1\", \"2\", \"3\", \"4\", \"5\")\n                       )\n\ndata$f_bel_4 = factor(data$bel_4,\n                       ordered = TRUE,\n                       levels = c(\"1\", \"2\", \"3\", \"4\", \"5\")\n                       )\n\ndata$f_bel_5 = factor(data$bel_5,\n                       ordered = TRUE,\n                       levels = c(\"1\", \"2\", \"3\", \"4\", \"5\")\n                       )\n\ndata$f_bel_6 = factor(data$bel_6,\n                       ordered = TRUE,\n                       levels = c(\"1\", \"2\", \"3\", \"4\", \"5\")\n                       )\n  \n# recode data\n  \ndata <- data %>% \n  mutate(gender = recode(gender,\n                            `1` = \"Female\",\n                            `2` = \"Male\")) %>%\n  mutate(race = recode(race,\n                            `1` = \"Asian\",\n                            `2` = \"Black\",\n                            `3` = \"Hispanic\",\n                            `4` = \"White\",\n                            `5` = \"Other\")) %>%\n  mutate(across(bel_1:bel_6, \n                ~ recode(.x, `1` = \"Not at all true\",\n                             `2` = \"Slightly true\",\n                             `3` = \"Moderately true\",\n                             `4` = \"Mostly true\",\n                             `5` = \"Completely true\"))) \n\n\n\n\nExamine Data\n\n\nCode\n# examine data\n\ndescribe_data <- describe(x=data) %>% \n  select(c(vars, n, mean, sd, median, min, max, range))\n\nkable(describe_data) %>% \n  kable_styling(\"striped\")\n\n\n\n\n \n  \n      \n    vars \n    n \n    mean \n    sd \n    median \n    min \n    max \n    range \n  \n \n\n  \n    gender* \n    1 \n    223 \n    1.233184 \n    0.4238096 \n    1 \n    1 \n    2 \n    1 \n  \n  \n    race* \n    2 \n    223 \n    3.318386 \n    1.7037249 \n    3 \n    1 \n    5 \n    4 \n  \n  \n    bel_1* \n    3 \n    223 \n    2.479821 \n    1.2148837 \n    2 \n    1 \n    5 \n    4 \n  \n  \n    bel_2* \n    4 \n    223 \n    3.829596 \n    0.8261619 \n    4 \n    1 \n    5 \n    4 \n  \n  \n    bel_2_r* \n    5 \n    223 \n    1.645740 \n    1.1605634 \n    1 \n    1 \n    5 \n    4 \n  \n  \n    bel_3* \n    6 \n    223 \n    3.094170 \n    1.3672858 \n    3 \n    1 \n    5 \n    4 \n  \n  \n    bel_4* \n    7 \n    223 \n    3.735426 \n    1.0725698 \n    4 \n    1 \n    5 \n    4 \n  \n  \n    bel_4_r* \n    8 \n    223 \n    1.973094 \n    1.1965389 \n    1 \n    1 \n    5 \n    4 \n  \n  \n    bel_5* \n    9 \n    223 \n    3.991031 \n    0.7473137 \n    4 \n    1 \n    5 \n    4 \n  \n  \n    bel_5_r* \n    10 \n    223 \n    1.488789 \n    0.9341421 \n    1 \n    1 \n    5 \n    4 \n  \n  \n    bel_6* \n    11 \n    223 \n    2.206278 \n    1.1978043 \n    2 \n    1 \n    5 \n    4 \n  \n  \n    bel_sum \n    12 \n    223 \n    23.614350 \n    4.4846254 \n    25 \n    6 \n    30 \n    24 \n  \n  \n    f_bel_1* \n    13 \n    223 \n    3.686099 \n    1.0905551 \n    4 \n    1 \n    5 \n    4 \n  \n  \n    f_bel_2* \n    14 \n    223 \n    1.565022 \n    1.0107690 \n    1 \n    1 \n    5 \n    4 \n  \n  \n    f_bel_3* \n    15 \n    223 \n    2.932735 \n    1.2979385 \n    3 \n    1 \n    5 \n    4 \n  \n  \n    f_bel_4* \n    16 \n    223 \n    1.878924 \n    1.0690801 \n    1 \n    1 \n    5 \n    4 \n  \n  \n    f_bel_5* \n    17 \n    223 \n    1.381166 \n    0.8236397 \n    1 \n    1 \n    5 \n    4 \n  \n  \n    f_bel_6* \n    18 \n    223 \n    3.820628 \n    1.1756254 \n    4 \n    1 \n    5 \n    4 \n  \n\n\n\n\n\nCode\nstr(data)\n\n\ntibble [223 × 18] (S3: tbl_df/tbl/data.frame)\n $ gender : chr [1:223] \"Female\" \"Male\" \"Female\" \"Female\" ...\n $ race   : chr [1:223] \"White\" \"White\" \"Asian\" \"White\" ...\n $ bel_1  : chr [1:223] \"Mostly true\" \"Slightly true\" \"Completely true\" \"Mostly true\" ...\n $ bel_2  : chr [1:223] \"Moderately true\" \"Slightly true\" \"Not at all true\" \"Not at all true\" ...\n $ bel_2_r: chr [1:223] \"Moderately true\" \"Mostly true\" \"Completely true\" \"Completely true\" ...\n $ bel_3  : chr [1:223] \"Mostly true\" \"Mostly true\" \"Mostly true\" \"Mostly true\" ...\n $ bel_4  : chr [1:223] \"Slightly true\" \"Moderately true\" \"Mostly true\" \"Slightly true\" ...\n $ bel_4_r: chr [1:223] \"Mostly true\" \"Moderately true\" \"Slightly true\" \"Mostly true\" ...\n $ bel_5  : chr [1:223] \"Slightly true\" \"Slightly true\" \"Not at all true\" \"Not at all true\" ...\n $ bel_5_r: chr [1:223] \"Mostly true\" \"Mostly true\" \"Completely true\" \"Completely true\" ...\n $ bel_6  : chr [1:223] \"Mostly true\" \"Moderately true\" \"Completely true\" \"Completely true\" ...\n $ bel_sum: num [1:223] 23 20 26 27 28 24 17 23 25 29 ...\n $ f_bel_1: Ord.factor w/ 5 levels \"1\"<\"2\"<\"3\"<\"4\"<..: 4 2 5 4 4 3 4 3 4 4 ...\n $ f_bel_2: Ord.factor w/ 5 levels \"1\"<\"2\"<\"3\"<\"4\"<..: 3 2 1 1 1 1 4 1 1 1 ...\n $ f_bel_3: Ord.factor w/ 5 levels \"1\"<\"2\"<\"3\"<\"4\"<..: 4 4 4 4 4 5 3 2 1 5 ...\n $ f_bel_4: Ord.factor w/ 5 levels \"1\"<\"2\"<\"3\"<\"4\"<..: 2 3 4 2 1 3 3 1 1 1 ...\n $ f_bel_5: Ord.factor w/ 5 levels \"1\"<\"2\"<\"3\"<\"4\"<..: 2 2 1 1 1 1 2 1 1 1 ...\n $ f_bel_6: Ord.factor w/ 5 levels \"1\"<\"2\"<\"3\"<\"4\"<..: 4 3 5 5 5 3 1 3 5 5 ...\n\n\n\n\nExamine Variables\n\n\nCode\n# create xtabs and bar plot to visualize variables\n\n# gender\n\nxt_gender <- xtabs(~gender, data = data)\n\nkable(xt_gender) %>% \n  kable_styling(\"striped\")\n\n\n\n\n \n  \n    gender \n    Freq \n  \n \n\n  \n    Female \n    171 \n  \n  \n    Male \n    52 \n  \n\n\n\n\n\nCode\nbarplot(xt_gender, \n        xlab = \"Gender\",\n        ylab = \"Frequency\")\n\n\n\n\n\nCode\n# race\n\nxt_race <- xtabs(~race, data = data)\n\nkable(xt_race) %>% \n  kable_styling(\"striped\")\n\n\n\n\n \n  \n    race \n    Freq \n  \n \n\n  \n    Asian \n    59 \n  \n  \n    Black \n    21 \n  \n  \n    Hispanic \n    33 \n  \n  \n    Other \n    10 \n  \n  \n    White \n    100 \n  \n\n\n\n\n\nCode\nbarplot(xt_race, \n        xlab = \"Race\",\n        ylab = \"Frequency\")\n\n\n\n\n\nCode\n# bel_sum\n\nxt_sum <- xtabs(~bel_sum, data = data)\n\nkable(xt_sum) %>% \n  kable_styling(\"striped\")\n\n\n\n\n \n  \n    bel_sum \n    Freq \n  \n \n\n  \n    6 \n    1 \n  \n  \n    8 \n    1 \n  \n  \n    10 \n    1 \n  \n  \n    11 \n    2 \n  \n  \n    12 \n    1 \n  \n  \n    13 \n    2 \n  \n  \n    14 \n    2 \n  \n  \n    15 \n    3 \n  \n  \n    16 \n    1 \n  \n  \n    17 \n    8 \n  \n  \n    18 \n    9 \n  \n  \n    19 \n    7 \n  \n  \n    20 \n    7 \n  \n  \n    21 \n    12 \n  \n  \n    22 \n    15 \n  \n  \n    23 \n    19 \n  \n  \n    24 \n    17 \n  \n  \n    25 \n    32 \n  \n  \n    26 \n    21 \n  \n  \n    27 \n    25 \n  \n  \n    28 \n    12 \n  \n  \n    29 \n    12 \n  \n  \n    30 \n    13 \n  \n\n\n\n\n\nCode\nbarplot(xt_sum, \n        xlab = \"Belonging\",\n        ylab = \"Frequency\")\n\n\n\n\n\nCode\n# f_bel_1\n\nxt_1 <- xtabs(~f_bel_1, data = data)\n\nkable(xt_1) %>% \n  kable_styling(\"striped\")\n\n\n\n\n \n  \n    f_bel_1 \n    Freq \n  \n \n\n  \n    1 \n    9 \n  \n  \n    2 \n    23 \n  \n  \n    3 \n    55 \n  \n  \n    4 \n    78 \n  \n  \n    5 \n    58 \n  \n\n\n\n\n\nCode\nbarplot(xt_1, \n        xlab = \"I feel like a real part of this class\",\n        ylab = \"Frequency\")\n\n\n\n\n\nCode\n# f_bel_2\n\nxt_2 <- xtabs(~f_bel_2, data = data)\n\nkable(xt_2) %>% \n  kable_styling(\"striped\")\n\n\n\n\n \n  \n    f_bel_2 \n    Freq \n  \n \n\n  \n    1 \n    157 \n  \n  \n    2 \n    28 \n  \n  \n    3 \n    20 \n  \n  \n    4 \n    14 \n  \n  \n    5 \n    4 \n  \n\n\n\n\n\nCode\nbarplot(xt_2, \n        xlab = \"Sometimes I feel as if I don’t belong in this class\",\n        ylab = \"Frequency\")\n\n\n\n\n\nCode\n# f_bel_3\n\nxt_3 <- xtabs(~f_bel_3, data = data)\n\nkable(xt_3) %>% \n  kable_styling(\"striped\")\n\n\n\n\n \n  \n    f_bel_3 \n    Freq \n  \n \n\n  \n    1 \n    38 \n  \n  \n    2 \n    50 \n  \n  \n    3 \n    55 \n  \n  \n    4 \n    49 \n  \n  \n    5 \n    31 \n  \n\n\n\n\n\nCode\nbarplot(xt_3, \n        xlab = \"I am included in lots of activities in this class\",\n        ylab = \"Frequency\")\n\n\n\n\n\nCode\n# f_bel_4\n\nxt_4 <- xtabs(~f_bel_4, data = data)\n\nkable(xt_4) %>% \n  kable_styling(\"striped\")\n\n\n\n\n \n  \n    f_bel_4 \n    Freq \n  \n \n\n  \n    1 \n    112 \n  \n  \n    2 \n    51 \n  \n  \n    3 \n    40 \n  \n  \n    4 \n    15 \n  \n  \n    5 \n    5 \n  \n\n\n\n\n\nCode\nbarplot(xt_4, \n        xlab = \"I feel very different from most other students in this class\",\n        ylab = \"Frequency\")\n\n\n\n\n\nCode\n# f_bel_5\n\nxt_5 <- xtabs(~f_bel_5, data = data)\n\nkable(xt_5) %>% \n  kable_styling(\"striped\")\n\n\n\n\n \n  \n    f_bel_5 \n    Freq \n  \n \n\n  \n    1 \n    169 \n  \n  \n    2 \n    36 \n  \n  \n    3 \n    10 \n  \n  \n    4 \n    3 \n  \n  \n    5 \n    5 \n  \n\n\n\n\n\nCode\nbarplot(xt_5, \n        xlab = \"I wish I were in a different class\",\n        ylab = \"Frequency\")\n\n\n\n\n\nCode\n# f_bel_6\n\nxt_6 <- xtabs(~f_bel_6, data = data)\n\nkable(xt_6) %>% \n  kable_styling(\"striped\")\n\n\n\n\n \n  \n    f_bel_6 \n    Freq \n  \n \n\n  \n    1 \n    13 \n  \n  \n    2 \n    15 \n  \n  \n    3 \n    54 \n  \n  \n    4 \n    58 \n  \n  \n    5 \n    83 \n  \n\n\n\n\n\nCode\nbarplot(xt_6, \n        xlab = \"I feel proud of belonging to this class\",\n        ylab = \"Frequency\")"
  },
  {
    "objectID": "posts/abigailbalint_hw1.html",
    "href": "posts/abigailbalint_hw1.html",
    "title": "Homework 1",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(readxl)\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/abigailbalint_hw1.html#question-1---lung-capacity",
    "href": "posts/abigailbalint_hw1.html#question-1---lung-capacity",
    "title": "Homework 1",
    "section": "Question 1 - Lung Capacity",
    "text": "Question 1 - Lung Capacity\nReading in LungCapData –\n\n\nCode\nlung <- read_excel(\"_data/LungCapData.xls\")\nhead(lung,2)\n\n\n# A tibble: 2 × 6\n  LungCap   Age Height Smoke Gender Caesarean\n    <dbl> <dbl>  <dbl> <chr> <chr>  <chr>    \n1    6.48     6   62.1 no    male   no       \n2   10.1     18   74.7 yes   female no       \n\n\nLooking at some basic descriptive stats –\n\n\nCode\nglimpse(lung)\n\n\nRows: 725\nColumns: 6\n$ LungCap   <dbl> 6.475, 10.125, 9.550, 11.125, 4.800, 6.225, 4.950, 7.325, 8.…\n$ Age       <dbl> 6, 18, 16, 14, 5, 11, 8, 11, 15, 11, 19, 17, 12, 10, 10, 13,…\n$ Height    <dbl> 62.1, 74.7, 69.7, 71.0, 56.9, 58.7, 63.3, 70.4, 70.5, 59.2, …\n$ Smoke     <chr> \"no\", \"yes\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\",…\n$ Gender    <chr> \"male\", \"female\", \"female\", \"male\", \"male\", \"female\", \"male\"…\n$ Caesarean <chr> \"no\", \"no\", \"yes\", \"no\", \"no\", \"no\", \"yes\", \"no\", \"no\", \"no\"…\n\n\nCode\nmean(lung$LungCap, na.rm = T)\n\n\n[1] 7.863148\n\n\nCode\nvar(lung$LungCap, na.rm = T)\n\n\n[1] 7.086288\n\n\nCode\nsd(lung$LungCap, na.rm = T)\n\n\n[1] 2.662008\n\n\nCode\nrange(lung$LungCap, na.rm = T)\n\n\n[1]  0.507 14.675\n\n\n\nWhat does the distribution of LungCap look like?\n\n\n\nCode\nggplot(lung, aes(x = LungCap)) +\n  geom_histogram()\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nThe distribution looks relatively normal. There is a clear concentration of the sample around 7-8 and the outliers are only a very small portion of the sample.\n\nCompare the probability distribution of the LungCap with respect to Males and Females\n\n\n\nCode\nggplot(lung, aes(x=LungCap)) + \n    geom_boxplot(fill=\"slateblue\", alpha=0.2) + \n    xlab(\"Lung Capacity\") +\n  facet_wrap(\"Gender\")\n\n\n\n\n\nThe probability distribution is pretty similar between male and female, but males skew to a higher lung capacity overall and the median line is at around 8 whereas female is closer to 7.5.\n\nCompare the mean lung capacities for smokers and non-smokers. Does it make sense?\n\n\n\nCode\nlung %>%\n  group_by(Smoke) %>%\n  summarise(mean = mean(LungCap), n = n())\n\n\n# A tibble: 2 × 3\n  Smoke  mean     n\n  <chr> <dbl> <int>\n1 no     7.77   648\n2 yes    8.65    77\n\n\nWe would expect the lung capacities for non smokers to be higher but the mean for smokers is actually a little bit higher.\n\nExamine the relationship between Smoking and Lung Capacity within age groups: “less than or equal to 13”, “14 to 15”, “16 to 17”, and “greater than or equal to 18”.\n\nRecoding the age groups –\n\n\nCode\nlunggroup <- lung %>%\n  mutate(`AgeGroup` = dplyr::case_when(\n    `Age` >= 0 & `Age` < 14 ~ \"0-13\",\n    `Age` >= 14 & `Age` < 16 ~ \"14-15\",\n    `Age` >= 16 & `Age` < 18 ~ \"16-17\",\n    `Age` >= 18 ~ \"18+\" ))\nhead(lunggroup)\n\n\n# A tibble: 6 × 7\n  LungCap   Age Height Smoke Gender Caesarean AgeGroup\n    <dbl> <dbl>  <dbl> <chr> <chr>  <chr>     <chr>   \n1    6.48     6   62.1 no    male   no        0-13    \n2   10.1     18   74.7 yes   female no        18+     \n3    9.55    16   69.7 no    female yes       16-17   \n4   11.1     14   71   no    male   no        14-15   \n5    4.8      5   56.9 no    male   no        0-13    \n6    6.22    11   58.7 no    female no        0-13    \n\n\nMean lung capacity by age group –\n\n\nCode\nlunggroup %>%\ngroup_by(Smoke, AgeGroup) %>%\n  summarise(mean = mean(LungCap), n = n())\n\n\n`summarise()` has grouped output by 'Smoke'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 8 × 4\n# Groups:   Smoke [2]\n  Smoke AgeGroup  mean     n\n  <chr> <chr>    <dbl> <int>\n1 no    0-13      6.36   401\n2 no    14-15     9.14   105\n3 no    16-17    10.5     77\n4 no    18+      11.1     65\n5 yes   0-13      7.20    27\n6 yes   14-15     8.39    15\n7 yes   16-17     9.38    20\n8 yes   18+      10.5     15\n\n\nFor both smokers and non-smokers, the lung capacity goes up as the age increases with 18+ having the highest average capacity. In all age ranges besides 0-13 (the broadest range), the mean is higher for non-smokers than smokers.\n\nCompare the lung capacities for smokers and non-smokers within each age group. Is your answer different from the one in part c. What could possibly be going on here?\n\n\n\nCode\nggplot(lunggroup, aes(x=LungCap, fill=Smoke)) + \n    geom_boxplot() + \n    xlab(\"Lung Capacity\") +\n  facet_wrap(\"AgeGroup\")\n\n\n\n\n\nI’m seeing that the results by age group are slightly different than in part C. Above I can see that the average for all age ranges is higher for non-smokers, besides age group 0-13. I can see in my results in part D that the sample size for 0-13 non-smokers is extremely high, much higher than any other group of smokers or non-smokers, so with this higher sample size comes more variance. The median lines are actually pretty close but the outliers are probably affecting the mean."
  },
  {
    "objectID": "posts/abigailbalint_hw1.html#question-2",
    "href": "posts/abigailbalint_hw1.html#question-2",
    "title": "Homework 1",
    "section": "Question 2",
    "text": "Question 2\nCreating a data frame –\n\n\nCode\npriorconviction <- c(0,1,2,3,4)\nprisoners <- c(128,434,160,64,24)\nq2 <- data.frame(priorconviction, prisoners)\nhead(q2)\n\n\n  priorconviction prisoners\n1               0       128\n2               1       434\n3               2       160\n4               3        64\n5               4        24\n\n\n\nWhat is the probability that a randomly selected inmate has exactly 2 prior convictions?\n\n\n\nCode\n160/810\n\n\n[1] 0.1975309\n\n\nI found it to be .1975 or 19.75%\n\nWhat is the probability that a randomly selected inmate has fewer than 2 prior convictions?\n\n\n\nCode\n(434+128)/810\n\n\n[1] 0.6938272\n\n\nTo get this I added the sample of 0 or 1 prior conviction and it comes out to .69 or 69%.\n\nWhat is the probability that a randomly selected inmate has 2 or fewer prior convictions?\n\n\n\nCode\n(128+434+160)/810\n\n\n[1] 0.891358\n\n\nTo get this I added the sample of 0 or 1 or 2 prior convictions and it comes out to .89 or 89%.\n\nWhat is the probability that a randomly selected inmate has more than 2 prior convictions?\n\n\n\nCode\n(64+24)/810\n\n\n[1] 0.108642\n\n\nTo get this I added the sample of 3 or 4 prior convictions and it comes out to .108 or 11%.\n\nWhat is the expected value1 for the number of prior convictions?\n\n\n\nCode\nsum(q2$priorconviction*prisoners)\n\n\n[1] 1042\n\n\nCode\n1042/810\n\n\n[1] 1.28642\n\n\nTo get this I summed all of the numbers of prior convictions by the amount of prisoners (1042) then divided this by total sample (810) to get a final expected value of 1.28 prior convictions.\n\nCalculate the variance and the standard deviation for the Prior Convictions.\n\n\n\nCode\nvar(q2$priorconviction)\n\n\n[1] 2.5\n\n\nCode\nvar(q2$priorconviction)*(5-1)/5\n\n\n[1] 2\n\n\nI used the above code to find a sample variance of 2.5 and a population variance of 2.\n\n\nCode\nsd(q2$priorconviction)\n\n\n[1] 1.581139\n\n\nI used the standard deviation function to calculate the above.\n:::"
  },
  {
    "objectID": "posts/HW3_JustineShakespeare.html",
    "href": "posts/HW3_JustineShakespeare.html",
    "title": "Homework 3",
    "section": "",
    "text": "Before we dive into the homework let’s load the data."
  },
  {
    "objectID": "posts/HW3_JustineShakespeare.html#a",
    "href": "posts/HW3_JustineShakespeare.html#a",
    "title": "Homework 3",
    "section": "a",
    "text": "a\nIdentify the predictor and the response.\nFirst we’ll load the data and take a look.\n\n\nCode\ndata(UN11)\nglimpse(UN11)\n\n\nRows: 199\nColumns: 6\n$ region    <fct> Asia, Europe, Africa, Africa, Caribbean, Latin Amer, Asia, C…\n$ group     <fct> other, other, africa, africa, other, other, other, other, oe…\n$ fertility <dbl> 5.968, 1.525, 2.142, 5.135, 2.000, 2.172, 1.735, 1.671, 1.94…\n$ ppgdp     <dbl> 499.0, 3677.2, 4473.0, 4321.9, 13750.1, 9162.1, 3030.7, 2285…\n$ lifeExpF  <dbl> 49.49, 80.40, 75.00, 53.17, 81.10, 79.89, 77.33, 77.75, 84.2…\n$ pctUrban  <dbl> 23, 53, 67, 59, 100, 93, 64, 47, 89, 68, 52, 84, 89, 29, 45,…\n\n\nFertility is the response variable and ppgdp is the predictor variable."
  },
  {
    "objectID": "posts/HW3_JustineShakespeare.html#b",
    "href": "posts/HW3_JustineShakespeare.html#b",
    "title": "Homework 3",
    "section": "b",
    "text": "b\nDraw the scatterplot of fertility on the vertical axis versus ppgdp on the horizontal axis and summarize the information in this graph. Does a straight-line mean function seem to be plausible for a summary of this graph?\nWe’ll use ggplot() to draw the scatterplot.\n\n\nCode\nggplot(UN11, aes(ppgdp, fertility)) + geom_point() +\n  labs(title = \"Fertility by GDP per Capita\", x = \"GDP per capita\", \n       y = \"Fertility (birth rate per 1000 females)\")\n\n\n\n\n\nThis graph shows that fertility levels can vary widely for countries with low gdp per capita, but as the gdp per capita gets larger, the fertility levels are relatively low. Given the shape, it does not seem like a straight-line mean function will be appropriate as a summary of this graph."
  },
  {
    "objectID": "posts/HW3_JustineShakespeare.html#c",
    "href": "posts/HW3_JustineShakespeare.html#c",
    "title": "Homework 3",
    "section": "c",
    "text": "c\nDraw the scatterplot of log(fertility) versus log(ppgdp) using natural logarithms. Does the simple linear regression model seem plausible for a summary of this graph? If you use a different base of logarithms, the shape of the graph won’t change, but the values on the axes will change.\nWe’ll use the log() function to get the natural logarithms of both variables.\n\n\nCode\nUN11$log_fertility <- log(UN11$fertility)\nUN11$log_ppgdp <- log(UN11$ppgdp)\n\nggplot(UN11, aes(log_ppgdp, log_fertility)) + geom_point() +\n  labs(title = \"Fertility by GDP per Capita: Logarithm Edition\", \n       x = \"Log of GDP per capita\", \n       y = \"Log of Fertility (birth rate per 1000 females)\") + \n  geom_smooth(method = \"lm\")\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nThis graph does lend itself well to using a linear regression model. We’ve added an OLS line here using the geom_smooth() function."
  },
  {
    "objectID": "posts/HW3_JustineShakespeare.html#a-1",
    "href": "posts/HW3_JustineShakespeare.html#a-1",
    "title": "Homework 3",
    "section": "a",
    "text": "a\nHow, if at all, does the slope of the prediction equation change?\nTo answer this question we’ll run a linear regression for both x_US and y, and x_UK and y and compare the coefficients.\n\n\nCode\n## US dollars\nLR_US <- lm(formula = y ~ x_US, data = df)\nsummary(LR_US)$coefficients\n\n\n            Estimate Std. Error   t value     Pr(>|t|)\n(Intercept) 3.150123 2.59153772  1.215542 2.270796e-01\nx_US        1.949919 0.05204747 37.464236 6.901293e-60\n\n\nCode\n## UK pounds sterling\nLR_UK <- lm(formula = y ~ x_UK, data = df)\nsummary(LR_UK)$coefficients\n\n\n            Estimate Std. Error   t value     Pr(>|t|)\n(Intercept) 3.150123 2.59153772  1.215542 2.270796e-01\nx_UK        2.593392 0.06922314 37.464236 6.901293e-60\n\n\nThe coefficients are different, indicating that the slope did change with the different currency. The signs are the same (both slopes are positive and indicate a positive relationship), but the slope of the regression with pounds sterling is steeper."
  },
  {
    "objectID": "posts/HW3_JustineShakespeare.html#b-1",
    "href": "posts/HW3_JustineShakespeare.html#b-1",
    "title": "Homework 3",
    "section": "b",
    "text": "b\nHow, if at all, does the correlation change?\nWe’ll run the cor() function to compare the correlations.\n\n\nCode\ncor(df$x_US, df$y, method = \"pearson\")\n\n\n[1] 0.9668169\n\n\nCode\ncor(df$x_UK, df$y, method = \"pearson\")\n\n\n[1] 0.9668169\n\n\nThe correlations appears to be the same for pounds sterling and US dollars."
  },
  {
    "objectID": "posts/HW3_JustineShakespeare.html#a-2",
    "href": "posts/HW3_JustineShakespeare.html#a-2",
    "title": "Homework 3",
    "section": "a",
    "text": "a\nGraphically portray how the explanatory variable relates to the outcome variable in each of the two cases.\nWe can explore this relationship and our finding with a visual depiction of the data.\nThe scatterplot of this data is a little awkward because of the nature of the data (that it was originally ordinal variables).\n\n\nCode\nggplot(student.survey_i, aes(religiosity, pol_ideo)) + \n  geom_point() + \n  geom_smooth(method = \"lm\") +\n  labs(title = \"Political Ideology and Religiosity\", x = \"Religiosity\", y = \"Political Ideology\")\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nLet’s see what it looks like in a bar chart. We’ll use the original variables so that R recognizes them as ordinal.\n\n\nCode\nggplot(student.survey_i, aes(re, fill = pi)) + \n  geom_bar() +\n  labs(title = \"Political Ideology and Religiosity\", x = \"How often respondents attend religious services\", y = \"Count\", fill = \"Political Ideology\") +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\nThis chart illustrates the relationship a little better. You can see that pretty much all of the respondents who identified as “very conservative” also reported attending religious services “every week”, which is the most frequent choice available."
  },
  {
    "objectID": "posts/HW3_JustineShakespeare.html#b-2",
    "href": "posts/HW3_JustineShakespeare.html#b-2",
    "title": "Homework 3",
    "section": "b",
    "text": "b\nSummarize and interpret results of inferential analyses.\nWe can run the linear regression with the numeric variables we made earlier.\n\n\nCode\npol_reg <- lm(pol_ideo ~ religiosity, data = student.survey_i)\nsummary(pol_reg)\n\n\n\nCall:\nlm(formula = pol_ideo ~ religiosity, data = student.survey_i)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.81243 -0.87160  0.09882  1.12840  3.09882 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   0.9308     0.4252   2.189   0.0327 *  \nreligiosity   0.9704     0.1792   5.416 1.22e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.345 on 58 degrees of freedom\nMultiple R-squared:  0.3359,    Adjusted R-squared:  0.3244 \nF-statistic: 29.34 on 1 and 58 DF,  p-value: 1.221e-06\n\n\nThe results of this regression indicate that there is a positive and significant relationship between religiosity and political ideology (p-value 1.22e-06 <0.001), with people who report being more conservative also reporting being more likely to attend religious services often. The R-squared value is also fairly high (0.3359), indicating that the model is a relatively good fit.\nNow let’s take a look at the second set of variables and explore their relationship. (ii) y = high school GPA and x = hours of TV watching."
  },
  {
    "objectID": "posts/HW3_JustineShakespeare.html#a-3",
    "href": "posts/HW3_JustineShakespeare.html#a-3",
    "title": "Homework 3",
    "section": "a",
    "text": "a\nGraphically portray how the explanatory variable relates to the outcome variable in each of the two cases.\nSince both of these variables are continuous this relationship lends itself well to being visualized with a scatterplot.\n\n\nCode\nggplot(student.survey_i, aes(tv, hi)) + \n  geom_point() + \n  labs(title = \"High school GPA and hours of of TV watched per week\", x = \"Hours of TV watched per week\", y = \"High school GPA\")\n\n\n\n\n\nThis visualization indicates there is a negative relationship between the number of hours of tv watched per week and high school GPA."
  },
  {
    "objectID": "posts/HW3_JustineShakespeare.html#b-3",
    "href": "posts/HW3_JustineShakespeare.html#b-3",
    "title": "Homework 3",
    "section": "b",
    "text": "b\nSummarize and interpret results of inferential analyses.\n\n\nCode\ngpa_tv <- lm(hi ~ tv, data = student.survey_i)\nsummary(gpa_tv)\n\n\n\nCall:\nlm(formula = hi ~ tv, data = student.survey_i)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.2583 -0.2456  0.0417  0.3368  0.7051 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  3.441353   0.085345  40.323   <2e-16 ***\ntv          -0.018305   0.008658  -2.114   0.0388 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4467 on 58 degrees of freedom\nMultiple R-squared:  0.07156,   Adjusted R-squared:  0.05555 \nF-statistic: 4.471 on 1 and 58 DF,  p-value: 0.03879\n\n\nThis linear regression model indicates that the relationship between these variables is negative, as we could see from the scatterplot above. It is also significant (p-vlue 0.0388 < 0.05). The R-squared is not as large as the previous relationship we explored, indicating the model is not as good of a fit."
  },
  {
    "objectID": "posts/HW1_solution_Pang.html",
    "href": "posts/HW1_solution_Pang.html",
    "title": "Homework 1 Solution",
    "section": "",
    "text": "First, let’s read in the data from the Excel file:\n\n\nCode\nlibrary(readxl)\nlibrary(dplyr, warn.conflicts = F)\nlibrary(magrittr)\ndf <- read_excel(\"_data/LungCapData.xls\")\n\n\nThe distribution of LungCap looks as follows:\n\n\nCode\nhist(df$LungCap, xlab = 'Lung Capacity', main = '', freq = F)\n\n\n\n\n\nThe histogram suggests that the distribution is close to a normal distribution. Most of the observations are close to the mean. Very few observations are close to the margins (0 and 15).\n\n\n\n\n\nCode\nboxplot(LungCap ~ Gender, data = df)\n\n\n\n\n\nThe shape of the distribution is similar for males and females. The median, first quartile, third quartile lung capacity values all seem to be somewhat higher for males.\n\n\n\n\n\nCode\ndf %>%\n  group_by(Smoke) %>%\n  summarize(LungCap = mean(LungCap))\n\n\n# A tibble: 2 × 2\n  Smoke LungCap\n  <chr>   <dbl>\n1 no       7.77\n2 yes      8.65\n\n\nThe lung capacity for smokers seems to be higher than non-smokers. It goes against the common idea that smoking would hurt lung capacity.\n\n\n\n\nLess than or equal to 13\n\n\n\nCode\ndf %>%\n  filter(Age <= 13) %>%\n  group_by(Smoke) %>%\n  summarize(LungCap = mean(LungCap))\n\n\n# A tibble: 2 × 2\n  Smoke LungCap\n  <chr>   <dbl>\n1 no       6.36\n2 yes      7.20\n\n\n\n14 to 15\n\n\n\nCode\ndf %>%\n  filter(Age == 14 | Age == 15) %>%\n  group_by(Smoke) %>%\n  summarize(LungCap = mean(LungCap))\n\n\n# A tibble: 2 × 2\n  Smoke LungCap\n  <chr>   <dbl>\n1 no       9.14\n2 yes      8.39\n\n\n\n16 to 17\n\n\n\nCode\ndf %>%\n  filter(Age == 16 | Age == 17) %>%\n  group_by(Smoke) %>%\n  summarize(LungCap = mean(LungCap))\n\n\n# A tibble: 2 × 2\n  Smoke LungCap\n  <chr>   <dbl>\n1 no      10.5 \n2 yes      9.38\n\n\n\nGreater than or equal to 18\n\n\n\nCode\ndf %>%\n  filter(Age >= 18) %>%\n  group_by(Smoke) %>%\n  summarize(LungCap = mean(LungCap))\n\n\n# A tibble: 2 × 2\n  Smoke LungCap\n  <chr>   <dbl>\n1 no       11.1\n2 yes      10.5\n\n\n\n\n\nFor three out of the four groups, lung capacity if smaller for smokers. This makes another explanation plausible. Smoking is inversely related to lung capacity, but older people both smoke more and have more lung capacity. Thus, considering the relationship between smoking and lung capacity without looking at age makes the relationship look the opposite of what it is."
  },
  {
    "objectID": "posts/HW1_solution_Pang.html#a-1",
    "href": "posts/HW1_solution_Pang.html#a-1",
    "title": "Homework 1 Solution",
    "section": "a",
    "text": "a\n\n\nCode\ntb %>%\n  filter(X == 2) %>%\n  pull(Frequency) %>%\n  divide_by(n)\n\n\n[1] 0.1975309"
  },
  {
    "objectID": "posts/HW1_solution_Pang.html#b-1",
    "href": "posts/HW1_solution_Pang.html#b-1",
    "title": "Homework 1 Solution",
    "section": "b",
    "text": "b\n\n\nCode\ntb %>%\n  filter(X < 2) %>%\n  pull(Frequency) %>%\n  sum() %>%\n  divide_by(n)\n\n\n[1] 0.6938272"
  },
  {
    "objectID": "posts/HW1_solution_Pang.html#c-1",
    "href": "posts/HW1_solution_Pang.html#c-1",
    "title": "Homework 1 Solution",
    "section": "c",
    "text": "c\n\n\nCode\ntb %>%\n  filter(X <= 2) %>%\n  pull(Frequency) %>%\n  sum() %>%\n  divide_by(n)\n\n\n[1] 0.891358"
  },
  {
    "objectID": "posts/HW1_solution_Pang.html#d-1",
    "href": "posts/HW1_solution_Pang.html#d-1",
    "title": "Homework 1 Solution",
    "section": "d",
    "text": "d\n\n\nCode\ntb %>%\n  filter(X > 2) %>%\n  pull(Frequency) %>%\n  sum() %>%\n  divide_by(n)\n\n\n[1] 0.108642"
  },
  {
    "objectID": "posts/HW1_solution_Pang.html#e-1",
    "href": "posts/HW1_solution_Pang.html#e-1",
    "title": "Homework 1 Solution",
    "section": "e",
    "text": "e\nExpected number of prior convictions is just a weighted average of the number of prior convictions.\n\nMethod 1: Multiply every value with their frequency, then divide by total frequency i.e. (0 * 128 + 1 * 434 + 2 * 160 ……) / 810.\n\n\n\nCode\nsum(tb$X * tb$Frequency) / n\n\n\n[1] 1.28642\n\n\n\nMethod 2: Multiply every value with their probility, sum them up.\n\n\n\nCode\ntb %>%\n  mutate(probability = Frequency / n) -> tb\n\nprint(tb)\n\n\n# A tibble: 5 × 3\n      X Frequency probability\n  <dbl>     <dbl>       <dbl>\n1     0       128      0.158 \n2     1       434      0.536 \n3     2       160      0.198 \n4     3        64      0.0790\n5     4        24      0.0296\n\n\n\n\nCode\nsum(tb$X * tb$probability)\n\n\n[1] 1.28642\n\n\n\nMethod 3: Recreate the whole sample (a vector that has 128 zeroes, 434 ones, 160 twos, ….) with a total length/size of 810. Take the mean.\n\n\n\nCode\nsample <- c(rep(0, 128), rep(1, 434), rep(2, 160), rep(3, 64), rep(4, 24))\nmean(sample)\n\n\n[1] 1.28642"
  },
  {
    "objectID": "posts/HW1_solution_Pang.html#f",
    "href": "posts/HW1_solution_Pang.html#f",
    "title": "Homework 1 Solution",
    "section": "f",
    "text": "f\n\nMethod 1: Let’s start from the end: we have the sample, just call var() and sd()\n\n\n\nCode\ncat('Variance:', var(sample))\n\n\nVariance: 0.8572937\n\n\nCode\ncat('\\nStandard Deviation:', sd(sample))\n\n\n\nStandard Deviation: 0.9259016\n\n\nMethod 2: Manually apply the formula using weights.\nStandard deviation is square root of variance. So let’s calculate variance first. For that we need the mean. Let’s pull the expected value from the previous section:\n\n\nCode\nm <- sum(tb$X * tb$Frequency) / n\n\n\nFor every observation, we’ll need the squared difference from mean (squared deviation from mean).\n\n\nCode\ntb %>%\n  mutate(sq_deviation = (X - m)^2) -> tb \nprint(tb)\n\n\n# A tibble: 5 × 4\n      X Frequency probability sq_deviation\n  <dbl>     <dbl>       <dbl>        <dbl>\n1     0       128      0.158        1.65  \n2     1       434      0.536        0.0820\n3     2       160      0.198        0.509 \n4     3        64      0.0790       2.94  \n5     4        24      0.0296       7.36  \n\n\nThen, we can now multiply them with probability.\n\n\nCode\nsum(tb$sq_deviation * tb$probability)\n\n\n[1] 0.8562353\n\n\nThis gives us the ‘population’ variance. If we wanted the ‘sample’ variance, what the var() function does, we could manually apply the Bessel’s correction:\n\n\nCode\nvariance <- sum(tb$sq_deviation * tb$probability) * (n / (n-1))\nprint(variance)\n\n\n[1] 0.8572937\n\n\nStandard deviation is then just the square root:\n\n\nCode\nsqrt(variance)\n\n\n[1] 0.9259016\n\n\nThis replicated what we found directly using the sample."
  },
  {
    "objectID": "posts/XiaoyanHu_Finalproject1.html",
    "href": "posts/XiaoyanHu_Finalproject1.html",
    "title": "Final Project Checkin-1",
    "section": "",
    "text": "Code\nlibrary(tidyr)\nlibrary(dplyr)\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nCode\nlibrary(readxl)\nlibrary(ggplot2)"
  },
  {
    "objectID": "posts/XiaoyanHu_Finalproject1.html#introduction-and-background",
    "href": "posts/XiaoyanHu_Finalproject1.html#introduction-and-background",
    "title": "Final Project Checkin-1",
    "section": "Introduction and background",
    "text": "Introduction and background\nThe Chinese government implemented the one-child policy in 1979, which resulted in the increasing proportion of one-child families and the “four-two-one” family structure consisting of four grandparents, two parents, and one child. Despite being blessed with relatively more family and social resources, only children may face physical and socio-psychological problems during development, including an elevated risk for overweight and obesity and negative psychosocial consequences. Previous studies have shown that only children had a higher likelihood of overweight or obesity, compared with children who had one or more siblings. Over obesity, mental healthy is also interesting to explore that how it is related to overweight/obesity, as well as sib-size, in young adolescents affects mental health.。"
  },
  {
    "objectID": "posts/XiaoyanHu_Finalproject1.html#research-questions",
    "href": "posts/XiaoyanHu_Finalproject1.html#research-questions",
    "title": "Final Project Checkin-1",
    "section": "research questions",
    "text": "research questions\n\nDoes obesity positively related to mental health?\nwhat are factors that affects mental healthy?\ndoes sibling or obeisty directily related to mental health?"
  },
  {
    "objectID": "posts/XiaoyanHu_Finalproject1.html#key-predictors",
    "href": "posts/XiaoyanHu_Finalproject1.html#key-predictors",
    "title": "Final Project Checkin-1",
    "section": "key predictors",
    "text": "key predictors\n\nmental health\nsibling number\nobisity rate\ngender"
  },
  {
    "objectID": "posts/XiaoyanHu_Finalproject1.html#hypothesis",
    "href": "posts/XiaoyanHu_Finalproject1.html#hypothesis",
    "title": "Final Project Checkin-1",
    "section": "hypothesis",
    "text": "hypothesis\n\nHigher obesity rate increase the risk of depression\nhigher family income increase the rate of obesity\nMore sibling reduce the risk of both depression and anxiety."
  },
  {
    "objectID": "posts/XiaoyanHu_Finalproject1.html#data-description",
    "href": "posts/XiaoyanHu_Finalproject1.html#data-description",
    "title": "Final Project Checkin-1",
    "section": "data description",
    "text": "data description\n\n\nCode\ndata<-read_excel(\"/Users/cassie199/Desktop/23spring/603_Spring_2023-1/posts/_data/mentalhealth_data.xlsx\")\nhead(data)\n\n\n# A tibble: 6 × 29\n  T0depres…¹ T0anx…² T1dep…³ T1anx…⁴ Height Weight    WC    HC   SBP   DBP   FBG\n       <dbl>   <dbl>   <dbl>   <dbl>  <dbl>  <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n1         31      35      41      35   153.   34.6    58  67      98    60   4.4\n2         35      24      35      25   172.   46.1    63  78     110    70   3.9\n3         31      34      37      26   146.   38.9    72  77.7   102    62   4.6\n4         27      31      42      35   162.   46.8    62  80     116    80   4.5\n5         31      26      49      33   154.   36.4    56  72      90    60   4.2\n6         30      28      47      32   164.   40.6    55  73     102    70   3.7\n# … with 18 more variables: TC <dbl>, TG <dbl>, `HDL-C` <dbl>, `LDL-C` <dbl>,\n#   BMI <dbl>, WHR <dbl>, WtHR <dbl>, `Family location` <dbl>,\n#   `Number of siblings` <dbl>,\n#   `How much time do you spend with your father in elementary school?` <dbl>,\n#   `How much time do you spend with your mother in elementary school?` <dbl>,\n#   `Father’s education level` <dbl>, `Mother’s education level` <dbl>,\n#   `Family financial situation` <dbl>, `Sleeping hours` <dbl>, …\n\n\nCode\nglimpse(data)\n\n\nRows: 1,348\nColumns: 29\n$ T0depression                                                        <dbl> 31…\n$ T0anxiety                                                           <dbl> 35…\n$ T1depression                                                        <dbl> 41…\n$ T1anxiety                                                           <dbl> 35…\n$ Height                                                              <dbl> 15…\n$ Weight                                                              <dbl> 34…\n$ WC                                                                  <dbl> 58…\n$ HC                                                                  <dbl> 67…\n$ SBP                                                                 <dbl> 98…\n$ DBP                                                                 <dbl> 60…\n$ FBG                                                                 <dbl> 4.…\n$ TC                                                                  <dbl> 3.…\n$ TG                                                                  <dbl> 0.…\n$ `HDL-C`                                                             <dbl> 0.…\n$ `LDL-C`                                                             <dbl> 2.…\n$ BMI                                                                 <dbl> 14…\n$ WHR                                                                 <dbl> 0.…\n$ WtHR                                                                <dbl> 0.…\n$ `Family location`                                                   <dbl> 2,…\n$ `Number of siblings`                                                <dbl> 2,…\n$ `How much time do you spend with your father in elementary school?` <dbl> 5,…\n$ `How much time do you spend with your mother in elementary school?` <dbl> 5,…\n$ `Father’s education level`                                          <dbl> 4,…\n$ `Mother’s education level`                                          <dbl> 3,…\n$ `Family financial situation`                                        <dbl> 3,…\n$ `Sleeping hours`                                                    <dbl> 3,…\n$ `Skipping breakfast`                                                <dbl> 1,…\n$ Vigorous                                                            <dbl> 1,…\n$ Moderate                                                            <dbl> 2,…\n\n\nCode\nsum(is.na(data))\n\n\n[1] 728\n\n\nCode\nplot(data$T0depression~data$BMI)\n\n\n\n\n\nThis dataset including 1348 variables and 29 columns. there are 728 NA in this data set. all variables was presented as numberic data. descriptive data was also presented as degrees such as education level, family financial situation and depression rate. By pre-plotting depression rate vs BMI, we can see that some ouliers may need to deal with and there is no siginifcant disrtibution on graph. More data processing is needed in future process."
  },
  {
    "objectID": "posts/abigailbalint_hw3.html",
    "href": "posts/abigailbalint_hw3.html",
    "title": "Homework 3",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(readxl)\nlibrary(alr4)\nlibrary(smss)\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/abigailbalint_hw3.html#question-1",
    "href": "posts/abigailbalint_hw3.html#question-1",
    "title": "Homework 3",
    "section": "Question 1",
    "text": "Question 1\n\nThe predictor is the ppgdp variable and the response is fertility.\nScatterplot below: A straight line function here doesn’t make sense because at the low end of GDP the fertility rates have a huge range.\n\n\n\nCode\nggplot(data = UN11, aes(x = ppgdp, y = fertility)) +\n  geom_point() +\n  geom_smooth(method = 'lm', se=F)\n\n\n`geom_smooth()` using formula 'y ~ x'\n\n\n\n\n\n\nScatterplot with log applied:\n\nApplying the log smooths out the graphs bringing the distribution closer to a straight line.\n\n\nCode\nggplot(data = UN11, aes(x = log(ppgdp), y = log(fertility))) +\n  geom_point() +\n  geom_smooth(method = 'lm', se=F)\n\n\n`geom_smooth()` using formula 'y ~ x'"
  },
  {
    "objectID": "posts/Final_Project_Check-In_Saisrinivas.html",
    "href": "posts/Final_Project_Check-In_Saisrinivas.html",
    "title": "603_Project_Check_In_Saisrinivas_Ambatipudi",
    "section": "",
    "text": "The blood transfusion dataset contain 748 samples with 5 input features: Input Features: • Recency (number of months since the last donation) • Frequency (total number of donations) • Monetary (total blood donated in c.c.) • Time (number of months since the first donation) • Age (age of the donor)\nSource: https://www.kaggle.com/datasets/ninalabiba/blood-transfusion-dataset\n\nBD <- read_csv(\"C:/UMass/DACSS_603/603_Spring_2023/posts/_data/transfusion_saisrinivas.csv\")\n\nRows: 748 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (5): Recency (months), Frequency (times), Monetary (c.c. blood), Time (m...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nBD\n\n# A tibble: 748 × 5\n   `Recency (months)` `Frequency (times)` Monetary (c.c. blood…¹ Time …² wheth…³\n                <dbl>               <dbl>                  <dbl>   <dbl>   <dbl>\n 1                  2                  50                  12500      98       1\n 2                  0                  13                   3250      28       1\n 3                  1                  16                   4000      35       1\n 4                  2                  20                   5000      45       1\n 5                  1                  24                   6000      77       0\n 6                  4                   4                   1000       4       0\n 7                  2                   7                   1750      14       1\n 8                  1                  12                   3000      35       0\n 9                  2                   9                   2250      22       1\n10                  5                  46                  11500      98       1\n# … with 738 more rows, and abbreviated variable names\n#   ¹​`Monetary (c.c. blood)`, ²​`Time (months)`,\n#   ³​`whether he/she donated blood in March 2007`"
  },
  {
    "objectID": "posts/Final_Project_Check-In_Saisrinivas.html#research-questions",
    "href": "posts/Final_Project_Check-In_Saisrinivas.html#research-questions",
    "title": "603_Project_Check_In_Saisrinivas_Ambatipudi",
    "section": "Research Questions:",
    "text": "Research Questions:\nBlood Donation Prediction Frequency:\nThe aim of this study is to develop linear regression, logistic regression machine learning model, to accurately predict whether a blood donor is likely to donate in the future. The results of this study could be useful in developing targeted strategies for donor recruitment and retention, ultimately improving the availability and accessibility of blood donations.\nIdentify Factors that Affect Blood Donation:\nHow do donation behaviors vary across different regions, and what factors may contribute to these variations? Using hypothesis testing, this research project aims to compare the donation patterns of donors from different regions based on demographic and donation-related variables, such as age, gender, donation frequency, and time since last donation. The findings can provide insights into the regional variations in blood donation behaviors and inform targeted strategies to address these differences, potentially leading to increased donation rates and more efficient allocation of resources for blood donation organizations. The study will visualize the results using plots and charts to identify any significant patterns or trends that could help healthcare and blood donation organizations to develop effective strategies to increase blood donation rates.\nSegmentation of Blood Donors:\nUse clustering techniques to segment blood donors based on their demographic and donation history characteristics. Explore methods such as K-means clustering or hierarchical clustering to identify groups of donors with similar characteristics. Examine the differences between these groups and explore their donation patterns over time.\nBuilding a Donor Retention Strategy:\nUse the insights gained from the previous projects to develop a donor retention strategy for the blood donation center. Identify factors that are associated with donor churn (i.e., donors who stop donating blood), and develop a plan to mitigate these factors. This can help healthcare and blood donation organizations to develop strategies to retain donors over the long term."
  },
  {
    "objectID": "posts/Final_Project_Check-In_Saisrinivas.html#summary-of-the-data",
    "href": "posts/Final_Project_Check-In_Saisrinivas.html#summary-of-the-data",
    "title": "603_Project_Check_In_Saisrinivas_Ambatipudi",
    "section": "Summary of the data",
    "text": "Summary of the data\n\nsummary(BD)\n\n Recency (months) Frequency (times) Monetary (c.c. blood) Time (months)  \n Min.   : 0.000   Min.   : 1.000    Min.   :  250         Min.   : 2.00  \n 1st Qu.: 2.750   1st Qu.: 2.000    1st Qu.:  500         1st Qu.:16.00  \n Median : 7.000   Median : 4.000    Median : 1000         Median :28.00  \n Mean   : 9.507   Mean   : 5.515    Mean   : 1379         Mean   :34.28  \n 3rd Qu.:14.000   3rd Qu.: 7.000    3rd Qu.: 1750         3rd Qu.:50.00  \n Max.   :74.000   Max.   :50.000    Max.   :12500         Max.   :98.00  \n whether he/she donated blood in March 2007\n Min.   :0.000                             \n 1st Qu.:0.000                             \n Median :0.000                             \n Mean   :0.238                             \n 3rd Qu.:0.000                             \n Max.   :1.000"
  },
  {
    "objectID": "posts/Final_Project_Check-In_Saisrinivas.html#descrpition-of-the-variables",
    "href": "posts/Final_Project_Check-In_Saisrinivas.html#descrpition-of-the-variables",
    "title": "603_Project_Check_In_Saisrinivas_Ambatipudi",
    "section": "Descrpition of the Variables:",
    "text": "Descrpition of the Variables:\nThis summary function presents a statistical description of a dataset related to blood donation, with five variables: Recency, Frequency, Monetary, Time, and whether the individual donated blood in March 2007. Here’s a breakdown of each variable:\nRecency (months): This variable represents the number of months since the last blood donation. The minimum value is 0 months, indicating that some individuals donated blood very recently. The mean is 9.507 months, suggesting that, on average, people donated blood around 9.5 months ago. The maximum value is 74 months, which means the longest gap between donations is 74 months.\nFrequency (times): This variable shows the total number of times an individual has donated blood. The minimum value is 1, meaning that at least one person has only donated blood once. The mean is 5.515 times, indicating that people, on average, have donated blood about 5.5 times. The maximum value is 50 times, showing that some individuals have donated blood quite frequently.\nMonetary (c.c. blood): This variable represents the total volume of blood donated by an individual, measured in cubic centimeters (c.c.). The minimum value is 250 c.c., which corresponds to the minimum single donation volume. The mean is 1379 c.c., suggesting that, on average, individuals have donated around 1.379 liters of blood. The maximum value is 12,500 c.c., indicating that the highest total volume donated by a person is 12.5 liters.\nTime (months): This variable measures the length of time an individual has been donating blood. The minimum value is 2 months, suggesting that some individuals are relatively new to blood donation. The mean is 34.28 months, indicating that, on average, people have been donating blood for about 34.3 months. The maximum value is 98 months, showing that some individuals have been donating blood for a long time.\nWhether he/she donated blood in March 2007: This is a binary variable that indicates if an individual donated blood in March 2007. The mean is 0.238, which means that about 23.8% of the individuals in the dataset donated blood in that specific month.\nThe summary function provides an overview of the dataset’s key statistics, such as minimum, 1st quartile, median, mean, 3rd quartile, and maximum values for each variable. This information helps to understand the distribution, central tendency, and spread of the data."
  },
  {
    "objectID": "posts/Final_Project_Check-In_Saisrinivas.html#expected-contribution",
    "href": "posts/Final_Project_Check-In_Saisrinivas.html#expected-contribution",
    "title": "603_Project_Check_In_Saisrinivas_Ambatipudi",
    "section": "Expected Contribution:",
    "text": "Expected Contribution:\n\nAkhilesh Kumar: Will work on the Segmentation of Blood Donors and Building a Donor Retention Strategy\nSai Srinivas: Will work on the Blood Donation Prediction Frequency and Identify Factors that Affect Blood Donation"
  },
  {
    "objectID": "posts/Final_Project_Check-In_Saisrinivas.html#references",
    "href": "posts/Final_Project_Check-In_Saisrinivas.html#references",
    "title": "603_Project_Check_In_Saisrinivas_Ambatipudi",
    "section": "References:",
    "text": "References:\nKaggle: https://www.kaggle.com/datasets/ninalabiba/blood-transfusion-dataset\nOriginal Owner and Donor: Prof. I-Cheng Yeh, Department of Information Management, Chung-Hua University, Hsin Chu, Taiwan 30067, R.O.C., e-mail:icyeh ‘@’ chu.edu.tw, TEL:886-3-5186511, Date Donated: October 3, 2008"
  },
  {
    "objectID": "posts/HW2_MiguelCuriel.html",
    "href": "posts/HW2_MiguelCuriel.html",
    "title": "Homework 2",
    "section": "",
    "text": "Question 1\nThe time between the date a patient was recommended for heart surgery and the surgery date for cardiac patients in Ontario was collected by the Cardiac Care Network (“Wait Times Data Guide,” Ministry of Health and Long-Term Care, Ontario, Canada, 2006). The sample mean and sample standard deviation for wait times (in days) of patients for two cardiac procedures are given in the accompanying table. Assume that the sample is representative of the Ontario population.\n\n\n\n\n\n\n\n\n\nSurgical Procedure\nSample Size\nMean Wait Time\nStandard Deviation\n\n\n\n\nBypass\n539\n19\n10\n\n\nAngiography\n847\n18\n9\n\n\n\nConstruct the 90% confidence interval to estimate the actual mean wait time for each of the two procedures. Is the confidence interval narrower for angiography or bypass surgery?\n\n\nCode\n# setting seed for reproducible results\nset.seed(0)\n\n# 90% confidence interval for bypass\nbypass_ci <- t.test(x=c(rnorm(539, mean=19, sd=10)), conf.level=0.90)$conf.int\n\n# 90% confidence interval for bypass\nangiography_ci <- t.test(x=c(rnorm(847, mean=18, sd=9)), conf.level=0.90)$conf.int\n\n# print results\ncat(\"Bypass 90% confidence interval:\", bypass_ci, \"\\n\"\n    , \"Angiography 90% confidence interval:\", angiography_ci, \"\\n\")\n\n\nBypass 90% confidence interval: 18.32586 19.74194 \n Angiography 90% confidence interval: 17.09532 18.10881 \n\n\nAs we can see from the results, bypass’ 90% confidence interval is 18.6-20.1, while angiography’s is 17.5-18.5. Therefore, angiography’s confidence interval is narrower.\n\n\nQuestion 2\nA survey of 1031 adult Americans was carried out by the National Center for Public Policy. Assume that the sample is representative of adult Americans. Among those surveyed, 567 believed that college education is essential for success. Find the point estimate, p, of the proportion of all adult Americans who believe that a college education is essential for success. Construct and interpret a 95% confidence interval for p.\n\n\nCode\n# setting seedd for reproducible results\nset.seed(0)\n\n# point estimate\npoint_estimate <- 567/1031\n\n# 95% confidence interval\nproportion_test <- prop.test(567, 1031, conf.level = 0.95)$conf.int\n\n# print results\ncat(\"Point estimate:\", point_estimate, \"\\n\"\n    , \"95% confidence interval:\", proportion_test, \"\\n\")\n\n\nPoint estimate: 0.5499515 \n 95% confidence interval: 0.5189682 0.580558 \n\n\nThe results indicate that we are 95% confident that the true proportion of all adult Americans who believe that college education is essential for success lies between .52-.58. In other words, we can say with 95% certainty that 52%-58% of adult Americans believe that college is essential for success.\n\n\nQuestion 3\nSuppose that the financial aid office of UMass Amherst seeks to estimate the mean cost of textbooks per semester for students. The estimate will be useful if it is within $5 of the true population mean (i.e. they want the confidence interval to have a length of $10 or less). The financial aid office is pretty sure that the amount spent on books varies widely, with most values between $30 and $200. They think that the population standard deviation is about a quarter of this range (in other words, you can assume they know the population standard deviation). Assuming the significance level to be 5%, what should be the size of the sample?\n\n\nCode\n# margin of error formula\n## ME = z * (sigma / sqrt(n))\n### solving for n\n#### n = (z * sigma/ME)^2\n##### replacing values\n###### z = significance level 5% = critical z value for 95% conf int = 1.96\n###### sigma = 1/4 of range of textbook costs = (200-30)/4 = 42.5\n###### ME = estimate within 5 of the true population mean = 5\n\n# calculate result\nn <- round((1.96*42.5/5)^2)\n\n# print result\ncat(\"Ideal sample size:\", n, \"\\n\")\n\n\nIdeal sample size: 278 \n\n\nUsing the margin of error formula for confidence intervals and solving for n (sample size), we see that the ideal sample size (rounded to the nearest integer) is 278.\n\n\nQuestion 4\nAccording to a union agreement, the mean income for all senior-level workers in a large service company equals $500 per week. A representative of a women’s group decides to analyze whether the mean income μ for female employees matches this norm. For a random sample of nine female employees, ȳ = $410 and s = 90.\n\nTest whether the mean income of female employees differs from $500 per week. Include assumptions, hypotheses, test statistic, and P-value. Interpret the result.\n\nAssumptions:\n\nThe data is normally distributed.\nThe sample is a simple random sample.\nThe standard deviation of the population is unknown.\n\nHypotheses:\n\nH0: μ = 500\nH1: Ha: μ ≠ 500\n\nTest statistic:\n\nt = (ȳ - μ) / (s / sqrt(n)) = (410 - 500) / (90 / sqrt(9)) = -3\n\nP-value:\n\nThe P-value is 0.01707168, which is less than the level of significance of 0.05. Therefore, we reject the null hypothesis and conclude that there is sufficient evidence to suggest that the mean income of female employees differs from $500 per week.\n\n\nReport the P-value for Ha: μ < 500. Interpret.\n\nThe P-value is 0.008535841, which is less than the level of significance of 0.05. Therefore, we reject the null hypothesis and conclude that there is sufficient evidence to suggest that the mean income of female employees is less than $500 per week.\n\nReport and interpret the P-value for Ha: μ > 500.\n\nThe P-value is 0.9914642, which is more than the level of significance of 0.05. Therefore, we fail to reject the null hypothesis and conclude that there is sufficient evidence to suggest that the mean income of female employees is greater than $500 per week.\n\n\n\n\nCode\n# A.3.a. Test statistic\n(410 - 500) / (90 / sqrt(9))\n\n# A.4.a. P-value two-sided\npt(-3, 8) * 2\n\n# B.1. P-value μ < 500\npt(-3, 8)\n\n# C.1. P-value μ > 500\npt(-3, 8, lower.tail=FALSE)\n\n\n\n\nQuestion 5\nJones and Smith separately conduct studies to test H0: μ = 500 against Ha: μ ≠ 500, each with n = 1000. Jones gets ȳ = 519.5, with se = 10.0. Smith gets ȳ = 519.7, with se = 10.0.\n\nShow that t = 1.95 and P-value = 0.051 for Jones. Show that t = 1.97 and P-value = 0.049 for Smith.\n\nJones:\n\nt = (ȳ - μ) / se = (519.5 - 500) / 10 = 19.5 / 10 = 1.95\nP-value = round(2 * pt(q=1.95, df=999, lower.tail=FALSE), 3) = 0.051\n\nSmith:\n\nt = (ȳ - μ) / se = (519.7 - 500) / 10 = 19.7 / 10 = 1.97\nP-value = round(2 * pt(q=1.97, df=999, lower.tail=FALSE), 3) = 0.049\n\n\nUsing α = 0.05, for each study indicate whether the result is “statistically significant.”\n\nSince Jones’ P-value is greater than 0.05, we fail to reject the null hypothesis, indicating that his results are not statistically significant. In contrast, Smith’s P-value is less than 0.05, therefore we reject the null hypothesis and find his results statistically significant.\n\nUsing this example, explain the misleading aspects of reporting the result of a test as “P ≤ 0.05” versus “P > 0.05,” or as “reject H0” versus “Do not reject H0,” without reporting the actual P-value.\n\n“P ≤ 0.05” or “reject H0” without reporting the actual P-value can be misleading because it doesn’t provide information on how strong the evidence is against the null hypothesis. For example, both Jones and Smith barely pass the 0.05 threshold, having 0.049 and 0.051, respectively. Reporting this would help readers and analysts to take the strength of the evidence in consideration (in this case, “rejecting H0” or “failing to reject H0” should be taken with a grain of salt).\n\n\n\n\nCode\n# A.1.b. Proving Jones' p-value\nround(2 * pt(q=1.95, df=999, lower.tail=FALSE), 3)\n\n# A.2.b. Proving Smith's p-value\nround(2 * pt(q=1.97, df=999, lower.tail=FALSE), 3)\n\n\n\n\nQuestion 6\nA school nurse wants to determine whether age is a factor in whether children choose a healthy snack after school. She conducts a survey of 300 middle school students, with the results below. Test at α = 0.05 the claim that the proportion who choose a healthy snack differs by grade level. What is the null hypothesis? Which test should we use? What is the conclusion?\n\n\n\nGrade level\n6th grade\n7th grade\n8th grade\n\n\n\n\nHealthy snack\n31\n43\n51\n\n\nUnhealthy snack\n69\n57\n49\n\n\n\n\nNull hypothesis: There is no difference in the proportion of students who choose a healthy snack based on grade level.\nTest: Chi-squared test because we are assessing whether proportions of outcomes (choosing healthy versus unhealthy snacks) in each grade are equal or different.\nConclusion: Since the p-value is 0.01547, we reject the null hypothesis at the 0.05 level of significance and conclude that there is a significant difference in the proportion of healthy snack choices among the different grade levels.\n\n\n\nCode\n# create a matrix of the observed values\nobserved <- matrix(c(31, 43, 51, 69, 57, 49), nrow = 2, byrow = TRUE)\n\n# perform the chi-squared test\nresult <- chisq.test(observed)\n\n# print the results\nprint(observed)\n\n\n     [,1] [,2] [,3]\n[1,]   31   43   51\n[2,]   69   57   49\n\n\nCode\nprint(result)\n\n\n\n    Pearson's Chi-squared test\n\ndata:  observed\nX-squared = 8.3383, df = 2, p-value = 0.01547\n\n\n\n\nQuestion 7\nPer-pupil costs (in thousands of dollars) for cyber charter school tuition for school districts in three areas are shown. Test the claim that there is a difference in means for the three areas, using an appropriate test. What is the null hypothesis? Which test should we use? What is the conclusion?\n\n\n\nArea 1\n6.2\n9.3\n6.8\n6.1\n6.7\n7.5\n\n\nArea 2\n7.5\n8.2\n8.5\n8.2\n7.0\n9.3\n\n\nArea 3\n5.8\n6.4\n5.6\n7.1\n3.0\n3.5\n\n\n\n\nNull hypothesis: There is no difference in means for the three areas.\nTest: Analysis of Variance (ANOVA) because we are computing the difference between the means of three or more groups.\nConclusion: Given that the P-value associated to the F-statistic is 0.00397, we reject the null hypothesis and conclude that there is a significant difference in means for the three areas.\n\n\n\nCode\narea1 <- c(6.2, 9.3, 6.8, 6.1, 6.7, 7.5)\narea2 <- c(7.5, 8.2, 8.5, 8.2, 7.0, 9.3)\narea3 <- c(5.8, 6.4, 5.6, 7.1, 3.0, 3.5)\n\nanova_result <- aov(c(area1, area2, area3) ~ rep(c(\"Area 1\", \"Area 2\", \"Area 3\")\n                                                 , c(6, 6, 6)))\nprint(summary(anova_result))\n\n\n                                                 Df Sum Sq Mean Sq F value\nrep(c(\"Area 1\", \"Area 2\", \"Area 3\"), c(6, 6, 6))  2  25.66  12.832   8.176\nResiduals                                        15  23.54   1.569        \n                                                  Pr(>F)   \nrep(c(\"Area 1\", \"Area 2\", \"Area 3\"), c(6, 6, 6)) 0.00397 **\nResiduals                                                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "posts/HW1_XiaoyanHu.html",
    "href": "posts/HW1_XiaoyanHu.html",
    "title": "Homework 1",
    "section": "",
    "text": "Code\nlibrary(tidyr)\nlibrary(dplyr)\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nCode\nlibrary(readxl)\nlibrary(ggplot2)\ndata<-read_excel(\"/Users/cassie199/Desktop/23spring/603_Spring_2023-1/posts/_data/LungCapData.xls\")\nhead(data)\n\n\n# A tibble: 6 × 6\n  LungCap   Age Height Smoke Gender Caesarean\n    <dbl> <dbl>  <dbl> <chr> <chr>  <chr>    \n1    6.48     6   62.1 no    male   no       \n2   10.1     18   74.7 yes   female no       \n3    9.55    16   69.7 no    female yes      \n4   11.1     14   71   no    male   no       \n5    4.8      5   56.9 no    male   no       \n6    6.22    11   58.7 no    female no       \n\n\n\nQuestion 1\n\nUse the LungCapData to answer the following questions. (Hint: Using dplyr, especially group_by() and summarize() can help you answer the following questions relatively efficiently.)\n\n\nWhat does the distribution of LungCap look like? (Hint: Plot a histogram with probability density on the y axis)\n\n-the distribution of LungCap looks like a normal distribution\n\n\nCode\nggplot(data, aes(x=LungCap)) + geom_histogram()\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\nCompare the probability distribution of the LungCap with respect to Males and Females? (Hint:make boxplots separated by gender using the boxplot() function)\n\n\n\nCode\nboxplot(data$LungCap~data$Gender)\n\n\n\n\n\n\nCompare the mean lung capacities for smokers and non-smokers. Does it make sense?\n\n\n\nCode\nboxplot(data$LungCap~data$Smoke)\n\n\n\n\n\n\nExamine the relationship between Smoking and Lung Capacity within age groups: “less than or equal to 13”, “14 to 15”, “16 to 17”, and “greater than or equal to 18”.\n\n\n\nCode\nage_ranges <- c(\"<=13\", \"14-15\", \"16-17\",  \">18\")\ndata$age_ranges <- cut(data$Age, breaks = c(13, 14,15, 16,17, 18),\n                     include.lowest = TRUE)\nggplot(data, aes(x = age_ranges, y = LungCap)) +\n  geom_boxplot() +\n  labs(x = \"Age Range\", y = \"LungCap\")\n\n\n\n\n\n\nCompare the lung capacities for smokers and non-smokers within each age group. Is your answer different from the one in part c. What could possibly be going on here?\n\n#Question 2\n\nLet X = number of prior convictions for prisoners at a state prison at which there are 810 prisoners.\n\nX 0 1 2 3 4 Frequency 128 434 160 64 24\n\nWhat is the probability that a randomly selected inmate has exactly 2 prior convictions?\n\n\n\nCode\nn<-810\nX<-tibble(x=0:4,\n          F=c(128,434,160,64,24))\nX\n\n\n# A tibble: 5 × 2\n      x     F\n  <int> <dbl>\n1     0   128\n2     1   434\n3     2   160\n4     3    64\n5     4    24\n\n\nCode\npa<-160/n\npa\n\n\n[1] 0.1975309\n\n\n\nWhat is the probability that a randomly selected inmate has fewer than 2 prior convictions?\n\n\n\nCode\npb<-(128+434)/n\npb\n\n\n[1] 0.6938272\n\n\n\nWhat is the probability that a randomly selected inmate has 2 or fewer prior convictions?\n\n\n\nCode\npc<-(128+434+160)/n\npc\n\n\n[1] 0.891358\n\n\n\nWhat is the probability that a randomly selected inmate has more than 2 prior convictions?\n\n\n\nCode\npd<-(64+24)/n\npd\n\n\n[1] 0.108642\n\n\n\nWhat is the expected value1 for the number of prior convictions?\n\n\n\nCode\nprior<-c(434,160,64,24)\nmean(prior)\n\n\n[1] 170.5\n\n\n\nCalculate the variance and the standard deviation for the Prior Convictions.\n\n\n\nCode\nvar(prior)\n\n\n[1] 34115.67\n\n\nCode\nsd(prior)\n\n\n[1] 184.7043"
  },
  {
    "objectID": "posts/HW1_MiguelCuriel.html",
    "href": "posts/HW1_MiguelCuriel.html",
    "title": "Homework 1",
    "section": "",
    "text": "First, let’s read in the data from the Excel file:\n\n\nCode\nlibrary(readxl)\ndf <- read_excel(\"_data/LungCapData.xls\")\n\n\nThe distribution of LungCap looks as follows:\n\n\nCode\nhist(df$LungCap)\n\n\n\n\n\nThe histogram suggests that the distribution is close to a normal distribution. Most of the observations are close to the mean. Very few observations are close to the margins (0 and 15).\n\n\n\nCompare the probability distribution of the LungCap with respect to Males and Females?\n\n\nCode\nsuppressPackageStartupMessages(library(dplyr))\nboxplot(LungCap~Gender, data=df)\n\n\n\n\n\nThe boxplot suggests that males tend to have a slightly higher lung capacity than females. While the mean of both genders is close to each other (~8), males’ interquartile range is slightly higher (approximately from 6.5 to 10) when compared to the females’ (approximately from 6 to 9).\n\n\n\nCompare the mean lung capacities for smokers and non-smokers. Does it make sense?\n\n\nCode\ndf %>%\n  group_by(Smoke) %>%\n  summarise(mean = mean(LungCap), n = n())\n\n\n# A tibble: 2 × 3\n  Smoke  mean     n\n  <chr> <dbl> <int>\n1 no     7.77   648\n2 yes    8.65    77\n\n\nSmokers have a mean lung capacity of 8.65, versus non-smokers who have a lung capacity of 7.77. These results do not make sense, as we would expect non-smokers to have greater lung capacity.\n\n\n\nExamine the relationship between Smoking and Lung Capacity within age groups: “less than or equal to 13”, “14 to 15”, “16 to 17”, and “greater than or equal to 18”.\n\n\nCode\nage_groups <- df %>% \n  mutate(\n    # Create categories\n    AgeGroup = case_when(\n      Age <= 13            ~ \"0 to 13\",\n      Age == 14 | Age == 15 ~ \"14 to 15\",\n      Age == 16 | Age == 17 ~ \"16 to 17\",\n      Age >= 18             ~ \"18 and above\"\n    ),\n    # Convert to factor\n    AgeGroup = factor(\n      AgeGroup,\n      level = c(\"0 to 13\", \"14 to 15\",\"16 to 17\", \"18 and above\")\n    )\n  )\n\nboxplot(LungCap~AgeGroup, data=age_groups)\n\n\n\n\n\nThe boxplot suggests that the greater the age, the greater the lung capacity. This is especially evident when comparing the lowest age group (0-13) versus the rest of them, but between the two oldest age groups (16-17 vs 18+), this is the least evident. This makes sense as the greatest physical growth usually happens earlier in life - from childhood to puberty - but then this slows down in the late teens and it almost entirely stops during young adulthood.\n\n\n\nCompare the lung capacities for smokers and non-smokers within each age group. Is your answer different from the one in part c. What could possibly be going on here?\n\n\nCode\nlibrary(ggplot2)\n\nage_groups %>% ggplot(aes(x=AgeGroup, y=LungCap, fill=Smoke)) + geom_boxplot()\n\n\n\n\n\nThe answer is different than what I found in 1.C. I previously found that the lung capacity mean for non-smokers was less than the smoking counterpart; however, this new box plot suggests that most non-smoking age groups actually have better lung capacity than smokers. The only age group where this condition isn’t met is in the youngest ages (0-13) so it is likely that this group that is skewing the overall smoker versus non-smoker analysis."
  },
  {
    "objectID": "posts/HW1_MiguelCuriel.html#a-1",
    "href": "posts/HW1_MiguelCuriel.html#a-1",
    "title": "Homework 1",
    "section": "a",
    "text": "a\nWhat is the probability that a randomly selected inmate has exactly 2 prior convictions?\n160/810 = .1975 = 19.75%"
  },
  {
    "objectID": "posts/HW1_MiguelCuriel.html#b-1",
    "href": "posts/HW1_MiguelCuriel.html#b-1",
    "title": "Homework 1",
    "section": "b",
    "text": "b\nWhat is the probability that a randomly selected inmate has fewer than 2 prior convictions?\n(128+434)/810 = 562/810 = .6938 = 69.38%"
  },
  {
    "objectID": "posts/HW1_MiguelCuriel.html#c-1",
    "href": "posts/HW1_MiguelCuriel.html#c-1",
    "title": "Homework 1",
    "section": "c",
    "text": "c\nWhat is the probability that a randomly selected inmate has 2 or fewer prior convictions?\n(128+434+160)/810 = 722/810 = .8914 = 89.14%"
  },
  {
    "objectID": "posts/HW1_MiguelCuriel.html#d-1",
    "href": "posts/HW1_MiguelCuriel.html#d-1",
    "title": "Homework 1",
    "section": "d",
    "text": "d\nWhat is the probability that a randomly selected inmate has more than 2 prior convictions?\n(64+24)/810 = 88/810 = .1086 = 10.86%"
  },
  {
    "objectID": "posts/HW1_MiguelCuriel.html#e-1",
    "href": "posts/HW1_MiguelCuriel.html#e-1",
    "title": "Homework 1",
    "section": "e",
    "text": "e\nWhat is the expected value for the number of prior convictions? (The expected value of a discrete random variable X, symbolized as E(X), is often referred to as the long-term average or mean)\n\n\nCode\n(0*128/810) + (1*434/810) + (2*160/810) + (3*64/810) + (4*24/810) \n\n\n[1] 1.28642"
  },
  {
    "objectID": "posts/HW1_MiguelCuriel.html#f",
    "href": "posts/HW1_MiguelCuriel.html#f",
    "title": "Homework 1",
    "section": "f",
    "text": "f\nCalculate the variance and the standard deviation for the Prior Convictions.\nBelow is the variance:\n\n\nCode\n((0-1.28642)^2 * 0) + ((1-1.28642)^2 * .5358) + ((2-1.28642)^2 * .395) + ((3-1.28642)^2 * .237) + ((4-1.28642)^2 * .1185)\n\n\n[1] 1.813581\n\n\nAnd below is the standard deviation:\n\n\nCode\nsqrt(((0-1.28642)^2 * 0) + ((1-1.28642)^2 * .5358) + ((2-1.28642)^2 * .395) + ((3-1.28642)^2 * .237) + ((4-1.28642)^2 * .1185))\n\n\n[1] 1.346693"
  },
  {
    "objectID": "posts/AdithyaParupudi_finalproject1.html",
    "href": "posts/AdithyaParupudi_finalproject1.html",
    "title": "Final Project - Post 1",
    "section": "",
    "text": "Description\nThe birthwt dataset(part of the MASS package) is a widely-used data collection in the field of medical statistics and public health research, focusing on the factors influencing birth weight in newborns. It contains records of various factors such as maternal age, weight, race, smoking habits during pregnancy, and the number of prenatal visits, among others. By analyzing the relationships between these variables and birth weight, researchers and medical professionals can identify potential risk factors, better understand the determinants of low birth weight, and develop effective interventions to improve maternal and neonatal health outcomes.\n\n\nResearch Questions\n\nDoes maternal smoking during pregnancy have a significant impact on newborn birth weight?\nIs there a correlation between maternal age and the number of prenatal visits?\nDo racial differences influence birth weight, when controlling for other factors such as maternal age, weight, and smoking habits?\n\n\n\nHypothesis\n\nNull Hypothesis (H0): There is no significant relationship between maternal smoking during pregnancy and newborn birth weight, after controlling for other factors such as maternal age, weight, and race.\nThere is a significant relationship between maternal smoking during pregnancy and newborn birth weight.\nThe relationship between maternal age and newborn birth weight is moderated by the number of prenatal visits, such that the positive association between maternal age and birth weight is stronger for mothers with a higher number of prenatal visits.\n\n\n\nDescriptive Statistics\nThe birthwt data frame has 189 rows and 10 columns. The data were collected at Baystate Medical Center, Springfield, Mass during 1986.\n\nlow: an indicator of birth weight less than 2.5 kg.\nage: mother’s age in years.\nlwt: mother’s weight in pounds at last menstrual period.\nrace: mother’s race (1 = white, 2 = black, 3 = other).\nsmoke: smoking status during pregnancy.\nptl: number of previous premature labors.\nht: history of hypertension.\nui: presence of uterine irritability.\niv: number of physician visits during the first trimester.\nbwt: birth weight in grams.\n\n\n\nExploratory Data Analysis\n\n\nCode\n# column names\ncolnames(birthwt)\n\n\n [1] \"low\"   \"age\"   \"lwt\"   \"race\"  \"smoke\" \"ptl\"   \"ht\"    \"ui\"    \"ftv\"  \n[10] \"bwt\"  \n\n\n\n\nCode\nglimpse(birthwt)\n\n\nRows: 189\nColumns: 10\n$ low   <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ age   <int> 19, 33, 20, 21, 18, 21, 22, 17, 29, 26, 19, 19, 22, 30, 18, 18, …\n$ lwt   <int> 182, 155, 105, 108, 107, 124, 118, 103, 123, 113, 95, 150, 95, 1…\n$ race  <int> 2, 3, 1, 1, 1, 3, 1, 3, 1, 1, 3, 3, 3, 3, 1, 1, 2, 1, 3, 1, 3, 1…\n$ smoke <int> 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0…\n$ ptl   <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0…\n$ ht    <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ ui    <int> 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1…\n$ ftv   <int> 0, 3, 1, 2, 0, 0, 1, 1, 1, 0, 0, 1, 0, 2, 0, 0, 0, 3, 0, 1, 2, 3…\n$ bwt   <int> 2523, 2551, 2557, 2594, 2600, 2622, 2637, 2637, 2663, 2665, 2722…\n\n\n\n\nCode\nhead(birthwt)\n\n\n   low age lwt race smoke ptl ht ui ftv  bwt\n85   0  19 182    2     0   0  0  1   0 2523\n86   0  33 155    3     0   0  0  0   3 2551\n87   0  20 105    1     1   0  0  0   1 2557\n88   0  21 108    1     1   0  0  1   2 2594\n89   0  18 107    1     1   0  0  1   0 2600\n91   0  21 124    3     0   0  0  0   0 2622\n\n\n\n\nReferences\nVenables, W. N. and Ripley, B. D. (2002) Modern Applied Statistics with S. Fourth edition. Springer"
  },
  {
    "objectID": "posts/HW3_SaisrinivasAmbatipudi.html",
    "href": "posts/HW3_SaisrinivasAmbatipudi.html",
    "title": "603Homework_Saisrinivas_Ambatipudi",
    "section": "",
    "text": "library(tidyverse)\nlibrary(ggplot2)\nlibrary(stats)\nlibrary(alr4)\nlibrary(smss)\n\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/HW3_SaisrinivasAmbatipudi.html#question-1",
    "href": "posts/HW3_SaisrinivasAmbatipudi.html#question-1",
    "title": "603Homework_Saisrinivas_Ambatipudi",
    "section": "Question 1:",
    "text": "Question 1:\nUnited Nations (Data file: UN11in alr4) The data in the file UN11 contains several variables, including ppgdp, the gross national product per person in U.S. dollars, and fertility, the birth rate per 1000 females, both from the year 2009. The data are for 199 localities, mostly UN member countries, but also other areas such as Hong Kong that are not independent countries. The data were collected from the United Nations (2011). We will study the dependence of fertility on ppgdp. (a) Identify the predictor and the response. (b) Draw the scatterplot of fertility on the vertical axis versus ppgdp on the horizontal axis and summarize the information in this graph. Does a straight-line mean function seem to be plausible for a summary of this graph? (c) Draw the scatterplot of log(fertility) versus log(ppgdp) using natural logarithms. Does the simple linear regression model seem plausible for a summary of this graph? If you use a different base of logarithms, the shape of the graph won’t change, but the values on the axes will change."
  },
  {
    "objectID": "posts/HW3_SaisrinivasAmbatipudi.html#answer",
    "href": "posts/HW3_SaisrinivasAmbatipudi.html#answer",
    "title": "603Homework_Saisrinivas_Ambatipudi",
    "section": "Answer:",
    "text": "Answer:"
  },
  {
    "objectID": "posts/HW3_SaisrinivasAmbatipudi.html#a",
    "href": "posts/HW3_SaisrinivasAmbatipudi.html#a",
    "title": "603Homework_Saisrinivas_Ambatipudi",
    "section": "(a)",
    "text": "(a)\nThe dependence of fertility is examined in this study. Thus, the predictor (independent variable) is ppgdp (gross national product per person), and the response (dependent variable) is fertility (birth rate per 1000 females)."
  },
  {
    "objectID": "posts/HW3_SaisrinivasAmbatipudi.html#b",
    "href": "posts/HW3_SaisrinivasAmbatipudi.html#b",
    "title": "603Homework_Saisrinivas_Ambatipudi",
    "section": "(b)",
    "text": "(b)\n\ndata(UN11)\nUN11\n\n                                        region  group fertility    ppgdp\nAfghanistan                               Asia  other  5.968000    499.0\nAlbania                                 Europe  other  1.525000   3677.2\nAlgeria                                 Africa africa  2.142000   4473.0\nAngola                                  Africa africa  5.135000   4321.9\nAnguilla                             Caribbean  other  2.000000  13750.1\nArgentina                           Latin Amer  other  2.172000   9162.1\nArmenia                                   Asia  other  1.735000   3030.7\nAruba                                Caribbean  other  1.671000  22851.5\nAustralia                              Oceania   oecd  1.949000  57118.9\nAustria                                 Europe   oecd  1.346000  45158.8\nAzerbaijan                                Asia  other  2.148000   5637.6\nBahamas                              Caribbean  other  1.877000  22461.6\nBahrain                                   Asia  other  2.430000  18184.1\nBangladesh                                Asia  other  2.157000    670.4\nBarbados                             Caribbean  other  1.575000  14497.3\nBelarus                                 Europe  other  1.479000   5702.0\nBelgium                                 Europe   oecd  1.835000  43814.8\nBelize                              Latin Amer  other  2.679000   4495.8\nBenin                                   Africa africa  5.078000    741.1\nBermuda                              Caribbean  other  1.760000  92624.7\nBhutan                                    Asia  other  2.258000   2047.2\nBolivia                             Latin Amer  other  3.229000   1977.9\nBosnia and Herzegovina                  Europe  other  1.134000   4477.7\nBotswana                                Africa africa  2.617000   7402.9\nBrazil                              Latin Amer  other  1.800000  10715.6\nBrunei Darussalam                         Asia  other  1.984000  32647.6\nBulgaria                                Europe  other  1.546000   6365.1\nBurkina Faso                            Africa africa  5.750000    519.7\nBurundi                                 Africa africa  4.051000    176.6\nCambodia                                  Asia  other  2.422000    797.2\nCameroon                                Africa africa  4.287000   1206.6\nCanada                           North America   oecd  1.691000  46360.9\nCape Verde                              Africa africa  2.279000   3244.0\nCayman Islands                       Caribbean  other  1.600000  57047.9\nCentral African Republic                Africa africa  4.423000    450.8\nChad                                    Africa africa  5.737000    727.4\nChile                               Latin Amer   oecd  1.832000  11887.7\nChina                                     Asia  other  1.559000   4354.0\nColombia                            Latin Amer  other  2.293000   6222.8\nComoros                                 Africa africa  4.742000    736.6\nCongo                                   Africa africa  4.442000   2665.1\nCook Islands                           Oceania  other  2.530806  12212.1\nCosta Rica                          Latin Amer  other  1.812000   7703.8\nCote dIvoire                            Africa africa  4.224000   1154.1\nCroatia                                 Europe  other  1.501000  13819.5\nCuba                                 Caribbean  other  1.451000   5704.4\nCyprus                                    Asia  other  1.458000  28364.3\nCzech Republic                          Europe   oecd  1.501000  18838.8\nDemocratic Republic of the Congo        Africa africa  5.485000    200.6\nDenmark                                 Europe   oecd  1.885000  55830.2\nDjibouti                                Africa africa  3.589000   1282.6\nDominica                             Caribbean  other  3.000000   7020.8\nDominican Republic                   Caribbean  other  2.490000   5195.4\nEast Timor                                Asia  other  5.918000    706.1\nEcuador                             Latin Amer  other  2.393000   4072.6\nEgypt                                   Africa africa  2.636000   2653.7\nEl Salvador                         Latin Amer  other  2.171000   3425.6\nEquatorial Guinea                       Africa africa  4.980000  16852.4\nEritrea                                 Africa africa  4.243000    429.1\nEstonia                                 Europe   oecd  1.702000  14135.4\nEthiopia                                Africa africa  3.848000    324.6\nFiji                                   Oceania  other  2.602000   3545.7\nFinland                                 Europe   oecd  1.875000  44501.7\nFrance                                  Europe   oecd  1.987000  39545.9\nFrench Polynesia                       Oceania  other  2.033000  24669.0\nGabon                                   Africa africa  3.195000  12468.8\nGambia                                  Africa africa  4.689000    579.1\nGeorgia                                   Asia  other  1.528000   2680.3\nGermany                                 Europe   oecd  1.457000  39857.1\nGhana                                   Africa africa  3.988000   1333.2\nGreece                                  Europe   oecd  1.540000  26503.8\nGreenland                        NorthAtlantic  other  2.217000  35292.7\nGrenada                              Caribbean  other  2.171000   7429.0\nGuatemala                           Latin Amer  other  3.840000   2882.3\nGuinea                                  Africa africa  5.032000    427.5\nGuinea-Bissau                           Africa africa  4.877000    539.4\nGuyana                              Latin Amer  other  2.190000   2996.0\nHaiti                                Caribbean  other  3.159000    612.7\nHonduras                            Latin Amer  other  2.996000   2026.2\nHong Kong                                 Asia  other  1.137000  31823.7\nHungary                                 Europe   oecd  1.430000  12884.0\nIceland                                 Europe  other  2.098000  39278.0\nIndia                                     Asia  other  2.538000   1406.4\nIndonesia                                 Asia  other  2.055000   2949.3\nIran                                      Asia  other  1.587000   5227.1\nIraq                                      Asia  other  4.535000    888.5\nIreland                                 Europe   oecd  2.097000  46220.3\nIsrael                                    Asia   oecd  2.909000  29311.6\nItaly                                   Europe   oecd  1.476000  33877.1\nJamaica                              Caribbean  other  2.262000   4899.0\nJapan                                     Asia   oecd  1.418000  43140.9\nJordan                                    Asia  other  2.889000   4445.3\nKazakhstan                                Asia  other  2.481000   9166.7\nKenya                                   Africa africa  4.623000    801.8\nKiribati                               Oceania  other  3.500000   1468.2\nKuwait                                    Asia  other  2.251000  45430.4\nKyrgyzstan                                Asia  other  2.621000    865.4\nLaos                                      Asia  other  2.543000   1047.6\nLatvia                                  Europe  other  1.506000  10663.0\nLebanon                                   Asia  other  1.764000   9283.7\nLesotho                                 Africa africa  3.051000    980.7\nLiberia                                 Africa africa  5.038000    218.6\nLibya                                   Africa africa  2.410000  11320.8\nLithuania                               Europe  other  1.495000  10975.5\nLuxembourg                              Europe   oecd  1.683000 105095.4\nMacao                                     Asia  other  1.163000  49990.2\nMadagascar                              Africa africa  4.493000    421.9\nMalawi                                  Africa africa  5.968000    357.4\nMalaysia                                  Asia  other  2.572000   8372.8\nMaldives                                  Asia  other  1.668000   4684.5\nMali                                    Africa africa  6.117000    598.8\nMalta                                   Europe  other  1.284000  19599.2\nMarshall Islands                       Oceania  other  4.384466   3069.4\nMauritania                              Africa africa  4.361000   1131.1\nMauritius                               Africa africa  1.590000   7488.3\nMexico                              Latin Amer   oecd  2.227000   9100.7\nMicronesia                             Oceania  other  3.307000   2678.2\nMoldova                                 Europe  other  1.450000   1625.8\nMongolia                                  Asia  other  2.446000   2246.7\nMontenegro                              Europe  other  1.630000   6509.8\nMorocco                                 Africa africa  2.183000   2865.0\nMozambique                              Africa africa  4.713000    407.5\nMyanmar                                   Asia  other  1.939000    876.2\nNamibia                                 Africa africa  3.055000   5124.7\nNauru                                  Oceania  other  3.300000   6190.1\nNepal                                     Asia  other  2.587000    534.7\nNeth Antilles                        Caribbean  other  1.900000  20321.1\nNetherlands                             Europe   oecd  1.794000  46909.7\nNew Caledonia                          Oceania  other  2.091000  35319.5\nNew Zealand                            Oceania   oecd  2.135000  32372.1\nNicaragua                           Latin Amer  other  2.500000   1131.9\nNiger                                   Africa africa  6.925000    357.7\nNigeria                                 Africa africa  5.431000   1239.8\nNorth Korea                               Asia  other  1.988000    504.0\nNorway                                  Europe   oecd  1.948000  84588.7\nOman                                      Asia  other  2.146000  20791.0\nPakistan                                  Asia  other  3.201000   1003.2\nPalau                                  Oceania  other  2.000000  10821.8\nPalestinian Territory                     Asia  other  4.270000   1819.5\nPanama                              Latin Amer  other  2.409000   7614.0\nPapua New Guinea                       Oceania  other  3.799000   1428.4\nParaguay                            Latin Amer  other  2.858000   2771.1\nPeru                                Latin Amer  other  2.410000   5410.7\nPhilippines                               Asia  other  3.050000   2140.1\nPoland                                  Europe   oecd  1.415000  12263.2\nPortugal                                Europe   oecd  1.312000  21437.6\nPuerto Rico                          Caribbean  other  1.757000  26461.0\nQatar                                     Asia  other  2.204000  72397.9\nRepublic of Korea                         Asia  other  1.389000  21052.2\nRomania                                 Europe  other  1.428000   7522.4\nRussian Federation                      Europe  other  1.529000  10351.4\nRwanda                                  Africa africa  5.282000    532.3\nSaint Lucia                          Caribbean  other  1.907000   6677.1\nSamoa                                  Oceania  other  3.763000   3343.3\nSao Tome and Principe                   Africa africa  3.488000   1283.3\nSaudi Arabia                              Asia  other  2.639000  15835.9\nSenegal                                 Africa africa  4.605000   1032.7\nSerbia                                  Europe  other  1.562000   5123.2\nSeychelles                              Africa africa  2.340000  11450.6\nSierra Leone                            Africa africa  4.728000    351.7\nSingapore                                 Asia  other  1.367000  43783.1\nSlovakia                                Europe   oecd  1.372000  15976.0\nSlovenia                                Europe   oecd  1.477000  23109.8\nSolomon Islands                        Oceania  other  4.041000   1193.5\nSomalia                                 Africa africa  6.283000    114.8\nSouth Africa                            Africa africa  2.383000   7254.8\nSpain                                   Europe  other  1.504000  30542.8\nSri Lanka                                 Asia  other  2.235000   2375.3\nSt Vincent and Grenadines            Caribbean  other  1.995000   6171.7\nSudan                                   Africa africa  4.225000   1824.9\nSuriname                            Latin Amer  other  2.266000   7018.0\nSwaziland                               Africa africa  3.174000   3311.2\nSweden                                  Europe   oecd  1.925000  48906.2\nSwitzerland                             Europe   oecd  1.536000  68880.2\nSyria                                     Asia  other  2.772000   2931.5\nTajikistan                                Asia  other  3.162000    816.0\nTanzania                                Africa africa  5.499000    516.0\nTFYR Macedonia                          Europe  other  1.397000   4434.5\nThailand                                  Asia  other  1.528000   4612.8\nTogo                                    Africa africa  3.864000    524.6\nTonga                                  Oceania  other  3.783000   3543.1\nTrinidad and Tobago                  Caribbean  other  1.632000  15205.1\nTunisia                                 Africa africa  1.909000   4222.1\nTurkey                                    Asia   oecd  2.022000  10095.1\nTurkmenistan                              Asia  other  2.316000   4587.5\nTuvalu                                 Oceania  other  3.700000   3187.2\nUganda                                  Africa africa  5.901000    509.0\nUkraine                                 Europe  other  1.483000   3035.0\nUnited Arab Emirates                      Asia  other  1.707000  39624.7\nUnited Kingdom                          Europe   oecd  1.867000  36326.8\nUnited States                    North America   oecd  2.077000  46545.9\nUruguay                             Latin Amer  other  2.043000  11952.4\nUzbekistan                                Asia  other  2.264000   1427.3\nVanuatu                                Oceania  other  3.750000   2963.5\nVenezuela                           Latin Amer  other  2.391000  13502.7\nViet Nam                                  Asia  other  1.750000   1182.7\nYemen                                     Asia  other  4.938000   1437.2\nZambia                                  Africa africa  6.300000   1237.8\nZimbabwe                                Africa africa  3.109000    573.1\n                                 lifeExpF pctUrban\nAfghanistan                      49.49000       23\nAlbania                          80.40000       53\nAlgeria                          75.00000       67\nAngola                           53.17000       59\nAnguilla                         81.10000      100\nArgentina                        79.89000       93\nArmenia                          77.33000       64\nAruba                            77.75000       47\nAustralia                        84.27000       89\nAustria                          83.55000       68\nAzerbaijan                       73.66000       52\nBahamas                          78.85000       84\nBahrain                          76.06000       89\nBangladesh                       70.23000       29\nBarbados                         80.26000       45\nBelarus                          76.37000       75\nBelgium                          82.81000       97\nBelize                           77.81000       53\nBenin                            58.66000       42\nBermuda                          82.30000      100\nBhutan                           69.84000       35\nBolivia                          69.40000       67\nBosnia and Herzegovina           78.40000       49\nBotswana                         51.34000       62\nBrazil                           77.41000       87\nBrunei Darussalam                80.64000       76\nBulgaria                         77.12000       72\nBurkina Faso                     57.02000       27\nBurundi                          52.58000       11\nCambodia                         65.10000       20\nCameroon                         53.56000       59\nCanada                           83.49000       81\nCape Verde                       77.70000       62\nCayman Islands                   83.80000      100\nCentral African Republic         51.30000       39\nChad                             51.61000       28\nChile                            82.35000       89\nChina                            75.61000       48\nColombia                         77.69000       75\nComoros                          63.18000       28\nCongo                            59.33000       63\nCook Islands                     76.24547       76\nCosta Rica                       81.99000       65\nCote dIvoire                     57.71000       51\nCroatia                          80.37000       58\nCuba                             81.33000       75\nCyprus                           82.14000       71\nCzech Republic                   81.00000       74\nDemocratic Republic of the Congo 50.56000       36\nDenmark                          81.37000       87\nDjibouti                         60.04000       76\nDominica                         78.20000       67\nDominican Republic               76.57000       70\nEast Timor                       64.20000       29\nEcuador                          78.91000       68\nEgypt                            75.52000       44\nEl Salvador                      77.09000       65\nEquatorial Guinea                52.91000       40\nEritrea                          64.41000       22\nEstonia                          79.95000       70\nEthiopia                         61.59000       17\nFiji                             72.27000       52\nFinland                          83.28000       85\nFrance                           84.90000       86\nFrench Polynesia                 78.07000       51\nGabon                            64.32000       86\nGambia                           60.30000       59\nGeorgia                          77.31000       53\nGermany                          82.99000       74\nGhana                            65.80000       52\nGreece                           82.58000       62\nGreenland                        71.60000       84\nGrenada                          77.72000       40\nGuatemala                        75.10000       50\nGuinea                           56.39000       36\nGuinea-Bissau                    50.40000       30\nGuyana                           73.45000       29\nHaiti                            63.87000       54\nHonduras                         75.92000       52\nHong Kong                        86.35000      100\nHungary                          78.47000       68\nIceland                          83.77000       94\nIndia                            67.62000       30\nIndonesia                        71.80000       45\nIran                             75.28000       71\nIraq                             72.60000       66\nIreland                          83.17000       62\nIsrael                           84.19000       92\nItaly                            84.62000       69\nJamaica                          75.98000       52\nJapan                            87.12000       67\nJordan                           75.17000       79\nKazakhstan                       72.84000       59\nKenya                            59.16000       23\nKiribati                         63.10000       44\nKuwait                           75.89000       98\nKyrgyzstan                       72.36000       35\nLaos                             69.42000       34\nLatvia                           78.51000       68\nLebanon                          75.07000       87\nLesotho                          48.11000       28\nLiberia                          58.59000       48\nLibya                            77.86000       78\nLithuania                        78.28000       67\nLuxembourg                       82.67000       85\nMacao                            83.80000      100\nMadagascar                       68.61000       31\nMalawi                           55.17000       20\nMalaysia                         76.86000       73\nMaldives                         78.70000       41\nMali                             53.14000       37\nMalta                            82.29000       95\nMarshall Islands                 70.60000       72\nMauritania                       60.95000       42\nMauritius                        76.89000       42\nMexico                           79.64000       78\nMicronesia                       70.17000       23\nMoldova                          73.48000       48\nMongolia                         72.83000       63\nMontenegro                       77.37000       61\nMorocco                          74.86000       59\nMozambique                       51.81000       39\nMyanmar                          67.87000       34\nNamibia                          63.04000       39\nNauru                            57.10000      100\nNepal                            70.05000       19\nNeth Antilles                    79.86000       93\nNetherlands                      82.79000       83\nNew Caledonia                    80.49000       57\nNew Zealand                      82.77000       86\nNicaragua                        77.45000       58\nNiger                            55.77000       17\nNigeria                          53.38000       51\nNorth Korea                      72.12000       60\nNorway                           83.47000       80\nOman                             76.44000       73\nPakistan                         66.88000       36\nPalau                            72.10000       84\nPalestinian Territory            74.81000       74\nPanama                           79.07000       75\nPapua New Guinea                 65.52000       13\nParaguay                         74.91000       62\nPeru                             76.90000       77\nPhilippines                      72.57000       49\nPoland                           80.56000       61\nPortugal                         82.76000       61\nPuerto Rico                      83.20000       99\nQatar                            78.24000       96\nRepublic of Korea                83.95000       83\nRomania                          77.95000       58\nRussian Federation               75.01000       73\nRwanda                           57.13000       19\nSaint Lucia                      77.54000       28\nSamoa                            76.02000       20\nSao Tome and Principe            66.48000       63\nSaudi Arabia                     75.57000       82\nSenegal                          60.92000       43\nSerbia                           77.05000       56\nSeychelles                       78.00000       56\nSierra Leone                     48.87000       39\nSingapore                        83.71000      100\nSlovakia                         79.53000       55\nSlovenia                         82.84000       49\nSolomon Islands                  70.00000       19\nSomalia                          53.38000       38\nSouth Africa                     54.09000       62\nSpain                            84.76000       78\nSri Lanka                        78.40000       14\nSt Vincent and Grenadines        74.73000       50\nSudan                            63.82000       41\nSuriname                         74.18000       70\nSwaziland                        48.54000       21\nSweden                           83.65000       85\nSwitzerland                      84.71000       74\nSyria                            77.72000       56\nTajikistan                       71.23000       26\nTanzania                         60.31000       27\nTFYR Macedonia                   77.14000       59\nThailand                         77.76000       34\nTogo                             59.40000       44\nTonga                            75.38000       24\nTrinidad and Tobago              73.82000       14\nTunisia                          77.05000       68\nTurkey                           76.61000       70\nTurkmenistan                     69.40000       50\nTuvalu                           65.10000       51\nUganda                           55.44000       13\nUkraine                          74.58000       69\nUnited Arab Emirates             78.02000       84\nUnited Kingdom                   82.42000       80\nUnited States                    81.31000       83\nUruguay                          80.66000       93\nUzbekistan                       71.90000       36\nVanuatu                          73.58000       26\nVenezuela                        77.73000       94\nViet Nam                         77.44000       31\nYemen                            67.66000       32\nZambia                           50.04000       36\nZimbabwe                         52.72000       39\n\n\n\nplot(UN11$ppgdp, UN11$fertility,\n     main=\"Fertility vs PPGDP\",\n     xlab=\"PPGDP (Gross National Product per Person)\",\n     ylab=\"Fertility (Birth Rate per 1000 Females)\")\n\nmodel <- lm(fertility ~ ppgdp, data = UN11)\n\nabline(model, col = \"red\", lwd = 2)\n\n\n\n\nThe graph first displays a severe negative association between a country’s gross national product per person and fertility rate, but after this point, there appears to be no change in fertility in relation to ppgdp. A straight-line mean function does not appear to be an adequate metric for this graph’s summary."
  },
  {
    "objectID": "posts/HW3_SaisrinivasAmbatipudi.html#c",
    "href": "posts/HW3_SaisrinivasAmbatipudi.html#c",
    "title": "603Homework_Saisrinivas_Ambatipudi",
    "section": "(c)",
    "text": "(c)\n\nplot(log(UN11$ppgdp), log(UN11$fertility),\n     main=\"Log(Fertility) vs Log(PPGDP)\",\n     xlab=\"Log(PPGDP)\",\n     ylab=\"Log(Fertility)\")\n\nmodel <- lm(log(fertility) ~ log(ppgdp), data = UN11)\n\nabline(model, col = \"red\", lwd = 2)\n\n\n\n\nThroughout the graph, the connection between the variables appears to be negative. For a description of this graph, basic linear regression is feasible."
  },
  {
    "objectID": "posts/HW3_SaisrinivasAmbatipudi.html#question-2",
    "href": "posts/HW3_SaisrinivasAmbatipudi.html#question-2",
    "title": "603Homework_Saisrinivas_Ambatipudi",
    "section": "Question 2:",
    "text": "Question 2:\nAnnual income, in dollars, is an explanatory variable in a regression analysis. For a British version of the report on the analysis, all responses are converted to British pounds sterling (1 pound equals about 1.33 dollars, as of 2016).\n\nHow, if at all, does the slope of the prediction equation change?\nHow, if at all, does the correlation change?"
  },
  {
    "objectID": "posts/HW3_SaisrinivasAmbatipudi.html#answer-1",
    "href": "posts/HW3_SaisrinivasAmbatipudi.html#answer-1",
    "title": "603Homework_Saisrinivas_Ambatipudi",
    "section": "Answer:",
    "text": "Answer:"
  },
  {
    "objectID": "posts/HW3_SaisrinivasAmbatipudi.html#a-1",
    "href": "posts/HW3_SaisrinivasAmbatipudi.html#a-1",
    "title": "603Homework_Saisrinivas_Ambatipudi",
    "section": "(a)",
    "text": "(a)\n\nUN11$british <- 1.33 * UN11$ppgdp\nsummary(lm(fertility ~ british, UN11))\n\n\nCall:\nlm(formula = fertility ~ british, data = UN11)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.9006 -0.8801 -0.3547  0.6749  3.7585 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  3.178e+00  1.048e-01  30.331  < 2e-16 ***\nbritish     -2.407e-05  3.500e-06  -6.877  7.9e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.206 on 197 degrees of freedom\nMultiple R-squared:  0.1936,    Adjusted R-squared:  0.1895 \nF-statistic: 47.29 on 1 and 197 DF,  p-value: 7.903e-11\n\nsummary(lm(fertility ~ ppgdp, UN11))\n\n\nCall:\nlm(formula = fertility ~ ppgdp, data = UN11)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.9006 -0.8801 -0.3547  0.6749  3.7585 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  3.178e+00  1.048e-01  30.331  < 2e-16 ***\nppgdp       -3.201e-05  4.655e-06  -6.877  7.9e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.206 on 197 degrees of freedom\nMultiple R-squared:  0.1936,    Adjusted R-squared:  0.1895 \nF-statistic: 47.29 on 1 and 197 DF,  p-value: 7.903e-11\n\n\nThe slope of the prediction equation will change when income is converted from dollars to pounds sterling. Since 1 pound equals about 133. dollars (as of 2016), to convert the annual income to pounds, you will need to divide the annual income in dollars by 133. The slope of the equation in pounds will be the same as the original slope in dollars. The magnitude of the slope changes due to the change in units, but the relationship between the variables remains the same."
  },
  {
    "objectID": "posts/HW3_SaisrinivasAmbatipudi.html#b-1",
    "href": "posts/HW3_SaisrinivasAmbatipudi.html#b-1",
    "title": "603Homework_Saisrinivas_Ambatipudi",
    "section": "(b)",
    "text": "(b)\n\ncor(UN11$ppgdp, UN11$fertility)\n\n[1] -0.4399891\n\ncor(UN11$british, UN11$fertility)\n\n[1] -0.4399891\n\n\nWhen converting from dollars to pounds sterling, the correlation between variables will not change. The correlation coefficients are only a measure of the relative relationship between the two variables, not the units they are measured in. The correlation coefficients of the variables remain the same since the conversion from dollars to pounds only involves a linear transformation."
  },
  {
    "objectID": "posts/HW3_SaisrinivasAmbatipudi.html#question-3",
    "href": "posts/HW3_SaisrinivasAmbatipudi.html#question-3",
    "title": "603Homework_Saisrinivas_Ambatipudi",
    "section": "Question 3:",
    "text": "Question 3:\nWater runoff in the Sierras (Data file: water in alr4) Can Southern California’s water supply in future years be predicted from past data? One factor affecting water availability is stream runoff. If runoff could be predicted, engineers, planners, and policy makers could do their jobs more efficiently. The data file contains 43 years’ worth of precipitation measurements taken at six sites in the Sierra Nevada mountains (labeled APMAM, APSAB, APSLAKE, OPBPC, OPRC, and OPSLAKE) and stream runoff volume at a site near Bishop, California, labeled BSAAM. Draw the scatterplot matrix for these data and summarize the information available from these plots. (Hint: Use the pairs() function.)"
  },
  {
    "objectID": "posts/HW3_SaisrinivasAmbatipudi.html#answer-2",
    "href": "posts/HW3_SaisrinivasAmbatipudi.html#answer-2",
    "title": "603Homework_Saisrinivas_Ambatipudi",
    "section": "Answer:",
    "text": "Answer:\n\ndata(water)\nwater\n\n   Year APMAM APSAB APSLAKE OPBPC  OPRC OPSLAKE  BSAAM\n1  1948  9.13  3.58    3.91  4.10  7.43    6.47  54235\n2  1949  5.28  4.82    5.20  7.55 11.11   10.26  67567\n3  1950  4.20  3.77    3.67  9.52 12.20   11.35  66161\n4  1951  4.60  4.46    3.93 11.14 15.15   11.13  68094\n5  1952  7.15  4.99    4.88 16.34 20.05   22.81 107080\n6  1953  9.70  5.65    4.91  8.88  8.15    7.41  67594\n7  1954  5.02  1.45    1.77 13.57 12.45   13.32  65356\n8  1955  6.70  7.44    6.51  9.28  9.65    9.80  67909\n9  1956 10.50  5.85    3.38 21.20 18.55   17.42  92715\n10 1957  9.10  6.13    4.08  9.55  9.20    8.25  70024\n11 1958  8.75  5.23    5.90 15.25 14.80   17.48  99216\n12 1959  8.10  3.77    4.56  9.05  6.85    9.56  55786\n13 1960  3.75  1.47    1.78  4.57  6.10    7.65  46153\n14 1961 10.15  5.09    4.86  8.90  7.15    9.00  47947\n15 1962  6.15  3.52    3.30 16.90 14.75   17.68  76877\n16 1963 12.75  8.17   10.16 16.75 11.55   15.53  88443\n17 1964  7.35  4.33    4.85  5.25  7.45    8.20  54634\n18 1965 11.25  6.56    7.60  8.40 13.20   13.29  78806\n19 1966  4.05  1.90    2.00 10.85  8.25   12.56  56542\n20 1967 12.65  6.62    7.14 23.25 17.00   23.66 116244\n21 1968  4.65  3.84    3.34  7.10  6.80    8.28  60857\n22 1969  5.35  3.62    4.62 43.37 24.85   33.07 146345\n23 1970  4.05  1.98    2.94  8.95 11.25   11.00  73726\n24 1971  5.90  5.72    5.42  8.45 10.90   10.82  65530\n25 1972  9.45  4.82    6.79  7.90  7.60    8.06  60772\n26 1973  3.45  2.63    2.88 14.80 14.70   15.86  91696\n27 1974  4.25  2.54    2.36 18.05 16.90   16.42  87377\n28 1975  7.90  4.42    6.78 11.50  9.55   12.56  77306\n29 1976  9.38  8.30    9.70  6.80  5.25    4.73  44756\n30 1977  7.08  4.40    3.90  4.05  4.35    4.60  41785\n31 1978 11.92  5.78    6.70 25.30 20.55   21.94 112653\n32 1979  3.88  2.26    3.10 15.97 11.83   13.88  79975\n33 1980  5.80  3.10    3.34 24.40 19.15   23.78 106821\n34 1981  2.70  2.22    2.48  8.99  9.45   12.14  69177\n35 1982 18.08 11.96   13.02 18.55 18.40   19.45 120463\n36 1983  8.20  4.98    5.76 19.25 22.90   23.86 135043\n37 1984  7.65  5.30    5.74 14.45 13.15   14.42 102001\n38 1985  5.22  4.42    4.04 11.45 10.16   13.06  77790\n39 1986  4.93  3.26    4.58 26.47 15.33   26.46 118144\n40 1987  5.99  2.76    3.98  4.80  6.85    6.36  61229\n41 1988  6.83  6.82    5.18  7.20  9.01    9.88  58942\n42 1989  8.80  5.06    4.92  8.05  9.60    9.58  53965\n43 1990  7.10  5.06    6.05  5.80  6.50    8.41  49774\n\n\n\npairs(water)\n\n\n\n\nAccording to the above figure, the stream run-off variable has a link with the ‘O’ named lakes but no significant association with the ’A named lakes."
  },
  {
    "objectID": "posts/HW3_SaisrinivasAmbatipudi.html#question-4",
    "href": "posts/HW3_SaisrinivasAmbatipudi.html#question-4",
    "title": "603Homework_Saisrinivas_Ambatipudi",
    "section": "Question 4:",
    "text": "Question 4:\nProfessor ratings (Data file: Rateprof in alr4) In the website and online forum RateMyProfessors.com, students rate and comment on their instructors. Launched in 1999, the site includes millions of ratings on thousands of instructors. The data file includes the summaries of the ratings of 364 instructors at a large campus in the Midwest (Bleske-Rechek and Fritsch, 2011). Each instructor included in the data had at least 10 ratings over a several year period. Students provided ratings of 1–5 on quality, helpfulness, clarity, easiness of instructor’s courses, and raterInterest in the subject matter covered in the instructor’s courses. The data file provides the averages of these five ratings. Create a scatterplot matrix of these five variables. Provide a brief description of the relationships between the five ratings."
  },
  {
    "objectID": "posts/HW3_SaisrinivasAmbatipudi.html#answer-3",
    "href": "posts/HW3_SaisrinivasAmbatipudi.html#answer-3",
    "title": "603Homework_Saisrinivas_Ambatipudi",
    "section": "Answer:",
    "text": "Answer:\n\ndata(Rateprof)\nRateprof\n\n    gender numYears numRaters numCourses pepper discipline\n1     male        7        11          5     no        Hum\n2     male        6        11          5     no        Hum\n3     male       10        43          2     no        Hum\n4     male       11        24          5     no        Hum\n5     male       11        19          7     no        Hum\n6     male       10        15          9     no        Hum\n7     male        7        17          3     no        Hum\n8     male       11        16          3     no        Hum\n9     male       11        12          4     no        Hum\n10    male        7        18          4     no        Hum\n11    male       11        11          4     no        Hum\n12    male        6        33          4    yes        Hum\n13    male        4        23          5     no        Hum\n14    male       10        34          2    yes        Hum\n15    male       11        23          8    yes        Hum\n16    male        3        27          5    yes        Hum\n17    male        1        14          2    yes        Hum\n18    male       11        19          7     no        Hum\n19    male        6        11          5     no        Hum\n20    male       11        14          5     no        Hum\n21    male        7        59          9     no        Hum\n22    male       10        28          5     no        Hum\n23    male        1        13          3     no        Hum\n24    male       10        15          5     no        Hum\n25    male        7        16          2     no        Hum\n26    male       11        35          6     no        Hum\n27    male        7        11          2     no        Hum\n28    male       11        23         10     no        Hum\n29    male       11        27          6     no        Hum\n30    male       11        42          5    yes        Hum\n31    male       11        38          4     no        Hum\n32    male       11        20          8     no        Hum\n33    male       11        44          6     no        Hum\n34    male       11        57          3    yes        Hum\n35    male       10        15          3     no        Hum\n36    male        9        46          5     no        Hum\n37    male       11        10          4     no        Hum\n38    male        3        18          2     no        Hum\n39    male       11        79          7     no        Hum\n40    male       11        18          2     no        Hum\n41    male        3        26          5     no        Hum\n42    male       11        52          2     no        Hum\n43    male       11        10          2     no        Hum\n44    male        7        26          5     no        Hum\n45    male       10        15          3     no        Hum\n46    male       11        45          8     no        Hum\n47    male        3        12          3    yes        Hum\n48    male        7        12          2     no        Hum\n49    male       11        16          3     no        Hum\n50    male        9        36          8     no        Hum\n51    male       11        24          5     no        Hum\n52    male        3        29          4     no        Hum\n53    male       11        65          4     no        Hum\n54    male       11        11          4     no        Hum\n55    male        2        14          4    yes        Hum\n56    male       11        14          7     no        Hum\n57    male       11        29          6     no        Hum\n58    male       11        54          6    yes        Hum\n59    male        4        12          3     no        Hum\n60    male       11        57          5     no        Hum\n61    male        9        13          6     no        Hum\n62    male        5        33          3     no        Hum\n63    male        8        46          7     no        Hum\n64    male       11        19          4     no        Hum\n65  female       11        18          6    yes        Hum\n66  female        9        28          3     no        Hum\n67  female       11        21          7     no        Hum\n68  female        8        26          8    yes        Hum\n69  female       11        10          2     no        Hum\n70  female        2        14          4     no        Hum\n71  female        7        27          6     no        Hum\n72  female       10        22          3     no        Hum\n73  female        4        36          6     no        Hum\n74  female        2        14          4    yes        Hum\n75  female       11        19          6     no        Hum\n76  female       11        86          8     no        Hum\n77  female        8        25          8     no        Hum\n78  female        8        26          7    yes        Hum\n79  female       10        16          2     no        Hum\n80  female        9        20          3     no        Hum\n81  female       11        30          4     no        Hum\n82  female        7        12          3     no        Hum\n83  female       10        21          3     no        Hum\n84  female        1        19          4    yes        Hum\n85  female       11        12          3    yes        Hum\n86  female        4        31          3    yes        Hum\n87  female        4        22          2    yes        Hum\n88  female        4        13          5     no        Hum\n89  female       11        22          5     no        Hum\n90  female       11        26          8     no        Hum\n91  female       11        15          3     no        Hum\n92  female        2        20          5     no        Hum\n93  female       11        24          7     no        Hum\n94  female       11        69          3     no        Hum\n95  female       10        37          6     no        Hum\n96  female        9        36          7     no        Hum\n97  female        7        32          3     no        Hum\n98  female        2        11          3     no        Hum\n99  female       11        41          2     no        Hum\n100 female        7        34          7     no        Hum\n101 female        5        15          4     no        Hum\n102 female       11        38          3     no        Hum\n103 female        2        10          4    yes        Hum\n104 female       11        29          5     no        Hum\n105 female        1        10          4    yes        Hum\n106 female       11        27          5     no        Hum\n107 female       11        36          4     no        Hum\n108 female        9        19          3     no        Hum\n109 female       11        14          2     no        Hum\n110 female        8        17          5     no        Hum\n111 female       10        54          3     no        Hum\n112 female       11        21          5     no        Hum\n113 female        5        33          6     no        Hum\n114 female       11        47          8     no        Hum\n115 female       11        58          5     no        Hum\n116 female        7        14          4    yes        Hum\n117 female       11        10          5     no        Hum\n118 female        9        36          6     no        Hum\n119 female        7        14          5    yes        Hum\n120 female       11        26          5     no        Hum\n121 female        8        39          5     no        Hum\n122 female       11        16          6     no        Hum\n123 female        5        67          3     no        Hum\n124 female        9        29          4     no        Hum\n125 female       11        26          6     no        Hum\n126 female        6        24          7     no        Hum\n127 female        7        16          5    yes        Hum\n128 female        5        20          2     no        Hum\n129 female       10        35          6     no        Hum\n130 female        5        36          9     no        Hum\n131 female        4        28          3     no        Hum\n132 female       11        10          4     no        Hum\n133 female        9        53          7     no        Hum\n134 female        2        13          2     no        Hum\n135   male        7        10          2     no     SocSci\n136   male        7        21          2     no     SocSci\n137   male        2        33          2     no     SocSci\n138   male        7        11          2     no     SocSci\n139   male        6        18          3     no     SocSci\n140   male       11        30          2     no     SocSci\n141   male        7        14          2     no     SocSci\n142   male        4        15          5    yes     SocSci\n143   male       11        50          4     no     SocSci\n144   male        3        10          3     no     SocSci\n145   male       11        30          5     no     SocSci\n146   male       11        53          3     no     SocSci\n147   male       11        46          4    yes     SocSci\n148   male       11        30          5     no     SocSci\n149   male       11        42          7     no     SocSci\n150   male       11        74          8     no     SocSci\n151   male       10        51          4     no     SocSci\n152   male       11        31          4     no     SocSci\n153   male       11        19          2     no     SocSci\n154   male       11        56          4     no     SocSci\n155   male       11        72          7     no     SocSci\n156   male        8        67          4     no     SocSci\n157   male       11        85          7     no     SocSci\n158   male        9        14          1     no     SocSci\n159   male        9        47          7     no     SocSci\n160   male       10        69          2     no     SocSci\n161   male        2        11          2     no     SocSci\n162   male       11        56          6     no     SocSci\n163   male       10        14          5     no     SocSci\n164   male        7        23          7     no     SocSci\n165   male        6        36          5     no     SocSci\n166   male       11        44          8     no     SocSci\n167 female        6        19          4     no     SocSci\n168 female        4        11          5     no     SocSci\n169 female       11        10          1    yes     SocSci\n170 female       10        56          2     no     SocSci\n171 female       11        85          3     no     SocSci\n172 female        2        16          1    yes     SocSci\n173 female        7        39          5    yes     SocSci\n174 female       11        31          4     no     SocSci\n175 female        8        24          3     no     SocSci\n176 female       11        67          3     no     SocSci\n177 female        2        15          3     no     SocSci\n178 female        6        46          3     no     SocSci\n179 female        9        13          6     no     SocSci\n180 female        4        27          3     no     SocSci\n181 female       11        35          3     no     SocSci\n182 female        1        32          4     no     SocSci\n183 female        3        22          2    yes     SocSci\n184 female       11        51          3     no     SocSci\n185 female        8        11          2     no     SocSci\n186 female       10        52          3     no     SocSci\n187 female       10        62          5     no     SocSci\n188 female        3        13          1     no     SocSci\n189 female        2        20          2     no     SocSci\n190 female       11        32          4     no     SocSci\n191 female        5        23          3     no     SocSci\n192 female       11        62          1     no     SocSci\n193 female        5        42          6    yes     SocSci\n194 female       11        24          2     no     SocSci\n195 female        5        26          6     no     SocSci\n196 female        3        12          4    yes     SocSci\n197 female       11        32          4     no     SocSci\n198 female        9        50          5     no     SocSci\n199 female       11        57          4     no     SocSci\n200 female       11        15          4     no     SocSci\n201   male        8        32          4    yes       STEM\n202   male       11        17          9     no       STEM\n203   male       11        23          5     no       STEM\n204   male       11        13          4     no       STEM\n205   male        1        13          2     no       STEM\n206   male        6        18          7     no       STEM\n207   male        1        11          3    yes       STEM\n208   male       10        26          4     no       STEM\n209   male        6        15          4     no       STEM\n210   male       11        25          2     no       STEM\n211   male       11        21          3     no       STEM\n212   male       11        43          3     no       STEM\n213   male       11        51          6     no       STEM\n214   male       11        11          2     no       STEM\n215   male       11        27          2     no       STEM\n216   male       11        37          5     no       STEM\n217   male       11        35          3     no       STEM\n218   male       11        29          2     no       STEM\n219   male        4        30          2     no       STEM\n220   male       11        28          6    yes       STEM\n221   male       11        33          3     no       STEM\n222   male       11        28          6     no       STEM\n223   male       11        62          5     no       STEM\n224   male        5        38          7     no       STEM\n225   male       11        14          1     no       STEM\n226   male        5        25          3     no       STEM\n227   male       11        53          8     no       STEM\n228   male        7        20          3     no       STEM\n229   male       11        21         10     no       STEM\n230   male        6        40          6     no       STEM\n231   male       11        13          3     no       STEM\n232   male       11        67          4     no       STEM\n233   male       11        19          5     no       STEM\n234   male       10        17          2     no       STEM\n235   male       11        22          7     no       STEM\n236   male       11        16          3     no       STEM\n237   male        6        35          6     no       STEM\n238   male        8        38          5     no       STEM\n239   male       11        67          6     no       STEM\n240   male       11        35          4     no       STEM\n241   male       11        32          5     no       STEM\n242   male       11        58          4     no       STEM\n243   male       11        21          3     no       STEM\n244   male       10        12          4     no       STEM\n245   male       11        45          7     no       STEM\n246   male        3        14          2     no       STEM\n247   male        9        41          5     no       STEM\n248   male        1        10          3     no       STEM\n249   male       11        30          3     no       STEM\n250   male       11        25         12     no       STEM\n251   male       11        37          7     no       STEM\n252   male       11        23          4     no       STEM\n253   male       11        65          3     no       STEM\n254   male       11        54          7     no       STEM\n255   male        8        41          5     no       STEM\n256   male        9        13          6     no       STEM\n257   male       11        52          6     no       STEM\n258   male        4        33          8    yes       STEM\n259   male        8        20          6     no       STEM\n260   male        2        13          2     no       STEM\n261   male       11        52          5     no       STEM\n262   male        6        20          3     no       STEM\n263   male        5        39          3     no       STEM\n264   male       11        53          8     no       STEM\n265   male       11        49          7     no       STEM\n266   male       11        49          6     no       STEM\n267   male       11        57          7     no       STEM\n268   male       11        17          5     no       STEM\n269   male       11        30         10     no       STEM\n270   male       11        13          3     no       STEM\n271   male       11        29          6     no       STEM\n272   male       11        29          3     no       STEM\n273   male       11        11          6     no       STEM\n274   male       10        12          2     no       STEM\n275   male       11        18          5     no       STEM\n276   male        3        15          3     no       STEM\n277 female       11        25          3     no       STEM\n278 female        8        10          4     no       STEM\n279 female       11        13          3    yes       STEM\n280 female       11        47          8     no       STEM\n281 female        5        11          6     no       STEM\n282 female       11        18          5     no       STEM\n283 female        4        21          1     no       STEM\n284 female        7        42          2     no       STEM\n285 female        9        33          1     no       STEM\n286 female        3        27          3     no       STEM\n287 female       11        12          3     no       STEM\n288 female       11        45          4     no       STEM\n289 female       11        49          3     no       STEM\n290 female       11        36          3    yes       STEM\n291 female       11        54          4     no       STEM\n292 female       10        31          6     no       STEM\n293 female        6        17          4     no       STEM\n294 female        7        30          4     no       STEM\n295 female        4        11          3     no       STEM\n296 female        2        17          5     no       STEM\n297 female        8        80          8     no       STEM\n298 female       11        39          4     no       STEM\n299 female       11        26          4     no       STEM\n300 female       11        29          4     no       STEM\n301 female       11        60          7     no       STEM\n302 female        1        16          3     no       STEM\n303 female        3        10          5     no       STEM\n304   male        3        14          2     no   Pre-prof\n305   male        6        14          7     no   Pre-prof\n306   male        7        12          4     no   Pre-prof\n307   male       11        13          3     no   Pre-prof\n308   male       11        11          5     no   Pre-prof\n309   male        8        39          2     no   Pre-prof\n310   male        8        32          3    yes   Pre-prof\n311   male        2        10          2     no   Pre-prof\n312   male       10        21          5    yes   Pre-prof\n313   male       11        10          3     no   Pre-prof\n314   male        7        12          3     no   Pre-prof\n315   male        5        15          3    yes   Pre-prof\n316   male       11        53          6     no   Pre-prof\n317   male        8        68          3     no   Pre-prof\n318   male        7        48          4     no   Pre-prof\n319   male        2        10          1     no   Pre-prof\n320   male       11        36          3     no   Pre-prof\n321   male        8        15          2    yes   Pre-prof\n322   male        3        32          3    yes   Pre-prof\n323   male        9        23          5     no   Pre-prof\n324   male        4        14          4     no   Pre-prof\n325   male        8        12          4     no   Pre-prof\n326   male       11        31          2     no   Pre-prof\n327   male       11        24          3    yes   Pre-prof\n328   male        8        33          4     no   Pre-prof\n329   male        6        50          2    yes   Pre-prof\n330   male        8        11          6     no   Pre-prof\n331   male       11        19          3     no   Pre-prof\n332   male        6        19          1     no   Pre-prof\n333   male       11        57          5     no   Pre-prof\n334   male       11        34          3     no   Pre-prof\n335   male        6        16          2     no   Pre-prof\n336   male        8        29          3     no   Pre-prof\n337   male       11        45          6     no   Pre-prof\n338   male       11        21          4     no   Pre-prof\n339 female        9        10          1     no   Pre-prof\n340 female        7        10          4     no   Pre-prof\n341 female       11        24          3     no   Pre-prof\n342 female        5        10          2     no   Pre-prof\n343 female        7        10          2     no   Pre-prof\n344 female       11        16          4     no   Pre-prof\n345 female       11        16          2     no   Pre-prof\n346 female       11        10          5     no   Pre-prof\n347 female        2        15          1     no   Pre-prof\n348 female        2        29          2    yes   Pre-prof\n349 female        3        24          2     no   Pre-prof\n350 female        2        11          4     no   Pre-prof\n351 female       11        22          9    yes   Pre-prof\n352 female       11        62          3     no   Pre-prof\n353 female       11        21          3     no   Pre-prof\n354 female        9        53          4     no   Pre-prof\n355 female        5        12          2     no   Pre-prof\n356 female       11        11          2     no   Pre-prof\n357 female       11        38          2     no   Pre-prof\n358 female       11        46          4     no   Pre-prof\n359 female       11        10          3     no   Pre-prof\n360 female        3        24          4     no   Pre-prof\n361 female       11        27          4     no   Pre-prof\n362 female        5        21          3     no   Pre-prof\n363 female        4        15          2     no   Pre-prof\n364 female        2        10          3     no   Pre-prof\n365 female        9        11          5     no   Pre-prof\n366 female        2        11          4     no   Pre-prof\n                           dept  quality helpfulness  clarity easiness\n1                       English 4.636364    4.636364 4.636364 4.818182\n2             Religious Studies 4.318182    4.545455 4.090909 4.363636\n3                           Art 4.790698    4.720930 4.860465 4.604651\n4                       English 4.250000    4.458333 4.041667 2.791667\n5                       Spanish 4.684211    4.684211 4.684211 4.473684\n6                       Spanish 4.233333    4.266667 4.200000 4.533333\n7                       Spanish 4.382353    4.352941 4.411765 4.117647\n8                       English 2.062500    2.062500 2.062500 1.437500\n9                         Music 2.041667    2.166667 2.000000 1.750000\n10                      English 4.111111    4.222222 4.000000 3.666667\n11                   Philosophy 4.727273    4.909091 4.545455 4.000000\n12                   Philosophy 3.724242    3.848485 3.606060 4.242424\n13                        Music 2.804348    2.695652 2.913043 2.217391\n14                        Music 4.838235    4.823529 4.852941 4.676471\n15                      Spanish 4.565217    4.565217 4.565217 2.826087\n16            Religious Studies 4.944444    4.962963 4.925926 3.703704\n17                      English 4.464286    4.714286 4.214286 3.214286\n18                      History 4.184211    4.368421 4.000000 3.631579\n19                          Art 3.909091    4.090909 3.727273 2.272727\n20                          Art 3.500000    3.285714 3.714286 3.285714\n21                   Philosophy 3.474576    3.542373 3.406780 2.355932\n22                      English 3.696429    3.714286 3.678571 3.642857\n23                      English 3.576923    3.461538 3.692308 4.615385\n24                   Philosophy 1.633333    1.600000 1.666667 2.266667\n25                      History 3.531250    3.312500 3.750000 2.562500\n26                      History 3.114286    3.057143 3.171429 3.857143\n27                        Music 4.909091    5.000000 4.818182 4.181818\n28                      English 4.239130    4.434783 4.043478 2.826087\n29                      Spanish 3.981481    4.037037 3.925926 3.148148\n30                      English 4.392857    4.500000 4.285714 3.166667\n31                      History 4.526316    4.473684 4.578947 3.131579\n32                       German 4.075000    4.150000 4.000000 3.250000\n33                   Philosophy 2.829546    3.022727 2.636364 2.045454\n34            Religious Studies 4.552632    4.561404 4.543860 3.614035\n35                        Music 3.700000    3.600000 3.800000 3.133333\n36                      English 1.891304    2.239130 1.543478 3.869565\n37                       French 4.277778    4.222222 4.333333 3.888889\n38                      English 4.416667    4.611111 4.222222 3.111111\n39                      History 2.569620    2.696203 2.443038 2.658228\n40                      Theater 3.472222    3.111111 3.833333 3.388889\n41                      English 2.442308    2.692308 2.192308 2.230769\n42            Religious Studies 3.067308    3.403846 2.692308 3.076923\n43            Religious Studies 3.600000    3.600000 3.600000 3.200000\n44                      Spanish 4.115385    4.230769 4.000000 2.884615\n45                      English 2.900000    3.200000 2.600000 3.800000\n46                      English 3.388889    3.711111 3.111111 2.844444\n47                   Philosophy 4.708334    4.750000 4.666666 3.666667\n48                         FLTR 3.291667    3.333333 3.250000 3.333333\n49                      History 2.250000    2.062500 2.437500 2.812500\n50                      English 3.555556    3.722222 3.388889 2.777778\n51                       French 3.854167    3.875000 3.833333 3.000000\n52                      History 3.913793    3.965517 3.862069 2.965517\n53                      History 3.469231    3.353846 3.584615 3.061538\n54                          Art 3.045455    3.090909 3.000000 3.363636\n55                      Spanish 3.464286    3.285714 3.642857 3.357143\n56                       German 4.000000    4.000000 4.000000 2.500000\n57                        Music 3.982759    3.862069 4.103448 3.310345\n58                      History 4.518519    4.518519 4.518519 4.074074\n59                      Theater 2.958333    2.833333 3.083333 2.500000\n60                      History 2.085965    2.280702 1.894737 2.631579\n61                      Theater 3.538462    3.461538 3.615385 3.230769\n62                      English 2.924242    2.696970 3.151515 3.454545\n63                      English 2.293478    2.282609 2.304348 2.456522\n64                      English 3.684211    3.684211 3.684211 3.157895\n65                       German 4.666666    4.888889 4.444445 3.055556\n66               Womens Studies 4.732143    4.857143 4.607143 4.500000\n67               Womens Studies 4.880952    4.952381 4.809524 4.809524\n68                      English 4.653846    4.615385 4.692308 4.076923\n69                      History 3.100000    3.500000 2.700000 3.000000\n70                      Spanish 2.107143    1.928571 2.285714 2.000000\n71                      Spanish 4.180000    4.320000 4.040000 3.720000\n72                      English 3.500000    3.500000 3.500000 2.272727\n73                      English 4.625000    4.666667 4.583333 4.527778\n74                      Spanish 4.428571    4.642857 4.214286 4.000000\n75                      English 2.394737    2.263158 2.526316 1.736842\n76                      English 1.674419    1.686047 3.360465 1.558140\n77                      English 3.760000    3.840000 3.680000 2.920000\n78                      English 4.538462    4.769231 4.307692 3.500000\n79            Religious Studies 1.875000    2.187500 1.562500 2.687500\n80            Religious Studies 4.025000    3.950000 4.100000 2.650000\n81                      Spanish 3.550000    3.666667 3.433333 1.966667\n82                      Spanish 3.166667    3.333333 3.000000 3.916667\n83                      English 2.523810    2.428571 2.619048 1.857143\n84                      Spanish 3.710526    3.894737 3.526316 2.789474\n85                          Art 4.458333    4.500000 4.416667 4.166667\n86                      Spanish 4.709677    4.806452 4.612903 3.645161\n87                      English 4.500000    4.545455 4.454545 3.636364\n88                       French 3.461538    3.307692 3.615385 2.923077\n89                       French 4.545455    4.545455 4.545455 3.500000\n90                      English 3.500000    3.807692 3.192308 3.692308\n91                      English 2.200000    2.000000 2.400000 2.400000\n92                      English 3.925000    4.000000 3.500000 3.850000\n93                      History 3.958333    3.916667 4.000000 2.750000\n94            Religious Studies 2.782609    3.000000 2.565218 3.768116\n95                      English 1.756757    1.810811 1.702703 1.783784\n96                      English 4.851351    4.945946 4.756757 3.729730\n97                      English 3.218750    3.343750 3.093750 2.156250\n98                      Spanish 3.312500    3.416667 3.208333 2.458333\n99               Womens Studies 3.695122    3.731707 3.658537 3.390244\n100                 Art History 3.794118    3.794118 3.794118 4.088235\n101                     Spanish 3.233333    3.266667 3.200000 3.400000\n102                     Spanish 3.236842    3.631579 2.842105 3.421053\n103                     English 4.700000    4.700000 4.700000 3.700000\n104                     English 1.482759    1.517241 1.448276 2.068966\n105                     Spanish 3.950000    3.900000 4.000000 3.400000\n106                     English 3.537037    3.629630 3.444444 2.814815\n107                     English 3.208333    3.222222 3.194444 2.083333\n108                     English 2.394737    2.684211 2.105263 1.789474\n109                       Dance 3.500000    3.714286 3.285714 3.500000\n110                     English 3.470588    3.529412 3.411765 3.294118\n111                     English 4.305556    4.259259 4.351852 3.203704\n112                     English 3.095238    3.142857 3.047619 3.857143\n113                  Philosophy 2.469697    2.454545 2.484848 2.666667\n114                     English 2.904255    3.021277 2.787234 3.361702\n115                     History 3.189655    3.206897 3.172414 3.672414\n116                     Spanish 4.678571    4.714286 4.642857 4.071429\n117              Art and design 3.200000    2.800000 3.600000 2.000000\n118                     History 3.833333    3.805556 3.861111 2.638889\n119                       Music 3.785714    3.500000 4.071429 3.285714\n120                       Music 3.442308    3.346154 3.500000 4.038462\n121                     History 2.589744    2.641026 2.538462 3.923077\n122                     History 2.437500    2.375000 2.500000 3.187500\n123                     English 2.977612    2.955224 3.000000 2.373134\n124                     English 3.224138    3.172414 3.275862 2.965517\n125                      German 2.750000    2.961539 2.538461 3.038461\n126                     English 3.729167    4.041666 3.416667 3.166667\n127                    Japanese 4.625000    4.812500 4.437500 3.062500\n128                     English 3.825000    4.000000 3.650000 2.550000\n129                     History 3.200000    3.171429 3.228571 3.028571\n130                     English 3.347222    3.333333 3.361111 3.500000\n131                     English 3.000000    3.074074 2.851852 3.592593\n132                       Music 4.100000    4.100000 4.100000 2.600000\n133                     English 2.792453    2.698113 2.886792 2.735849\n134                     Spanish 3.076923    3.076923 3.076923 3.461538\n135                  Psychology 4.800000    4.900000 4.700000 4.400000\n136               Communication 4.571429    4.571429 4.571429 4.238095\n137                  Psychology 4.757576    4.818182 4.696970 3.969697\n138               Communication 3.636364    3.545455 3.727273 3.727273\n139               Communication 3.583333    3.222222 3.944444 3.833333\n140                  Psychology 4.400000    4.366667 4.433333 4.133333\n141               Communication 4.071429    4.071429 4.142857 3.142857\n142                  Psychology 4.333333    4.333333 4.333333 2.933333\n143                   Sociology 4.250000    4.220000 4.280000 2.480000\n144                   Geography 4.150000    4.100000 4.200000 2.800000\n145           Political Science 3.366667    3.566667 3.166667 2.366667\n146                Anthropology 4.490566    4.283019 4.698113 3.603774\n147           Political Science 4.130435    3.934783 4.326087 3.217391\n148                  Psychology 2.700000    2.866667 2.533333 1.633333\n149           Political Science 3.142857    3.119048 3.166667 2.547619\n150                   Geography 2.621622    2.662162 2.581081 2.175676\n151                  Psychology 4.372549    4.176471 4.568627 3.588235\n152                  Psychology 1.467742    1.483871 1.451613 2.483871\n153                  Psychology 3.421053    3.578947 3.263158 4.368421\n154                Anthropology 2.419643    2.375000 2.464286 2.142857\n155           Political Science 3.833333    3.805556 3.861111 2.611111\n156           Political Science 4.216418    4.104477 4.328358 2.537314\n157           Political Science 2.494118    2.764706 2.223529 2.082353\n158                  Psychology 4.178571    4.142857 4.214286 2.428571\n159                   Sociology 2.872340    3.340426 2.404255 3.808511\n160                   Sociology 3.086957    3.420290 2.753623 3.579710\n161                Anthropology 3.409091    3.454545 3.363636 3.818182\n162           Political Science 2.535714    2.446429 2.625000 2.500000\n163               Communication 1.604167    1.500000 1.708333 2.083333\n164               Communication 2.630435    3.173913 2.086957 3.173913\n165               Communication 2.916667    3.138889 2.694444 3.500000\n166                   Sociology 2.733333    2.622222 2.844444 3.177778\n167     Communication Disorders 4.684211    4.894737 4.473684 4.210526\n168               Communication 4.000000    4.272727 3.727273 3.636364\n169               Communication 4.600000    4.700000 4.500000 4.500000\n170                   Sociology 4.544643    4.517857 4.571429 4.303571\n171                  Psychology 4.311765    4.305882 4.317647 4.435294\n172                  Psychology 4.937500    4.937500 4.937500 4.000000\n173                  Psychology 4.115385    4.179487 4.051282 2.410256\n174               Communication 2.435484    2.451613 2.419355 1.870968\n175                  Psychology 4.574074    4.703704 4.444444 2.888889\n176                   Sociology 3.880597    3.746269 4.029851 3.910448\n177                   Sociology 3.666667    3.866667 3.466667 3.533333\n178                   Sociology 4.195652    4.413043 3.978261 3.869565\n179                Anthropology 4.153846    4.384615 3.923077 4.076923\n180               Communication 2.000000    1.962963 2.037037 3.185185\n181               Communication 3.742857    3.714286 3.771429 2.885714\n182                   Sociology 2.171875    2.437500 1.906250 4.093750\n183                  Psychology 4.295455    4.318182 4.272727 3.954545\n184                  Psychology 3.686275    3.686275 3.686275 2.764706\n185               Communication 3.636364    4.090909 3.181818 3.363636\n186                  Psychology 4.076923    4.076923 4.076923 3.423077\n187                Anthropology 3.435484    3.274194 3.596774 2.322581\n188               Communication 2.730769    2.230769 3.230769 3.461538\n189                  Psychology 3.625000    3.550000 3.700000 3.500000\n190                  Psychology 4.562500    4.468750 4.656250 3.875000\n191                   Sociology 3.173913    3.130435 3.217391 3.173913\n192               Communication 3.741935    3.612903 3.870968 3.483871\n193                  Psychology 4.535714    4.547619 4.523810 2.500000\n194               Communication 4.083333    3.958333 4.208333 3.333333\n195     Communication Disorders 2.807692    2.730769 2.884615 3.576923\n196                   Geography 3.791667    3.500000 4.083334 3.000000\n197               Communication 3.343750    3.218750 3.468750 2.718750\n198                  Psychology 2.580000    2.600000 2.560000 2.860000\n199           Political Science 2.728070    2.526316 2.929825 2.684210\n200               Communication 3.966667    3.800000 4.133333 3.400000\n201                        Math 4.921875    5.000000 4.843750 3.812500\n202                        Math 2.529412    2.411765 2.647059 2.235294\n203                   Chemistry 2.565218    2.565218 2.565218 1.391304\n204                   Chemistry 4.269231    4.461538 4.076923 2.615385\n205                        Math 3.730769    3.692308 3.769231 3.846154\n206                        Math 2.638889    2.833333 2.444444 2.055556\n207                     Geology 4.954545    4.909091 5.000000 4.363636\n208                   Chemistry 4.480769    4.384615 4.576923 3.038462\n209                     Biology 2.646667    2.800000 2.533333 2.000000\n210                     Physics 4.720000    4.760000 4.680000 2.560000\n211                   Chemistry 2.619048    2.904762 2.333333 1.571429\n212           Astronomy/Physics 4.918605    5.000000 4.837209 3.279070\n213                        Math 4.549020    4.627451 4.470588 4.039216\n214                     Biology 4.045455    4.090909 4.000000 3.000000\n215                   Chemistry 4.796296    4.925926 4.666667 3.148148\n216                   Chemistry 3.067568    3.108108 3.027027 2.189189\n217                     Biology 3.385714    3.028571 3.742857 1.628571\n218                   Chemistry 4.000000    3.655172 4.344828 2.137931\n219                   Chemistry 1.750000    2.166667 1.333333 2.666667\n220            Computer Science 4.321429    4.392857 4.250000 2.714286\n221                     Physics 4.666667    4.757576 4.575758 3.212121\n222                        Math 2.714286    2.892857 2.535714 2.750000\n223                        Math 3.338710    3.645161 3.048387 3.193548\n224                        Math 3.986842    4.026316 3.947368 3.842105\n225       Physics and Astronomy 3.821429    4.071429 3.571429 3.214286\n226                     Geology 3.482759    3.482759 3.482759 3.206897\n227                   Chemistry 3.490566    3.924528 3.056604 3.000000\n228                     Geology 4.625000    4.600000 4.650000 3.700000\n229         Physics & Astronomy 3.452381    3.523810 3.380952 2.238095\n230                     Biology 2.287500    2.450000 2.125000 1.900000\n231                     Biology 3.038462    3.384615 2.692308 2.230769\n232                     Biology 4.365672    4.447761 4.283582 1.820896\n233                     Physics 3.684211    3.947368 3.421053 2.157895\n234                     Geology 3.500000    3.294118 3.705882 2.823529\n235                     Physics 4.159091    4.363636 3.954545 3.090909\n236                        Math 3.562500    3.437500 3.687500 3.375000\n237                   Chemistry 3.562500    3.437500 3.687500 3.375000\n238                     Geology 3.960526    4.000000 3.921053 2.578947\n239                        Math 4.231343    4.164179 4.298507 4.164179\n240                     Biology 2.671429    2.885714 2.457143 1.971429\n241                        Math 4.015625    3.937500 4.093750 4.406250\n242                     Biology 3.103448    3.275862 2.931034 2.224138\n243       Physics and Astronomy 4.166667    4.428571 3.904762 3.095238\n244                     Biology 2.791667    2.750000 2.833333 1.750000\n245                        Math 3.966667    4.088889 3.844444 2.533333\n246                     Geology 4.107143    4.142857 4.142857 3.071429\n247                   Chemistry 3.329268    3.512195 3.146341 3.000000\n248                     Geology 2.800000    2.900000 2.700000 2.400000\n249                     Biology 4.066667    3.900000 4.233333 2.933333\n250            Computer Science 4.620000    4.720000 4.520000 2.480000\n251                        Math 3.000000    3.081081 2.918919 3.054054\n252                   Chemistry 2.586957    2.608696 2.565217 2.695652\n253                        Math 1.915385    2.076923 1.723077 2.107692\n254                     Geology 4.527778    4.537037 4.518519 2.888889\n255                   Chemistry 3.890244    4.073171 3.707317 2.634146\n256                        Math 4.384615    4.461538 4.307692 3.846154\n257                        Math 3.451923    3.596154 3.269231 3.692308\n258                        Math 3.878788    4.030303 3.727273 3.515152\n259                     Physics 3.950000    3.900000 4.000000 3.200000\n260                   Chemistry 4.846154    4.923077 4.769231 3.769231\n261                        Math 2.865385    2.826923 2.903846 2.250000\n262                   Chemistry 3.000000    3.400000 2.600000 2.300000\n263                     Biology 3.166667    3.333333 3.000000 2.153846\n264                        Math 3.264151    3.339623 3.188679 3.528302\n265                        Math 2.928571    3.081633 2.775510 3.591837\n266         Physics & Astronomy 4.265306    4.265306 4.265306 3.204082\n267                        Math 2.508772    2.561404 2.473684 3.017544\n268                   Chemistry 4.235294    4.235294 4.235294 2.705882\n269            Computer Science 2.850000    2.933333 2.766667 2.200000\n270                     Geology 3.307692    3.461539 3.153846 2.769231\n271                        Math 2.620690    2.758621 2.482759 3.034483\n272                   Chemistry 3.103448    3.172414 3.034483 2.758621\n273            Computer Science 3.727273    4.000000 3.454545 3.272727\n274                     Geology 2.375000    2.416667 2.333333 2.750000\n275                        Math 3.722222    3.944444 3.500000 3.500000\n276                   Chemistry 2.500000    2.733333 2.266667 2.733333\n277                        Math 4.940000    5.000000 4.880000 4.800000\n278                     Biology 2.900000    3.100000 2.700000 1.500000\n279                     Biology 4.692308    4.769231 4.615385 4.230769\n280                        Math 3.436170    3.531915 3.340425 4.744681\n281                        Math 3.772727    3.818182 3.727273 3.727273\n282                   Chemistry 2.750000    2.777778 2.722222 2.444444\n283                   Chemistry 2.619048    3.190476 2.047619 2.523810\n284                     Geology 4.285714    4.452381 4.119048 3.619048\n285                     Biology 4.015152    4.181818 3.848485 1.878788\n286                        Math 4.981481    5.000000 4.962963 3.962963\n287                        Math 4.583333    4.500000 4.666667 3.666667\n288                     Biology 2.422222    2.533333 2.311111 2.022222\n289                        Math 2.336735    2.591837 2.081633 2.183673\n290                     Biology 4.500000    4.555556 4.444444 3.305556\n291                     Geology 3.555556    3.555556 3.555556 2.500000\n292                        Math 4.064516    4.096774 4.032258 3.258065\n293                     Biology 3.529412    3.470588 3.588235 2.117647\n294                     Biology 4.233333    4.366667 4.100000 2.933333\n295                     Biology 3.454545    3.363636 3.545455 2.636364\n296                        Math 2.352941    2.411765 2.294118 2.000000\n297                        Math 2.087500    2.375000 1.800000 2.387500\n298                     Physics 3.653846    4.076923 3.230769 2.794872\n299                     Biology 2.153846    2.346154 1.961538 2.115385\n300            Computer Science 2.655172    2.689655 2.620690 2.655172\n301                        Math 2.608333    2.716667 2.466667 3.483333\n302       Physics and Astronomy 2.406250    2.375000 2.437500 1.937500\n303                        Math 2.900000    3.200000 2.600000 3.100000\n304                        Kins 2.392857    2.142857 2.642857 1.428572\n305                        Kins 4.892857    4.928571 4.857143 4.857143\n306                        Kins 2.958333    3.000000 2.916667 3.833333\n307                        Kins 2.000000    2.000000 2.000000 2.846154\n308                    Business 3.954545    4.363636 3.545455 3.636364\n309                   Economics 4.294872    4.384615 4.205128 3.769231\n310                        Kins 4.453125    4.343750 4.562500 4.625000\n311                  Accounting 4.200000    4.600000 4.200000 2.400000\n312                        Kins 4.904762    4.857143 4.952381 4.809524\n313                     Finance 3.100000    3.500000 2.700000 1.900000\n314 Environmental Public Health 3.166667    3.166667 3.250000 3.583333\n315         Information Systems 3.433333    3.533333 3.333333 3.600000\n316                   Economics 4.811321    4.849057 4.773585 4.132075\n317                   Economics 3.919118    4.250000 3.588235 2.911765\n318                  Accounting 4.062500    4.229167 3.895833 1.750000\n319                    Business 2.600000    3.100000 2.200000 4.300000\n320                    Business 2.791667    2.750000 2.777778 2.194444\n321                     Finance 4.166667    4.266667 4.066667 3.000000\n322         Information Systems 4.250000    4.218750 4.281250 4.125000\n323                   Economics 4.260870    4.478261 4.043478 4.130435\n324                    Business 4.178571    4.071429 4.285714 3.142857\n325          Managerial Science 2.916667    3.000000 2.833333 2.333333\n326                  Accounting 2.919355    2.645161 3.193548 2.322581\n327                   Marketing 4.562500    4.791667 4.333333 3.666667\n328                   Economics 4.393939    4.424242 4.363636 3.454545\n329            Criminal Justice 4.640000    4.560000 4.720000 3.940000\n330  Curriculum and Instruction 2.590909    2.636364 2.545454 2.272727\n331             Library Science 4.394737    4.473684 4.315789 3.368421\n332                     Finance 3.447368    3.105263 3.789474 2.473684\n333                   Economics 3.719298    3.912281 3.526316 2.561404\n334                    Business 2.720588    2.764706 2.676471 2.647059\n335                    Business 3.687500    3.812500 3.562500 3.437500\n336           Special Education 3.136364    3.000000 3.272727 2.090909\n337                   Economics 4.288889    4.466667 4.111111 3.155556\n338                 Social Work 3.095238    2.809524 3.380952 2.666667\n339                    Business 4.050000    3.900000 4.200000 4.900000\n340           Special Education 4.850000    4.900000 4.800000 4.900000\n341                  Accounting 4.861111    4.833333 4.888889 4.722222\n342                        Kins 4.850000    4.800000 4.900000 4.000000\n343                  Management 3.350000    3.600000 3.100000 3.700000\n344                   Marketing 3.562500    3.687500 3.437500 4.125000\n345                    Business 4.687500    4.625000 4.750000 3.500000\n346                   Marketing 3.600000    3.600000 3.600000 2.700000\n347          Managerial Science 3.433333    3.666667 3.200000 3.200000\n348                   Economics 4.120690    4.344828 3.896552 4.034483\n349                   Economics 4.083330    4.583330 3.583330 4.166670\n350                 Social Work 3.181818    3.272727 3.090909 3.545455\n351                        Kins 4.340909    4.545455 4.090909 4.454545\n352                  Accounting 3.491935    3.612903 3.370968 2.467742\n353                  Management 4.500000    4.428571 4.571429 3.619048\n354                   Economics 3.584906    3.415094 3.754717 3.037736\n355                        Kins 3.708333    3.583333 3.833333 4.250000\n356                  Accounting 3.318182    3.090909 3.545455 2.181818\n357                   Economics 3.513158    3.657895 3.368421 3.868421\n358                   Economics 4.228261    4.304348 4.152174 3.369565\n359                        Kins 1.900000    1.900000 1.900000 3.800000\n360                   Economics 1.937500    2.250000 1.625000 2.333333\n361                    Business 3.462963    3.333333 3.592593 3.111111\n362                 Social Work 2.619048    2.714286 2.523810 3.619048\n363                  Accounting 2.966667    3.066667 2.866667 2.666667\n364                  Accounting 3.250000    3.200000 3.300000 3.000000\n365                     Nursing 1.909091    1.909091 1.909091 2.272727\n366                   Marketing 1.409091    1.363636 1.454545 2.636364\n    raterInterest  sdQuality sdHelpfulness sdClarity sdEasiness sdRaterInterest\n1        3.545455 0.55185640     0.6741999 0.5045250  0.4045199       1.1281521\n2        4.000000 0.90201795     0.9341987 0.9438798  0.5045250       1.0744356\n3        3.432432 0.45293432     0.6663898 0.4129681  0.5407021       1.2369438\n4        3.181818 0.93250483     0.9315329 0.9990938  0.5882300       1.3322506\n5        4.214286 0.65001124     0.8200699 0.5823927  0.6117753       0.9749613\n6        3.916667 0.86327170     1.0327956 0.7745967  0.6399405       0.6685579\n7        3.812500 0.94421613     0.9963167 0.9393364  0.6966305       1.2230427\n8        2.937500 1.19547760     1.3400871 1.1236103  0.7274384       1.6111590\n9        3.750000 1.07573090     1.3371159 1.0444659  0.7537783       1.2880570\n10       4.176471 0.90025413     1.0032627 0.9074852  0.7669650       1.5381123\n11       2.900000 0.34377584     0.3015113 0.5222330  0.7745967       1.1972190\n12       3.333333 1.23870660     1.3257359 1.2975793  0.7917663       1.2909944\n13       3.217391 0.98556780     1.0632191 1.1246431  0.7952428       1.1660548\n14       4.718750 0.51816386     0.6262243 0.4357058  0.8060600       0.5811210\n15       3.952381 0.99206334     1.0368697 0.9920634  0.8340577       1.0712698\n16       3.592593 0.16012815     0.1924501 0.2668803  0.8688992       1.1522306\n17       2.357143 0.60333323     0.4688072 0.8925824  0.8925824       1.1507284\n18       3.526316 0.69142620     0.6839856 0.8164966  0.8950808       1.1239030\n19       4.000000 1.26131250     1.4459976 1.1908744  0.9045340       1.0000000\n20       3.230769 1.25575600     1.3827827 1.2666473  0.9138736       1.2351685\n21       3.537037 1.59585600     1.6433989 1.5878705  0.9240599       1.3418231\n22       3.250000 1.23482860     1.2724180 1.3348206  0.9511898       1.3228756\n23       2.923077 1.57911040     1.6641006 1.6012815  0.9607689       1.4978617\n24       3.400000 0.71879530     0.9102590 1.0465362  0.9611501       1.2983506\n25       3.800000 0.93930380     1.1383468 0.9309493  0.9639329       0.9411239\n26       3.000000 1.27236300     1.3271566 1.3823619  0.9744639       1.5811388\n27       4.909091 0.20225996     0.0000000 0.4045199  0.9816498       0.3015113\n28       3.333333 1.12683750     1.1609591 1.2605288  0.9840627       1.3284223\n29       4.153846 1.08735290     1.1596247 1.1410496  0.9885383       0.8338972\n30       3.516129 1.00932060     1.1529390 0.9947598  1.0101115       1.3384311\n31       4.029412 0.62544435     0.6872130 0.7580765  1.0179750       1.0294245\n32       3.631579 1.38862860     1.3484884 1.4867839  1.0195458       1.1160708\n33       3.475000 1.35519330     1.5017607 1.3655378  1.0332731       1.3772417\n34       3.901961 0.72999640     0.8867586 0.8252743  1.0480319       1.1533413\n35       3.933333 1.26491100     1.2983506 1.3201731  1.0600988       0.7988086\n36       2.347826 1.03769530     1.4634022 0.8084697  1.0668478       1.1588767\n37       3.882353 0.84404874     1.0602750 0.7669650  1.0786096       0.9926198\n38       3.500000 0.57522374     0.9785276 0.7320845  1.0786096       0.9851844\n39       2.956522 1.17597260     1.3141676 1.2273260  1.0846963       1.3109806\n40       4.000000 1.18162680     1.4095844 1.0981267  1.0921586       1.0954451\n41       2.692308 1.37351320     1.5942203 1.2967415  1.1066234       1.2890068\n42       3.442308 1.14630200     1.2408033 1.2452020  1.1175280       1.1784580\n43       4.300000 1.50554530     1.5055453 1.5776213  1.1352924       0.4830459\n44       4.320000 0.86380196     0.8629110 1.0954451  1.1428709       0.9882645\n45       2.733333 1.28452320     1.3732131 1.3522468  1.1464230       1.3870146\n46       3.185185 1.42577290     1.4866408 1.5109031  1.1472409       1.3877773\n47       3.416667 0.54181236     0.4522670 0.6513389  1.1547005       1.4433757\n48       3.416667 1.33923880     1.3026779 1.5447860  1.1547005       1.4433757\n49       3.200000 0.96609180     1.1236103 1.0935416  1.1672617       1.2649110\n50       3.290322 1.40802690     1.4660171 1.4595063  1.1737878       1.2556325\n51       4.227273 1.27244340     1.2958965 1.3405601  1.1795356       0.9725675\n52       4.320000 1.40196910     1.4010904 1.4814119  1.1796718       0.9451631\n53       3.280702 1.28049350     1.3855366 1.3450407  1.1842313       1.3059532\n54       3.636364 1.42222620     1.6403991 1.2649111  1.2060454       0.9244163\n55       2.785714 1.51231200     1.7288756 1.3926810  1.2157393       1.4238934\n56       3.875000 1.19292780     1.4142136 1.0377490  1.2247449       0.6408699\n57       4.333333 1.24987690     1.4814119 1.2633523  1.2846191       1.2038585\n58       4.340909 0.70015470     0.7458246 0.8182065  1.2863852       1.0330173\n59       3.500000 1.23322070     1.4034589 1.3113722  1.3142575       1.0000000\n60       3.166667 1.15177480     1.2642670 1.2054076  1.3447196       1.4241846\n61       3.800000 1.16299780     1.5607362 0.8697185  1.3634421       1.6193277\n62       2.696970 1.28160800     1.3803271 1.3257359  1.3713795       1.2370542\n63       3.806452 1.33988430     1.4089089 1.3311576  1.3777038       1.3271361\n64       2.933333 1.18098970     1.2495613 1.2495613  1.3849652       1.1629192\n65       3.937500 0.42008403     0.3233808 0.7047922  1.0556415       0.7719024\n66       3.333333 0.44058344     0.4483951 0.5669467  0.5091751       1.2089410\n67       3.428571 0.21821790     0.2182179 0.4023739  0.5117663       1.2478553\n68       3.550000 0.62879616     0.6972473 0.6793662  0.6275716       1.0505954\n69       2.800000 1.12546290     1.3540064 1.0593499  0.6666667       1.2292726\n70       3.166667 1.04105290     1.2066665 1.1387288  0.6793662       1.4034589\n71       3.840000 0.77567180     0.8524475 0.8888194  0.7371115       0.9865766\n72       3.000000 1.38873020     1.5039630 1.4392458  0.7672969       1.4832397\n73       3.722222 0.59009683     0.6324555 0.7319251  0.7740842       1.0852547\n74       3.214286 0.26726124     0.4972452 0.5789342  0.7844645       1.1217138\n75       3.066667 1.51454940     1.6276126 1.6113632  0.8056816       1.4375906\n76       2.814815 1.07585110     1.1808435 1.0584168  0.8059288       1.2258784\n77       3.631579 1.20864940     1.2330483 1.2489996  0.8124038       1.2115429\n78       3.730769 0.54631630     0.4296689 0.7358930  0.8602325       1.1156233\n79       2.666667 0.82663980     1.1086779 0.8139410  0.8732125       1.2909944\n80       3.812500 1.15251450     1.2343760 1.1652874  0.8750940       1.1672618\n81       4.000000 1.15482500     1.0933445 1.2507469  0.8899180       1.1126973\n82       3.777778 1.05169420     1.0730867 1.2060454  0.9003366       1.7159384\n83       2.450000 1.25972410     1.4342743 1.3219754  0.9102590       1.3562720\n84       4.157895 1.26178660     1.2864567 1.3485968  0.9176629       1.2588865\n85       3.750000 0.68947720     0.9045340 0.6685579  0.9374369       1.1381804\n86       3.870968 0.49622230     0.4774484 0.6672041  0.9503819       1.4081178\n87       3.545455 0.80178374     0.8578641 0.9116846  0.9534626       1.0107646\n88       3.153846 0.98871840     1.1094004 1.1929279  0.9540736       1.2142318\n89       3.818182 0.87163080     1.0568269 0.8004328  0.9636241       1.2960145\n90       3.153846 1.20830460     1.3272296 1.2006409  0.9703290       1.3473621\n91       2.769231 1.34695420     1.3093073 1.4540584  0.9856108       1.4232502\n92       3.250000 1.05475120     1.1697953 1.0894228  0.9880869       1.2926920\n93       3.666667 1.13172300     1.4116492 0.9780193  0.9890707       1.3904436\n94       2.903226 1.24699130     1.3612279 1.2541491  1.0021291       1.3755437\n95       2.545455 1.06472230     1.1263964 1.1514451  1.0037467       1.5429458\n96       3.750000 0.30878040     0.2292434 0.4947168  1.0178586       1.2181424\n97       2.937500 1.24393690     1.3102419 1.4223872  1.0194678       1.1341474\n98       3.125000 1.46563900     1.5012072 1.5316705  1.0206207       1.2618999\n99       3.358974 1.24939010     1.3968606 1.2571745  1.0217154       1.1582014\n100      2.794118 1.14228400     1.2499554 1.2739681  1.0259555       1.2739681\n101      3.400000 1.34783780     1.4864467 1.3732131  1.0555973       1.3522468\n102      3.457143 1.42722960     1.4597492 1.5338754  1.0560400       1.2912114\n103      3.400000 0.78881060     0.9486833 0.6749486  1.0593499       1.0749677\n104      2.400000 0.66120505     0.7847060 0.6316762  1.0667385       1.3844373\n105      3.900000 0.89597870     1.4491377 0.6666667  1.0749677       0.8755950\n106      3.166667 1.31504550     1.3414650 1.3681355  1.0754976       1.4345630\n107      2.647059 1.15495820     1.3117297 1.0907257  1.0790207       1.2524486\n108      2.842105 1.03519920     1.2495613 1.1002392  1.0841765       1.3849652\n109      3.846154 1.17669680     1.2666474 1.2043876  1.0919284       1.7246330\n110      2.875000 1.15204420     1.3284223 1.1757351  1.1048024       1.6683325\n111      2.920000 1.01133510     1.1190492 0.9935150  1.1053836       1.2590894\n112      2.850000 1.51343190     1.6212870 1.5961263  1.1084094       1.0021622\n113      3.363636 1.36324180     1.4809395 1.4603341  1.1086779       1.0552897\n114      3.380952 1.35389260     1.5671819 1.3011061  1.1117071       1.1032630\n115      3.333333 1.21694030     1.3861295 1.2860647  1.1299423       0.9712859\n116      4.071429 0.63872296     0.6112498 0.8418974  1.1411388       1.1411388\n117      3.000000 1.41813650     1.6193277 1.3498971  1.1547005       1.2472191\n118      3.166667 1.06904500     1.2608261 1.0994226  1.1748016       1.2761549\n119      3.714286 0.87077080     1.0919285 0.7300459  1.2043875       1.2043875\n120      1.098039 1.57052170     1.7191232 1.7191232  1.2159200       1.1972190\n121      3.105263 1.33215200     1.4232502 1.3542556  1.2222631       1.4292216\n122      3.062500 1.22304260     1.5438048 1.0954451  1.2230427       1.4361407\n123      2.552239 1.47307360     1.6554120 1.4142136  1.2288923       1.1452614\n124      2.827586 1.36660560     1.5369024 1.3600565  1.2387424       1.4159541\n125      3.277778 1.19373370     1.3705698 1.1740791  1.2483835       1.2307339\n126      3.333333 1.33025940     1.3376712 1.5012072  1.2740442       1.1671841\n127      4.533333 0.64549720     0.5439056 0.8920949  1.2893797       0.6399405\n128      3.235294 1.29039980     1.3764944 1.3869694  1.3168943       1.1472473\n129      3.657143 1.31842150     1.3823619 1.3303187  1.3169866       1.1867617\n130      3.500000 1.53910660     1.5856499 1.6062873  1.3416408       1.3471506\n131      3.148148 1.27097790     1.3846945 1.3503192  1.3660516       1.0635103\n132      3.300000 1.24275680     1.3703203 1.1972190  1.4298407       1.6363917\n133      2.913043 1.35317990     1.3948381 1.4366143  1.4431662       1.2441166\n134      2.615385 1.32045050     1.4978617 1.3204506  1.4500221       1.0439078\n135      4.500000 0.25819890     0.3162278 0.4830459  0.5163978       0.5477226\n136      4.095238 0.63807750     0.6761234 0.6761234  0.7003401       0.9952267\n137      4.000000 0.28287620     0.3916747 0.4666937  0.7699370       0.7905694\n138      3.333333 1.22659910     1.3684763 1.1908744  0.7862454       1.3228757\n139      3.000000 1.15363870     1.2628425 1.2113300  0.8574929       1.2649111\n140      4.000000 0.75885576     0.7648905 0.8583598  0.8603661       0.9325048\n141      3.500000 1.10692130     1.3847680 1.0271052  0.8644378       1.0801235\n142      3.933333 0.91936830     1.0465362 0.8997354  0.8837151       0.5936168\n143      3.312500 0.66432410     0.7899884 0.8091316  0.8861750       1.1697654\n144      3.800000 1.17968920     1.1972190 1.2292726  0.9189366       1.3165612\n145      3.296296 1.22427560     1.3047217 1.3412124  0.9278575       1.0308628\n146      3.428571 0.62396500     0.9482242 0.5401177  0.9474586       1.3844373\n147      3.057143 0.87834935     0.9752988 0.9440892  0.9640895       1.4939655\n148      3.192308 1.36836170     1.5698305 1.2793677  0.9643055       1.2335066\n149      3.428571 1.23607220     1.2916690 1.3954048  0.9678334       1.5324595\n150      3.218750 1.30806460     1.4456077 1.3447692  0.9702155       1.2657735\n151      4.083334 0.76709280     1.0675591 0.6963624  0.9900296       0.7810788\n152      3.225806 0.88445354     0.9956896 0.8500474  0.9956896       1.5429339\n153      3.277778 1.49316180     1.6095475 1.5578513  1.0116283       1.6379886\n154      3.111111 1.40057380     1.4962073 1.4008346  1.0344708       1.3963114\n155      3.606557 1.17485390     1.2631159 1.1904024  1.0421478       1.2685778\n156      3.500000 0.91793525     1.0020332 0.9752709  1.0777130       1.2460464\n157      3.222222 1.24521490     1.4528056 1.2571365  1.0824549       1.3862730\n158      3.785714 1.01160850     1.0994504 1.1217138  1.0894096       1.3688047\n159      2.857143 1.13476720     1.4337030 1.0766206  1.0962049       1.2010448\n160      3.019608 1.21557450     1.3438947 1.1931890  1.1298854       1.2726381\n161      3.090909 1.39316510     1.5075567 1.4333686  1.1677484       1.3003496\n162      2.365854 1.39106610     1.4259879 1.5203170  1.1908744       1.3182583\n163      2.454545 0.90864410     0.9780193 0.9990938  1.2128539       1.4384937\n164      3.650000 1.17953560     1.5270939 1.0406748  1.2303796       1.0399899\n165      2.861111 1.33897610     1.5147424 1.3484265  1.2305632       1.2224747\n166      3.097561 1.33399600     1.4027390 1.4027390  1.3973279       1.0721450\n167      4.368421 0.34199280     0.3153018 0.5129892  0.6306035       0.7608859\n168      3.636364 0.80622580     0.9045340 0.9045340  0.6741999       0.9244163\n169      3.400000 0.65828060     0.6749486 0.7071068  0.7071068       1.5776213\n170      3.285714 0.51589980     0.6321988 0.6566344  0.7608522       1.0738933\n171      3.705882 0.91604894     0.9883070 0.9905718  0.7935313       1.1070847\n172      2.187500 0.17078250     0.2500000 0.2500000  0.8164966       1.5585784\n173      3.828571 1.23789690     1.2746887 1.2343487  0.8181477       1.0427823\n174      3.357143 1.19542130     1.2606535 1.2589465  0.8462441       1.3666473\n175      3.833333 0.58348596     0.5417078 0.7510676  0.8473185       1.0494995\n176      3.566667 1.06986810     1.2103504 1.0726652  0.8656989       1.1404232\n177      3.466667 1.34518540     1.5055453 1.4573296  0.9154754       1.1872337\n178      3.369565 0.98589080     1.1070699 0.9997584  0.9335403       1.1226694\n179      3.692308 1.16161910     1.1929279 1.2557560  0.9540736       1.1094004\n180      2.740741 1.05611770     1.2854655 1.0912759  0.9622504       1.1298654\n181      3.483871 1.24482120     1.4465258 1.2622509  0.9631880       1.1509697\n182      3.281250 1.29271770     1.6051831 1.2536238  0.9954534       1.3009767\n183      3.590909 0.95940320     1.0413528 0.9847319  0.9989172       0.7341397\n184      3.500000 0.96416175     1.0486219 1.0860975  1.0116963       1.1832160\n185      4.714286 1.00227010     1.4459976 1.0787198  1.0269106       0.4879500\n186      3.659574 1.21826220     1.3112807 1.2342522  1.0355666       1.1087909\n187      3.241935 1.17516610     1.3203248 1.2208527  1.0366117       1.1878850\n188      2.923077 1.12944280     1.3634421 1.0127394  1.0500305       1.3204506\n189      3.950000 1.26569100     1.5035047 1.2607433  1.0513150       0.9986833\n190      4.266667 0.83037110     1.0467885 0.7006621  1.0701221       0.9802650\n191      4.086957 1.54925720     1.5463843 1.6502485  1.0724727       0.6683115\n192      2.870968 1.07787720     1.2460408 1.0937059  1.0825279       1.1232837\n193      3.761905 0.66619470     0.8025077 0.7066960  1.0876244       0.8499505\n194      2.708333 1.26548390     1.3014763 1.2846643  1.1293194       1.1970677\n195      4.272727 1.64970860     1.7101507 1.6811168  1.2057554       1.1621744\n196      3.833333 0.81067690     1.2431631 0.5149286  1.2060454       1.2673044\n197      2.586207 1.33463480     1.4081416 1.3674647  1.2504032       1.0527936\n198      3.100000 1.30290100     1.4568627 1.3425532  1.2779128       1.2152872\n199      2.957447 1.24308870     1.3771770 1.2936610  1.3114591       1.3180593\n200      3.307692 1.21694387     1.3732131 1.2459458  1.5946339       1.4366985\n201      2.900000 0.18445101     0.0000000 0.3689020  0.5922892       1.3222238\n202      3.333333 1.21797200     1.2776357 1.2718675  0.6642112       0.8164966\n203      3.650000 1.38419940     1.3968564 1.4405203  0.7223151       1.2258188\n204      3.333333 0.63296210     0.7762500 0.6405126  0.7679476       1.3228757\n205      2.846154 1.21818480     1.1821319 1.3008873  0.8006408       0.9870962\n206      2.444444 1.30390310     1.4652846 1.2935233  0.8023658       1.2472191\n207      3.181818 0.15075567     0.3015113 0.0000000  0.8090398       1.4012981\n208      4.043478 0.84238670     1.0228166 0.8086075  0.8236878       1.2239378\n209      3.200000 1.46671000     1.6124516 1.3557637  0.8451543       1.3201731\n210      3.320000 0.43493295     0.5228129 0.4760952  0.8698659       1.2819256\n211      3.142857 1.46547570     1.6704718 1.3540064  0.8701396       1.1952286\n212      3.047619 0.21630646     0.0000000 0.4326129  0.8817078       1.2484601\n213      3.000000 0.59375840     0.7472827 0.6435197  0.8935499       1.3070323\n214      2.909091 1.12815210     1.3003496 1.0954451  0.8944272       1.2210279\n215      2.904762 0.31802452     0.2668803 0.5547002  0.9073929       1.3749459\n216      3.028571 1.42965700     1.5773357 1.4431156  0.9079231       1.3169866\n217      3.228571 1.20101500     1.4242793 1.2209653  0.9102590       1.2622509\n218      3.107143 0.91612536     1.1733914 0.9364012  0.9151166       1.3968029\n219      2.961538 0.80676025     1.2058288 0.6064784  0.9222661       1.1482428\n220      3.642857 0.91504186     0.9164863 1.0408330  0.9371803       0.9440892\n221      3.354839 0.60810500     0.6139169 0.7084447  0.9603898       1.4270806\n222      3.346154 1.22798060     1.4991179 1.1379690  0.9670497       1.1642099\n223      2.796610 1.15508210     1.2557641 1.2337789  0.9723844       1.1563865\n224      3.894737 1.19394220     1.4423487 1.0640912  0.9733285       1.1806886\n225      3.357143 0.91161615     1.1411388 0.9376145  0.9749613       1.2774459\n226      3.896552 1.27813170     1.6822750 1.1838403  0.9775812       1.2913124\n227      2.851064 1.00320000     1.1715584 1.0413763  0.9784634       1.3984265\n228      3.882353 0.55901700     0.6805570 0.5871429  0.9787210       1.1114379\n229      2.722222 1.42218820     1.5039630 1.5321942  0.9952267       1.2274103\n230      3.375000 1.18693470     1.4667249 1.1137256  1.0076629       1.3716451\n231      4.000000 1.39136530     1.5566236 1.4366985  1.0127394       1.2472191\n232      3.611940 1.03199600     1.0770834 1.1120002  1.0139239       1.5270810\n233      3.000000 1.01667380     1.2681432 1.2163602  1.0145145       1.3228757\n234      2.875000 1.03077640     0.9195587 1.3585243  1.0145993       1.1474610\n235      3.363636 0.89157915     0.9021379 1.0455016  1.0192944       1.3471507\n236      2.812500 1.22304260     1.3647344 1.2500000  1.0246951       1.0468206\n237      2.812500 1.22304260     1.3647344 1.2500000  1.0246951       1.0468206\n238      3.200000 1.01596430     1.1624764 1.0496223  1.0301293       1.2319282\n239      2.650794 0.97440100     1.0814835 1.0152612  1.0386036       1.2202423\n240      3.645161 1.24819200     1.3233520 1.3578282  1.0427823       1.1704241\n241      2.281250 1.31053040     1.3897667 1.3526408  1.0429293       1.1139692\n242      3.810345 1.23102660     1.4605106 1.1826534  1.0436626       1.4244112\n243      3.350000 1.01653000     1.1212238 1.0910895  1.0442587       1.2258187\n244      3.000000 1.21465170     1.2880570 1.1934163  1.0552897       1.1832160\n245      3.333333 1.35847510     1.4588635 1.3809673  1.0574412       1.2841818\n246      3.000000 0.62568640     0.9492623 0.5345225  1.0716117       1.0377490\n247      3.028571 1.21248590     1.5020311 1.1524100  1.0723805       1.3169866\n248      2.700000 1.22927260     1.2866839 1.2516656  1.0749677       1.2516656\n249      3.413793 0.76263310     1.0618786 0.7738544  1.0806554       0.9826074\n250      4.117647 0.72571800     0.6782330 0.9183318  1.0847427       1.2187264\n251      2.343750 1.33853150     1.4601822 1.3617468  1.1041826       1.1247760\n252      2.333333 1.29379800     1.4996706 1.3082287  1.1051443       1.0846523\n253      2.122807 0.97855616     1.2413160 0.9438567  1.1057107       1.2965638\n254      3.078431 0.84032700     0.8403270 0.7948120  1.1271381       1.2139710\n255      3.341463 1.20162490     1.2326850 1.2892615  1.1348149       1.2963363\n256      3.076923 1.13933180     1.1266014 1.1821319  1.1435437       1.4411534\n257      2.787234 1.20562260     1.2873383 1.2225878  1.1468364       1.2146957\n258      3.303030 1.17944840     1.3342800 1.1256311  1.1489455       1.3574988\n259      2.950000 1.30686450     1.5525870 1.1697953  1.1516578       1.1459310\n260      4.153846 0.31521258     0.2773501 0.4385290  1.1657506       1.2810252\n261      2.285714 1.25680650     1.3535886 1.2408033  1.1694644       1.3385315\n262      3.050000 1.15849270     1.2732057 1.1876558  1.1742859       1.1459310\n263      3.615385 1.20488770     1.3245324 1.2139540  1.1818465       1.1148610\n264      2.978723 1.13765860     1.3147594 1.1938400  1.1865233       1.1700866\n265      2.950000 1.17703720     1.2883571 1.2460823  1.2062056       1.1972190\n266      3.891304 1.15064310     1.3034491 1.1686087  1.2412962       1.0160547\n267      2.350877 1.35781870     1.5355030 1.2970470  1.2746320       1.2318863\n268      4.090909 1.14724730     1.2004901 1.1472473  1.3117119       1.0444659\n269      3.724138 1.34645130     1.5297810 1.3565507  1.3235272       1.2217245\n270      3.692308 1.31558700     1.3301244 1.3445045  1.3634421       1.1094004\n271      3.173913 1.25822410     1.4054784 1.1838403  1.3753638       1.0724727\n272      2.962963 1.30506780     1.3645765 1.3491468  1.4054784       1.2241632\n273      4.181818 1.64869094     1.6124516 1.8090681  1.4206273       0.9816498\n274      2.090909 1.13066760     1.4433757 1.3026779  1.4222262       0.8312094\n275      2.444444 1.36362650     1.4741786 1.4652846  1.4245742       1.3382263\n276      3.000000 1.06904500     1.4864467 1.0997835  1.4375906       1.4142136\n277      4.240000 0.16583124     0.0000000 0.3316625  0.5000000       0.8793937\n278      3.000000 1.02198060     0.8755950 1.3374935  0.5270463       1.3333333\n279      3.250000 0.59646390     0.4385290 0.8697185  0.7250111       1.1381804\n280      2.914894 1.11129090     1.2131716 1.1661270  0.7362680       1.4269120\n281      3.000000 1.45539750     1.6624188 1.3483997  0.9045340       1.4832397\n282      3.588235 1.37466570     1.5550886 1.4061025  0.9217772       0.9393364\n283      2.904762 1.19557330     1.4359334 1.1169687  0.9283883       1.0442587\n284      3.459459 0.99885650     0.9160461 1.1305596  0.9358023       1.1924437\n285      3.961538 0.79534630     0.8083372 0.9721501  0.9603898       0.8708970\n286      3.407407 0.09622505     0.0000000 0.1924501  0.9798541       1.3376000\n287      3.000000 0.73340220     0.6741999 0.8876254  0.9847319       1.6514456\n288      3.850000 1.27009100     1.4238233 1.2581050  1.0110501       1.1220403\n289      2.302326 1.20064120     1.3527498 1.2047948  1.0139335       0.9888638\n290      3.656250 0.70710677     0.6946508 0.8432740  1.0642085       1.0957211\n291      2.607843 1.06251160     1.2539247 1.0757475  1.0772081       1.2973578\n292      2.320000 1.40084460     1.4225526 1.4255729  1.0944631       1.3453624\n293      3.352941 1.23073390     1.3284223 1.2277430  1.1114379       1.2718675\n294      3.366667 1.08860350     1.2452207 1.0618786  1.1121068       1.2726116\n295      3.272727 1.40453820     1.5666989 1.3684763  1.1200649       1.4893562\n296      2.470588 0.91454744     0.9393364 1.1599949  1.1726039       1.2307339\n297      2.733333 1.09883430     1.3442329 1.0482052  1.2169036       1.2339054\n298      3.421053 1.27296710     1.2852322 1.4227760  1.2178386       1.0813300\n299      2.884615 1.02731920     1.3547637 0.9583640  1.2434443       1.4234303\n300      4.068966 1.34365870     1.4418106 1.3993313  1.2614012       1.3074248\n301      2.250000 1.35637750     1.4390636 1.4078071  1.2688132       1.0823902\n302      2.875000 1.14336860     1.3102163 1.0935416  1.3400871       1.3102163\n303      1.900000 1.07496770     1.3165612 0.9660918  1.3703203       1.1005049\n304      2.714286 1.19580300     1.2924124 1.1507283  0.5135526       1.2043875\n305      4.000000 0.28946713     0.2672612 0.3631365  0.5345225       1.1766968\n306      3.416667 1.49443412     1.4298407 1.6329932  0.6992059       1.3540064\n307      2.769231 1.24163870     1.5275252 1.0801234  0.8006408       1.4232502\n308      2.909091 1.15009880     1.2060454 1.2135598  0.8090398       1.1361818\n309      3.055556 0.76706850     0.8148421 0.8938234  0.8098583       1.1697239\n310      3.593750 0.87399140     1.0957211 0.8775883  0.8327955       1.5420844\n311      3.900000 0.78881064     0.6992059 0.6324555  0.8432740       1.2866839\n312      3.894737 0.20118695     0.3585686 0.2182179  0.8728716       1.1969747\n313      2.500000 1.32916010     1.5811388 1.4181365  0.8755950       0.9258201\n314      3.500000 1.11464080     1.6422453 1.0552897  0.9003366       1.1677484\n315      3.666667 1.26585190     1.3557637 1.2909944  0.9102590       1.5886502\n316      3.133333 0.34287268     0.4112002 0.4658123  0.9206548       1.2720778\n317      2.919355 1.06368230     1.1638203 1.2366939  0.9261587       1.1205734\n318      3.348837 1.09459510     1.0766296 1.2070640  0.9339917       1.3252802\n319      2.600000 1.02198060     1.3703203 1.3984118  0.9486833       1.3498971\n320      3.114286 1.38551180     1.5743479 1.3333333  0.9803627       1.2312459\n321      3.357143 0.79432510     0.9611501 0.7988086  1.0000000       1.4468609\n322      3.093750 1.03954090     1.2374369 1.0544644  1.0080323       1.4448881\n323      2.952381 0.91539320     0.8979555 1.0650762  1.0137396       1.2835961\n324      4.000000 1.11987540     1.1411388 1.2043876  1.0271052       1.0377490\n325      3.750000 1.41153260     1.8090681 1.1934162  1.0730867       0.7537783\n326      2.741935 1.17564420     1.2529535 1.2759142  1.0766335       1.2101719\n327      3.863636 0.86366886     0.8329709 1.0494995  1.0901403       1.0371873\n328      3.285714 0.94998010     1.0615526 0.9623598  1.0923286       1.1818737\n329      4.160000 0.63116350     0.8121526 0.5728554  1.0956314       0.8417668\n330      3.636364 1.13618180     1.3618170 1.3684763  1.1037128       1.5015144\n331      3.421053 0.71838840     0.7723284 0.8200699  1.1160708       1.5024347\n332      3.000000 1.41317940     1.5949482 1.4749368  1.1239030       1.1547005\n333      3.092593 1.04803190     1.1538863 1.1036508  1.1498066       1.2017051\n334      3.030303 1.29796560     1.4367224 1.3644976  1.1516090       1.1315048\n335      2.769231 1.34008700     1.3768926 1.3647344  1.1528949       1.4232502\n336      4.000000 1.30558240     1.4832397 1.3483997  1.2210279       0.8944272\n337      3.184211 1.00277390     1.0135446 1.0917505  1.2239198       1.3326516\n338      3.444444 1.15778930     1.3645163 1.1608700  1.2382784       1.0416176\n339      3.200000 0.92646280     1.1005049 0.9189366  0.3162278       1.5491933\n340      4.600000 0.24152295     0.3162278 0.4216370  0.3162278       0.6992059\n341      4.647059 0.41322106     0.5144958 0.3233808  0.5745131       0.6063391\n342      4.200000 0.33747430     0.4216370 0.3162278  0.6666667       0.6324555\n343      3.875000 1.54650140     1.5776213 1.7288403  0.6749486       1.3562027\n344      3.437500 1.44769930     1.5370426 1.4127397  0.7187953       1.0935416\n345      3.187500 0.57373047     0.6191392 0.5773503  0.8164966       0.9810708\n346      4.250000 1.64654520     1.7126977 1.6465452  0.8232726       0.8864053\n347      3.000000 1.48644670     1.5886502 1.4242793  0.9411239       1.4638501\n348      3.034483 0.96042377     1.2614012 0.9001916  0.9813532       1.0850529\n349      2.625000 0.68630000     0.7172800 0.8297000  1.0072200       1.0959400\n350      3.363636 1.30905940     1.4893562 1.2210279  1.0357255       0.6741999\n351      3.636364 0.80750000     0.9625004 0.9714540  1.0568269       1.3988245\n352      2.966102 1.37152520     1.4525821 1.4169220  1.0669052       1.3641481\n353      3.428571 0.80622580     1.0757057 0.7464200  1.0712698       1.0281745\n354      2.825000 1.14243270     1.2315400 1.2074394  1.0734965       1.0594508\n355      2.000000 1.43745890     1.5050420 1.4034589  1.1381804       1.4142136\n356      2.727273 1.67738970     1.7002674 1.7529196  1.1677484       1.4206273\n357      2.823529 1.14777600     1.2363048 1.2610817  1.2119016       1.1407225\n358      2.906977 0.97585590     1.0081791 1.0534049  1.2176242       1.1508579\n359      2.200000 1.02198060     1.1005049 1.1972190  1.2292726       1.1352924\n360      2.562500 1.18241190     1.3593477 1.1726039  1.2740441       1.3149778\n361      2.555556 1.32958880     1.4935760 1.2787993  1.3397283       1.4232502\n362      3.411765 1.54842470     1.5537972 1.6917165  1.4309504       1.0036697\n363      2.533333 1.28822500     1.3345233 1.3557637  1.4474937       1.1254629\n364      2.700000 1.20761480     1.4757296 1.0593499  1.5634719       0.9486833\n365      2.545455 1.37510340     1.5782614 1.3003496  1.6180797       1.6348478\n366      2.272727 0.91701096     0.9244163 1.0357255  1.6292776       1.4206273\n\n\n\nselected_data <- Rateprof[, c(\"quality\", \"helpfulness\", \"clarity\", \"easiness\", \"raterInterest\")]\npairs(selected_data)\n\n\n\n\nAccording to the scatter plot matrix of average professor evaluations for the themes of quality, clarity, helpfulness, easiness, and rater interest, the variables quality, clarity, and helpfulness appear to have substantial positive connections. Helpfulness, clarity, and quality appear to have a significantly less positive link with variable ease. Rater interest does not appear to be related to any of the other factors. As a result, we may conclude that Quality, helpfulness, and clarity have the most obvious linear links with one another, although Easiness and raterInterest do not appear to have linear ties with the other factors."
  },
  {
    "objectID": "posts/HW3_SaisrinivasAmbatipudi.html#question-5",
    "href": "posts/HW3_SaisrinivasAmbatipudi.html#question-5",
    "title": "603Homework_Saisrinivas_Ambatipudi",
    "section": "Question 5:",
    "text": "Question 5:\nFor the student.survey data file in the smss package, conduct regression analyses relating (by convention, y denotes the outcome variable, x denotes the explanatory variable) (i) y = political ideology and x = religiosity, (ii) y = high school GPA and x = hours of TV watching. (You can use ?student.survey in the R console, after loading the package, to see what each variable means.) (a) Graphically portray how the explanatory variable relates to the outcome variable in each of the two cases (b) Summarize and interpret results of inferential analyses."
  },
  {
    "objectID": "posts/HW3_SaisrinivasAmbatipudi.html#answer-4",
    "href": "posts/HW3_SaisrinivasAmbatipudi.html#answer-4",
    "title": "603Homework_Saisrinivas_Ambatipudi",
    "section": "Answer:",
    "text": "Answer:\n\ndata(student.survey)\nstudent.survey\n\n   subj ge ag  hi  co   dh    dr   tv sp ne ah    ve pa                    pi\n1     1  m 32 2.2 3.5    0  5.00  3.0  5  0  0 FALSE  r          conservative\n2     2  f 23 2.1 3.5 1200  0.30 15.0  7  5  6 FALSE  d               liberal\n3     3  f 27 3.3 3.0 1300  1.50  0.0  4  3  0 FALSE  d               liberal\n4     4  f 35 3.5 3.2 1500  8.00  5.0  5  6  3 FALSE  i              moderate\n5     5  m 23 3.1 3.5 1600 10.00  6.0  6  3  0 FALSE  i          very liberal\n6     6  m 39 3.5 3.5  350  3.00  4.0  5  7  0 FALSE  d               liberal\n7     7  m 24 3.6 3.7    0  0.20  5.0 12  4  2 FALSE  i               liberal\n8     8  f 31 3.0 3.0 5000  1.50  5.0  3  3  1 FALSE  i               liberal\n9     9  m 34 3.0 3.0 5000  2.00  7.0  5  3  0 FALSE  i          very liberal\n10   10  m 28 4.0 3.1  900  2.00  1.0  1  2  1 FALSE  i      slightly liberal\n11   11  m 23 2.3 2.6  253  1.50 10.0 15  1  1 FALSE  r slightly conservative\n12   12  f 27 3.5 3.6  190  3.00 14.0  3  7  0 FALSE  d               liberal\n13   13  m 36 3.3 3.5  245  1.50  6.0 15 12  5 FALSE  d          very liberal\n14   14  m 28 3.2 3.2  500  6.00  3.0 10  1  2 FALSE  i              moderate\n15   15  f 28 3.0 3.5 3500  1.00  4.0  3  1  0 FALSE  d          very liberal\n16   16  f 25 3.8 3.3  210 10.00  7.0  6  1  0 FALSE  i               liberal\n17   17  f 41 4.0 3.0 1000 15.00  6.0  7  3 10 FALSE  i      slightly liberal\n18   18  m 50 3.8 3.8    0  3.00  5.0  9  6 10 FALSE  d               liberal\n19   19  m 71 4.0 3.5 5000  3.00  6.0 12  2  2 FALSE  i               liberal\n20   20  f 28 3.0 3.8  120  1.00 25.0  0  0  2 FALSE  d          very liberal\n21   21  f 26 3.7 3.7 8000  8.00  4.0  4  4  1 FALSE  i              moderate\n22   22  f 27 4.0 3.7    2  2.50  4.0  2  7  0 FALSE  i               liberal\n23   23  m 31 2.7 3.5 1700  5.00  7.0  7  2  0 FALSE  r     very conservative\n24   24  f 23 3.7 3.7    2  2.00  7.0  4  2  0 FALSE  i              moderate\n25   25  m 23 3.2 3.8  450  4.00  0.0  7  7  3 FALSE  i          very liberal\n26   26  f 44 3.0 3.0    0  2.00  2.0  3  2  3 FALSE  i      slightly liberal\n27   27  m 26 3.7 3.0 1000  3.00  8.0  2  7  0 FALSE  d               liberal\n28   28  f 31 3.7 3.8  850 10.00 10.0  3  7  0 FALSE  r slightly conservative\n29   29  m 24 3.3 3.1  420  2.00 10.0  6  5  0 FALSE  d              moderate\n30   30  f 26 3.3 3.3 1200  0.75 10.0  0  3  0 FALSE  r               liberal\n31   31  m 26 3.3 3.5 1000  1.50  0.0  3  3  3 FALSE  d               liberal\n32   32  f 32 3.5 3.9  150 12.00 10.0  2  0  0 FALSE  d               liberal\n33   33  m 26 3.4 3.4 2000  1.50  2.0  7 14  0 FALSE  d               liberal\n34   34  f 22 3.2 2.8  316  2.00 10.0  3  5  2 FALSE  i               liberal\n35   35  f 24 3.5 3.9  900  1.75  8.0  0  0  1 FALSE  d          very liberal\n36   36  m 24 3.6 3.3  250  2.00  4.0  6  3  1 FALSE  r slightly conservative\n37   37  m 23 3.8 3.7  180  0.50 10.0  5  7  0 FALSE  i               liberal\n38   38  m 33 3.4 3.4 6000  1.50  8.0  5  6  2 FALSE  i               liberal\n39   39  m 23 2.8 3.2  950  2.00 37.0 10  5  0 FALSE  r slightly conservative\n40   40  m 31 3.8 3.5 1100  0.75  0.5  3  5  2 FALSE  r          conservative\n41   41  m 26 3.4 3.4 1300  1.20  0.0  8  2  0 FALSE  i               liberal\n42   42  m 28 2.0 3.0  360  0.25 10.0  8  3  0 FALSE  d      slightly liberal\n43   43  f 24 3.8 3.9 1800  2.00  2.0  5  4  1 FALSE  r          conservative\n44   44  m 23 3.0 3.6  900 15.00 12.0  0  5  0 FALSE  r slightly conservative\n45   45  f 25 3.0 4.0 5000  5.00  1.5  0  4  0 FALSE  i              moderate\n46   46  f 24 3.0 3.5  300  1.00 10.0  5  5  0 FALSE  d               liberal\n47   47  f 27 3.0 3.8 2000 20.00 28.0  7 14  2 FALSE  r      slightly liberal\n48   48  m 24 3.3 3.8  630  1.30  2.0  3  5  0 FALSE  r     very conservative\n49   49  f 26 3.8 4.0 1200  1.00  0.0  4  3  1 FALSE  d               liberal\n50   50  f 27 3.0 4.0  580  2.00  5.0 15  1  2 FALSE  d          very liberal\n51   51  m 32 3.0 3.0 2000  5.00  5.0  5  2  1 FALSE  r slightly conservative\n52   52  f 41 4.0 4.0    0  8.00  8.0  4  2  2 FALSE  r              moderate\n53   53  f 29 3.0 3.9  300  3.70  2.0  5  1 11 FALSE  d               liberal\n54   54  f 50 3.5 3.8    6  6.00  7.0  3  7  0 FALSE  d               liberal\n55   55  f 22 3.4 3.7   80  7.00 10.0  1  2  2 FALSE  i               liberal\n56   56  f 23 3.6 3.2  375  1.50  5.0 10  5  0 FALSE  r          conservative\n57   57  m 26 3.5 3.6 2000  0.30 16.0  8  3  0 FALSE  d              moderate\n58   58  m 30 3.0 3.0    1  1.10  1.0  4  3  0 FALSE  i      slightly liberal\n59   59  f 23 3.0 3.0  112  0.50 15.0  3  3  0 FALSE  i              moderate\n60   60  f 22 3.4 3.0  650  4.00  8.0 16  7  1 FALSE  i              moderate\n             re    ab    aa    ld\n1    most weeks FALSE FALSE FALSE\n2  occasionally FALSE FALSE    NA\n3    most weeks FALSE FALSE    NA\n4  occasionally FALSE FALSE FALSE\n5         never FALSE FALSE FALSE\n6  occasionally FALSE FALSE    NA\n7  occasionally FALSE FALSE FALSE\n8  occasionally FALSE FALSE FALSE\n9  occasionally FALSE FALSE    NA\n10        never FALSE FALSE FALSE\n11 occasionally FALSE FALSE FALSE\n12 occasionally FALSE FALSE    NA\n13 occasionally FALSE FALSE FALSE\n14 occasionally FALSE FALSE FALSE\n15        never FALSE FALSE FALSE\n16   every week FALSE FALSE FALSE\n17   every week FALSE    NA FALSE\n18        never FALSE FALSE FALSE\n19        never FALSE FALSE FALSE\n20 occasionally FALSE FALSE FALSE\n21 occasionally FALSE FALSE FALSE\n22 occasionally FALSE FALSE FALSE\n23   every week FALSE FALSE FALSE\n24        never FALSE FALSE FALSE\n25        never FALSE FALSE FALSE\n26   most weeks FALSE FALSE FALSE\n27 occasionally FALSE FALSE    NA\n28   most weeks FALSE FALSE FALSE\n29 occasionally FALSE FALSE    NA\n30 occasionally FALSE FALSE    NA\n31 occasionally FALSE FALSE FALSE\n32 occasionally FALSE FALSE FALSE\n33        never FALSE FALSE FALSE\n34 occasionally FALSE FALSE    NA\n35 occasionally FALSE FALSE    NA\n36   every week FALSE FALSE FALSE\n37        never FALSE FALSE    NA\n38        never FALSE FALSE FALSE\n39   most weeks FALSE FALSE FALSE\n40   most weeks FALSE FALSE    NA\n41 occasionally FALSE FALSE FALSE\n42        never FALSE FALSE    NA\n43   every week FALSE FALSE FALSE\n44        never FALSE FALSE FALSE\n45 occasionally FALSE FALSE FALSE\n46        never FALSE FALSE FALSE\n47 occasionally FALSE FALSE FALSE\n48   every week FALSE FALSE FALSE\n49        never FALSE FALSE FALSE\n50 occasionally FALSE FALSE FALSE\n51   every week FALSE FALSE FALSE\n52 occasionally FALSE FALSE FALSE\n53 occasionally FALSE FALSE FALSE\n54 occasionally FALSE FALSE    NA\n55        never FALSE FALSE    NA\n56   every week FALSE FALSE FALSE\n57 occasionally FALSE FALSE    NA\n58   every week FALSE FALSE FALSE\n59   most weeks FALSE FALSE FALSE\n60 occasionally FALSE FALSE FALSE"
  },
  {
    "objectID": "posts/HW3_SaisrinivasAmbatipudi.html#i",
    "href": "posts/HW3_SaisrinivasAmbatipudi.html#i",
    "title": "603Homework_Saisrinivas_Ambatipudi",
    "section": "(i)",
    "text": "(i)\n\n## plot(student.survey$re, student.survey$pi, main=\"Political Ideology vs Religiosity\", xlab=\"Religiosity\", ylab=\"Political Ideology\")\n\nstudent.survey %>%\n  select(c(pi, re)) %>%\n  ggplot() + \n  geom_bar(aes(x = re, fill = as.factor(pi)), position = \"dodge\") +\n  xlab(\"Religiosity\") +\n  ylab(\"Political Ideology\") +\n  scale_fill_manual(values = c(\"#FF0000\", \"#FF8000\", \"#FFFF00\", \"#80FF00\", \"#00FF00\", \"#00FF80\", \"#00FFFF\", \"#0080FF\", \"#0000FF\", \"#8000FF\", \"#FF00FF\", \"#FF0080\")) +\n  labs(fill = \"Political Ideology\")\n\n\n\n\nReligiosity and political ideology seem to have a positive relationship."
  },
  {
    "objectID": "posts/HW3_SaisrinivasAmbatipudi.html#ii",
    "href": "posts/HW3_SaisrinivasAmbatipudi.html#ii",
    "title": "603Homework_Saisrinivas_Ambatipudi",
    "section": "(ii)",
    "text": "(ii)\n\nstudent.survey %>%\n  select(tv, hi) %>%\n  ggplot(aes(x = tv, y = hi)) +\n  geom_point(color = \"steelblue\", alpha = 0.6) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\", linetype = \"solid\", size = 1) +\n  labs(title = \"High School GPA vs Hours of TV Watching\",\n       x = \"Hours of TV Watching\",\n       y = \"High School GPA\") +\n  theme_minimal() +\n  theme(panel.grid.major = element_line(color = \"gray\", linetype = \"dotted\"))\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nHigh school GPA and TV-watching appear to have a negative relationship."
  },
  {
    "objectID": "posts/HW3_SaisrinivasAmbatipudi.html#a-2",
    "href": "posts/HW3_SaisrinivasAmbatipudi.html#a-2",
    "title": "603Homework_Saisrinivas_Ambatipudi",
    "section": "(a)",
    "text": "(a)\n\nmodel1 <- lm(as.numeric(pi) ~ as.numeric(re), data=student.survey)\nsummary(model1)\n\n\nCall:\nlm(formula = as.numeric(pi) ~ as.numeric(re), data = student.survey)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.81243 -0.87160  0.09882  1.12840  3.09882 \n\nCoefficients:\n               Estimate Std. Error t value Pr(>|t|)    \n(Intercept)      0.9308     0.4252   2.189   0.0327 *  \nas.numeric(re)   0.9704     0.1792   5.416 1.22e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.345 on 58 degrees of freedom\nMultiple R-squared:  0.3359,    Adjusted R-squared:  0.3244 \nF-statistic: 29.34 on 1 and 58 DF,  p-value: 1.221e-06\n\n\nThere is a statistically significant link between religion and political ideology at a significance level of 0.01. The association is modest and positive, implying that as weekly church attendance grows, political ideology shifts toward the right."
  },
  {
    "objectID": "posts/HW3_SaisrinivasAmbatipudi.html#b-2",
    "href": "posts/HW3_SaisrinivasAmbatipudi.html#b-2",
    "title": "603Homework_Saisrinivas_Ambatipudi",
    "section": "(b)",
    "text": "(b)\n\nmodel2 <- lm(as.numeric(hi) ~ as.numeric(tv), data=student.survey)\nsummary(model2)\n\n\nCall:\nlm(formula = as.numeric(hi) ~ as.numeric(tv), data = student.survey)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.2583 -0.2456  0.0417  0.3368  0.7051 \n\nCoefficients:\n                Estimate Std. Error t value Pr(>|t|)    \n(Intercept)     3.441353   0.085345  40.323   <2e-16 ***\nas.numeric(tv) -0.018305   0.008658  -2.114   0.0388 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4467 on 58 degrees of freedom\nMultiple R-squared:  0.07156,   Adjusted R-squared:  0.05555 \nF-statistic: 4.471 on 1 and 58 DF,  p-value: 0.03879\n\n\nThere is a negative relationship between weekly TV viewing hours and high school GPA, with a slope of -0.018, indicating that as weekly TV viewing hours increase, a student’s GPA tends to fall. At a significance threshold of 0.05, there is a statistically significant association between weekly television viewing and GPA. The R-squared value, however, is close to zero, indicating that the regression model does not make a strong forecast for the observed variables. This is not surprising given that there does not appear to be a linear trend in the data when looking at the scatter plot with hours of television viewed and GPA."
  },
  {
    "objectID": "posts/HW3_MiguelCuriel.html",
    "href": "posts/HW3_MiguelCuriel.html",
    "title": "Homework 3",
    "section": "",
    "text": "Code\n# load necessary packages\nlibrary(tidyverse)\nlibrary(alr4)\nlibrary(smss)\n\n\n\nQuestion 1\nUnited Nations (Data file: UN11in alr4) The data in the file UN11 contains several variables, including ppgdp, the gross national product per person in U.S. dollars, and fertility, the birth rate per 1000 females, both from the year 2009. The data are for 199 localities, mostly UN member countries, but also other areas such as Hong Kong that are not independent countries. The data were collected from the United Nations (2011). We will study the dependence of fertility on ppgdp.\n\nIdentify the predictor and the response.\n\nPredictor = ppgdp\nResponse = fertility\n\nDraw the scatterplot of fertility on the vertical axis versus ppgdp on the horizontal axis and summarize the information in this graph. Does a straight-line mean function seem to be plausible for a summary of this graph?\n\nNo, a straight-line mean function does not seem plausible because the distribution seems to be somewhat curvilinear.\n\nDraw the scatterplot of log(fertility) versus log(ppgdp) using natural logarithms. Does the simple linear regression model seem plausible for a summary of this graph? If you use a different base of logarithms, the shape of the graph won’t change, but the values on the axes will change.\n\nYes, a simple linear regression model with a logarithmic transformation seems to be plausible as it is better at capturing the curvilinear relationship between variables.\n\n\n\n\nCode\nggplot(data = UN11, aes(x = ppgdp, y = fertility)) +\n  geom_point() +\n  geom_smooth(method = 'lm', se=F) +\n  labs(title='1.b. Scatterplot of fertility versus ppgdp')\n\n\n\n\n\n\n\nCode\nggplot(data = UN11, aes(x = log(ppgdp), y = log(fertility))) +\n  geom_point() +\n  geom_smooth(method = 'lm', se=F) +\n  labs(title='1.c. Scatterplot of log(fertility) versus log(ppgdp)')\n\n\n\n\n\n\n\nQuestion 2\nAnnual income, in dollars, is an explanatory variable in a regression analysis. For a British version of the report on the analysis, all responses are converted to British pounds sterling (1 pound equals about 1.33 dollars, as of 2016).\n\nHow, if at all, does the slope of the prediction equation change?\n\nUsing the standard slope-intercept equation - y = mx + b - the slope (m) will not change as this this relationship is independent of the unit of measurement. What will change is the intercept (b), meaning that the starting point will be 1.33 instead of 1.\n\nHow, if at all, does the correlation change?\n\nSimilarly to the slope, the correlation does not change as this is affected by the relationship between two variables (y = mx) rather than the starting point or, in this case, the unit of measurement (b).\n\n\n\n\nQuestion 3\nWater runoff in the Sierras (Data file: water in alr4) Can Southern California’s water supply in future years be predicted from past data? One factor affecting water availability is stream runoff. If runoff could be predicted, engineers, planners, and policy makers could do their jobs more efficiently. The data file contains 43 years’ worth of precipitation measurements taken at six sites in the Sierra Nevada mountains (labeled APMAM, APSAB, APSLAKE, OPBPC, OPRC, and OPSLAKE) and stream runoff volume at a site near Bishop, California, labeled BSAAM. Draw the scatterplot matrix for these data and summarize the information available from these plots.\n\n\nCode\npairs(water[,-1])\n\n\n\n\n\nAs we can see from the scatterplot matrix, the sites that seem to correlate the most to BSAAM are OPSLAKE, OPRC, and OPBPC, while APSLAKE, APSAB, and APMAM do not seem to be correlated at all. Therefore, if a model were to be created to predict runoff, a good idea would be to start by including the precipitation of sites that begin with an “O” and exclude those that begin “A”.\n\n\nQuestion 4\nProfessor ratings (Data file: Rateprof in alr4) In the website and online forum RateMyProfessors.com, students rate and comment on their instructors. Launched in 1999, the site includes millions of ratings on thousands of instructors. The data file includes the summaries of the ratings of 364 instructors at a large campus in the Midwest (Bleske-Rechek and Fritsch, 2011). Each instructor included in the data had at least 10 ratings over a several year period. Students provided ratings of 1–5 on quality, helpfulness, clarity, easiness of instructor’s courses, and raterInterest in the subject matter covered in the instructor’s courses. The data file provides the averages of these five ratings. Create a scatterplot matrix of these five variables. Provide a brief description of the relationships between the five ratings.\n\n\nCode\npairs(~ quality + helpfulness + clarity + easiness + raterInterest, data=Rateprof)\n\n\n\n\n\nJust by looking at the scatterplot matrix, there are a couple of variables that are seemingly correlated. In particular, quality seems to have a nearly perfect, positive correlation with helpfulness and clarity. Similarly, helpfulness seems to have a moderate-to-high positive correlation. In contrast, easiness of the instructor’s courses and raterInterest do not seem to be highly correlated with any of the variables.\n\n\nQuestion 5\nFor the student.survey data file in the smss package, conduct regression analyses relating (i) y = political ideology and x = religiosity, and (ii) y = high school GPA and x = hours of TV watching.\n\nGraphically portray how the explanatory variable relates to the outcome variable in each of the two cases.\n\nPlots can be found below.\n\nSummarize and interpret results of inferential analyses.\n\nFrom the numerical analysis, both instances are statistically significant, however scenario i (political ideology against religiosity) is much more significant than scenario ii (high school GPA against hours spent watching TV). Further, from the graphics we can see that only scenario i (political ideology against religiosity) is clearly linear, whereas scenario ii (high school GPA against hours spent watching TV) could benefit from preprocessing techniques such as a logarithmic transformation or treating outliers/missing data.\n\n\n\n\nCode\ndata(\"student.survey\", package = \"smss\")\ndf <- student.survey\n\ndf$pi_numeric <- factor(df$pi, levels = c(\n  \"very conservative\"\n  ,\"conservative\"\n  ,\"slightly conservative\"\n  ,\"moderate\"\n  ,\"slightly liberal\"\n  ,\"liberal\"\n  ,\"very liberal\"\n), labels = c(1,2,3,4,5,6,7)\n)\n\ndf$pi_numeric <- as.numeric(as.character(df$pi_numeric))\n\ndf$re_numeric <- factor(df$re, levels = c(\n  \"never\"\n  ,\"occasionally\"\n  ,\"most weeks\"\n  ,\"every week\"\n), labels = c(1,2,3,4)\n)\n\ndf$re_numeric <- as.numeric(as.character(df$re_numeric))\n\nsummary(lm(pi_numeric ~ re_numeric, data=df))\n\n\n\nCall:\nlm(formula = pi_numeric ~ re_numeric, data = df)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-3.09882 -1.12840 -0.09882  0.87160  2.81243 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   7.0692     0.4252  16.624  < 2e-16 ***\nre_numeric   -0.9704     0.1792  -5.416 1.22e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.345 on 58 degrees of freedom\nMultiple R-squared:  0.3359,    Adjusted R-squared:  0.3244 \nF-statistic: 29.34 on 1 and 58 DF,  p-value: 1.221e-06\n\n\n\n\nCode\nggplot(data = df, aes(x = re_numeric, y = pi_numeric)) +\n  geom_point() +\n  geom_smooth(method = 'lm', se=F) +\n  labs(title='5.a.i. Scatterplot of political ideology versus religiosity'\n       ,x=\"religiosity\"\n       ,y='political ideology') +\n  scale_y_continuous(labels = c(\n  \"very conservative\"\n  ,\"conservative\"\n  ,\"slightly conservative\"\n  ,\"moderate\"\n  ,\"slightly liberal\"\n  ,\"liberal\"\n  ,\"very liberal\"\n    ), breaks = c(1,2,3,4,5,6,7)) +\n  scale_x_continuous(labels = c(\n  \"never\"\n  ,\"occasionally\"\n  ,\"most weeks\"\n  ,\"every week\"\n    ), breaks = c(1,2,3,4))\n\n\n\n\n\n\n\nCode\nggplot(data = df, aes(x = tv, y = hi)) +\n  geom_point() +\n  geom_smooth(method = 'lm', se=F) +\n  labs(title='5.a.ii. Scatterplot of high school GPA against hours spent watching TV'\n       ,x='hours spent watching TV'\n       ,y='high school GPA')"
  },
  {
    "objectID": "posts/HW2_Guanhua_Tan.html",
    "href": "posts/HW2_Guanhua_Tan.html",
    "title": "Homework 2",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.0      ✔ purrr   0.3.5 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.4.1 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\n\nQuestion 1\n\n\nCode\n# Bypass \ns_mean<-19\ns_size <-539\nstandard_error <- 10/539\nstandard_error\n\n\n[1] 0.01855288\n\n\nCode\n# t-value\nconfidence_level <- 0.9\ntail_area <- (1-confidence_level)/2\nt_score <- qt(p = 1-tail_area, df = s_size-1)\nt_score\n\n\n[1] 1.647691\n\n\nCode\n# plug everything back in\nCI <- c(s_mean - t_score * standard_error,\n        s_mean + t_score * standard_error)\nprint(CI)\n\n\n[1] 18.96943 19.03057\n\n\n18.97 <= CI_bypass <= 19.03\n\n\nCode\n# Angiography\ns_mean_a<-18\ns_size_a<-847\nstandard_error_a <- 9/847\nstandard_error_a\n\n\n[1] 0.01062574\n\n\nCode\n# t-value\nconfidence_level <- 0.9\ntail_area <- (1-confidence_level)/2\nt_score<- qt(p = 1-tail_area, df = s_size-1)\nt_score\n\n\n[1] 1.647691\n\n\nCode\n# plug everything back in\nCI_a <- c(s_mean_a- t_score * standard_error_a,\n        s_mean_a + t_score * standard_error_a)\nprint(CI_a)\n\n\n[1] 17.98249 18.01751\n\n\n17.98 <= CI_angiograpy<= 18.02\nThe Confidence Interval is narrower for Angiography surgery because it has a smaller standard_error.\n\n\nQuestion 2\n\n\nCode\np2 <- 567/1031\np2\n\n\n[1] 0.5499515\n\n\nCode\nSE2 <-sqrt(p2*(1-p2)/1031)\ntail_area2 <-(1-0.95)/2\nt_score2 <-qt(p-tail_area2, df=1030)\n\n\nError in qt(p - tail_area2, df = 1030): object 'p' not found\n\n\nCode\nCI2_A<-p2-t_score2*SE2\n\n\nError in eval(expr, envir, enclos): object 't_score2' not found\n\n\nCode\nCI2_B <-p2+t_score2*SE2\n\n\nError in eval(expr, envir, enclos): object 't_score2' not found\n\n\nCode\nCI2_A\n\n\nError in eval(expr, envir, enclos): object 'CI2_A' not found\n\n\nCode\nCI2_B\n\n\nError in eval(expr, envir, enclos): object 'CI2_B' not found\n\n\n0.549 <= P <= 0.551\n\n\nQuestion 3\n\n\nCode\nsd_question3 <- (200-30)/4\nMargin3 <-5\nn <- (1.96*sd_question3/Margin3)^2\nn\n\n\n[1] 277.5556\n\n\nthe size of students is 277\n#Question 4\nNull hypothesis: The mean income of female employees is equal to $500 per week. H0: μ = $500 Alternative hypothesis: The mean income of female employees is different from $500 per week. Ha: μ ≠ $500 t.test suggests the mean income of female employees is different from $500 per week. We reject the Null hypothesis.\n\n\nCode\nfemale_group_mean <-410\nsd_4<-90\nn_4<-9\nt_stat4<-(female_group_mean-500)/(sd_4/sqrt(n_4))\nP_value_4 <-(1-pt(t_stat4, df = n_4-1, lower.tail = F))*2\n\nt_stat4\n\n\n[1] -3\n\n\nCode\nP_value_4\n\n\n[1] 0.01707168\n\n\nt-statistic is -3. p-value is 0.017.\nB. Report the P-value for Ha: μ < 500. Interpret. C. Report and interpret the P-value for Ha: μ > 500.\n\n\nCode\nP_value_lower4<-pt(t_stat4, df=n_4-1, lower.tail=TRUE)\nP_value_high4<-pt(t_stat4, df=n_4-1, lower.tail = F)\n\nP_value_lower4\n\n\n[1] 0.008535841\n\n\nCode\nP_value_high4\n\n\n[1] 0.9914642\n\n\nFor Ha: mu<500, we run the pt function and p-value is 0.008, which suggests that we reject the Null hypothesis and the mean income of female employees is much less than 500.\nFor Ha: mu >500, we run the pt function and p-value is 0.99, which suggests we fail to reject the Null hypothesis and we are unable to demonstrate the income mean of female employees is greater thant 500.\n\n\nQuestion 5\n\n\nCode\n# Question 5\nt_score_5_Jones <-(519.5-500)/10\np_value_5_Jones<- 2*(1-pt(t_score_5_Jones, df=999))\nt_score_5_Jones\n\n\n[1] 1.95\n\n\nCode\np_value_5_Jones\n\n\n[1] 0.05145555\n\n\nCode\nt_score_5_Smith <-(519.7-500)/10\np_value_5_Smith <- 2*(1-pt(t_score_5_Smith, df=999))\nt_score_5_Smith\n\n\n[1] 1.97\n\n\nCode\np_value_5_Smith\n\n\n[1] 0.04911426\n\n\nB If α=0.5, Smith is statically significant because his p-value is smaller than α. C If we don’t get the actual p-value, we can only conclude that Smith is statically significant without that there is a very tiny difference between two groups. Also, we will ignore that Smith’s p-value is barely smaller than α, which suggests that it is not extremely significant.\n\n\nQuestion 6\n\n\nCode\ndf_6<-data.frame(\"Grade Level\"=c(\"Heathy sanck\", \"Unhealth snack\"), \"6th grade\"=c(31,69), \"7th grade\"=c(43,57), \"8th grade\"=c(51,49))\n\nchisq.test(df_6[,-1], correct=F)\n\n\n\n    Pearson's Chi-squared test\n\ndata:  df_6[, -1]\nX-squared = 8.3383, df = 2, p-value = 0.01547\n\n\nNull hypothesis: means of 3 grades to choose two types of snack are equal. We should use chisq test to test the correlation between grades and the counts of healthy and unhealthy snacks. Chisq suggests that we should reject the null hypothesis because p-value is 0.01547, which is smaller than 0.5. In other words, different grades show differen choices of snacks.\n\n\nQuestion 7\n\n\nCode\n# Question 7\ndf_7<- data.frame(\"Area1\"=c(6.2,9.3,6.8,6.1,6.7,7.5),\n                  \"Area2\"=c(7.5,8.2,8.5,8.2,7.0,9.3),\n                  \"Area3\"=c(5.8,6.5,5.6,7.1,3.0,3.5))\ndf_7_long <- df_7 %>%\n  pivot_longer(cols=c(Area1, Area2, Area3),names_to=\"Area\", values_to = \"Fee\")\n\nmy.anova_7<-aov(Fee ~ Area, df_7_long)\nsummary(my.anova_7)\n\n\n            Df Sum Sq Mean Sq F value  Pr(>F)   \nArea         2  25.35  12.674   7.993 0.00433 **\nResiduals   15  23.78   1.586                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nNull hypothesis: mean of three areas are equal. We should use anova test. Anova test suggests that we should reject the null hypothesis because p-value is 0.0043, which is much smaller than 0.5. In other words, tutions are highly related to areas."
  },
  {
    "objectID": "posts/HW3_solutions_Pang.html",
    "href": "posts/HW3_solutions_Pang.html",
    "title": "Homework 3",
    "section": "",
    "text": "Please check your answers against the solutions."
  },
  {
    "objectID": "posts/HW3_solutions_Pang.html#question-1",
    "href": "posts/HW3_solutions_Pang.html#question-1",
    "title": "Homework 3",
    "section": "Question 1",
    "text": "Question 1\nLoad the necessary packages.\n\nlibrary(alr4)\n\nError in library(alr4): there is no package called 'alr4'\n\nlibrary(smss)\n\nError in library(smss): there is no package called 'smss'\n\nlibrary(ggplot2)\nlibrary(stargazer)\n\nError in library(stargazer): there is no package called 'stargazer'\n\n\nLoad data:\n\ndata(UN11)\n\nWarning in data(UN11): data set 'UN11' not found\n\n\n\n(a)\nThe predictor is ppgdp, i.e. GDP per capita. The response is fertility, the birth rate per 1000 women.\n\n\n(b)\n\nggplot(data = UN11, aes(x = ppgdp, y = fertility)) +\n  geom_point()\n\nError in ggplot(data = UN11, aes(x = ppgdp, y = fertility)): object 'UN11' not found\n\n\nA straight line is not appropriate, because the relationship has an L-shaped structure (or the left half of a U-shape).\n\n\n(c)\n\nggplot(data = UN11, aes(x = log(ppgdp), y = log(fertility))) +\n  geom_point()\n\nError in ggplot(data = UN11, aes(x = log(ppgdp), y = log(fertility))): object 'UN11' not found\n\n\nYes, now a simple linear regression model is more plausible. We can imagine a negative-sloped straight line going through those points."
  },
  {
    "objectID": "posts/HW3_solutions_Pang.html#question-2",
    "href": "posts/HW3_solutions_Pang.html#question-2",
    "title": "Homework 3",
    "section": "Question 2",
    "text": "Question 2\n\n(a)\nThe conversion from USD to British pound will mean the numerical value of the response will be divided by 1.33. To offset that, the slope will also become divided by 1.33.\n\n\n(b)\nCorrelation will not change because it is a standardized measure that is not influenced by the unit of measurement.\nBoth outcomes can easily be shown via simulation."
  },
  {
    "objectID": "posts/HW3_solutions_Pang.html#question-3",
    "href": "posts/HW3_solutions_Pang.html#question-3",
    "title": "Homework 3",
    "section": "Question 3",
    "text": "Question 3\n\ndata(water)\n\nWarning in data(water): data set 'water' not found\n\npairs(water)\n\nError in pairs(water): object 'water' not found\n\n\n\nYear appears to be largely unrelated to each of the other variables\nThe three variables starting with “O” seem to be correlated with each other, meaning that all the plot including two of these variables exhibit a dependence between the variables that is stronger than the dependence between the “O” variables and other variables. The three variables starting with “A” also seem to be another correlated group\nBSAAM is more closely related to the “O” variables than the “A” variables"
  },
  {
    "objectID": "posts/HW3_solutions_Pang.html#question-4",
    "href": "posts/HW3_solutions_Pang.html#question-4",
    "title": "Homework 3",
    "section": "Question 4",
    "text": "Question 4\n\ndata(Rateprof)\n\nWarning in data(Rateprof): data set 'Rateprof' not found\n\npairs(Rateprof[,c('quality', 'clarity', 'helpfulness',\n                  'easiness', 'raterInterest')])\n\nError in pairs(Rateprof[, c(\"quality\", \"clarity\", \"helpfulness\", \"easiness\", : object 'Rateprof' not found\n\n\nThe very strong pair-wise correlation among quality, clarity, and helpfulness is very striking. easiness is also correlated fairly highly with the other three. raterInterest is also moderately correlated, but raters almost always say they are at least moderately interested in the subject. Overall, the results might show that people don’t necessarily distinguish all these dimensions very well in their minds—or that professors that do one in one dimension tend to do well on the others too."
  },
  {
    "objectID": "posts/HW3_solutions_Pang.html#question-5",
    "href": "posts/HW3_solutions_Pang.html#question-5",
    "title": "Homework 3",
    "section": "Question 5",
    "text": "Question 5\n\n(a)\nOne way of visually representing the relationship between religiosity and political ideology is as follows (and there are other ways). As we go towards bars to the right (more religiousity), we see lighter colors pop up (more conservatism)\n\ndata(student.survey)\n\nWarning in data(student.survey): data set 'student.survey' not found\n\nggplot(data = student.survey, aes(x = re, fill = pi)) +\n    geom_bar(position = \"fill\")\n\nError in ggplot(data = student.survey, aes(x = re, fill = pi)): object 'student.survey' not found\n\n\nThe relationship between high school GPA and hours of watching TV can be shown with a good old scatter plot.\n\nggplot(data = student.survey, aes(x = tv, y = hi)) +\n  geom_point() \n\nError in ggplot(data = student.survey, aes(x = tv, y = hi)): object 'student.survey' not found\n\n\n\n\n(b)\nDealing with ordinal variables in linear regression is a difficult problem. We’ll just go ahead and assume that we can just convert them to numeric and use them. This would be done for political ideology and religiosity. High school GPA and hours of TV are already continuous.\n\nm1 <- lm(as.numeric(pi) ~ as.numeric(re), \n         data = student.survey)\n\nError in is.data.frame(data): object 'student.survey' not found\n\nm2 <- lm(hi ~ tv, data = student.survey)\n\nError in is.data.frame(data): object 'student.survey' not found\n\nstargazer(m1, m2, type = 'text', \n          dep.var.labels = c('Pol. Ideology', 'HS GPA'),\n          covariate.labels = c('Religiosity', 'Hours of TV')\n          )\n\nError in stargazer(m1, m2, type = \"text\", dep.var.labels = c(\"Pol. Ideology\", : could not find function \"stargazer\"\n\n\nReligiosity is positively and statistically significantly (at the 0.01 significance level) associated with conservatism.\nHours of TV is negatively and statistically significantly (at the 0.05 significance level) associated with High School GPA. Watching an average of 1 more hour of TV per week is associated with a 0.018 decline in High School GPA."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "DACSS 603 Introduction to Quantitative Analysis Spring 2023",
    "section": "",
    "text": "The blog posts here are contributed by students enrolled in DACSS 603, Introduction to Quantitative Analysis.\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nApr 12, 2023\n\n\n603Homework_Saisrinivas_Ambatipudi\n\n\nSaisrinivas Ambatipudi\n\n\n\n\nApr 11, 2023\n\n\nHomework 3\n\n\nFelix Betanourt\n\n\n\n\nApr 11, 2023\n\n\nHomework 3\n\n\nAlexa Potter\n\n\n\n\nApr 11, 2023\n\n\nHomework 3\n\n\nYoung Soo Choi\n\n\n\n\nApr 11, 2023\n\n\nHomework 3\n\n\nDiana Rinker\n\n\n\n\nApr 11, 2023\n\n\nHomework 3\n\n\nAbigail Balint\n\n\n\n\nApr 11, 2023\n\n\nHomework 3\n\n\nMiguel Curiel\n\n\n\n\nApr 10, 2023\n\n\nHomework 3\n\n\nAdithya Parupudi\n\n\n\n\nApr 10, 2023\n\n\nHomework 3\n\n\nJustine Shakespeare\n\n\n\n\nApr 7, 2023\n\n\nDACSS 603 Homework 3\n\n\nLaura Collazo\n\n\n\n\nApr 3, 2023\n\n\nHomework 2\n\n\nAbigail Balint\n\n\n\n\nApr 2, 2023\n\n\nHomework2\n\n\nRahul Somu\n\n\n\n\nMar 29, 2023\n\n\nHomework 2\n\n\nRosemary Pang\n\n\n\n\nMar 28, 2023\n\n\nHomework 2\n\n\nAsch Harwood\n\n\n\n\nMar 28, 2023\n\n\nHomework 2\n\n\nEmma Narkewicz\n\n\n\n\nMar 28, 2023\n\n\nHomework 2\n\n\nSaisrinivas Ambatipudi\n\n\n\n\nMar 28, 2023\n\n\nHomework 2\n\n\nFelix Betanourt\n\n\n\n\nMar 28, 2023\n\n\nHomework 2\n\n\nCaitlin Rowley\n\n\n\n\nMar 28, 2023\n\n\nHomework 2\n\n\nAlexa Potter\n\n\n\n\nMar 28, 2023\n\n\nHomework 2\n\n\nMiguel Curiel\n\n\n\n\nMar 27, 2023\n\n\nFinal Project check-in (1)\n\n\nDiana Rinker\n\n\n\n\nMar 27, 2023\n\n\nHomework 2\n\n\nJustine Shakespeare\n\n\n\n\nMar 27, 2023\n\n\nHomework 2\n\n\nYoung Soo Choi\n\n\n\n\nMar 26, 2023\n\n\nBlog Post #2\n\n\nAlexis Gamez\n\n\n\n\nMar 25, 2023\n\n\nHomework 2\n\n\nDiana Rinker\n\n\n\n\nMar 24, 2023\n\n\nHomework 2\n\n\nXiaoyan\n\n\n\n\nMar 24, 2023\n\n\nFinal Project Check-in 1\n\n\nRahul Somu\n\n\n\n\nMar 23, 2023\n\n\nDACSS 603 Homework 2\n\n\nLaura Collazo\n\n\n\n\nMar 23, 2023\n\n\nHomework 2\n\n\nGuanhua Tan\n\n\n\n\nMar 22, 2023\n\n\nFinal Project Check In\n\n\nYoung Soo Choi\n\n\n\n\nMar 22, 2023\n\n\nFinal Project check in 1\n\n\nThrishul Pola\n\n\n\n\nMar 22, 2023\n\n\n603_Project_Check_In_Saisrinivas_Ambatipudi\n\n\nAkhilesh Kumar & Saisrinivas Ambatipudi\n\n\n\n\nMar 21, 2023\n\n\nFinal Project Part 1\n\n\nAlexa Potter\n\n\n\n\nMar 21, 2023\n\n\nFinal Project - Check-In 1\n\n\nCaitlin Rowley\n\n\n\n\nMar 21, 2023\n\n\n603_Project_Check_In_Akhilesh\n\n\nAkhilesh Kumar & Saisrinivas Ambatipudi\n\n\n\n\nMar 21, 2023\n\n\nFinal Project Proposal\n\n\nEmma Narkewicz\n\n\n\n\nMar 21, 2023\n\n\nFinal Project Check in 1\n\n\nKristin Abijaoude\n\n\n\n\nMar 21, 2023\n\n\nFinalPart1\n\n\nLiam Tucksmith\n\n\n\n\nMar 21, 2023\n\n\nFinal Project Part 1\n\n\nTyler Tewksbury\n\n\n\n\nMar 21, 2023\n\n\nThe Pandemic’s Toll on Mental Health: A Look at the Tech Sector (DRAFT)\n\n\nMiguel Curiel\n\n\n\n\nMar 21, 2023\n\n\nMaternal Mortality and Women's Empowerment: Exploring Socio-Political Factors Associated with Better Outcomes for Mothers Across the Globe\n\n\nJustine Shakespeare\n\n\n\n\nMar 20, 2023\n\n\nFinal Project Initial Research\n\n\nAbigail Balint\n\n\n\n\nMar 20, 2023\n\n\nEstimating National Food Waste with State/Local Waste Characterization Studies in the United States\n\n\nAsch Harwood\n\n\n\n\nMar 20, 2023\n\n\nHomework 1\n\n\nJerin Jacob\n\n\n\n\nMar 20, 2023\n\n\nFinal Project - Post 1\n\n\nAdithya Parupudi\n\n\n\n\nMar 18, 2023\n\n\nProject Proposal\n\n\nAlexis Gamez\n\n\n\n\nMar 18, 2023\n\n\nFinal Project - Check point 1\n\n\nFelix Betanourt\n\n\n\n\nMar 17, 2023\n\n\nFinal Project Checkin-1\n\n\nXiaoyan\n\n\n\n\nMar 16, 2023\n\n\nHw 2 by Kristin Abijaoude\n\n\nKristin Abijaoude\n\n\n\n\nMar 12, 2023\n\n\nHW2\n\n\nLiam Tucksmith\n\n\n\n\nMar 10, 2023\n\n\nFinal Project Proposal Check-in1\n\n\nMeredith Derian-Toth\n\n\n\n\nMar 10, 2023\n\n\nFinal Project 603\n\n\nJerin Jacob\n\n\n\n\nMar 4, 2023\n\n\nFinal Project Check 1\n\n\nGuanhua Tan\n\n\n\n\nMar 1, 2023\n\n\nHomework 1 Solution\n\n\nRosemary Pang\n\n\n\n\nFeb 28, 2023\n\n\nHomework 1\n\n\nOllie Murphy\n\n\n\n\nFeb 28, 2023\n\n\nHomework 1 - Akhilesh Kumar\n\n\nAkhilesh Kumar Meghwal\n\n\n\n\nFeb 28, 2023\n\n\nHomework 1\n\n\nEmma Narkewicz\n\n\n\n\nFeb 28, 2023\n\n\nHomework 1\n\n\nZhiyuan Zhou\n\n\n\n\nFeb 28, 2023\n\n\nHomework 1\n\n\nChristine Brydges\n\n\n\n\nFeb 28, 2023\n\n\nHomework 1\n\n\nAsch Harwood\n\n\n\n\nFeb 28, 2023\n\n\nHomework - 1\n\n\nDarron Bunt\n\n\n\n\nFeb 27, 2023\n\n\nHomework 1\n\n\nSaisrinivas Ambatipudi\n\n\n\n\nFeb 27, 2023\n\n\nHomework 2\n\n\nAkhilesh Kumar Meghwal\n\n\n\n\nFeb 27, 2023\n\n\nHw 1 by Kristin Abijaoude\n\n\nKristin Abijaoude\n\n\n\n\nFeb 27, 2023\n\n\nHomework 1\n\n\nYoung Soo Choi\n\n\n\n\nFeb 27, 2023\n\n\nHomework1\n\n\nRahul Somu\n\n\n\n\nFeb 27, 2023\n\n\nHomework 2\n\n\nAdithya Parupudi\n\n\n\n\nFeb 27, 2023\n\n\nHomework 1\n\n\nXiaoyan\n\n\n\n\nFeb 26, 2023\n\n\nHomework 1\n\n\nCaitlin Rowley\n\n\n\n\nFeb 26, 2023\n\n\nHomework 1\n\n\nJustine Shakespeare\n\n\n\n\nFeb 25, 2023\n\n\nHomework 1\n\n\nAlexa Potter\n\n\n\n\nFeb 24, 2023\n\n\nHomework 1\n\n\nFelix Betanourt\n\n\n\n\nFeb 23, 2023\n\n\nHomework1 - EDA of LungCap Data\n\n\nAdithya Parpudi\n\n\n\n\nFeb 23, 2023\n\n\nHomework 1\n\n\nAbigail Balint\n\n\n\n\nFeb 20, 2023\n\n\nBlog Post #1\n\n\nAlexis Gamez\n\n\n\n\nFeb 20, 2023\n\n\nHW1\n\n\nLiam Tucksmith\n\n\n\n\nFeb 20, 2023\n\n\nHomework 1\n\n\nMiguel Curiel\n\n\n\n\nFeb 19, 2023\n\n\nDACSS 603 Homework 1\n\n\nLaura Collazo\n\n\n\n\nFeb 17, 2023\n\n\nHomework 1\n\n\nDiana Rinker\n\n\n\n\nFeb 5, 2023\n\n\nHomework3\n\n\nRahul Somu\n\n\n\n\nFeb 5, 2023\n\n\nHomework 1\n\n\nGuanhua Tan\n\n\n\n\nFeb 5, 2023\n\n\nHomework - 2\n\n\nThrishul\n\n\n\n\nFeb 5, 2023\n\n\nHomework_One\n\n\nMeredith Derian-Toth\n\n\n\n\nFeb 5, 2023\n\n\nHomework - 1\n\n\nThrishul\n\n\n\n\nInvalid Date\n\n\nProposal for DACSS 603 Final Project\n\n\nLaura Collazo\n\n\n\n\nApr 12, 2022\n\n\nHomework 3\n\n\nRosemary Pang\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about/Rowena.Kosher.html",
    "href": "about/Rowena.Kosher.html",
    "title": "Rowena",
    "section": "",
    "text": "##Instructions"
  },
  {
    "objectID": "about/Rowena.Kosher.html#educationwork-background",
    "href": "about/Rowena.Kosher.html#educationwork-background",
    "title": "Rowena",
    "section": "Education/Work Background",
    "text": "Education/Work Background\n-BA Columbia University in Human Rights & Gender & Sexuality Studies -Background in nonprofit, academic, research and activism -current main jobs: Consultant and data analyst at Unsiloed, DEI Consulting Firm- focuses on psychological safety, inclusive leadership, neurodiversity, antiracism, dialogue for transformational change -Also freelance writer, teaching assistant"
  },
  {
    "objectID": "about/Rowena.Kosher.html#r-experience",
    "href": "about/Rowena.Kosher.html#r-experience",
    "title": "Rowena",
    "section": "R experience",
    "text": "R experience\n-minimal to none, though eager to learn"
  },
  {
    "objectID": "about/Rowena.Kosher.html#research-interests",
    "href": "about/Rowena.Kosher.html#research-interests",
    "title": "Rowena",
    "section": "Research interests",
    "text": "Research interests\n-Sociology PhD in the pipeline (hopefully), with intent to focus on gender and secualtiy studies and queer theory -interests include: trans and nonbinary menstruation (for eg. conducted cyberethnography of language use in YouTube videos), queer identity and familial structures, gender identity development and theory, intersectional queer identity, neurodiversity"
  },
  {
    "objectID": "about/Rowena.Kosher.html#hometown",
    "href": "about/Rowena.Kosher.html#hometown",
    "title": "Rowena",
    "section": "Hometown",
    "text": "Hometown\nGrew up in Connecticut, have lived in New York City since 2017 (currently in Brooklyn)"
  },
  {
    "objectID": "about/Rowena.Kosher.html#hobbies",
    "href": "about/Rowena.Kosher.html#hobbies",
    "title": "Rowena",
    "section": "Hobbies",
    "text": "Hobbies\n-Yoga and movement -spending time with my two italian greyhounds and one cat- an entire zoo in one tiny apartment -spending my time with one foot in the art world thanks to my talented friends and a fun foray into film production"
  },
  {
    "objectID": "about/Rowena.Kosher.html#fun-fact",
    "href": "about/Rowena.Kosher.html#fun-fact",
    "title": "Rowena",
    "section": "Fun fact",
    "text": "Fun fact\nI barista and bartend- my go-to coffee is a Black eye (Black drip w/ 2 shots of espresso) and my favorite cocktail is the negroni"
  },
  {
    "objectID": "about/MiguelCuriel.html",
    "href": "about/MiguelCuriel.html",
    "title": "Miguel Curiel",
    "section": "",
    "text": "Bachelor in Psychology and currently pursuing an MS in Data Analytics and Computational Social Science.\nCurrently working as a Computational Social Scientist at a supply chain technology company. Previously engaged several applied research settings, such as a market research agency, a neuroscience institute, and a non-profit."
  },
  {
    "objectID": "about/MiguelCuriel.html#r-experience",
    "href": "about/MiguelCuriel.html#r-experience",
    "title": "Miguel Curiel",
    "section": "R experience",
    "text": "R experience\n\nStarted getting deep into R around May 2022; before that, I had some months of experience in SQL and Python."
  },
  {
    "objectID": "about/MiguelCuriel.html#research-interests",
    "href": "about/MiguelCuriel.html#research-interests",
    "title": "Miguel Curiel",
    "section": "Research interests",
    "text": "Research interests\n\nInterested in modeling and understanding complex social systems, such as social media, economic ecosystems, and supply chains. Also interested in neuropsychology, neuroscience, and mental health in general."
  },
  {
    "objectID": "about/MiguelCuriel.html#hometown",
    "href": "about/MiguelCuriel.html#hometown",
    "title": "Miguel Curiel",
    "section": "Hometown",
    "text": "Hometown\n\nOriginally from Colima, Mexico; currently living in Washington, DC."
  },
  {
    "objectID": "about/MiguelCuriel.html#hobbies",
    "href": "about/MiguelCuriel.html#hobbies",
    "title": "Miguel Curiel",
    "section": "Hobbies",
    "text": "Hobbies\n\nPlaying guitar and video games, hanging out with family and friends, traveling, and discovering new bars and restaurants."
  },
  {
    "objectID": "about/MiguelCuriel.html#fun-fact",
    "href": "about/MiguelCuriel.html#fun-fact",
    "title": "Miguel Curiel",
    "section": "Fun fact",
    "text": "Fun fact\n\nIn a past life, I attempted to be a YouTube blogger - had one channel creating comedy sketches with friends; and another where I posted guitar covers."
  },
  {
    "objectID": "about/PranavKomaravolu.html",
    "href": "about/PranavKomaravolu.html",
    "title": "Pranav Bharadwaj Komaravolu",
    "section": "",
    "text": "A Computer Science Graduate student who is ever ready to take on challenges and eager to leverage my knowledge and passion for computers to create an impact on the organization or project I will be part of."
  },
  {
    "objectID": "about/PranavKomaravolu.html#educationwork-background",
    "href": "about/PranavKomaravolu.html#educationwork-background",
    "title": "Pranav Bharadwaj Komaravolu",
    "section": "Education/Work Background",
    "text": "Education/Work Background\n\nMaster of Science in Computer Science  from University of Massachusetts Amherst (Aug 2022 - )\nIntegrated Master of Technology  in Computer Science from University of Hyderabad, India  (Jul 2017 - Jun 2022)"
  },
  {
    "objectID": "about/PranavKomaravolu.html#r-experience",
    "href": "about/PranavKomaravolu.html#r-experience",
    "title": "Pranav Bharadwaj Komaravolu",
    "section": "R experience",
    "text": "R experience\nLearnt some R as a part of Stat-501 course. Slight acquaintance with mosaic library. But willing to master it through this course."
  },
  {
    "objectID": "about/PranavKomaravolu.html#research-interests",
    "href": "about/PranavKomaravolu.html#research-interests",
    "title": "Pranav Bharadwaj Komaravolu",
    "section": "Research interests",
    "text": "Research interests\nCurrently interested in the following areas:\n\nBio-NLP, which involves the use of natural language techniques and models to interpret genome sequences and also identify protein structures.\nDistributed learning, which involves the task of distributing the training task of a large machine learning model over various nodes in a compute cluster."
  },
  {
    "objectID": "about/PranavKomaravolu.html#hometown",
    "href": "about/PranavKomaravolu.html#hometown",
    "title": "Pranav Bharadwaj Komaravolu",
    "section": "Hometown",
    "text": "Hometown\nHyderabad, India"
  },
  {
    "objectID": "about/PranavKomaravolu.html#hobbies",
    "href": "about/PranavKomaravolu.html#hobbies",
    "title": "Pranav Bharadwaj Komaravolu",
    "section": "Hobbies",
    "text": "Hobbies\n\nSwimming\nTennis\nReading comics\nI also like taking walk (only when the weather is pleasant)"
  },
  {
    "objectID": "about/PranavKomaravolu.html#fun-fact",
    "href": "about/PranavKomaravolu.html#fun-fact",
    "title": "Pranav Bharadwaj Komaravolu",
    "section": "Fun fact",
    "text": "Fun fact\nWill learn some by the end of this semester :)"
  },
  {
    "objectID": "about/RahulSomu.html",
    "href": "about/RahulSomu.html",
    "title": "Rahul Somu",
    "section": "",
    "text": "##Instructions"
  },
  {
    "objectID": "about/RahulSomu.html#educationwork-background",
    "href": "about/RahulSomu.html#educationwork-background",
    "title": "Rahul Somu",
    "section": "Education/Work Background",
    "text": "Education/Work Background\nBachelors in Electrical engineering at BITS Pilani -India 3years of professional work experience as software engineer"
  },
  {
    "objectID": "about/RahulSomu.html#r-experience",
    "href": "about/RahulSomu.html#r-experience",
    "title": "Rahul Somu",
    "section": "R experience",
    "text": "R experience\nBeginner"
  },
  {
    "objectID": "about/RahulSomu.html#research-interests",
    "href": "about/RahulSomu.html#research-interests",
    "title": "Rahul Somu",
    "section": "Research interests",
    "text": "Research interests\nBig Data Analytics"
  },
  {
    "objectID": "about/RahulSomu.html#hometown",
    "href": "about/RahulSomu.html#hometown",
    "title": "Rahul Somu",
    "section": "Hometown",
    "text": "Hometown\nHyderabad - India"
  },
  {
    "objectID": "about/RahulSomu.html#hobbies",
    "href": "about/RahulSomu.html#hobbies",
    "title": "Rahul Somu",
    "section": "Hobbies",
    "text": "Hobbies\nSoccer and gaming"
  },
  {
    "objectID": "about/RahulSomu.html#fun-fact",
    "href": "about/RahulSomu.html#fun-fact",
    "title": "Rahul Somu",
    "section": "Fun fact",
    "text": "Fun fact"
  },
  {
    "objectID": "about/AdithyaParupudi_about.html",
    "href": "about/AdithyaParupudi_about.html",
    "title": "Adithya Parupudi",
    "section": "",
    "text": "University Of Massachusetts Amherst Amherst, MA M.S in Data Analytics and Computational Social Science | August 2022 - Present\nJawaharlal Nehru Technological University Hyderabad, Telangana, India B. Tech in Computer Science and Engineering | Aug 2022 – Exp. Dec 2023"
  },
  {
    "objectID": "about/AdithyaParupudi_about.html#r-experience",
    "href": "about/AdithyaParupudi_about.html#r-experience",
    "title": "Adithya Parupudi",
    "section": "R Experience",
    "text": "R Experience\nCompleted 601, Text-As-Data in Summer and Fall."
  },
  {
    "objectID": "about/AdithyaParupudi_about.html#research-interests",
    "href": "about/AdithyaParupudi_about.html#research-interests",
    "title": "Adithya Parupudi",
    "section": "Research interests",
    "text": "Research interests\nInterested in healthcare, finance"
  },
  {
    "objectID": "about/AdithyaParupudi_about.html#hometown",
    "href": "about/AdithyaParupudi_about.html#hometown",
    "title": "Adithya Parupudi",
    "section": "Hometown",
    "text": "Hometown\nHyderabad, India"
  },
  {
    "objectID": "about/AdithyaParupudi_about.html#hobbies",
    "href": "about/AdithyaParupudi_about.html#hobbies",
    "title": "Adithya Parupudi",
    "section": "Hobbies",
    "text": "Hobbies\n\ncooking\nworking out\nanime"
  },
  {
    "objectID": "about/AdithyaParupudi_about.html#fun-fact",
    "href": "about/AdithyaParupudi_about.html#fun-fact",
    "title": "Adithya Parupudi",
    "section": "Fun fact",
    "text": "Fun fact\nError 404"
  },
  {
    "objectID": "about/about_abbybalint603.html",
    "href": "about/about_abbybalint603.html",
    "title": "Abby Balint",
    "section": "",
    "text": "I graduated from UMass Amherst SBS in 2017 with a double major in Communications and Sociology. After graduation, I began a career in market research. Throughout the last five years my career has included project management, full cycle survey research, cleaning and weighting data, preparing and writing reports in various formats, and vendor/client management. I currently work at S&P Global Market Intelligence as a Research Analyst with a focus in consumer Digital Endpoints research."
  },
  {
    "objectID": "about/about_abbybalint603.html#r-experience",
    "href": "about/about_abbybalint603.html#r-experience",
    "title": "Abby Balint",
    "section": "R experience",
    "text": "R experience\nI have no experience in R at all outside of taking 601 last semester. In my day to day I typically use Excel, SPSS, and various online data processing platforms."
  },
  {
    "objectID": "about/about_abbybalint603.html#research-interests",
    "href": "about/about_abbybalint603.html#research-interests",
    "title": "Abby Balint",
    "section": "Research interests",
    "text": "Research interests\nGiven my background in Comm/Sociology, I am personally interested in social science research and topics like wealth inequality, or any data about the way humans behave. Professionally, I am interested in researching ways we anticipate the world will look different in the coming years - things like fin-tech changing financial planning accessibility, or the pandemic changing the way we work forever."
  },
  {
    "objectID": "about/about_abbybalint603.html#hometown",
    "href": "about/about_abbybalint603.html#hometown",
    "title": "Abby Balint",
    "section": "Hometown",
    "text": "Hometown\nCurrently living in Providence, RI but from Connecticut"
  },
  {
    "objectID": "about/about_abbybalint603.html#hobbies",
    "href": "about/about_abbybalint603.html#hobbies",
    "title": "Abby Balint",
    "section": "Hobbies",
    "text": "Hobbies\nPainting, writing poetry, going to music festivals, and traveling"
  },
  {
    "objectID": "about/about_abbybalint603.html#fun-fact",
    "href": "about/about_abbybalint603.html#fun-fact",
    "title": "Abby Balint",
    "section": "Fun fact",
    "text": "Fun fact\nI have two orange brother cats that are one year old, and they love to make appearances on Zoom calls :)"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Contributors",
    "section": "",
    "text": "Find out more about our DACSS students who contributed to the blog.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAbby Balint\n\n\n\n\n\n\n\n\n\n\n\n\n\nAdithya Parupudi\n\n\n\n\n\n\n\n\n\n\n\n\n\nMiguel Curiel\n\n\n\n\n\n\n\n\n\n\n\n\n\nPranav Bharadwaj Komaravolu\n\n\n\n\n\n\n\n\n\n\n\n\n\nRahul Somu\n\n\n\n\n\n\n\n\n\n\n\n\n\nRowena\n\n\n\n\n\n\n\nNo matching items"
  }
]