---
title: "Final Project check-in 2"
author: "Diana Rinker"
description: "Final project DACSS 603"
date: "4/17/2023"
format:
  html:
    toc: true
    code-fold: true
    code-copy: true
    code-tools: true
editor: 
  markdown: 
    wrap: 72
---

## DACSS 603, spring 2023

## Final Project check-in 2, Diana Rinker.

# Online engagement

It is well known that online engagement with the web resource is a
highly valuable metric and is driving site revenue. However, engagement
and popularity might also be associated with other factors, that
websties are trying to avoid, such as online violence, inappropriate
behavior and misinformation. This research project is exploring how
these factos impact readers' engagement in social media conversations
and what are main forces of site popularity.

To do that I will use the data from an online blog on the news website.
The author of this blog is posting articles about interpersonal
relationships every work day (Mon- Fri). The posts are formulated as a
letter from a reader with the situation and a question about
relationships. The author gives an advice about the situation. Website
readers are free to comment under each post, but cannot make their own
posts.

All post methadata and comments are public. They are saved by the
website and available for the analysis. Using this data set, I will
explore how readers' engagement connected with blogs's author
engagement, site comments', web source of readers and negative behaviors
online.

My research question is: Does the authors engagement in the conversation
around the post makes readers more engaged and promotes positive
interactions among them?

**DV:** My dependent construct is **"user's engagement**", I will
measure users' engagement at the level of individual post, using the
following metrics:

1.  Page views
2.  Page visits (one visit can contain a few views, if the person visits
    other pages on the site)
3.  Unique users. One person can visit the same page a few times (it is
    calculated for all time since the page was posted).
4.  Number of votes: "thums up" or "thumbs down"
5.  Number of comments
6.  Exit rate or "bounces". When the visitor is coming to the page and
    then leaving, i.e. not opening other pages on this website.

L2 - page readers \* Reveal letter \* Reveal comments

**IV:** My main independent variable is **Blog's author engagement.** I
will measure authors engagement as the factor variable, with the
following levels:

A. Unspecified comment

B. Featured comment

C. Engagement in conversation

To control for **confounders,** I will also measure the follwing
variables:

1.  Topic of the post ("post tag"), categorical variable.

2.  Source of the readers, also categorical variable.

3.  Mood of the conversation , derivative continuous variable calculated
    as the ratio of "likes" to "dislikes".

4.  Blocked and flagged comments.

```{r, echo=F}
#Loading necessary libraries: 
library(readxl)
library(dplyr)
library(ggplot2)
library(tidyverse)
```

# Data

To answer my research question I will use two datasets. the first data
set has information about all comments associated with each post by post
ID. The second data set is analytics data for the web bage. It contains
one post per row and variables describe each post as a whole without
breaking down to the comment level.

In this project I will analyze posts for January 2021 - February 2023.

First, I will load the data set with the comment level data:

```{r, echo=T}
getwd()
raw <- as_tibble (read_csv("C:\\Users\\Diana\\OneDrive - University Of Massachusetts Medical School\\Documents\\R\\R working directory\\DACSS\\603\\my study files for dacss603\\globe\\ data.2021.plus.csv"))

comments.data<-raw 
colnames (comments.data)
head(comments.data$written_at)
comments.data <-comments.data%>%
              mutate(com.year = format(written_at,format = "%Y" ))
range(comments.data$com.year)
dim(comments.data)
```

Second, loadng post-level data :

```{r, echo=T}

merged <- as_tibble (read_csv("C:\\Users\\Diana\\OneDrive - University Of Massachusetts Medical School\\Documents\\R\\R working directory\\DACSS\\603\\my study files for dacss603\\globe\\data.merged.csv"))
# colnames(merged)
# str(merged)
```

\# Variable description

To begin, I will review available variables and evaluate whether it is a
good measure for this study.

# Engagement (DV) metrics:

### Page views

This is the most general engagement metric, representing how many views
the post received. Views do not distinguish repeated views by the same
person.

```{r, echo=T}
# str(merged)
merged$post.month <-as.numeric(merged$post.month)
merged$year_month <- paste0(merged$post.year, "-", sprintf("%02d", merged$post.month))

ggplot( data=merged, mapping=aes(y=`Page views`, x=merged$year_month))+
          geom_boxplot()+
  labs(title="Number of post wiews per month", x="Month", y="Number of vews")+ 
  theme(axis.text.x = element_text(angle = 45, hjust = 1))


```

From this graph we can see that the numbers of views is increased over
time. To get a better understanding of it, lets review other metrics.

### Page visits and unique users

Views - represent the number of times page was viewed by all users at
all times. 
Visit - is an instance of a user engaging with bdc website.
There can be many visits by the same viewer. Once th user is leaving the
website, the visit is over. 
To count unique viewers, we can use "Uniques" variable. It tells us how many unique users saw the post.

The relationship will always be that Unique < Visits < Views, so we will plot them together:

```{r, echo=T}
# str(merged)
ggplot(merged, aes(x =post.date )) +
     geom_bar(aes(y = `Page views`, fill = "Page views"), stat = "identity", position = "dodge", width = 0.6) +
     geom_bar(aes(y = Visits, fill = "Visits"), stat = "identity", position = "dodge", width = 0.6) +
     geom_bar(aes(y = Uniques, fill = "Unique users"), stat = "identity", position = "dodge", width = 0.6)    +
     labs(title = "Views, visits and unique users per month", x = "Post.date", y = "Value") + 
     theme(axis.text.x = element_text(angle = 45, hjust = 1))




```
This graph also shows increase in all three  metrics in 2022-2023. To see how these metrics crrespond to each other,  I will calculate "visits ratio" and "unique.ratio".

### Pagevisit and unique ratios: 
"visits ratio"  isnumber of visits per total vies. this metric telss us how offten the page being re-viewed. The lower the ratio means more people viwed page more than once during their visit. 

"unique ratio" - number of unique users per total views is telling uys how often unique viewers re-visited the page. Lower ratio indicate more views by the same individual. 

Both ratios are always in the range of 0 - 1. 

```{r, echo=T}
  merged <- merged %>%
     mutate(uniques.ratio = Uniques / `Page views`)%>%
      mutate(visits.ratio = Visits / `Page views`)
# str(merged)

     ggplot(merged, aes( y = uniques.ratio, x =post.date, )) +
          geom_point(color ="green" )    +
          labs(title = "Unique.ratio per post ", x = "Post.date", y = "Value") + 
          theme(axis.text.x = element_text(angle = 45, hjust = 1))  

      ggplot(merged, aes( y = visits.ratio, x =post.date, )) +
          geom_point(color ="blue" )    +
          labs(title = "Unique.ratio per post ", x = "Post.date", y = "Value") + 
          theme(axis.text.x = element_text(angle = 45, hjust = 1))
     
```
From the two  graphs below we can see that the two variables are distributed very similarly. 

If we plot them against each other and calculate their correlation, we can see that they highly correlated especially in higher values. 
```{r, echo=T}
      plot(merged$uniques.ratio, merged$visits.ratio)
correlation <- cor(merged$uniques.ratio, merged$visits.ratio)
correlation
```

Density of the distribution also showing that there are more posts that are visited once and by unique users, i.w post is read once by each user. 

```{r, echo=T}
# DENSITY DISTRIBUTION HERE 
     
```

### Exit rate

This variable is measuring how many people visited the page and then left the website after the first view. We can also see consistent increase of this value, similar to the trend seen before: over time, there are more of less engaged users. 


```{r, echo=T}
# str(merged)
merged$`Exit rate` <- as.numeric(sub("%", "", merged$`Exit rate`)) / 100
ggplot(merged, mapping = aes(x=year_month , y=`Exit rate`, fill=year_month ))+
  geom_boxplot() +
  labs(title = "distribution of `Exit rate` per post ", y = "Exit rate" , x="Month")+ 
     theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

### Number of comments

```{r, echo=T}
# merged$year_month 
ggplot(merged, mapping = aes(x=year_month , y=n.comments, fill=year_month ))+
  geom_boxplot() +
  labs(title = "distribution of comments per post ", y = "Number of comments" )+
  scale_y_continuous(breaks = seq (from=0, to= 10000, by= 100)) + 
     theme(axis.text.x = element_text(angle = 45, hjust = 1))
   

```
It is interesting to mention that overal trend of comments is different from Views, visits and uniques trend. While the first tree seemded to increase over time, this variable overall decreased.  It si important to note, that commenting on the platform  requires logging in from a user, which is an indicator of engagement. 
It is also more likely that the user who logged in would return to the post (=add views or visits while not changing "uniques" count). If that assumption holds, an increase of first three variables (vies, visits and uniques) is consistent with decrease of comments. 



##Independent variables (IV):

#### Authors comments

To identify, how much the author of the bblog is engaged in the post, I
will create an additional variable derived from a user_name field.

```{r, echo=T}
# str(merged)
comments.data$user_name<-  ifelse (is.na(comments.data$user_name), 0, comments.data$user_name)
comments.data$author<-  ifelse (comments.data$user_name=="MeredithGoldstein", 1, 0)

comments.grouped <-comments.data %>%
  group_by(post_id)%>%
  summarize(n.comments=n(),
            author.sum = sum(author))

# dim(comments.grouped )
# colnames(comments.grouped )
# class(comments.grouped$author.sum)

# Comments data contains rows that dont actually reporesent posts, and were crearted by web support team for troubleshooting. I need to remove these rows. They typically have very low number of comments
comments.grouped <-comments.grouped %>%
filter(n.comments >100)  # removing invalid posts created by the  website management team.

comments.grouped <-comments.grouped %>%
  select(post_id, author.sum)
 dim(comments.grouped)
 
 
#adding author.sum to main data set: 
merged <- merge( merged , comments.grouped, by = "post_id", all = TRUE)

ggplot(merged, mapping = aes(x=year_month , y=author.sum, fill=year_month ))+
  geom_boxplot() +
  labs(title = "distribution of Author's comments by months", y = "Author's comments" , x="Month")+ 
     theme(axis.text.x = element_text(angle = 45, hjust = 1))

```

### Mood of the post.

This is a numerical variable, calculated as percentage of "thumbs up"
from all likes (both "thumbs up" and "thumbs down").

```{r, echo=T}
ggplot(merged, mapping = aes(x=year_month , y=pct.positive, fill=year_month ))+
  geom_boxplot() +
  labs(title = "distribution of Mood per post ", y = "Mood" , x="Month")+ 
     theme(axis.text.x = element_text(angle = 45, hjust = 1))


```

### Blocked comments per post.

Now I will visualize amount of blocked comments per post:

```{r, echo=T}

ggplot(merged, mapping = aes(x=year_month , y=blocked.sum, fill=year_month ))+
  geom_boxplot() +
  labs(title = "Number Blocked comments  per post ", y = "Number of blocked comments per post" , x="Month")+ 
     theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

### Referral sources:

The web analytics provides information on where the viewers are coming
from to the LL page. There are 5 sources of referrals, each
corresponding with a variable in the data set.

```         
        "Search + amp referral visits"
        "Direct (non-email) referral visits"
        "Other website referral visits"
        "Social referral visits"
        "BDC referral visits"
        "Visits when post was on LL HP" 
```

```{r, echo=T}
merged<-merged%>%
  rename(google ="Search + amp referral visits",
         direct ="Direct (non-email) referral visits",
         other.web = "Other website referral visits",
          social= "Social referral visits",
          bdc= "BDC referral visits",
          ll= "Visits when post was on LL HP" )

ggplot(merged, mapping=aes(x=post.date))+
  geom_point(aes(y=google), color="red")+
  geom_point(aes(y=direct), color="green")+
  geom_point(aes(y=other.web), color="yellow")+
  geom_point(aes(y=social), color="purple")+
  geom_point(aes(y=bdc), color="blue")+
  geom_point(aes(y=ll), color="pink")  +
  labs(title = "referral sources per post ", y = "Number of referrals" , x="Post")+ 
     theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

# Analysis

First, I will use "page views" as dependent variable and regress it
against all other independent variable:

##Page views as DV

```{r, echo=T}
colnames(merged)
summary(lm(`Page views` ~ pct.positive+author.sum +uniques.ratio +visits.ratio +blocked.sum, data = merged))

```

## Number of comments as DV

```{r, echo=T}
colnames(merged)
summary(lm(n.comments ~ pct.positive+author.sum +uniques.ratio +visits.ratio +blocked.sum, data = merged))

```

## Exit rate as DV

```{r, echo=T}
colnames(merged)
summary(lm(`Exit rate`~ pct.positive+author.sum +uniques.ratio +visits.ratio +blocked.sum, data = merged))

```

##Page views as DV

```{r, echo=T}
colnames(merged)
summary(lm(google~ ll, data = merged))
summary(lm(google~ n.comments, data = merged))
summary(lm(google~ Uniques, data = merged))
summary(lm(google~ Uniques + n.comments +ll, data = merged))

```

### Blocked comments and Post mood:

```{r, echo=T}

plot(blocked.sum ~ mood.score, data= grouped )
  
  
summary(grouped)  

fit <- lm(blocked.sum ~ mood.score, data = grouped)

sum(is.na(grouped$mood.score))

summary(fit)

  
```

### 

This variable indicates the date of the comment. Using the range of the
dates per post, I can estimate how long each post was in active
discussion.

```{r, echo=T}


```

### Second wave causes:

Some posts demonstrate increased amount of views, coming from search
engines. These views are coming 2-3 days after the post date (ADD DATA
HERE ).

We are interested to find out, what characteristics of the post (prior
to it being picked up by google), correlate with its appearance on
google news.

Number of comments within 1st 10 hours from the post -\> google wave

```{r, echo=T}
#  calculating number of comments within first 10 hours 

# date and time of the post 
post.date  <-comments.data %>%
  group_by(post_id)%>%
  arrange(written_at)%>%
  summarize(post.dt = first(written_at), 
            n.coms =n())%>%
  filter (n.coms>100)%>%
  select(post_id, post.dt, n.coms)%>%
arrange(desc(post_id))
head(post.date )

# Calculating age of the post 
comments.data.1 <- merge( comments.data , post.date , by = "post_id", all = TRUE)
comments.data.1 <-comments.data.1 %>%
  mutate (age = difftime(  written_at,post.dt, units = "hours"), 
          early = ifelse(age<10, 1, 0))

early.coms  <-comments.data.1 %>%
  group_by(post_id)%>%
  arrange(written_at)%>%
  summarize(early.sum = sum(early), 
            n.coms =n())%>%
  filter (n.coms>100)%>%
  select(post_id, early.sum)%>%
arrange(desc(post_id))

# Adding number of early comments to merged dataset:
merged.1 <- merge( merged, early.coms , by = "post_id", all = TRUE)

```

Now, as I calculated number of early comments for all posts, I can see
if that number correlated with google referrals

```{r, echo=T}

merged.2<-merged.1 %>%
  filter(google>10000)

correlation.2 <- cor(merged.2$early.sum, merged.2$google)
correlation.2
plot(merged.2$early.sum, merged.2$google)

summary(lm(google ~early.sum, data=merged.2))

```

We can see significant connection between early comments and google
referrals above 10000 visits. Howeve, R\^2 is low in this model, whic
suggests that there are other factors that are not considered in this
model. I will explore how days of the week and time of the post
contribute to this relationship.

```{r, echo=T}
# calculating day of the week for the post: 

# Calculating age of the post 
library(lubridate)
comments.data.1 <-comments.data.1 %>%
  mutate (weekday = wday(post.dt, label = TRUE), 
          weekend = ifelse(weekday =="Fri" | weekday == "Sat", 1, 0))
  

# class(week.days$weekday)
# levels(week.days$weekday) 
# 
# week.days$weekend

weekdays <-comments.data.1 %>%
  group_by(post_id)%>%
  arrange(written_at)%>%
  summarize(post.weekday = first(weekday), 
            post.weekend =first(weekend),
            n.coms =n())%>%
  filter (n.coms>100)%>%
  select(post_id, post.weekday,post.weekend )%>%
arrange(desc(post_id))


# Adding number of early comments to merged dataset:
merged.weekdays <- merge( merged.2, weekdays , by = "post_id", all = TRUE)
colnames(merged.weekdays)

levels(merged.weekdays$post.weekday)
```

### Adding IV to the model

Now, as I have necessary variables to run a regression, I will build the
model again with more independent variables:

```{r, echo=T}

merged.2<-merged.1 %>%
  filter(google>10000)

# correlation.2 <- cor(merged.weekdays$early.sum, merged.weekdays$google)
# correlation.2
 plot(merged.weekdays$post.weekday ,   merged.weekdays$google)

summary(lm(google ~early.sum +post.weekday , data=merged.weekdays))

table(merged.weekdays$post.weekday)

ggplot (merged.weekdays, mapping =aes(x=early.sum, y=google, color =post.weekday))+
  geom_point( ) +
  geom_smooth(method="lm")+
  labs(title = "Google referrals and early comments", y = "Google referrals" , x="Early comments")+ 
     theme(axis.text.x = element_text(angle = 45, hjust = 1))+
  facet_wrap(~post.weekday)


# ggplot(merged, mapping=aes(x=post.date))+
#   geom_point(aes(y=google), color="red")+
#   geom_point(aes(y=direct), color="green")+
#   geom_point(aes(y=other.web), color="yellow")+
#   geom_point(aes(y=social), color="purple")+
#   geom_point(aes(y=bdc), color="blue")+
#   geom_point(aes(y=ll), color="pink")  +
#   labs(title = "referral sources per post ", y = "Number of referrals" , x="Post")+ 
#      theme(axis.text.x = element_text(angle = 45, hjust = 1))



```
