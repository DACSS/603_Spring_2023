---
title: "Homework 5"
author: "Justine Shakespeare"
description: "Homework 5 for DACSS 603 spring 2023"
date: "05/01/2023"
format:
  html:
    toc: true
    code-fold: true
    code-copy: true
    code-tools: true
categories:
  - hw5
  - model diagnostics
  - Justine Shakespeare
---

```{r}
library(tidyverse)
library(alr4)
library(smss)
library(stargazer)
```

## Question 1

```{r}
# load the data
data(house.selling.price.2)
glimpse(house.selling.price.2)
```
First we'll recreate some of the calculations from the homework assignment. 

```{r}
# rename the variables according to the homework 
house.selling.price.2 <- rename(house.selling.price.2, 
                                Price = P, 
                                Size = S, 
                                Beds = Be, 
                                Baths = Ba)

# take a look at the co
cor(house.selling.price.2)

# run linear regression
summary(lm(formula = Price ~ Size + Beds + Baths + New, data = house.selling.price.2))
```

### A

*For backward elimination, which variable would be deleted first? Why?*

I would remove the Beds variable first because it is not significant and has the largest p-value. 

### B

*For forward selection, which variable would be added first? Why?*

I would add the Size variable first because the correlation between Size and Price is the strongest among all of the variables. The p-value for the Size variable in the regression output is also the smallest.

### C

*Why do you think that BEDS has such a large P-value in the multiple regression model, even though it has a substantial correlation with PRICE?*

Because when the other variables are included in the model, and therefore controlled for, the effect that only the variable Beds is responsible for is quite small. Other variables, such as Size, account for a larger proportion of the effect on Price. The explanatory power of Beds, once other variables are present, is not large in comparison to the other variables. 

### D

*Using software with these four predictors, find the model that would be selected using each criterion:*

To come up with a set of models to compare, I used backwards elimination to remove the variable with the highest p-value. That gave us the following four models to compare. 

```{r}

HPModel1 <- (lm(formula = Price ~ Size + Beds + Baths + New, data = house.selling.price.2))

HPModel2 <- (lm(formula = Price ~ Size + Baths + New, data = house.selling.price.2))

HPModel3 <- (lm(formula = Price ~ Size + New, data = house.selling.price.2))

HPModel4 <- (lm(formula = Price ~ Size, data = house.selling.price.2))

stargazer(HPModel1, HPModel2, HPModel3, HPModel4, type = "text")
```

HPModel1 and HPModel2 (first with all variables included, and second with only Beds removed), have the highest R^2 and adjusted R^2 numbers, indicating these are the strongest models. HPModel1 has the highest R^2 with 0.869, but the adjusted R^2 is 0.001 smaller than HPModel2. This is because HPModel1 was penalized for including more variables. Let's compare the AIC and BIC of those two models to determine which is strongest.

```{r}
broom::glance(HPModel1)
broom::glance(HPModel2)
```
HPModel1 has an AIC of 790.6225 and BIC of 805.8181, while HPModel2 has an AIC of 789.1366 and BIC of 801.7996. Because these numbers are slightly smaller for HPModel2, this indicates it is the slightly stronger model. 

Finally, we can calculate the PRESS statistic. 

```{r}
# calculate the residuals
res1 <- residuals(HPModel1)

# Calculate the PRESS statistic
pressStat1 <- sum((res1/(1-hatvalues(HPModel1)))^2)

# View the PRESS statistic for HPModel1
cat("HPModel1's PRESS statistics is", pressStat1, "\n")

# calculate the residuals
res2 <- residuals(HPModel2)

# Calculate the PRESS statistic
pressStat2 <- sum((res2/(1-hatvalues(HPModel2)))^2)

# View the PRESS statistic for HPModel2
cat("HPModel2's PRESS statistics is", pressStat2)
```

Again, HPModel2 has a slightly smaller PRESS statistic, indicating it is a stronger model.

### E

Given the adjusted R^2, AIC, BIC, and PRESS statistic, I would suggest keeping HPModel2, the model with all of the variables except for Beds since this model was the strongest according to those assessment tools (and we know R^2 gets larger with more variables, so it makes sense to pay attention to the adjusted R^2 in this case). 

## Question 2

```{r}
data(trees)
glimpse(trees)
```
From the documentation:
“This data set provides measurements of the diameter, height and volume of timber in 31 felled black cherry trees. Note that the diameter (in inches) is erroneously labeled Girth in the data. It is measured at 4 ft 6 in above the ground.”

*Tree volume estimation is a big deal, especially in the lumber industry. Use the trees data to build a basic model of tree volume prediction. In particular,*

### A 

*Fit a multiple regression model with the Volume as the outcome and Girth and Height as the explanatory variables*

```{r}
TreePred <- lm(formula = Volume ~ Girth + Height, data = trees)
summary(TreePred)
```

### B  

*Run regression diagnostic plots on the model. Based on the plots, do you think any of the regression assumptions is violated?*

#### Residuals vs Fitted plot.

```{r}
plot(TreePred, which = 1)
```
The line is not horizontal, indicating the assumption of linearity is violated here. While the residuals do not exactly "bounce randomly" around the zero line, they are also not in a funnel shape, so it is unclear whether the assumption of constant variance is completely violated. There are some residuals that stand out, indicating that there are outliers. 


#### Normal Q-Q plot

```{r}
plot(TreePred, which = 2)
```
Most of the points are along the line, indicating that the assumption of normality has been met. 


#### Scale-Location

```{r}
plot(TreePred, which = 3)
```
The red line is not horizontal and instead has a significant dip and then trends slightly upward. This suggests the model violates the assumption of constant variance. 


#### Cook's Distance

```{r}
plot(TreePred, which = 4)
```
To determine whether the influential observation assumption has been violated we must calculate whether the Cook's distance is larger than 1 or 4/n. Recall there are 31 observations in this data, indicating the calculation for Cook's distance is 4/31 = 0.13. It looks as though about three points are larger than that, indicating the influential observation assumption has been violated. 

## Question 3

```{r}
data(florida)

glimpse(florida)
```
(Data file: florida in alr R package)

In the 2000 election for U.S. president, the counting of votes in Florida was controversial. In Palm Beach County in south Florida, for example, voters used a so-called butterfly ballot. Some believe that the layout of the ballot caused some voters to cast votes for Buchanan when their intended choice was Gore.

The data has variables for the number of votes for each candidate—Gore, Bush, and Buchanan.

### A

*Run a simple linear regression model where the Buchanan vote is the outcome and the Bush vote is the explanatory variable. Produce the regression diagnostic plots. Is Palm Beach County an outlier based on the diagnostic plots? Why or why not?*

```{r}
votes <- lm(formula = Buchanan ~ Bush, data = florida)
summary(votes)

plot(votes, which = 1:4)
```
After running this regression and using diagnostic plots to examine it, it looks like Palm Beach is definitely a major outlier. In the first plot, residuals vs fitted, the Palm Beach residual is far from the red line and from the rest of the residuals. In the second plot, Normal Q-Q, Palm Beach once again is far from the rest of the data, which otherwise lines up fairly neatly against the line. In the Scale-Location plot again Palm Beach is far from the other residuals. And finally, in the Cook's Distance plot we can see that Palm Beach is a significant outlier since it is over 1 and 4/67 (0.06). 

Dade county also shows up as an outlier in some of these plots, but since this county includes Miami - a city with a very different demographic breakdown than the rest of the state - it makes sense this difference would be reflected in voting patterns. 

### B 

*Take the log of both variables (Bush vote and Buchanan Vote) and repeat the analysis in (A.) Does your findings change?*

```{r}
votesLogged <- lm(formula = log(Buchanan) ~ log(Bush), data = florida)
summary(votesLogged)

plot(votesLogged, which = 1:4)
```

If you log the variables Palm Beach remains an outlier in each diagnostic plot, but the difference between that county and the rest of the data is not as stark.

