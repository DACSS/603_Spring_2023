---
title: "Homework 4"
author: "Asch Harwood"
description: "Homework 4, Intro to Quant"
date: "4/25/2023"
format:
  html:
    toc: true
    code-fold: true
    code-copy: true
    code-tools: true
categories:
  - hw4
---

```{r}
library("dplyr")
library("knitr")
library(kableExtra)
library(xtable)
library(ggplot2)
library(GGally)
library(lme4)
library(car)
library(dplyr)
library(alr4)
library(stargazer)
library(smss)
```

# Question 1

For recent data in Jacksonville, Florida, on - y = selling price of home (in dollars)

-   x1 = size of home(in square feet)

-   x2 = lot size (in square feet),

-   prediction equation \<- ŷ = −10,536 + 53.8x1 + 2.84x2.

### A

A particular home of 1240 square feet on a lot of 18,000 square feet sold for \$145,000. Find the predicted selling price and the residual, and interpret.

```{r}
x1 <-  1240 #house size
x2 <- 18000 #lot size
y_observed <- 145000
```

```{r}
# find predicted selling price
y_hat <- (-10536) + (53.8*x1) + (2.84*x2)
residual <- y_observed - y_hat
cat('Predict sale price: $',  y_hat)
cat('\n')
cat('Residual: $', residual)
cat('\n')

cat('The predicted sale price of', y_hat, 'dollars is', residual, 'dollars less than the actual observed house price of', y_observed, '.')
```

### B

Holding the lot size fixed, each unit increase in square footage increases the house value by 53.8 dollars. This is because the square footage coefficient is 53.8.

###C

The lot size would need to increase by 19.94 square feet to increase the home value by 53.8 dollars.

# Question 2

(Data file: salary in alr4 R package). The data file concerns salary and other characteristics of all faculty in a small Midwestern college collected in the early 1980s for presentation in legal proceedings for which discrimination against women in salary was at issue. All persons in the data hold tenured or tenure track positions; temporary faculty are not included. The variables include degree, a factor with levels PhD and MS; rank, a factor with levels Asst, Assoc, and Prof; sex, a factor with levels Male and Female; Year, years in current rank; ysdeg, years since highest degree, and salary, academic year salary in dollars.

```{r}
data(salary)
salary
```

### A

We cannot reject the null hypothesis that there is a difference between the mean male and female salary at the 0.05 significance level given the observed p-value of 0.09. The 95 percent confidence interval supports because it contains zero.

```{r}
ggplot(data = salary, aes(x=salary, fill=sex)) + 
  geom_histogram(bins=5)
```

```{r}
male_salaries <- salary[salary$sex == "Male",]$salary
female_salaries <- salary[salary$sex == "Female",]$salary
t.test(male_salaries, female_salaries)
```

### B

```{r}
data(salary)
fit <- lm(salary ~ rank + sex + year + ysdeg + degree, data = salary)
sex_ci <- confint(fit)["sexFemale", ]
cat('sexFemale confidence interval: ', sex_ci)
```

### C

Intercept: The intercept of \$15746.05 is when all over coefficients are zero, which can be interpreted as the 'base' salary. In this case, our base reference is a male, assistant professor with zero years of experience and 0 years since graduation

rankAssoc: Holding all else equal, associate professors make \$5292.36 more than assistant professors. This finding is statistically significant, which means we can reject the null hypthosis that there is no different in salary between associate and assistant professors.

rankProf: Holding all else equal, professors make \$11118.76 more than assistant professors. This finding is statistically significant, which means we can reject the null hypthosis that there is no different in salary between professors and assistant professors.

sexFemale: Holding all else equal, female professors make \$1166.37 more than male professors. This finding is NOT statistically significant, which means we cannot reject the null hypothesis that there is no difference in salary between male and female professors.

year: Holding all else equal, for each year increase in years in current position, salary increases by \$476.31 This finding is statistically significant, which means we can reject the null hypothesis that an increase in years does not increase salary.

ysdeg: Holding all else equal, for each year increase in years since degree, salary decreases by \$124.57. This finding is NOT statistically significant, which means we cannot reject the null hypothesis that a change in years in position does not have a corresponding change in salary.

degreePhD: Holding all else equal, professors with PhD's make \$1388 more than those with masters degrees. This is NOT statistically significant.

```{r}
summary(fit)
```

###D

rankAsst: Holding all else equal, associate professors make \$11118.76 less than a professor. This finding is statistically significant.

rankAssoc: Holding all else equal, assistant professors make \$5826.40 less than a professor. This finding is statistically significant.

```{r}
salary$rank <- relevel(salary$rank, ref="Prof")
fit <- lm(salary ~ rank + sex + year + ysdeg + degree, data = salary)
summary(fit)
```

###E

By excluding rank:

-   the model as a whole is considerably less 'useful'. The r-squared has gone down from 86 to 63, as has the adjusted R-squared, even though we have simplified the model. This means the model without rank has less 'explanatory' power. We also see an increase in the residual standard error.

-   sexFemale in the new model is now correlated with a decreased salary of \$1,286.54 compared to the original models increase of \$1,166.37 That being said, sex is still not statistically significant.

-   There is a slight decrease in the coefficient for year.

-   In the new model, an increase in one year in number of years since degree is associated with a \$339.40 increase in salary, compared to \$124.57 reduction from the previous model. This finding is statistically significant.

-   In the new model, having a phd is associated with a decline in salary by \$3,299.35 compared to the \$1,388.61 bump in the original model.

```{r}
fit_no_rank <- lm(salary ~ sex + year + ysdeg + degree, data = salary)
```

```{r message=FALSE, warning=FALSE}
# Create a side-by-side table of the models using stargazer
stargazer(fit, fit_no_rank,type = "text",
          title = "Professor Salary Regression",
          align = TRUE,
          column.labels = c("M1:AllVari", "M2:NoRank"),
          ci = TRUE, # Show confidence intervals
          digits = 2) 
```

### F

To prevent multicollinearity, I excluded ys_deg from the model. A pair plot visual inspection suggests that ys_deg is correlated with years. Without a visual inspection, we already know it is correlated with dean_selected because dean_selected is derived from ys_deg. We want to remove multicollinearity because it can impact the other coefficients in the model, possibly obscuring the true relationship between the independent and dependent variable.

There is evidence to suggest that the dean is preferentially rewarding staff he has hired. Holding all else equal, those hired by the dean earn \$2160 more than those who were not. This number is statistically significant.

```{r message=FALSE}
#add dean selection
salary$dean_selected <- ifelse(salary$ysdeg<=15, 1, 0)

#dropping ysdeg b/c correlated with dean_selected
x_no_y <- subset(salary, select = -c(salary, ysdeg))
#pairs(x_no_y)

# not including ys_deg b/c correlation with dean_selected and year
fit_dean <- lm(salary ~ degree + rank + sex + dean_selected + year, data = salary)
summary(fit_dean)
```

```{r}
vif(fit_dean)
```

# Question 3


```{r}
data("house.selling.price")
house.selling.price$New <- as.factor(house.selling.price$New)
```

### A

Size: Holding all else equal, a one square foot increase in house size adds \$116 dollars to its sale price. This finding is statistically significant.

New: Holding all else equal, a new house sells for \$57,736 more than an 'old' house. This finding is also statistically significant.

```{r}
fit <- lm(Price ~ Size + New, data = house.selling.price)
summary(fit)
```

### B

**Combined Model**

`price = -40230.867 + 116.132*x_size + 57736.283*x_new`

In this model, our price prediction is based on our intercept to -40,230.867 plus 116.132 muliplied by the house square footage plus $57736 if the house is new.

**New House Model**

```price_new_house = 116.132*x_size + 17505.42```

**Old House Model**

```price_old_house = 116.132*x_size - 40230.867```

```{r}
-40230.867 + 57736.283
```


### C

Find the predicted selling price for a home of 3000 square feet that is (i) new, (ii) not new.

```{r}

# house size
x_size <- 3000

# new model
price_new_house <- 116.132*x_size + 17505.42

#old house model
price_old_house = 116.132*x_size - 40230.867

cat('New house estimate: ', price_new_house)
cat('\n')
cat('Old house estimate: ', price_old_house)
```


### D

```{r}
fit <- lm(Price ~ Size + New + Size*New, data = house.selling.price)
summary(fit)
```


### E
New1: Coefficient of $-78,537. Compared to old houses, new houses with zero square feet would sell for $78,527, which is not possible. This result, however, is NOT statisically significant.

Size:New1: For every unit increase in size, new homes sale price will increase by 61 dollars more than an old home. This coefficient is statistically significant.

### F

Find the predicted selling price for a home of 3000 square feet that is (i) new, (ii) not new.

predict(fit, )
```{r}
new_house <- data.frame(Size = 3000, New = 1)
new_house$New <- as.factor(new_house$New)
new_house_predicted_price <- as.numeric(predict(fit, newdata = new_house))


old_house <- data.frame(Size = 3000, New = 0)
old_house$New <- as.factor(old_house$New)
old_house_predicted_price <- as.numeric(predict(fit, newdata = old_house))

cat('New house predicted price: ', new_house_predicted_price)
cat('\n')
cat('Old house predicted price: ', old_house_predicted_price)
```

###G

Find the predicted selling price for a home of 1500 square feet that is (i) new, (ii) not new. Comparing to (F), explain how the difference in predicted selling prices changes as the size of home increases.

### H

Do you think the model with interaction or the one without it represents the relationship of size and new to the outcome price? What makes you prefer one model over another?
