---
title: "Final Project checkin 5"
author: "Diana Rinker"
description: "Final project DACSS 603"
date: "5/9/2023"
format:
  html:
    toc: true
    code-fold: true
    code-copy: true
    code-tools: true
editor: 
  markdown: 
    wrap: 72
editor_options: 
  chunk_output_type: console
---

## DACSS 603, spring 2023

## Final Project check-in 5, Diana Rinker.

# Introduction. Online engagement

It is well known that online engagement with the web resource is a
highly valuable metric and is contributing to the site revenue. This
research project is exploring which factors contribute to users online
engagement. To do that I will use the data of an online blog on the news
website. The author of this blog is posting articles about interpersonal
relationships every work day (Mon- Fri). The posts are formulated as a
letter from a reader with the situation and a question about
relationships. The author gives an advice about the situation. Website
readers are free to comment under each post, but cannot make their own
posts.

All post methadata and comments are public. They are saved by the
website and available for the analysis. Using this data set, I will
explore how readers' engagement connected with blogs's author
engagement, site comments', web source of readers and negative behaviors
online.

# Research Question and hypothesis.

**RQ:** Which factors influence user's engagement in online blog?


**DV:** My dependent construct is **"user's engagement**", I will
measure users' engagement at the level of individual post, using the
following metrics:

1\. Exit rate or "bounces". When the visitor is coming to the page and
then leaving, i.e. not opening other pages on this website. this will
represent all readers.

2\. Number of comments - is engagement metric, representing only loyal
readers, who have created an account and signed in.

**IV:** My main independent variables are

1.  Post popularity. Unique users - number of unique people viewed the
    post.
2.  Source of the readers - which web page the reader came from. This is
    represented by 6 different variables, each is continuous type.
3.  Mood of the conversation , derivative continuous variable calculated
    as the ratio of "likes" to all emotions (sum of thumbs up and thumbs
    down), with the range 0-1.
4.  Blocked and flagged comments.
5.  Number of author's comments.
6.  Weekday of the post.

```{r, echo=F}
#Loading necessary libraries: 
library(readxl)
library(dplyr)
library(ggplot2)
library(tidyverse)
library(png)
library(grid)
library(gridExtra)
library(stargazer)
```

### Data source and description.

To answer my research question I will use two datasets. The first data
set has information about all comments associated with each post by post
ID. The second data set is analytics data for the web page. It contains
one post per row and variables describe each post as a whole without
breaking down to the comment level.

In this project I will analyze posts for January 2022 - March 2023. Here
is the list of variables in each data set:

Comments data:

```{r, echo=T}
getwd()
# First, I will load the data set with the comment level data:
raw <- as_tibble (read_csv("C:\\Users\\Diana\\OneDrive - University Of Massachusetts Medical School\\Documents\\R\\R working directory\\DACSS\\603\\my study files for dacss603\\globe\\ data.2021.plus.csv"))
comments.data<-raw 
colnames (comments.data)
head(comments.data$written_at)
comments.data <-comments.data%>%
              mutate(com.year = format(written_at,format = "%Y" ))
# range(comments.data$com.year)
# dim(comments.data)
str(comments.data)
dim(comments.data)
comments.data <- comments.data%>%
  filter(written_at  >=	"2022-01-01")
```

Post data:

```{r, echo=T}
# Second, loadng post-level data :
merged <- as_tibble (read_csv("C:\\Users\\Diana\\OneDrive - University Of Massachusetts Medical School\\Documents\\R\\R working directory\\DACSS\\603\\my study files for dacss603\\globe\\data.merged.csv"))
# colnames(merged)
str(merged)
dim(merged)
# Limiting the dataset to 2022: 
merged <- merged%>%
  filter(post.date  >=	"2022-01-01")
dim(merged)

# Due to dataset of comments having more data than post dataset,  I will cut them to match: 
merged <- merged %>%
  filter (!is.na(merged$pct.positive))
```

To begin, I will review available variables and evaluate to idntify a
metric for each construct in my study.

# 1.DV: engagement.

## Exit rate

This variable is measuring how many people visited the page and then
left the website after the first view. This metric is the best measure
of engagement for all users, as it represents the first step after being
exposed to the post - either quitting the site or remaining on the site.

Here can see the distribution of this variable :

```{r, echo=T}
# str(merged)
merged <- merged %>%
  mutate(e.rate = Exits/`Page views`)
select (merged, `Exit rate`, e.rate)
# merged$`Exit rate` <- as.numeric(sub("%", "", merged$`Exit rate`)) / 100

ggplot(data=merged, mapping=aes(x=e.rate))+
    geom_histogram(binwidth = 0.02, fill = "grey50", alpha = 0.9)+
    geom_histogram(binwidth = 0.01, fill = "sandybrown", alpha = 0.7)

```

Distribution of exit rate over time:

```{r, echo=T}
# creating  year_month  variable 
merged$post.month <-as.numeric(merged$post.month)
merged$year_month <- paste0(merged$post.year, "-", sprintf("%02d", merged$post.month))

ggplot(merged, mapping = aes(x=year_month , y=`e.rate`, fill=year_month ))+
  geom_boxplot() +
  labs(title = "distribution of `Exit rate` per post ", y = "Exit rate" , x="Month")+ 
     theme(axis.text.x = element_text(angle = 45, hjust = 1))

```

## Number of comments:

engagement metric for loyal readers.

Commenting requires user to log in, which is an indicator of greater
engagement of an individual user. Therefore this variable represents
engagement of a subset of users,loyal readers, who have created an
account.

```{r, echo=T}
# merged$year_month 
ggplot(data=merged, mapping=aes(x=n.comments))+
  geom_histogram(fill = "seagreen4")
```

And change in the distribution over time:

```{r, echo=T}
ggplot(merged, mapping = aes(x=year_month , y=n.comments, fill=year_month ))+
  geom_boxplot() +
  labs(title = "distribution of comments per post ", y = "Number of comments" )+
  scale_y_continuous(breaks = seq (from=0, to= 10000, by= 100)) + 
     theme(axis.text.x = element_text(angle = 45, hjust = 1))

```

# 2.IV. Independent variables:

```{r, echo=T}

```

### 2.1.Popularity

"Uniques" variable represents number of unique people who came to the
page and viewed it at least once, his metric represents popularity of
the post. It's distribution shows us that not all posts are equally
popular:

```{r, echo=T}
# colnames(merged)
ggplot(data=merged, mapping=aes(x=Uniques))+
    geom_histogram(fill = "purple2")

```

We can see a long tail on the right, showing that there is a number of
posts who are way more popular than the majority of the sample.

```{r, echo=T}

ggplot( data=merged, mapping=aes(y=Uniques, x=merged$year_month))+
          geom_boxplot()+
  labs(title="Number of unique viewers per month", x="Month", y="Number of unique viewers")+ 
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```

Next plot will illustrate connection between popularity and engagement
metrics:

```{r, echo=T}
pairs(subset (merged, select=c(Uniques, e.rate, n.comments )))
```

We can see that popularity strongly correlated with exit rate,where an
increase in popularity causes decrease of engagement.It is not as
strongly correlated with number of comments. Engagement rate and number
of comments dont show obvious pattern between each other.

### 2.2. Referral sources.

The website analytics provides information on where the viewers are
coming from to the blog page. For example, if people clicked on the blog
link posted on FaceBook, that would be referral from social media. If
people clicked on the blog link within BDC website, that would be "BDC
referral visit".

There are 5 sources of referrals, each corresponding with a variable in
the data set. Variable's value is a number of visits from this referral
source.

```         
"Search + amp referral visits"
"Direct (non-email) referral visits"
"Other website referral visits"
"Social referral visits"
"BDC referral visits"
"Visits when post was on LL HP" 
```

```{r, echo=T}
#renaming variables for convenience: 
merged<-merged%>%
  rename(google ="Search + amp referral visits",
         direct ="Direct (non-email) referral visits",
         other.web = "Other website referral visits",
          social= "Social referral visits",
          bdc= "BDC referral visits",
          ll= "Visits when post was on LL HP" )

ggplot(merged, mapping=aes(x=post.date))+
  geom_point(aes(y=google), color="red")+
  geom_point(aes(y=direct), color="green")+
  geom_point(aes(y=other.web), color="yellow")+
  geom_point(aes(y=social), color="purple")+
  geom_point(aes(y=bdc), color="blue")+
  geom_point(aes(y=ll), color="pink")  +
  labs(title = "Referral sources per post ", y = "Number of referrals" , x="Post")+ 
  scale_color_manual(values = c("red", "green", "yellow", "purple", "blue", "pink"),
                     labels = c("Google", "Direct", "Other Web", "Social", "Bdc", "LL"))
```

This graph is showing of "Search + amp referral visits" have high
variability. Other referral sources range is much smaller.

```{r, echo=T}
colnames(comments.data)
# Calculating date of the post and weekday  from comments data 

library(lubridate)
post.date <-comments.data %>%
    group_by(post_id)%>%
    arrange(written_at)%>%
    summarize(post.dt = first(written_at))

post.date  <-post.date %>%
  mutate (weekday = wday(post.dt, label = TRUE),
          n.coms =n())%>%
  filter (n.coms>100)%>%
  select(post_id,post.dt,  weekday )

 # Adding weekdays to both data sets: comments level and post level:
comments.data <- merge(comments.data, post.date , by = "post_id", all = TRUE)
merged <- merge( merged, post.date , by = "post_id", all = TRUE)


```

### 2.3. Post weekdays

The posts are being published Monday through Friday (with rare
exceptions). No posts are published on the weekend. Below is a plot for
exit rate and number of comments' distributions per week day

```{r, echo=T}
 ggplot (merged,  mapping=aes(x=weekday, y=e.rate) )+
  geom_boxplot()+
  geom_point()+
  labs(title="Exit rate per weekday")

ggplot (merged,  mapping=aes(x=weekday, y=n.comments) )+
  geom_boxplot()+
  geom_point()+
  labs(title="Number of comments per weekday")

```

We can see that exit rate is overall lower on Tuesday, and number of
comments overall highest on Fridays.

### 2.4. Authors comments

To identify, how much the author of the blog is engaged in the post, I
will create an additional variable derived from a user_name field and
review the distribution of autor's comments :

```{r, echo=T}
# str(merged)
comments.data$user_name<-  ifelse (is.na(comments.data$user_name), 0, comments.data$user_name)
comments.data$author<-  ifelse (comments.data$user_name=="MeredithGoldstein", 1, 0)

comments.grouped <-comments.data %>%
  group_by(post_id)%>%
  summarize(n.comments=n(),
            author.sum = sum(author))

# Comments data contains rows that dont actually reporesent posts, and were crearted by web support team for troubleshooting. I need to remove these rows. They typically have very low number of comments
comments.grouped <-comments.grouped %>%
filter(n.comments >100)  # removing invalid posts created by the  website management team.

comments.grouped <-comments.grouped %>%
  select(post_id, author.sum)
 dim(comments.grouped)
 
#adding author.sum to main data set: 
merged <- merge( merged , comments.grouped, by = "post_id", all = TRUE)

 ggplot (merged,  mapping=aes(x=author.sum) )+
  geom_histogram()+
  labs(title="Authors comments distribution")

```

Authors comments per engagement metrics:

```{r, echo=T}

ggplot (merged,  mapping=aes(x=author.sum, y=e.rate ))+
  geom_point()+
  labs(title="Exit rate per author.sum")
  
ggplot (merged,  mapping=aes(x=author.sum, y=n.comments) )+
  geom_point()+
  labs(title="Number of comments per author.sum")

```

This graph shows, that majority of posts have no author's comments.

### 2.5. Mood of the post.

This is a numerical variable, calculated as percentage of "thumbs up"
from all likes (both "thumbs up" and "thumbs down").

```{r, echo=T}
# colnames(merged)
 ggplot (merged,  mapping=aes(x=pct.positive) )+
  geom_histogram(fill = "springgreen3")+
  labs(title="Mood of the post (pct.positive) distribution")
```

Plotting mood pf the post against engagement metyrics:

```{r, echo=T}
 ggplot (merged,  mapping=aes(x=pct.positive, y=e.rate ))+
  geom_point()+
  labs(title="Exit rate per pct.positive")
  
ggplot (merged,  mapping=aes(x=pct.positive, y=n.comments) )+
  geom_point()+
  labs(title="Number of comments per pct.positive")
```

Distribution of mood over time:

```{r, echo=T}
 ggplot(merged, mapping = aes(x=year_month , y=pct.positive, fill=year_month ))+
  geom_boxplot() +
  labs(title = "distribution of Mood per post ", y = "Mood" , x="Month")+ 
     theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

### 2.6. Blocked comments per post.

Now I will visualize amount of blocked comments per post: Distribution
of blocked comments:

```{r, echo=T}
 ggplot (merged,  mapping=aes(x=blocked.sum) )+
  geom_histogram(fill = "brown")+
  labs(title="blocked.sum distribution")
```

Plotting blocked comments against engagement metrics:

```{r, echo=T}

 ggplot (merged,  mapping=aes(x=blocked.sum, y=e.rate ))+
  geom_point()+
  labs(title="Exit rate per blocked.sum")
  
ggplot (merged,  mapping=aes(x=blocked.sum, y=n.comments) )+
  geom_point()+
  labs(title="Number of comments per blocked.sum")
```

Distribution of blocked comments over time:

```{r, echo=T}

ggplot(merged, mapping = aes(x=year_month , y=blocked.sum, fill=year_month ))+
  geom_boxplot() +
  labs(title = "Number Blocked comments  per post ", y = "Number of blocked comments per post" , x="Month")+ 
     theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

# 3. Creating a model for exit rate - all users engagement.

### 3.1 Basic model.

I will start with entering all of the independent variable in the model
and then eliminating ones that are not significant.

```{r, echo=T}
# colnames(merged)
model.1 <- lm(e.rate ~ log(Uniques)+ author.sum +n.comments +google +direct +other.web +social +bdc + ll + blocked.sum + pct.positive +weekday , data = merged)

model.2 <- lm(e.rate ~ log(Uniques)+ google +direct +other.web +social +bdc  +weekday , data = merged)

stargazer( model.1,model.2, type = 'text')
```

Elimination of non-significant factors slightly impacted coefficients of
significant variables, and did not change R\^2 or adjusted R\^2 much.

My next step is to diagnoze this model:

### 3.2. Diagnostic of linear models:

```{r, echo=T}

model.2 <- lm(e.rate ~ log(Uniques)+ google +direct +other.web +social +bdc  +weekday , data = merged)

plot(model.2, which = 1:6)
```

We can see the issue with distribution of residuals in "Residuals vs
fitted" plot. This suggests non-linear relationship between variables. I
will transform some of my variables to see if that gives me a better
model:

```{r, echo=T}
#original model
model.1 <- lm(e.rate ~ log(Uniques)+ google +direct +other.web +social +bdc  +weekday , data = merged)
# Logging Google referrals
model.2 <- lm(e.rate ~ log(Uniques)+log(google) +direct +other.web +social +bdc  +weekday , data = merged)
# squaring google referrals
model.3 <- lm(e.rate ~ log(Uniques)+I(google^2) +direct +other.web +social +bdc  +weekday , data = merged)
stargazer( model.1, model.2, model.3,type = 'text')
```
This table showing that google referrals are significant in either
form - squared or logged. However, its coefficient is much higher when
logged. Uniques variables's coefficient also changes significantly as I
am modifying google variable. That migh indicate interaction between
them.

Comparing diagnostic plots for model 2 (google logged) and  3 (google squared):
```{r, echo=T}
par(mfrow = c(1,2))
plot(model.2, which = 1)
plot(model.3, which = 1)
```
These two plots show ineven distribution of residuals around zero line. To address it,  I will transform  the variables further. 



 First , I will log the rest of referral variables.

```{r, echo=T}
#original model
model.1 <- lm(e.rate ~ log(Uniques)+ google +direct +other.web +social +bdc  +weekday , data = merged)
# all source variables are logged: 
model.3 <- lm(e.rate ~ log(Uniques)+log(google) +log(direct) +log(other.web) +log(social) +log(bdc)  +weekday , data = merged)
stargazer( model.1,model.3, type = 'text')
```
All referral sources except "other.web" and "social" show better fit
when logged. Moving forward I will use these variables not logeed.

```{r, echo=T}
par(mfrow = c(1,2))
plot(model.1, which = 1)
plot(model.3, which = 1)
```
We still observe curvi-linear distribution of residuals.

Next, I will check if the referral variables have an ineraction with
popularity (Uniques)

```{r, echo=T}
# last model
model.3 <- lm(e.rate ~ log(Uniques)+log(google) +log(direct) + other.web +social +log(bdc)  +weekday , data = merged)
# adding interaction: 
model.4 <- lm(e.rate ~ log(Uniques)*log(google) +log(direct) +other.web +social +log(bdc)  +weekday , data = merged)
stargazer( model.3,model.4, type = 'text')
```
Adding interaction between popularity and google referrals significcntly
improved the model: interaction term has negative correlation with
dependent variable, it also made "social" referrals significant, and
improved models R\^2 and adjusted R\^2.

It also significantly changed distribution of residuals: now distribution is equally spread around 0.
```{r, echo=T}
par(mfrow = c(1,2))
plot(model.3, which = 1)
plot(model.4, which = 1)
```


Lets review other diagnostic plots:

```{r, echo=T}
# last model
model.4 <- lm(e.rate ~ log(Uniques)*log(google) +log(direct) +other.web +social +log(bdc)  +weekday , data = merged)

par(mfrow = c(1,2))
plot(model.3, which = 2)
plot(model.4, which = 2)

par(mfrow = c(1,2))
plot(model.3, which = 3)
plot(model.4, which = 3)

par(mfrow = c(2,2))
plot(model.3, which = 4)
plot(model.4, which = 4)

par(mfrow = c(2,2))
plot(model.3, which = 5)
plot(model.4, which = 5)

par(mfrow = c(2,2))
plot(model.3, which = 6)
plot(model.4, which = 6)

```

We can see from both models, that variable 231 is an outlier that
significantly impacts the model. I will remove that observation and
re-evaluate the model:

```{r, echo=T}
merged.old <- merged
merged <- merged[-c(231), ]
model.4 <- lm(e.rate ~ log(Uniques)*log(google) +log(direct) +other.web +social +log(bdc)  +weekday , data = merged.old)
model.5 <- lm(e.rate ~ log(Uniques)*log(google) +log(direct) +other.web +social +log(bdc)  +weekday , data = merged)

stargazer( model.4,model.5, type = 'text')

par(mfrow = c(1,2))
plot(model.4, which = 1)
plot(model.5, which = 1)

par(mfrow = c(1,2))
plot(model.4, which = 2)
plot(model.5, which = 2)

par(mfrow = c(1,2))
plot(model.4, which = 2)
plot(model.5, which = 2)

par(mfrow = c(1,2))
plot(model.4, which = 3)
plot(model.5, which = 3)

par(mfrow = c(2,2))
plot(model.4, which = 4)
plot(model.5, which = 4)

par(mfrow = c(2,2))
plot(model.4, which = 5)
plot(model.5, which = 5)

par(mfrow = c(2,2))
plot(model.4, which = 6)
plot(model.5, which = 6)

```

New, the variable "other.web" became unsignificant, but model fit
improved.

### 3.3. Improved model.

I will check whether other referral sources have an interaction with
popularity:

```{r, echo=T}

model.5 <- lm(e.rate ~ log(Uniques)*log(google) +log(direct) +other.web +social +log(bdc)  +weekday , data = merged)
model.6 <- lm(e.rate ~ log(Uniques)*log(google) +log(Uniques)* log(direct) +log(Uniques)*other.web +log(Uniques)*social +log(Uniques)*log(bdc)  +log(Uniques)*weekday , data = merged)

model.7 <- lm(e.rate ~ log(Uniques)*log(google) +log(Uniques)* log(direct) +other.web +social +log(bdc)  +weekday , data = merged)

stargazer( model.5,model.6, model.7 , type = 'text')

par(mfrow = c(1,3))
plot(model.5, which = 1)
plot(model.6, which = 1)
plot(model.7, which = 1)

par(mfrow = c(1,3))
plot(model.5, which = 2)
plot(model.6, which = 2)
plot(model.7, which = 2)

par(mfrow = c(1,3))
plot(model.5, which = 3)
plot(model.6, which = 3)
plot(model.7, which = 3)

par(mfrow = c(1,3))
plot(model.5, which = 4)
plot(model.6, which = 4)
plot(model.7, which = 4)

par(mfrow = c(1,3))
plot(model.5, which = 5)
plot(model.6, which = 5)
plot(model.7, which = 5)

par(mfrow = c(1,3))
plot(model.5, which = 6)
plot(model.6, which = 6)
plot(model.7, which = 6)
```

All models have similar R\^2 and adjusted R\^2. However, the last model
(model.7) best accounts for all source variables.

Calculation of AIC and BIC:

```{r, echo=T}

AIC(model.5) 
AIC(model.6) 
AIC(model.7) 

BIC(model.5) 
BIC(model.6) 
BIC(model.7) 
```

AIC and BIC are very close to each other in all three models.

To summarize, the model \# 7 demonstrated the best fit with high R\^2
value and good results in diagnostic plots.

Te resulting model of my analysis is:

```{r, echo=T}
summary(model.7)
model.7 <- lm(e.rate ~ log(Uniques)*log(google) +log(Uniques)* log(direct) +other.web +social +log(bdc)  +weekday , data = merged)


```




# 4. Model for N.comments - loyal readers engagement:

## 4.1. Basic model for n.comments

Now as I have created a model for exit rate, I will do the same for
n.comments as dependent variable, which represents the measurement of
engagement of loyal readers.

```{r, echo=T}

colnames(merged)
summary(lm(n.comments ~ log(Uniques)+ author.sum + google +direct +other.web +social +bdc + ll + blocked.sum + pct.positive +weekday , data = merged))
     
model.1 <- lm(n.comments ~ log(Uniques)+ author.sum + google +direct +other.web +social +bdc + ll + blocked.sum + pct.positive +weekday , data = merged)
model.2 <- lm(n.comments ~ Uniques+ author.sum + google +direct +other.web +social +bdc + ll + blocked.sum + pct.positive +weekday , data = merged)
#removing unsignificant variables:
model.3 <- lm(n.comments ~  direct  +social + ll  + pct.positive +weekday , data = merged)
stargazer( model.1,model.2, model.3, type = 'text')

```

## 4.2. Diagnostics:

```{r, echo=T}

model.3 <- lm(n.comments ~  direct  +social + ll  + pct.positive +weekday , data = merged)

par(mfrow = c(2,3))
plot(model.3, which = 1:6)

```

Observation #48 looks like on outlier impacting the model. I will remove
this observation to see if that gives us any different results:

```{r, echo=T}
merged.old <- merged
merged <- merged[-c(48), ]
model.3 <- lm(n.comments ~  direct  +social + ll  + pct.positive +weekday , data = merged.old )
model.4 <- lm(n.comments ~  direct  +social + ll  + pct.positive +weekday , data = merged)
stargazer( model.3, model.4, type = 'text')

```

We can see that removing #48 changed some coefficients, did not impact
significance of variables. It also improved R\^2 and adjusted R\^2.

```{r, echo=T}
par(mfrow = c(2,3))
plot(model.4, which = 1:6)

```

### 4.3. QUESTION - do i need to improve mod fit here ?

# 4. Conlusion

We have build two models for different engagement metrics: engagement of
all readers and engagement of loyal readers. As we can see from the
models, different factors contribute to engagement of these two groups
of readers:

```{r, echo=T}
stargazer( model.7, model.4, type = 'text')
```

We can see, that different referral sourses contribute to users
engagement. While all readers'engagement (measured with exit rate) is
influenced by interaction of referrals from Google and Direct and
popularity, internal bdc referrals and somewhat referrals from other web
sources, loyal readers' engagement is highly impacted by the mood pf the
post, internal page referrals, social referrals. Thhey also show some
direct referrals' impact, but very small in compare with its influence
on all users' engagement.

We also see significant contibution of day of the week on readers
engagement. Specifically, Monday's engagement of all readers is
significantly higher than any other day. For loyal readers, Wednesday
showing thee lowest engagement.

It also important to point out, that popularity of the post (Uniques)
only plays a role for all user's engagement. Loyal readers appear not
being impacted by popularity.
