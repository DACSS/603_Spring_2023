---
title: "Homework_5_Saisrinivas_Ambatipudi"
author: "Saisrinivas Ambatipudi"
date: "2023-05-08"
output:
  word_document: default
  html_document: default
---

```{r setup, include=FALSE}
library(alr4)
library(smss)
```

## Question 1

### A

Backward elimination begins with a full model and gradually removes variables that do not add substantially to the model. As a result, because the variable Be/Beds has the highest p-value of 0.487, it is excluded first. We use the step() function to confirm this is correct.

```{r}
data(house.selling.price.2)
fit_Backward<-lm(P~.-Be,data = house.selling.price.2)
summary(fit_Backward)
```

Every variable in the model has a p-value of 0.05, indicating that it is statistically significant. With the elimination of the Be/Beds variable, the relevance of the remaining variables, S, Ba, and New, has increased.

For verification, we can use the stats package's step() function for backward stepwise prediction, which creates a regression model that incorporates all statistically significant predictor variables S, Ba, and New that are connected to the response variable.

```{r}
step(object = fit_Backward,direction = "backward",scope = 
       fit_Backward) #backward eliminate
```
### B

We begin with a model and gradually add variables, beginning with the most statistically significant.In this situation, the New variable with a p-value of 4.3e-06 would be added.

```{r}
fit_Forward<-lm(P~1,data = house.selling.price.2)
summary(fit_Forward)
```

```{r}
step_forward<-step(object=fit_Forward,direction = "forward",
                      scope = P~S+Be+Ba+New)
step_forward
```
Notably, the coefficients for both backward and forward choices are the same.

### C

BEDS has a huge p-value, indicating that it is statistically insignificant, and so we fail to reject the null hypothesis H0. Furthermore, it appears that BEDS is tightly related to the variable Size. This makes logical since the more bedrooms a house has, the larger it is. As a result, BEDS may already be tied to Size. Also worth noting is the tiny sample size (n=93), which may have contributed to the high p-value and significant association.

### D

```{r}
housing_price<-lm(P~1,data = house.selling.price.2)
all_housing_price_m1<-lm(P~.,data = house.selling.price.2)
stepwise_housing<-step(object=housing_price,direction = "both", scope = formula(all_housing_price_m1))
```

```{r}
summary(stepwise_housing)
```
The model's most helpful variables may be found via stepwise regression. We begin with the first model, which includes all of the variables in the housing data. The Broom package is used to extract more succinct information from an item. We will utilize the glance function in the Broom package.

```{r}
#model 1
all_housing_price_m1<-lm(P~.,data = house.selling.price.2)
summary(all_housing_price_m1)
```
```{r}
library(broom)
glance(all_housing_price_m1)
```

Our R^2 for Model_1 with all predictors is 0.8689.

The Beds variable is eliminated from Model_2 since we know it is tightly connected to size and has a high p-value, rendering it statistically unimportant.

```{r}
#model 2
housing_price_nobed_m2<-lm(P~.-Be,data = house.selling.price.2)
summary(housing_price_nobed_m2)
```
```{r}
glance(housing_price_nobed_m2)
```

R^2 for the Model_2 is 0.8681.

```{r}
#model 3
housing_price_nobed_noba_m3<-lm(P~.-Be,-Ba,data = house.selling.price.2)
summary(housing_price_nobed_noba_m3)
glance(housing_price_nobed_noba_m3)
```
We see the R^2 is 0.8654.

```{r}
#model 4
housing_price_new_m4<-lm(P~S+New,data = house.selling.price.2)
summary(housing_price_new_m4)
glance(housing_price_new_m4)
```
R^2 for Model_4 is 0.8484.

When we compare all four models for R^2, we observe that Model_1 with the highest R^2 of 0.8689 is the best model in this case.
When we compare the adjusted R2 of all four models, we observe that Model_2 with the highest adjusted R2 of 0.8637 is the best model in this scenario.

```{r}
PRESS <- function(all_housing_price_m1) {
    i <- residuals(all_housing_price_m1)/(1 - lm.influence(all_housing_price_m1)$hat)
    sum(i^2)
}

PRESS(all_housing_price_m1)        #model 1 - all variables
PRESS(housing_price_nobed_m2)      #model 2 - excludes variable Bed (includes size)
PRESS(housing_price_nobed_noba_m3) #model 3 - excludes variable Bed and Bath (includes size)
PRESS(housing_price_new_m4)        #model 4 - excludes every variable except New (inc.size)
```
Given many regression models, the one with the lowest PRESS should be chosen as the best performer. As a result, Model_3 with a PRESS of 27690.82 that removes variables Bed and Bath, as well as Model_2 with a PRESS of 27860.05 that excludes the variable Bed, should be evaluated.

AIC

Comparing all four models for AIC, and because the aim is to identify the model with the lowest AIC, we notice that Model_3 performs best at 765.8876 with variables Bed and Bath excluded. Model_2 omitting the variable Bed would be a realistic choice at 789.1366 since it is improbable that a house will be built without bedrooms or bathrooms.

```{r}
#AIC for all 4 models
AIC(all_housing_price_m1)           #model 1
AIC(housing_price_nobed_m2)         #model 2
AIC(housing_price_nobed_noba_m3)    #model 3
AIC(housing_price_new_m4)           #model 4
```

```{r}
#BIC for all 4 models
BIC(all_housing_price_m1)           #model 1
BIC(housing_price_nobed_m2)         #model 2
BIC(housing_price_nobed_noba_m3)    #model 3
BIC(housing_price_new_m4)           #model 4
```
Comparing all four models, as we did with AIC, we notice that BIC model_3 has the lowest BIC, which excludes variables Bed and Bath while including variables Size and New, at 778.3866, compared to Model_2, which includes variables Size,New, and Bath while omitting Bed and has a BIC of 801.7996.

### E

Model_2, which eliminated the Bed variable, was my favorite of all the models tested for best fit. It regularly delivered high predictive values with low AIC, BIC, and PRESS. Despite having lower AIC,BIC, and PRESS values, Model_3 had a lower adjusted R2 of 0.8607 compared to Model_2, which had a higher adjusted R2 of 0.8637.

## Question 2

### A

```{r}
data("trees")
head(trees,10)
summary(trees)
```
We rename the variable Girth to Diameter since in the instructions it says,that Girth is labelled erroneously.

```{r}
library(dplyr)
trees_d<-trees %>% rename(Diameter=Girth)
head(trees_d)
```

The model is now fitted using Volume as the outcome and Diameter and Height as explanatory factors.

```{r}
#Fitting a multiple regression model with the Volume as outcome
Tree_Vol<-lm(Volume~Diameter+Height,data=trees_d)
summary(Tree_Vol)
```

Diameter and Height both have tiny p-values, indicating statistical significance. This makes reasonable because they forecast tree volume. It is also worth noting that the model exhibits nearly perfect linear correlation, since the R2 is close to 1.00 at 0.948. At 0.9442, the corrected R2 is likewise near to 1.00.

### B

```{r}
par(mfrow = c(2,2))
plot(Tree_Vol,pch=18,col="blue",which = 1:6)
```

Fitted vs. Residuals

This illustration may be used to identify whether or not the residuals have non-linear patterns. If the red line across the center of the plot is nearly horizontal, the residuals are expected to follow a linear pattern. In the case of the trees, we can see that the red line deviates from a perfect horizontal line, but not much.The residuals appear to follow an approximately non-linear pattern, indicating that a non-linear model would be acceptable for this dataset, and so the regression assumptions are broken.

Normal Q-Q 

This figure is used to assess if the regression model's residuals are regularly distributed. We may infer the residuals are normally distributed if the points in this plot lie nearly along a straight diagonal line. The points in this data lie nearly along a straight diagonal line. A handful of the observations vary somewhat from the line towards the top, but not significantly enough to proclaim the residuals to be non-normally distributed.

Scale-Location

This figure verifies the assumption of equal variance, often known as "homoscedasticity" among the residuals in our regression model. The assumption of equal variance is likely satisfied if the red line is approximately horizontal across the plot. In this scenario, the red line is not horizontal across the plot, implying that equal variance may be broken.

The Cook's Distance

The Cook's Distance plot estimates the impact of a data item. It considers both the leverage and residue of each observation. Cook's Distance is a calculation that summarizes how much a regression model changes when the ith observation is eliminated.In this scenario, the Cook's Distance for each of the n=31 cases is considered. The observations with the highest/most influential Cooks Distances are 31 (0.605), 18 (.177), and 3 (.167).This is demonstrated below with the Cook's Distance function(). As a result, we can observe that certain observations in the data set are extremely impactful.

Using the Cook's Distance() tool to determine the influential observations:

```{r}
cooks_D <- cooks.distance(Tree_Vol)
influential <- cooks_D[(cooks_D > (3 * mean(cooks_D, na.rm = TRUE)))]
influential
```

Leverage vs. Residuals

This graphic is intended to highlight influential observations or those with a high level of leverage. If any of the spots in this plot are outside of Cook's distance (the dashed lines), it is a significant observation. Influence is evident in this case, as evidenced by highly leveraged observations.

Leverage versus Cooks Distance

This map, like the Cook's Distance plot, is suggestive of very influential data points. There are numerous extremely important data points in this plot, as well as in the Cook's Distance plot, particularly data points 3, 18, and 31.

## Question 3

### A

```{r}
data(florida)
head(florida,10)
summary(florida)
str(florida)

florida_vote<-(lm(Buchanan~Bush,data = florida))
summary(florida_vote)
```

We first observe that the Bush votes are statistically significant, with a p-value of 1.73e-08. I also see that the R^2 and modified R^2 are low, indicating a less-than-ideal fit.

```{r}
library(alr4)
data(florida)
fl_voting_model <- lm(Buchanan~Bush,data=florida)
par(mfrow=c(2,3))
plot(fl_voting_model,which=1:6)
```

Palm Beach County is an obvious outlier. Next, let us see if, with a better bottle, Palm Beach County will be less of an outlier.

### B

```{r}
data(florida)
fl_voting_model_log <- lm(log(Buchanan)~log(Bush),data=florida)
par(mfrow=c(2,3))
plot(fl_voting_model_log,which=1:6)
```

Here palm beach county has become a less of an outlier.