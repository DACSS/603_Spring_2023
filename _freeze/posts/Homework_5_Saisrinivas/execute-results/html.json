{
  "hash": "13fc41d9de0f3882587f924f2435bcb0",
  "result": {
    "markdown": "---\ntitle: \"Homework_5_Saisrinivas_Ambatipudi\"\nauthor: \"Saisrinivas Ambatipudi\"\ndate: \"2023-05-08\"\noutput:\n  word_document: default\n  html_document: default\n---\n\n\n\n\n## Question 1\n\n### A\n\nBackward elimination begins with a full model and gradually removes variables that do not add substantially to the model. As a result, because the variable Be/Beds has the highest p-value of 0.487, it is excluded first. We use the step() function to confirm this is correct.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(house.selling.price.2)\nfit_Backward<-lm(P~.-Be,data = house.selling.price.2)\nsummary(fit_Backward)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = P ~ . - Be, data = house.selling.price.2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-34.804  -9.496   0.917   7.931  73.338 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -47.992      8.209  -5.847 8.15e-08 ***\nS             62.263      4.335  14.363  < 2e-16 ***\nBa            20.072      5.495   3.653 0.000438 ***\nNew           18.371      3.761   4.885 4.54e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 16.31 on 89 degrees of freedom\nMultiple R-squared:  0.8681,\tAdjusted R-squared:  0.8637 \nF-statistic: 195.3 on 3 and 89 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n\nEvery variable in the model has a p-value of 0.05, indicating that it is statistically significant. With the elimination of the Be/Beds variable, the relevance of the remaining variables, S, Ba, and New, has increased.\n\nFor verification, we can use the stats package's step() function for backward stepwise prediction, which creates a regression model that incorporates all statistically significant predictor variables S, Ba, and New that are connected to the response variable.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstep(object = fit_Backward,direction = \"backward\",scope = \n       fit_Backward) #backward eliminate\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nStart:  AIC=523.21\nP ~ (S + Be + Ba + New) - Be\n\n       Df Sum of Sq   RSS    AIC\n<none>              23684 523.21\n- Ba    1      3550 27234 534.20\n- New   1      6349 30033 543.30\n- S     1     54898 78582 632.75\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = P ~ (S + Be + Ba + New) - Be, data = house.selling.price.2)\n\nCoefficients:\n(Intercept)            S           Ba          New  \n     -47.99        62.26        20.07        18.37  \n```\n:::\n:::\n\n### B\n\nWe begin with a model and gradually add variables, beginning with the most statistically significant.In this situation, the New variable with a p-value of 4.3e-06 would be added.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_Forward<-lm(P~1,data = house.selling.price.2)\nsummary(fit_Forward)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = P ~ 1, data = house.selling.price.2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-82.033 -26.633  -3.533  15.467 209.867 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   99.533      4.582   21.72   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 44.18 on 92 degrees of freedom\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nstep_forward<-step(object=fit_Forward,direction = \"forward\",\n                      scope = P~S+Be+Ba+New)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nStart:  AIC=705.63\nP ~ 1\n\n       Df Sum of Sq    RSS    AIC\n+ S     1    145097  34508 554.22\n+ Ba    1     91484  88121 641.41\n+ Be    1     62578 117028 667.79\n+ New   1     22833 156772 694.99\n<none>              179606 705.63\n\nStep:  AIC=554.22\nP ~ S\n\n       Df Sum of Sq   RSS    AIC\n+ New   1    7274.7 27234 534.20\n+ Ba    1    4475.6 30033 543.30\n<none>              34508 554.22\n+ Be    1      40.4 34468 556.11\n\nStep:  AIC=534.2\nP ~ S + New\n\n       Df Sum of Sq   RSS    AIC\n+ Ba    1    3550.1 23684 523.21\n+ Be    1     588.8 26645 534.17\n<none>              27234 534.20\n\nStep:  AIC=523.21\nP ~ S + New + Ba\n\n       Df Sum of Sq   RSS    AIC\n<none>              23684 523.21\n+ Be    1    130.55 23553 524.70\n```\n:::\n\n```{.r .cell-code}\nstep_forward\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = P ~ S + New + Ba, data = house.selling.price.2)\n\nCoefficients:\n(Intercept)            S          New           Ba  \n     -47.99        62.26        18.37        20.07  \n```\n:::\n:::\n\nNotably, the coefficients for both backward and forward choices are the same.\n\n### C\n\nBEDS has a huge p-value, indicating that it is statistically insignificant, and so we fail to reject the null hypothesis H0. Furthermore, it appears that BEDS is tightly related to the variable Size. This makes logical since the more bedrooms a house has, the larger it is. As a result, BEDS may already be tied to Size. Also worth noting is the tiny sample size (n=93), which may have contributed to the high p-value and significant association.\n\n### D\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhousing_price<-lm(P~1,data = house.selling.price.2)\nall_housing_price_m1<-lm(P~.,data = house.selling.price.2)\nstepwise_housing<-step(object=housing_price,direction = \"both\", scope = formula(all_housing_price_m1))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nStart:  AIC=705.63\nP ~ 1\n\n       Df Sum of Sq    RSS    AIC\n+ S     1    145097  34508 554.22\n+ Ba    1     91484  88121 641.41\n+ Be    1     62578 117028 667.79\n+ New   1     22833 156772 694.99\n<none>              179606 705.63\n\nStep:  AIC=554.22\nP ~ S\n\n       Df Sum of Sq    RSS    AIC\n+ New   1      7275  27234 534.20\n+ Ba    1      4476  30033 543.30\n<none>               34508 554.22\n+ Be    1        40  34468 556.11\n- S     1    145097 179606 705.63\n\nStep:  AIC=534.2\nP ~ S + New\n\n       Df Sum of Sq    RSS    AIC\n+ Ba    1      3550  23684 523.21\n+ Be    1       589  26645 534.17\n<none>               27234 534.20\n- New   1      7275  34508 554.22\n- S     1    129539 156772 694.99\n\nStep:  AIC=523.21\nP ~ S + New + Ba\n\n       Df Sum of Sq   RSS    AIC\n<none>              23684 523.21\n+ Be    1       131 23553 524.70\n- Ba    1      3550 27234 534.20\n- New   1      6349 30033 543.30\n- S     1     54898 78582 632.75\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(stepwise_housing)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = P ~ S + New + Ba, data = house.selling.price.2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-34.804  -9.496   0.917   7.931  73.338 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -47.992      8.209  -5.847 8.15e-08 ***\nS             62.263      4.335  14.363  < 2e-16 ***\nNew           18.371      3.761   4.885 4.54e-06 ***\nBa            20.072      5.495   3.653 0.000438 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 16.31 on 89 degrees of freedom\nMultiple R-squared:  0.8681,\tAdjusted R-squared:  0.8637 \nF-statistic: 195.3 on 3 and 89 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\nThe model's most helpful variables may be found via stepwise regression. We begin with the first model, which includes all of the variables in the housing data. The Broom package is used to extract more succinct information from an item. We will utilize the glance function in the Broom package.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#model 1\nall_housing_price_m1<-lm(P~.,data = house.selling.price.2)\nsummary(all_housing_price_m1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = P ~ ., data = house.selling.price.2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-36.212  -9.546   1.277   9.406  71.953 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -41.795     12.104  -3.453 0.000855 ***\nS             64.761      5.630  11.504  < 2e-16 ***\nBe            -2.766      3.960  -0.698 0.486763    \nBa            19.203      5.650   3.399 0.001019 ** \nNew           18.984      3.873   4.902  4.3e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 16.36 on 88 degrees of freedom\nMultiple R-squared:  0.8689,\tAdjusted R-squared:  0.8629 \nF-statistic: 145.8 on 4 and 88 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(broom)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: package 'broom' was built under R version 4.2.3\n```\n:::\n\n```{.r .cell-code}\nglance(all_housing_price_m1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 12\n  r.squared adj.r.squa…¹ sigma stati…²  p.value    df logLik   AIC   BIC devia…³\n      <dbl>        <dbl> <dbl>   <dbl>    <dbl> <dbl>  <dbl> <dbl> <dbl>   <dbl>\n1     0.869        0.863  16.4    146. 5.94e-38     4  -389.  791.  806.  23553.\n# … with 2 more variables: df.residual <int>, nobs <int>, and abbreviated\n#   variable names ¹​adj.r.squared, ²​statistic, ³​deviance\n```\n:::\n:::\n\n\nOur R^2 for Model_1 with all predictors is 0.8689.\n\nThe Beds variable is eliminated from Model_2 since we know it is tightly connected to size and has a high p-value, rendering it statistically unimportant.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#model 2\nhousing_price_nobed_m2<-lm(P~.-Be,data = house.selling.price.2)\nsummary(housing_price_nobed_m2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = P ~ . - Be, data = house.selling.price.2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-34.804  -9.496   0.917   7.931  73.338 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -47.992      8.209  -5.847 8.15e-08 ***\nS             62.263      4.335  14.363  < 2e-16 ***\nBa            20.072      5.495   3.653 0.000438 ***\nNew           18.371      3.761   4.885 4.54e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 16.31 on 89 degrees of freedom\nMultiple R-squared:  0.8681,\tAdjusted R-squared:  0.8637 \nF-statistic: 195.3 on 3 and 89 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nglance(housing_price_nobed_m2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 12\n  r.squared adj.r.squa…¹ sigma stati…²  p.value    df logLik   AIC   BIC devia…³\n      <dbl>        <dbl> <dbl>   <dbl>    <dbl> <dbl>  <dbl> <dbl> <dbl>   <dbl>\n1     0.868        0.864  16.3    195. 4.96e-39     3  -390.  789.  802.  23684.\n# … with 2 more variables: df.residual <int>, nobs <int>, and abbreviated\n#   variable names ¹​adj.r.squared, ²​statistic, ³​deviance\n```\n:::\n:::\n\n\nR^2 for the Model_2 is 0.8681.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#model 3\nhousing_price_nobed_noba_m3<-lm(P~.-Be,-Ba,data = house.selling.price.2)\nsummary(housing_price_nobed_noba_m3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = P ~ . - Be, data = house.selling.price.2, subset = -Ba)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-34.787  -9.785   0.897   8.108  73.084 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -49.016      8.588  -5.707 1.60e-07 ***\nS             61.863      4.473  13.832  < 2e-16 ***\nBa            20.996      5.760   3.645 0.000457 ***\nNew           18.197      3.821   4.763 7.68e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 16.5 on 86 degrees of freedom\nMultiple R-squared:  0.8654,\tAdjusted R-squared:  0.8607 \nF-statistic: 184.3 on 3 and 86 DF,  p-value: < 2.2e-16\n```\n:::\n\n```{.r .cell-code}\nglance(housing_price_nobed_noba_m3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 12\n  r.squared adj.r.squa…¹ sigma stati…²  p.value    df logLik   AIC   BIC devia…³\n      <dbl>        <dbl> <dbl>   <dbl>    <dbl> <dbl>  <dbl> <dbl> <dbl>   <dbl>\n1     0.865        0.861  16.5    184. 2.48e-37     3  -378.  766.  778.  23405.\n# … with 2 more variables: df.residual <int>, nobs <int>, and abbreviated\n#   variable names ¹​adj.r.squared, ²​statistic, ³​deviance\n```\n:::\n:::\n\nWe see the R^2 is 0.8654.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#model 4\nhousing_price_new_m4<-lm(P~S+New,data = house.selling.price.2)\nsummary(housing_price_new_m4)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = P ~ S + New, data = house.selling.price.2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-47.207  -9.763  -0.091   9.984  76.405 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -26.089      5.977  -4.365 3.39e-05 ***\nS             72.575      3.508  20.690  < 2e-16 ***\nNew           19.587      3.995   4.903 4.16e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 17.4 on 90 degrees of freedom\nMultiple R-squared:  0.8484,\tAdjusted R-squared:  0.845 \nF-statistic: 251.8 on 2 and 90 DF,  p-value: < 2.2e-16\n```\n:::\n\n```{.r .cell-code}\nglance(housing_price_new_m4)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 12\n  r.squared adj.r.squa…¹ sigma stati…²  p.value    df logLik   AIC   BIC devia…³\n      <dbl>        <dbl> <dbl>   <dbl>    <dbl> <dbl>  <dbl> <dbl> <dbl>   <dbl>\n1     0.848        0.845  17.4    252. 1.37e-37     2  -396.  800.  810.  27234.\n# … with 2 more variables: df.residual <int>, nobs <int>, and abbreviated\n#   variable names ¹​adj.r.squared, ²​statistic, ³​deviance\n```\n:::\n:::\n\nR^2 for Model_4 is 0.8484.\n\nWhen we compare all four models for R^2, we observe that Model_1 with the highest R^2 of 0.8689 is the best model in this case.\nWhen we compare the adjusted R2 of all four models, we observe that Model_2 with the highest adjusted R2 of 0.8637 is the best model in this scenario.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nPRESS <- function(all_housing_price_m1) {\n    i <- residuals(all_housing_price_m1)/(1 - lm.influence(all_housing_price_m1)$hat)\n    sum(i^2)\n}\n\nPRESS(all_housing_price_m1)        #model 1 - all variables\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 28390.22\n```\n:::\n\n```{.r .cell-code}\nPRESS(housing_price_nobed_m2)      #model 2 - excludes variable Bed (includes size)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 27860.05\n```\n:::\n\n```{.r .cell-code}\nPRESS(housing_price_nobed_noba_m3) #model 3 - excludes variable Bed and Bath (includes size)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 27690.82\n```\n:::\n\n```{.r .cell-code}\nPRESS(housing_price_new_m4)        #model 4 - excludes every variable except New (inc.size)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 31066\n```\n:::\n:::\n\nGiven many regression models, the one with the lowest PRESS should be chosen as the best performer. As a result, Model_3 with a PRESS of 27690.82 that removes variables Bed and Bath, as well as Model_2 with a PRESS of 27860.05 that excludes the variable Bed, should be evaluated.\n\nAIC\n\nComparing all four models for AIC, and because the aim is to identify the model with the lowest AIC, we notice that Model_3 performs best at 765.8876 with variables Bed and Bath excluded. Model_2 omitting the variable Bed would be a realistic choice at 789.1366 since it is improbable that a house will be built without bedrooms or bathrooms.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#AIC for all 4 models\nAIC(all_housing_price_m1)           #model 1\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 790.6225\n```\n:::\n\n```{.r .cell-code}\nAIC(housing_price_nobed_m2)         #model 2\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 789.1366\n```\n:::\n\n```{.r .cell-code}\nAIC(housing_price_nobed_noba_m3)    #model 3\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 765.8876\n```\n:::\n\n```{.r .cell-code}\nAIC(housing_price_new_m4)           #model 4\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 800.1262\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n#BIC for all 4 models\nBIC(all_housing_price_m1)           #model 1\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 805.8181\n```\n:::\n\n```{.r .cell-code}\nBIC(housing_price_nobed_m2)         #model 2\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 801.7996\n```\n:::\n\n```{.r .cell-code}\nBIC(housing_price_nobed_noba_m3)    #model 3\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 778.3866\n```\n:::\n\n```{.r .cell-code}\nBIC(housing_price_new_m4)           #model 4\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 810.2566\n```\n:::\n:::\n\nComparing all four models, as we did with AIC, we notice that BIC model_3 has the lowest BIC, which excludes variables Bed and Bath while including variables Size and New, at 778.3866, compared to Model_2, which includes variables Size,New, and Bath while omitting Bed and has a BIC of 801.7996.\n\n### E\n\nModel_2, which eliminated the Bed variable, was my favorite of all the models tested for best fit. It regularly delivered high predictive values with low AIC, BIC, and PRESS. Despite having lower AIC,BIC, and PRESS values, Model_3 had a lower adjusted R2 of 0.8607 compared to Model_2, which had a higher adjusted R2 of 0.8637.\n\n## Question 2\n\n### A\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(\"trees\")\nhead(trees,10)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   Girth Height Volume\n1    8.3     70   10.3\n2    8.6     65   10.3\n3    8.8     63   10.2\n4   10.5     72   16.4\n5   10.7     81   18.8\n6   10.8     83   19.7\n7   11.0     66   15.6\n8   11.0     75   18.2\n9   11.1     80   22.6\n10  11.2     75   19.9\n```\n:::\n\n```{.r .cell-code}\nsummary(trees)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     Girth           Height       Volume     \n Min.   : 8.30   Min.   :63   Min.   :10.20  \n 1st Qu.:11.05   1st Qu.:72   1st Qu.:19.40  \n Median :12.90   Median :76   Median :24.20  \n Mean   :13.25   Mean   :76   Mean   :30.17  \n 3rd Qu.:15.25   3rd Qu.:80   3rd Qu.:37.30  \n Max.   :20.60   Max.   :87   Max.   :77.00  \n```\n:::\n:::\n\nWe rename the variable Girth to Diameter since in the instructions it says,that Girth is labelled erroneously.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(dplyr)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'dplyr'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following object is masked from 'package:car':\n\n    recode\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n```\n:::\n\n```{.r .cell-code}\ntrees_d<-trees %>% rename(Diameter=Girth)\nhead(trees_d)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  Diameter Height Volume\n1      8.3     70   10.3\n2      8.6     65   10.3\n3      8.8     63   10.2\n4     10.5     72   16.4\n5     10.7     81   18.8\n6     10.8     83   19.7\n```\n:::\n:::\n\n\nThe model is now fitted using Volume as the outcome and Diameter and Height as explanatory factors.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#Fitting a multiple regression model with the Volume as outcome\nTree_Vol<-lm(Volume~Diameter+Height,data=trees_d)\nsummary(Tree_Vol)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = Volume ~ Diameter + Height, data = trees_d)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.4065 -2.6493 -0.2876  2.2003  8.4847 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -57.9877     8.6382  -6.713 2.75e-07 ***\nDiameter      4.7082     0.2643  17.816  < 2e-16 ***\nHeight        0.3393     0.1302   2.607   0.0145 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.882 on 28 degrees of freedom\nMultiple R-squared:  0.948,\tAdjusted R-squared:  0.9442 \nF-statistic:   255 on 2 and 28 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n\nDiameter and Height both have tiny p-values, indicating statistical significance. This makes reasonable because they forecast tree volume. It is also worth noting that the model exhibits nearly perfect linear correlation, since the R2 is close to 1.00 at 0.948. At 0.9442, the corrected R2 is likewise near to 1.00.\n\n### B\n\n\n::: {.cell}\n\n```{.r .cell-code}\npar(mfrow = c(2,2))\nplot(Tree_Vol,pch=18,col=\"blue\",which = 1:6)\n```\n\n::: {.cell-output-display}\n![](Homework_5_Saisrinivas_files/figure-html/unnamed-chunk-19-1.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](Homework_5_Saisrinivas_files/figure-html/unnamed-chunk-19-2.png){width=672}\n:::\n:::\n\n\nFitted vs. Residuals\n\nThis illustration may be used to identify whether or not the residuals have non-linear patterns. If the red line across the center of the plot is nearly horizontal, the residuals are expected to follow a linear pattern. In the case of the trees, we can see that the red line deviates from a perfect horizontal line, but not much.The residuals appear to follow an approximately non-linear pattern, indicating that a non-linear model would be acceptable for this dataset, and so the regression assumptions are broken.\n\nNormal Q-Q \n\nThis figure is used to assess if the regression model's residuals are regularly distributed. We may infer the residuals are normally distributed if the points in this plot lie nearly along a straight diagonal line. The points in this data lie nearly along a straight diagonal line. A handful of the observations vary somewhat from the line towards the top, but not significantly enough to proclaim the residuals to be non-normally distributed.\n\nScale-Location\n\nThis figure verifies the assumption of equal variance, often known as \"homoscedasticity\" among the residuals in our regression model. The assumption of equal variance is likely satisfied if the red line is approximately horizontal across the plot. In this scenario, the red line is not horizontal across the plot, implying that equal variance may be broken.\n\nThe Cook's Distance\n\nThe Cook's Distance plot estimates the impact of a data item. It considers both the leverage and residue of each observation. Cook's Distance is a calculation that summarizes how much a regression model changes when the ith observation is eliminated.In this scenario, the Cook's Distance for each of the n=31 cases is considered. The observations with the highest/most influential Cooks Distances are 31 (0.605), 18 (.177), and 3 (.167).This is demonstrated below with the Cook's Distance function(). As a result, we can observe that certain observations in the data set are extremely impactful.\n\nUsing the Cook's Distance() tool to determine the influential observations:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncooks_D <- cooks.distance(Tree_Vol)\ninfluential <- cooks_D[(cooks_D > (3 * mean(cooks_D, na.rm = TRUE)))]\ninfluential\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n        3        18        31 \n0.1673192 0.1775359 0.6052326 \n```\n:::\n:::\n\n\nLeverage vs. Residuals\n\nThis graphic is intended to highlight influential observations or those with a high level of leverage. If any of the spots in this plot are outside of Cook's distance (the dashed lines), it is a significant observation. Influence is evident in this case, as evidenced by highly leveraged observations.\n\nLeverage versus Cooks Distance\n\nThis map, like the Cook's Distance plot, is suggestive of very influential data points. There are numerous extremely important data points in this plot, as well as in the Cook's Distance plot, particularly data points 3, 18, and 31.\n\n## Question 3\n\n### A\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(florida)\nhead(florida,10)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n            Gore   Bush Buchanan\nALACHUA    47300  34062      262\nBAKER       2392   5610       73\nBAY        18850  38637      248\nBRADFORD    3072   5413       65\nBREVARD    97318 115185      570\nBROWARD   386518 177279      789\nCALHOUN     2155   2873       90\nCHARLOTTE  29641  35419      182\nCITRUS     25501  29744      270\nCLAY       14630  41745      186\n```\n:::\n\n```{.r .cell-code}\nsummary(florida)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      Gore             Bush           Buchanan     \n Min.   :   788   Min.   :  1316   Min.   :   9.0  \n 1st Qu.:  3055   1st Qu.:  4746   1st Qu.:  46.5  \n Median : 14152   Median : 20196   Median : 114.0  \n Mean   : 43341   Mean   : 43356   Mean   : 258.5  \n 3rd Qu.: 45974   3rd Qu.: 56542   3rd Qu.: 285.5  \n Max.   :386518   Max.   :289456   Max.   :3407.0  \n```\n:::\n\n```{.r .cell-code}\nstr(florida)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n'data.frame':\t67 obs. of  3 variables:\n $ Gore    : int  47300 2392 18850 3072 97318 386518 2155 29641 25501 14630 ...\n $ Bush    : int  34062 5610 38637 5413 115185 177279 2873 35419 29744 41745 ...\n $ Buchanan: int  262 73 248 65 570 789 90 182 270 186 ...\n```\n:::\n\n```{.r .cell-code}\nflorida_vote<-(lm(Buchanan~Bush,data = florida))\nsummary(florida_vote)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = Buchanan ~ Bush, data = florida)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-907.50  -46.10  -29.19   12.26 2610.19 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 4.529e+01  5.448e+01   0.831    0.409    \nBush        4.917e-03  7.644e-04   6.432 1.73e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 353.9 on 65 degrees of freedom\nMultiple R-squared:  0.3889,\tAdjusted R-squared:  0.3795 \nF-statistic: 41.37 on 1 and 65 DF,  p-value: 1.727e-08\n```\n:::\n:::\n\n\nWe first observe that the Bush votes are statistically significant, with a p-value of 1.73e-08. I also see that the R^2 and modified R^2 are low, indicating a less-than-ideal fit.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(alr4)\ndata(florida)\nfl_voting_model <- lm(Buchanan~Bush,data=florida)\npar(mfrow=c(2,3))\nplot(fl_voting_model,which=1:6)\n```\n\n::: {.cell-output-display}\n![](Homework_5_Saisrinivas_files/figure-html/unnamed-chunk-22-1.png){width=672}\n:::\n:::\n\n\nPalm Beach County is an obvious outlier. Next, let us see if, with a better bottle, Palm Beach County will be less of an outlier.\n\n### B\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(florida)\nfl_voting_model_log <- lm(log(Buchanan)~log(Bush),data=florida)\npar(mfrow=c(2,3))\nplot(fl_voting_model_log,which=1:6)\n```\n\n::: {.cell-output-display}\n![](Homework_5_Saisrinivas_files/figure-html/unnamed-chunk-23-1.png){width=672}\n:::\n:::\n\n\nHere palm beach county has become a less of an outlier.",
    "supporting": [
      "Homework_5_Saisrinivas_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}