{
  "hash": "39b28581888d0a9a8fd4d96f4b515aea",
  "result": {
    "markdown": "---\ntitle: \"Homework 5\"\nauthor: \"Felix Betanourt\"\ndesription: \"DACSS 603 HW5\"\ndate: \"05/10/2023\"\nformat:\n  html:\n    toc: true\n    code-fold: true\n    code-copy: true\n    code-tools: true\ncategories:\n  - hw5\n  - multiple regression\n  - simple regression\n  - correlation\n  - regression assumptions\n  - p value\n  - model fit\neditor: \n  markdown: \n    wrap: 72\n---\n\n::: {.cell}\n\n```{.r .cell-code}\nknitr::opts_chunk$set(echo = TRUE, warning = FALSE)\n```\n:::\n\n\n## Homework 5\n\nDACSS 603, Spring 2023\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Loading packages\n\nsuppressPackageStartupMessages(library(dplyr))\nsuppressPackageStartupMessages(library(tidyverse))\nlibrary(formattable)\nsuppressPackageStartupMessages(library(kableExtra))\nlibrary(ggplot2)\nsuppressPackageStartupMessages(library(alr4))\nsuppressPackageStartupMessages(library(smss))\nsuppressPackageStartupMessages(library(broom))\n```\n:::\n\n\nQuestion 1\n(Data file: house.selling.price.2 from smss R package)\nFor the house.selling.price.2 data the tables below show a correlation matrix and a model fit using four predictors of selling price.\n\n(Hint 1: You should be able to answer A, B, C just using the tables below, although you should \nfeel free to load the data in R and work with it if you so choose. They will be consistent with what you see on the tables.\n\n(Hint 2: The p-value of a variable in a simple linear regression is the same p-value one would get from a Pearson’s correlation (cor.test). The p-value is a function of the magnitude of the correlation coefficient (the higher the coefficient, the lower the p-value) and of sample size (larger samples lead to smaller p-values). For the correlations shown in the tables, they are between variables of the same length.)\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(\"house.selling.price.2\")\nhouse <- house.selling.price.2\nhouse <- rename(house, price = P\n         , size = S\n       , beds = Be\n       , baths = Ba\n       )\nstr(house)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n'data.frame':\t93 obs. of  5 variables:\n $ price: num  48.5 55 68 137 309.4 ...\n $ size : num  1.1 1.01 1.45 2.4 3.3 0.4 1.28 0.74 0.78 0.97 ...\n $ beds : int  3 3 3 3 4 1 3 3 2 3 ...\n $ baths: int  1 2 2 3 3 1 1 1 1 1 ...\n $ New  : int  0 0 0 0 1 0 0 0 0 0 ...\n```\n:::\n:::\n\n\nCorrelation Matrix:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Cor Matrix\ncor_matrix <- cor(house[, c(\"price\", \"size\", \"beds\", \"baths\", \"New\")], use = \"complete.obs\")\nround(cor_matrix, 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      price size beds baths  New\nprice  1.00 0.90 0.59  0.71 0.36\nsize   0.90 1.00 0.67  0.66 0.18\nbeds   0.59 0.67 1.00  0.33 0.27\nbaths  0.71 0.66 0.33  1.00 0.18\nNew    0.36 0.18 0.27  0.18 1.00\n```\n:::\n:::\n\nRegression model:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Fit a multiple regression model with predictor variables\nlm.model1 <- lm(price ~ size + beds + baths + New, data = house)\nsummary(lm.model1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = price ~ size + beds + baths + New, data = house)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-36.212  -9.546   1.277   9.406  71.953 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -41.795     12.104  -3.453 0.000855 ***\nsize          64.761      5.630  11.504  < 2e-16 ***\nbeds          -2.766      3.960  -0.698 0.486763    \nbaths         19.203      5.650   3.399 0.001019 ** \nNew           18.984      3.873   4.902  4.3e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 16.36 on 88 degrees of freedom\nMultiple R-squared:  0.8689,\tAdjusted R-squared:  0.8629 \nF-statistic: 145.8 on 4 and 88 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n\n\nWith these four predictors,\n\nA. For backward elimination, which variable would be deleted first? Why?\n\nNumber of bedrooms as is has the highest p-value.\n\n\nB. For forward selection, which variable would be added first? Why?\n\nSize as is has the lowest p-value.\n\nC. Why do you think that BEDS has such a large P-value in the multiple regression model, \neven though it has a substantial correlation with PRICE?\n\nWhen including other variables in the regression model, Beds lose predictability power for Price. Since Baths and Size are highly correlated to Price, they might be capturing part of the impact for Beds. In particular there is a high correlation between Size and Beds, so it might means multicollinearity.\n\nD. Using software with these four predictors, find the model that would be selected using each \ncriterion:\n\n1. R2\n2. Adjusted R2\n3. PRESS\n4. AIC\n5. BIC\n\nLet's compare Model 1 (above) with a model excluding Beds:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Fit a multiple regression model with predictor variables (-Beds)\nlm.model2 <- lm(price ~ size + baths + New, data = house)\nsummary(lm.model2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = price ~ size + baths + New, data = house)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-34.804  -9.496   0.917   7.931  73.338 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -47.992      8.209  -5.847 8.15e-08 ***\nsize          62.263      4.335  14.363  < 2e-16 ***\nbaths         20.072      5.495   3.653 0.000438 ***\nNew           18.371      3.761   4.885 4.54e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 16.31 on 89 degrees of freedom\nMultiple R-squared:  0.8681,\tAdjusted R-squared:  0.8637 \nF-statistic: 195.3 on 3 and 89 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n\n\nModel 1 (all variables):\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglance(lm.model1) %>%\n  dplyr::select(r.squared, adj.r.squared, AIC, BIC)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 4\n  r.squared adj.r.squared   AIC   BIC\n      <dbl>         <dbl> <dbl> <dbl>\n1     0.869         0.863  791.  806.\n```\n:::\n\n```{.r .cell-code}\n#PRESS\npr <- resid(lm.model1)/(1 - lm.influence(lm.model1)$hat)\nsum(pr^2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 28390.22\n```\n:::\n:::\n\n\nModel 2 (excluding Beds):\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglance(lm.model2) %>%\n  dplyr::select(r.squared, adj.r.squared, AIC, BIC)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 4\n  r.squared adj.r.squared   AIC   BIC\n      <dbl>         <dbl> <dbl> <dbl>\n1     0.868         0.864  789.  802.\n```\n:::\n\n```{.r .cell-code}\n#PRESS\npr2 <- resid(lm.model2)/(1 - lm.influence(lm.model2)$hat)\nsum(pr2^2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 27860.05\n```\n:::\n:::\n\n\nE. Explain which model you prefer and why.\n\nI prefer Model 2 it keeps the explanation power while simpler.\n\nQuestion 2\n(Data file: trees from base R)\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(\"trees\")\nstr(trees)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n'data.frame':\t31 obs. of  3 variables:\n $ Girth : num  8.3 8.6 8.8 10.5 10.7 10.8 11 11 11.1 11.2 ...\n $ Height: num  70 65 63 72 81 83 66 75 80 75 ...\n $ Volume: num  10.3 10.3 10.2 16.4 18.8 19.7 15.6 18.2 22.6 19.9 ...\n```\n:::\n:::\n\n\n\nFrom the documentation:\n\n“This data set provides measurements of the diameter, height and volume of timber in 31 felled \nblack cherry trees. Note that the diameter (in inches) is erroneously labeled Girth in the data. It is measured at 4 ft 6 in above the ground.”\n\nTree volume estimation is a big deal, especially in the lumber industry. Use the trees data to build a basic model of tree volume prediction. In particular, \n\nA. Fit a multiple regression model with the Volume as the outcome and Girth and Height as the explanatory variables \n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm.model3 <- lm(Volume ~ Girth + Height, data = trees)\nsummary(lm.model3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = Volume ~ Girth + Height, data = trees)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.4065 -2.6493 -0.2876  2.2003  8.4847 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -57.9877     8.6382  -6.713 2.75e-07 ***\nGirth         4.7082     0.2643  17.816  < 2e-16 ***\nHeight        0.3393     0.1302   2.607   0.0145 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.882 on 28 degrees of freedom\nMultiple R-squared:  0.948,\tAdjusted R-squared:  0.9442 \nF-statistic:   255 on 2 and 28 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n\n\nB. Run regression diagnostic plots on the model. Based on the plots, do you think any of the regression assumptions is violated?\n\n\n::: {.cell}\n\n```{.r .cell-code}\npar(mfrow = c(2,3))\nplot(lm.model3, which = 1:6)\n```\n\n::: {.cell-output-display}\n![](FelixBetancourt_HW5_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n\nYes, there is a violation of one assumption.There is Heteroskedasticity, the variance is not constant. We can see that red line with almost a funnel shape in the residual vs fitted graph and the trend line in the scale-location graph.\n\n\n\nQuestion 3\n\n(Data file: florida in alr R package)\n\nIn the 2000 election for U.S. president, the counting of votes in Florida was controversial. In Palm Beach County in south Florida, for example, voters used a so-called butterfly ballot. Some believe that the layout of the ballot caused some voters to cast votes for Buchanan when their intended choice was Gore.\n\nThe data has variables for the number of votes for each candidate—Gore, Bush, and Buchanan.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(\"florida\")\nflorida\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n               Gore   Bush Buchanan\nALACHUA       47300  34062      262\nBAKER          2392   5610       73\nBAY           18850  38637      248\nBRADFORD       3072   5413       65\nBREVARD       97318 115185      570\nBROWARD      386518 177279      789\nCALHOUN        2155   2873       90\nCHARLOTTE     29641  35419      182\nCITRUS        25501  29744      270\nCLAY          14630  41745      186\nCOLLIER       29905  60426      122\nCOLUMBIA       7047  10964       89\nDADE         328702 289456      561\nDE SOTO        3322   4256       36\nDIXIE          1825   2698       29\nDUVAL        107680 152082      650\nESCAMBIA      40958  73029      504\nFLAGLER       13891  12608       83\nFRANKLIN       2042   2448       33\nGADSDEN        9565   4750       39\nGILCHRIST      1910   3300       29\nGLADES         1420   1840        9\nGULF           2389   3546       71\nHAMILTON       1718   2153       24\nHARDEE         2341   3764       30\nHENDRY         3239   4743       22\nHERNANDO      32644  30646      242\nHIGHLANDS     14152  20196       99\nHILLSBOROUGH 166581 176967      836\nHOLMES         2154   4985       76\nINDIAN RIVER  19769  28627      105\nJACKSON        6868   9138      102\nJEFFERSON      3038   2481       29\nLAFAYETTE       788   1669       10\nLAKE          36555  49963      289\nLEE           73560 106141      305\nLEON          61425  39053      282\nLEVY           5403   6860       67\nLIBERTY        1011   1316       39\nMADISON        3011   3038       29\nMANATEE       49169  57948      272\nMARION        44648  55135      563\nMARTIN        26619  33864      108\nMONROE        16483  16059       47\nNASSAU         6952  16404       90\nOKALOOSA      16924  52043      267\nOKEECHOBEE     4588   5058       43\nORANGE       140115 134476      446\nOSCEOLA       28177  26216      145\nPALM BEACH   268945 152846     3407\nPASCO         69550  68581      570\nPINELLAS     199660 184312     1010\nPOLK          74977  90101      538\nPUTNAM        12091  13439      147\nST. JOHNS     19482  39497      229\nST. LUCIE     41559  34705      124\nSANTA ROSA    12795  36248      311\nSARASOTA      72854  83100      305\nSEMINOLE      58888  75293      194\nSUMTER         9634  12126      114\nSUWANNEE       4084   8014      108\nTAYLOR         2647   4051       27\nUNION          1399   2326       26\nVOLUSIA       97063  82214      396\nWAKULLA        3835   4511       46\nWALTON         5637  12176      120\nWASHINGTON     2796   4983       88\n```\n:::\n:::\n\n\nA. Run a simple linear regression model where the Buchanan vote is the outcome and the Bush vote is the explanatory variable. Produce the regression diagnostic plots.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm.model4 <- lm(Buchanan ~ Bush, data = florida)\nsummary(lm.model4)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = Buchanan ~ Bush, data = florida)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-907.50  -46.10  -29.19   12.26 2610.19 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 4.529e+01  5.448e+01   0.831    0.409    \nBush        4.917e-03  7.644e-04   6.432 1.73e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 353.9 on 65 degrees of freedom\nMultiple R-squared:  0.3889,\tAdjusted R-squared:  0.3795 \nF-statistic: 41.37 on 1 and 65 DF,  p-value: 1.727e-08\n```\n:::\n\n```{.r .cell-code}\npar(mfrow = c(2,3))\nplot(lm.model4, which = 1:6)\n```\n\n::: {.cell-output-display}\n![](FelixBetancourt_HW5_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n:::\n\n\nIs Palm Beach County an outlier based on the diagnostic plots? Why or why not?\n\nYes Palm Beach is an outliner. It stands out from the basic random pattern of residuals as we can see in the residual vs fitted plot. Also, in the Normal Q-Q plot, we can see how Palm Beach  falls far from the red line.\n\n\nB. Take the log of both variables (Bush vote and Buchanan Vote) and repeat the analysis in \n(A.) Does your findings change?\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm.model5 <- lm(log(Buchanan) ~ log(Bush), data = florida)\nsummary(lm.model5)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = log(Buchanan) ~ log(Bush), data = florida)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.96075 -0.25949  0.01282  0.23826  1.66564 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -2.57712    0.38919  -6.622 8.04e-09 ***\nlog(Bush)    0.75772    0.03936  19.251  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4673 on 65 degrees of freedom\nMultiple R-squared:  0.8508,\tAdjusted R-squared:  0.8485 \nF-statistic: 370.6 on 1 and 65 DF,  p-value: < 2.2e-16\n```\n:::\n\n```{.r .cell-code}\npar(mfrow = c(2,3))\nplot(lm.model5, which = 1:6)\n```\n\n::: {.cell-output-display}\n![](FelixBetancourt_HW5_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n:::\n\n\nYes, it make a correction of the model to make more precise but Palm Beach is still an outliner affecting the model along with other outliners like Calhoun and Collier.\n\n\n\n",
    "supporting": [
      "FelixBetancourt_HW5_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}