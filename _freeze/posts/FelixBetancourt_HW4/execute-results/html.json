{
  "hash": "95ffd7b64c360a32f11d51cdd787f318",
  "result": {
    "markdown": "---\ntitle: \"Homework 4\"\nauthor: \"Felix Betanourt\"\ndesription: \"DACSS 603 HW4\"\ndate: \"04/25/2023\"\nformat:\n  html:\n    toc: true\n    code-fold: true\n    code-copy: true\n    code-tools: true\ncategories:\n  - hw4\n  - simple regression\n  - multiple regression\n  - non-linear function\n  - p-value\n  - model fitting\neditor: \n  markdown: \n    wrap: 72\n---\n\n::: {.cell}\n\n```{.r .cell-code}\nknitr::opts_chunk$set(echo = TRUE, warning = FALSE)\n```\n:::\n\n\n## Homework 4\n\nDACSS 603, Spring 2023\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Loading packages\n\nsuppressPackageStartupMessages(library(dplyr))\nsuppressPackageStartupMessages(library(tidyverse))\nlibrary(formattable)\nsuppressPackageStartupMessages(library(kableExtra))\nlibrary(ggplot2)\nsuppressPackageStartupMessages(library(alr4))\nsuppressPackageStartupMessages(library(smss))\n```\n:::\n\n\n\nSome of the questions use data from the alr4 and smss R packages. You would need to call in those packages in R (no need for an  install.packages() call in your .qmd file, though—just use library()) and load the data using the data() function.\n\nQuestion 1\n\nFor recent data in Jacksonville, Florida, on y = selling price of home (in dollars), x1 = size of home (in square feet), and x2 = lot size (in square feet), the prediction equation is ŷ = −10,536 + 53.8x1 + 2.84x2.\n\nA. A particular home of 1240 square feet on a lot of 18,000 square feet sold for $145,000. Find the predicted selling price and the residual, and interpret.\n\nPredicted value:\n\n\n::: {.cell}\n\n```{.r .cell-code}\npredicted <- -10536 + 53.8*1240 + 2.84*18000\npredicted\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 107296\n```\n:::\n:::\n\n\nResidual:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nresidual <- 145000-107296\nresidual \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 37704\n```\n:::\n:::\n\n\nThe predicted value is $107,296 therefore the the residual is $37,704. The residual is positive, which means that the model is under-predicting. Would be appropriate to check with other actual values and adjusting the regression model for better fit.\n\n\nB. For fixed lot size, how much is the house selling price predicted to increase for each square foot increase in home size? Why?\n\nThe slope for lot size is 2.84, this means that for every square foot the price should increase by $2.84.\n\n\nC. According to this prediction equation, for fixed home size, how much would lot size need to increase to have the same impact as a one-square-foot increase in home size?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx2 <- (53.8/2.84)\nx2\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 18.94366\n```\n:::\n:::\n\n\nlot size should increase almost 19 times to have the same impact in the price as home size. \n\n\nQuestion 2\n\n(Data file: salary in alr4 R package). \n\n\n::: {.cell}\n\n```{.r .cell-code}\nstr(salary)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n'data.frame':\t52 obs. of  6 variables:\n $ degree: Factor w/ 2 levels \"Masters\",\"PhD\": 1 1 1 1 2 1 2 1 2 2 ...\n $ rank  : Factor w/ 3 levels \"Asst\",\"Assoc\",..: 3 3 3 3 3 3 3 3 3 3 ...\n $ sex   : Factor w/ 2 levels \"Male\",\"Female\": 1 1 1 2 1 1 2 1 1 1 ...\n $ year  : int  25 13 10 7 19 16 0 16 13 13 ...\n $ ysdeg : int  35 22 23 27 30 21 32 18 30 31 ...\n $ salary: int  36350 35350 28200 26775 33696 28516 24900 31909 31850 32850 ...\n```\n:::\n:::\n\n\nThe data file concerns salary and other characteristics of all faculty in a small Midwestern college collected in the early 1980s for presentation in legal proceedings for which discrimination against women in salary was at issue. \n\nAll persons in the data hold tenured or tenure track positions; temporary faculty are not included. The variables include degree, a factor with levels PhD and MS; rank, a factor with levels Asst, Assoc, and Prof; sex, a factor with levels Male and Female; Year, years in current rank; ysdeg, years since highest degree, and salary, academic year salary in dollars.\n\nA. Test the hypothesis that the mean salary for men and women is the same, without regard to any other variable but sex. Explain your findings.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nt.test(salary ~ sex, data = salary)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tWelch Two Sample t-test\n\ndata:  salary by sex\nt = 1.7744, df = 21.591, p-value = 0.09009\nalternative hypothesis: true difference in means between group Male and group Female is not equal to 0\n95 percent confidence interval:\n -567.8539 7247.1471\nsample estimates:\n  mean in group Male mean in group Female \n            24696.79             21357.14 \n```\n:::\n:::\n\nSince p-value > 0.05 , there is not significant difference between salary means for Males and Females.\n\n\nB. Run a multiple linear regression with salary as the outcome variable and everything else as predictors, including sex. Assuming no interactions between sex and the other predictors, obtain a 95% confidence interval for the difference in salary between males and females.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#salary2 <- salary%>%\n # mutate(degree_o = case_when(\n  #      degree == \"Masters\" ~ 0,\n  #       degree == \"PhD\" ~ 1,\n   #     )) %>%\n#  mutate(rank_o = case_when(\n #        rank == \"Asst\" ~ 1,\n  #       rank == \"Assoc\" ~ 2,\n   #      rank == \"Prof\" ~ 3,\n    #     )) %>%\n#  mutate(sex_o = case_when(\n #        sex == \"Male\" ~ 0,\n  #       sex == \"Female\" ~ 1,\n   #      ))\n\nsummary (model1 <- lm(salary ~ ysdeg + year + rank + sex + degree, data=salary))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = salary ~ ysdeg + year + rank + sex + degree, data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4045.2 -1094.7  -361.5   813.2  9193.1 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 15746.05     800.18  19.678  < 2e-16 ***\nysdeg        -124.57      77.49  -1.608    0.115    \nyear          476.31      94.91   5.018 8.65e-06 ***\nrankAssoc    5292.36    1145.40   4.621 3.22e-05 ***\nrankProf    11118.76    1351.77   8.225 1.62e-10 ***\nsexFemale    1166.37     925.57   1.260    0.214    \ndegreePhD    1388.61    1018.75   1.363    0.180    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2398 on 45 degrees of freedom\nMultiple R-squared:  0.855,\tAdjusted R-squared:  0.8357 \nF-statistic: 44.24 on 6 and 45 DF,  p-value: < 2.2e-16\n```\n:::\n\n```{.r .cell-code}\nt.test(salary ~ sex, data = salary, conf.int = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tWelch Two Sample t-test\n\ndata:  salary by sex\nt = 1.7744, df = 21.591, p-value = 0.09009\nalternative hypothesis: true difference in means between group Male and group Female is not equal to 0\n95 percent confidence interval:\n -567.8539 7247.1471\nsample estimates:\n  mean in group Male mean in group Female \n            24696.79             21357.14 \n```\n:::\n:::\n\n\nC. Interpret your finding for each predictor variable; discuss (a) statistical significance, (b) interpretation of the coefficient / slope in relation to the outcome variable and other variables.\n\na) The variables that predict salary are years in current rank, and rank, as those are variables with p-value<0.05.\n\nb) In terms of the coefficients:\n\n- Years in current rank: for each year in current rank the salary increases by 476 units.\n- Rank: the benchmark is assistant therefore by being associate there the salary is 5,292 higher than assistant and 11,118 higher for professor.\n- Gender/sex: females makes 1,166 more in salary than males. Not a significant variable to explain salary.\n- Degree: having a PhD means 1,388 more dollars than holding a Masters. Not a significant variable to explain salary.\n- Years since highest degree earned: in this case, more years means less salary, specifically 124 dollars less per year since highest degree was earned. Also, not a significant variable to explain salary.\n\n\nD. Change the baseline category for the rank variable. Interpret the coefficients related to rank again.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsalary$rank <- relevel(salary$rank, ref = \"Assoc\")\nsummary (model2 <- lm(salary ~ ysdeg + year + rank + sex + degree, data=salary))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = salary ~ ysdeg + year + rank + sex + degree, data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4045.2 -1094.7  -361.5   813.2  9193.1 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 21038.41    1109.12  18.969  < 2e-16 ***\nysdeg        -124.57      77.49  -1.608    0.115    \nyear          476.31      94.91   5.018 8.65e-06 ***\nrankAsst    -5292.36    1145.40  -4.621 3.22e-05 ***\nrankProf     5826.40    1012.93   5.752 7.28e-07 ***\nsexFemale    1166.37     925.57   1.260    0.214    \ndegreePhD    1388.61    1018.75   1.363    0.180    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2398 on 45 degrees of freedom\nMultiple R-squared:  0.855,\tAdjusted R-squared:  0.8357 \nF-statistic: 44.24 on 6 and 45 DF,  p-value: < 2.2e-16\n```\n:::\n\n```{.r .cell-code}\n#summary(salary$salary - model2$fitted.values)\n```\n:::\n\n\nBeing Assistant means 5,292 dollars less in salary compared to being Associate, and at the same time being Professor is 5,826 dollars more than Associate.\n\nE. Finkelstein (1980), in a discussion of the use of regression in discrimination cases, wrote, “[a] variable may reflect a position or status bestowed by the employer, in which case if there is discrimination in the award of the position or status, the variable may be ‘tainted.’ ” Thus, for example, if discrimination is at work in promotion of faculty to higher ranks, using rank to adjust salaries before comparing the sexes may not be acceptable to the courts.\nExclude the variable rank, refit, and summarize how your findings changed, if they did.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary (model3 <- lm(salary ~ ysdeg + year + sex + degree, data=salary))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = salary ~ ysdeg + year + sex + degree, data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8146.9 -2186.9  -491.5  2279.1 11186.6 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 17183.57    1147.94  14.969  < 2e-16 ***\nysdeg         339.40      80.62   4.210 0.000114 ***\nyear          351.97     142.48   2.470 0.017185 *  \nsexFemale   -1286.54    1313.09  -0.980 0.332209    \ndegreePhD   -3299.35    1302.52  -2.533 0.014704 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3744 on 47 degrees of freedom\nMultiple R-squared:  0.6312,\tAdjusted R-squared:  0.5998 \nF-statistic: 20.11 on 4 and 47 DF,  p-value: 1.048e-09\n```\n:::\n:::\n\n\nBy excluding rank, we have these changes in the model:\n\n- R squared is reduced from 0.85 to 0.63, this means that the rank has an relevant weight in explaining salary.\n\n- In this model years since highest degree earned has a significant relationship with salary, as well as Degree. Both variables were not significant in the model when rank is included.\n\n- Years in current rank reduces significance over salary, from a p-value < 0.001 to a p-value < 0.05.\n\n\nF. Everyone in this dataset was hired the year they earned their highest degree. It is also known that a new Dean was appointed 15 years ago, and everyone in the dataset who earned their highest degree 15 years ago or less than that has been hired by the new Dean. \n\nSome people have argued that the new Dean has been making offers that are a lot more generous to newly hired faculty than the previous one and that this might explain some of the variation in Salary.\n\nCreate a new variable that would allow you to test this hypothesis and run another multiple regression model to test this. Select variables carefully to make sure there is no multicollinearity. Explain why multicollinearity would be a concern in this case and how you avoided it. Do you find support for the hypothesis that the people hired by the new Dean are making higher than those that were not?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsalary$tenure <- ifelse(salary$ysdeg <= 15, \"15 or less\", \"More than 15\")\nsalary$tenure <- as.factor(salary$tenure)\n\nsalary2 <- salary%>%\n  mutate(degree_o = case_when(\n        degree == \"Masters\" ~ 0,\n         degree == \"PhD\" ~ 1,\n        )) %>%\n  mutate(rank_o = case_when(\n        rank == \"Asst\" ~ 1,\n        rank == \"Assoc\" ~ 2,\n         rank == \"Prof\" ~ 3,\n         )) %>%\n  mutate(sex_o = case_when(\n         sex == \"Male\" ~ 0,\n         sex == \"Female\" ~ 1,\n         )) %>%\n  mutate(tenure_o = case_when(\n         tenure == \"15 or less\" ~ 1,\n         tenure == \"More than 15\" ~ 0,\n         ))\n  \nt.test(salary ~ tenure_o, data = salary2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tWelch Two Sample t-test\n\ndata:  salary by tenure_o\nt = 5.6848, df = 48.319, p-value = 7.427e-07\nalternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0\n95 percent confidence interval:\n 4746.660 9940.417\nsample estimates:\nmean in group 0 mean in group 1 \n       27469.42        20125.88 \n```\n:::\n\n```{.r .cell-code}\ncor_matrix <- cor(salary2[, c(\"ysdeg\", \"rank_o\", \"year\", \"sex_o\", \"tenure_o\", \"degree_o\")], use = \"complete.obs\")\nround(cor_matrix, 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n         ysdeg rank_o  year sex_o tenure_o degree_o\nysdeg     1.00   0.70  0.64 -0.09    -0.84     0.48\nrank_o    0.70   1.00  0.51 -0.23    -0.72     0.01\nyear      0.64   0.51  1.00 -0.38    -0.54     0.14\nsex_o    -0.09  -0.23 -0.38  1.00     0.09    -0.08\ntenure_o -0.84  -0.72 -0.54  0.09     1.00    -0.24\ndegree_o  0.48   0.01  0.14 -0.08    -0.24     1.00\n```\n:::\n:::\n\n\nYears since highest degree earned has an strong correlation with tenure (new variable result of dividing the population in when they were hired). It was expected because tenure is a variable created based on years since highest degree earned (Everyone in this dataset was hired the year they earned their highest degree).\n\nSo there is multicollinearity for these 2 variables, so I will exclude years since highest degree earned from the model and keep tenure.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary (model4 <- lm(salary ~ year + tenure_o + rank + sex_o + degree_o, data=salary2))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = salary ~ year + tenure_o + rank + sex_o + degree_o, \n    data = salary2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3403.3 -1387.0  -167.0   528.2  9233.8 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 18301.04    1301.36  14.063  < 2e-16 ***\nyear          434.85      78.89   5.512 1.65e-06 ***\ntenure_o     2163.46    1072.04   2.018   0.0496 *  \nrankAsst    -4972.66     997.17  -4.987 9.61e-06 ***\nrankProf     6124.28    1028.58   5.954 3.65e-07 ***\nsex_o         907.14     840.54   1.079   0.2862    \ndegree_o      818.93     797.48   1.027   0.3100    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2362 on 45 degrees of freedom\nMultiple R-squared:  0.8594,\tAdjusted R-squared:  0.8407 \nF-statistic: 45.86 on 6 and 45 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\nSeems that with a significant p-value (<0.05), being hired 15 years ago or less means higher salary than faculty with more than 15 years. Specifically, being in the group of 15 years or less means 2,163 dollars more than the other group.\n\n\nQuestion 3\n\n(Data file: house.selling.price in smss R package)\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(\"house.selling.price\")\nhouse <- house.selling.price\nstr(house)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n'data.frame':\t100 obs. of  7 variables:\n $ case : int  1 2 3 4 5 6 7 8 9 10 ...\n $ Taxes: int  3104 1173 3076 1608 1454 2997 4054 3002 6627 320 ...\n $ Beds : int  4 2 4 3 3 3 3 3 5 3 ...\n $ Baths: int  2 1 2 2 3 2 2 2 4 2 ...\n $ New  : int  0 0 0 0 0 1 0 1 0 0 ...\n $ Price: int  279900 146500 237700 200000 159900 499900 265500 289900 587000 70000 ...\n $ Size : int  2048 912 1654 2068 1477 3153 1355 2075 3990 1160 ...\n```\n:::\n:::\n\n\nA. Using the house.selling.price data, run and report regression results modeling y = selling price (in dollars) in terms of size of home (in square feet) and whether the home is new (1 = yes; 0 = no). In particular, for each variable; discuss statistical significance and interpret the meaning of the coefficient.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary (model5 <- lm(Price ~ Size + New, data=house))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = Price ~ Size + New, data = house)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-205102  -34374   -5778   18929  163866 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -40230.867  14696.140  -2.738  0.00737 ** \nSize           116.132      8.795  13.204  < 2e-16 ***\nNew          57736.283  18653.041   3.095  0.00257 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 53880 on 97 degrees of freedom\nMultiple R-squared:  0.7226,\tAdjusted R-squared:  0.7169 \nF-statistic: 126.3 on 2 and 97 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n\nBoth independent variables (Size and if New or not) are statistically significant to predict the home price. In particular, for every square feet the home price increases by $116.13, and when the house is new the price is higher in $57,736 compared when is not new.\n\nB. Report and interpret the prediction equation, and form separate equations relating selling price to size for new and for not new homes.\n\nThe prediction equation is:\n\ny = selling price of home (in dollars), x1 = size of home (in square feet), and x2 = home is new (1 = yes; 0 = no), the prediction equation for new homes is\n\nŷ = -40230.867 + 116.132x1 + 57736.283\n\n\nFor the no new homes prediction equation we need to calculate the slope for no new (value 0 in the variable):\n\n\n::: {.cell}\n\n```{.r .cell-code}\nslope_nonew <- coef(model5)[1] - coef(model5)[3]\nslope_nonew\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(Intercept) \n  -97967.15 \n```\n:::\n:::\n\n\nPrediction equation for No New home:\n\nŷ = -40230.867 + 116.132x1 - 97967.15\n\nC. Find the predicted selling price for a home of 3000 square feet that is (i) new\n\n\n::: {.cell}\n\n```{.r .cell-code}\npredicted_new_house <- -40230.867 + 116.132*3000 + 57736.283\npredicted_new_house\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 365901.4\n```\n:::\n:::\n\n\n(ii) not new\n\n\n::: {.cell}\n\n```{.r .cell-code}\npredicted_nonew_house <- -40230.867 + 116.132*3000 - 97967.15\npredicted_nonew_house\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 210198\n```\n:::\n:::\n\n\nD. Fit another model, this time with an interaction term allowing interaction between size and new, and report the regression results\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary (model7 <- lm(Price ~ Size + New + Size*New, data=house))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = Price ~ Size + New + Size * New, data = house)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-175748  -28979   -6260   14693  192519 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -22227.808  15521.110  -1.432  0.15536    \nSize           104.438      9.424  11.082  < 2e-16 ***\nNew         -78527.502  51007.642  -1.540  0.12697    \nSize:New        61.916     21.686   2.855  0.00527 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 52000 on 96 degrees of freedom\nMultiple R-squared:  0.7443,\tAdjusted R-squared:  0.7363 \nF-statistic: 93.15 on 3 and 96 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n\n\nE. Report the lines relating the predicted selling price to the size for homes that are (i) new, (ii) not new.\n\nThe prediction formula for this model is:\n\nŷ = -22227.808 + 104.438*size - 78527.502*new (0 or 1) + 61.916*size*new (0 or 1).\n\nTo predict price for new or not new houses, we would need just to use the values 1 or 0 in the \"new\" variable.\n\n\nF. Find the predicted selling price for a home of 3000 square feet that is (i) new, \n\n\n::: {.cell}\n\n```{.r .cell-code}\nnewdata <- data.frame(\n  New = 1, \n  Size = 3000  \n)\n\npredicted_price <- predict(model7, newdata)\npredicted_price\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       1 \n398307.5 \n```\n:::\n:::\n\n\n(ii) not new.\n\n::: {.cell}\n\n```{.r .cell-code}\nnewdata <- data.frame(\n  New = 0, \n  Size = 3000  \n)\n\npredicted_price2 <- predict(model7, newdata)\npredicted_price2\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       1 \n291087.4 \n```\n:::\n:::\n\n\n\nG. Find the predicted selling price for a home of 1500 square feet that is (i) new, \n\n\n::: {.cell}\n\n```{.r .cell-code}\nnewdata <- data.frame(\n  New = 1, \n  Size = 1500  \n)\n\npredicted_price3 <- predict(model7, newdata)\npredicted_price3\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       1 \n148776.1 \n```\n:::\n:::\n\n\n\n(ii) not new. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nnewdata2 <- data.frame(\n  New = 0, \n  Size = 1500  \n)\n\npredicted_price4 <- predict(model7, newdata2)\npredicted_price4\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       1 \n134429.8 \n```\n:::\n:::\n\n\nComparing to (F), explain how the difference in predicted selling prices changes as the size of home increases.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#Difference in price for model in question F\n\ndiff_f <- predicted_price - predicted_price2\ndiff_f\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       1 \n107220.1 \n```\n:::\n\n```{.r .cell-code}\ndiff_g <- predicted_price3 - predicted_price4\ndiff_g\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       1 \n14346.32 \n```\n:::\n:::\n\n\nSeems that the difference in price between new or not new homes is bigger when the size is higher. For instance, for a new house with 3000 sqf is $107k higher than no new. But for 1500 sqf the new house is only $14k more. \n\n\nH. Do you think the model with interaction or the one without it represents the relationship of \nsize and new to the outcome price? What makes you prefer one model over another\n\n\n\nBoth models seems good to predict price given p-value for F-statistics is significant in both models, however the the value of F itself is a lot higher in the model with no interaction size-new, which means that there is better fit in this model compared to the interaction model.\n\nThe R2 and Adjusted R2 are similar in both models, so the interaction doesn't seems to add more explanation to the variations to the home price.\n\nOn the other hand, the interaction model provides an additional insight which is that the difference in price between new and no new houses is moderated by the size. This moderation is relevant to understand the price variation and it is not captured in the no-interaction model.\n\n\nFor those reasons, I would rather use the model with the interaction.\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}