{
  "hash": "841c3b87373e0e95cf9e6837369c6922",
  "result": {
    "markdown": "---\ntitle: \"Homework 5\"\nauthor: \"Miguel Curiel\"\ndescription: \"Multiple linear regression for DACSS 603.\"\ndate: \"05/09/2023\"\nformat:\n  pdf\n    # toc: true\n    # code-fold: true\n    # code-copy: true\n    # code-tools: true\ncategories:\n  - hw4\n  - linear regression\n  - multiple linear regression\neditor: \n  markdown: \n    wrap: 72\n---\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# load necessary packages\nlibrary(tidyverse)\nlibrary(alr4)\nlibrary(smss)\nlibrary(knitr)\n```\n:::\n\n\n# Question 1\n\n(Data file: house.selling.price.2 from smss R package)\n\nFor the house.selling.price.2 data the tables below show a correlation\nmatrix and a model fit using four predictors of selling price.\n\n|       | **Price** | **Size** | **Beds** | **Baths** | **New** |\n|-------|-----------|----------|----------|-----------|---------|\n| Price | 1         | 0.899    | 0.590    | 0.714     | .357    |\n| Size  | 0.899     | 1        | 0.669    | 0.662     | 0.176   |\n| Beds  | 0.590     | 0.669    | 1        | 0.334     | 0.267   |\n| Baths | 0.714     | 0.662    | 0.334    | 1         | 0.182   |\n| New   | 0.357     | 0.176    | 0.267    | 0.182     | 1       |\n\n: Correlation Matrix\n\n|             | Estimate | Std. Error | t value | Pr(\\> \\| t\\| ) |\n|-------------|----------|------------|---------|----------------|\n| (Intercept) | -41.795  | 12.104     | -3.453  | 0.001          |\n| Size        | 64.761   | 5.630      | 11.504  | 0              |\n| Beds        | -2.766   | 3.960      | -0.698  | 0.487          |\n| Baths       | 19.203   | 5.650      | 3.399   | 0.001          |\n| New         | 18.984   | 3.873      | 4.902   | 0.00000        |\n\n: Regression Output (Outcome: House Price)\n\nWith the these four predictors:\n\nA.  For backward elimination, which variable would be deleted first?\n    Why?\n\n    1.  Beds because it is moderately correlated to house price (.59),\n        but most importantly, because its P-value is the only one that\n        is not significant (.487).\n\nB.  For forward selection, which variable would be added first? Why?\n\n    1.  Size because it is the variable most correlated with price (.89)\n        and it seems to have the lowest P-value (0).\n\nC.  Why do I think BEDS has such a large P-value in the multiple\n    regression model, even though it has a substantial correlation with\n    PRICE?\n\n    1.  There are several reasons why this could happen\n        (multicollinearity, sample size, variable selection, nonlinear\n        relationship), but my guess is that this is due to\n        multicollinearity. This is because there are other variables\n        that measure similar things (e.g., one would expect beds to be\n        correlated to baths and size), therefore the regression model\n        cannot differentiate the exact effect that beds has on the\n        outcome variable.\n\nD.  Using software with these four predictors, find the model that would\n    be selected using each criterion:\n\n    1.  R-squared\n\n    2.  Adjusted R-squared\n\n    3.  PRESS\n\n    4.  AIC\n\n    5.  BIC\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Read in data\ndata(\"house.selling.price\", package = \"smss\")\ndata <- house.selling.price\n\n# Define the first regression model\nmodel1 <- lm(Price ~ Size + Beds + Baths + New, data = data)\n\n# Define the second regression model\nmodel2 <- lm(Price ~ Size + Beds + New, data = data)\n\n# Define the second regression model\nmodel3 <- lm(Price ~ Size + New, data = data)\n\n# Calculate the R-squared for each model\nrsq1 <- summary(model1)$r.squared\nrsq2 <- summary(model2)$r.squared\nrsq3 <- summary(model3)$r.squared\n\n# Calculate the adjusted R-squared for each model\nadj_rsq1 <- summary(model1)$adj.r.squared\nadj_rsq2 <- summary(model2)$adj.r.squared\nadj_rsq3 <- summary(model3)$adj.r.squared\n\n# Calculate the PRESS statistic for each model\npress1 <- sum(resid(model1)/(1 - hatvalues(model1))^2)\npress2 <- sum(resid(model2)/(1 - hatvalues(model2))^2)\npress3 <- sum(resid(model3)/(1 - hatvalues(model3))^2)\n\n# Calculate the AIC for each model\naic1 <- AIC(model1)\naic2 <- AIC(model2)\naic3 <- AIC(model3)\n\n# Calculate the BIC for each model\nbic1 <- BIC(model1)\nbic2 <- BIC(model2)\nbic3 <- BIC(model3)\n\n# Create a table comparing the models\nresults <- data.frame(Model = c(\"Model w/all predictors\"\n                                , \"Model w/out baths\"\n                                , \"Model w/out beds and baths\"),\n                      R_squared = c(rsq1, rsq2, rsq3),\n                      Adj_R_squared = c(adj_rsq1, adj_rsq2, adj_rsq3),\n                      PRESS = c(press1, press2, press3),\n                      AIC = c(aic1, aic2, aic3),\n                      BIC = c(bic1, bic2, bic3))\n\n# Print the results table\nkable(results, caption = \"Model Evaluation Metrics\")\n```\n\n::: {.cell-output-display}\nTable: Model Evaluation Metrics\n\n|Model                      | R_squared| Adj_R_squared|      PRESS|      AIC|      BIC|\n|:--------------------------|---------:|-------------:|----------:|--------:|--------:|\n|Model w/all predictors     | 0.7245489|     0.7129509| -38424.963| 2470.942| 2486.573|\n|Model w/out baths          | 0.7240775|     0.7154550| -68678.153| 2469.113| 2482.139|\n|Model w/out beds and baths | 0.7225963|     0.7168767|   6062.399| 2467.648| 2478.069|\n:::\n:::\n\n\nE.  Explain which model I prefer and why.\n\n    1.  Based on the previous metrics, I would choose the model that\n        **selects** `Size` and `New` as predictors and **excludes** the\n        `Beds` and `Baths` variables. I would choose this one because it\n        has the highest adjusted R-squared (0.7168767), the lowest AIC\n        (2467.648) and lowest BIC (2478.069). These metrics do not\n        evaluate a model's predictive performance; rather, they assess\n        how well a model explains variance in the outcome variable\n        (i.e., explanatory power and simplicity).\n\n# Question 2\n\n(Data file: trees from base R)\n\nFrom the documentation:\n\n\"This data set provides measurements of the diameter, height and volume\nof timber in 31 felled black cherry trees. Note that the diameter (in\ninches) is erroneously labeled Girth in the data. It is measured at 4 ft\n6 in above the ground.\"\n\nTree volume estimation is a big deal, especially in the lumber industry.\nUse the trees data to build a basic model of tree volume prediction. In\nparticular:\n\nA.  Fit a multiple regression model with the Volume as the outcome and\n    Girth and Height as the explanatory variables.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Read in data\ndata(\"trees\")\ndata <- trees\n\n# Define the regression model\nmodel4 <- lm(Volume ~ Girth + Height, data = data)\n```\n:::\n\n\nB.  Run regression diagnostic plots on the model. Based on the plots, do\n    you think any of the regression assumptions is violated?\n\n    1.  Based on the diagnostic plots below, there are a couple of\n        regression assumptions that seem to be violated, in particular\n        **nonlinearity and outliers**. Starting off with the Residuals\n        vs Fitted plot, it should have a roughly straight horizontal\n        line, but as we see there is a slight curvature which suggests\n        the relation between variables is not linear. The Normal Q-Q\n        plot looks as expected - a straight diagonal line - which\n        indicates that data is normally distributed. Moving on with the\n        Scale-Location plot, even though there is a slight curvature,\n        the line looks roughly horizontal which suggests there is\n        constant variance. Lastly, the Residuals vs Leverage plot should\n        yield no data points outside of the red dashed lines - in this\n        case, there is one point that is outside which indicates there\n        is an outlier in the data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Generate diagnostic plots\npar(mfrow = c(2, 2))\nplot(model4)\n```\n\n::: {.cell-output-display}\n![](HW5_MiguelCuriel_files/figure-pdf/unnamed-chunk-4-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n# Question 3\n\n(Data file: florida in alr4 R package)\n\nIn the 2000 election for U.S. president, the counting of votes in\nFlorida was controversial. In Palm Beach County in south Florida, for\nexample, voters used a so-called butterfly ballot. Some believe that the\nlayout of the ballot caused some voters to cast votes for Buchanan when\ntheir intended choice was Gore.\n\nThe data has variables for the number of votes for each\ncandidate---Gore, Bush, and Buchanan.\n\nA.  Run a simple linear regression model where the Buchanan vote is the\n    outcome and the Bush vote is the explanatory variable. Produce the\n    regression diagnostic plots. Is Palm Beach County an outlier based\n    on the diagnostic plots? Why or why not?\n\n    1.  Palm Beach County does seem to be an outlier based on the\n        diagnostic plots. All plots show it away from the rest of the\n        data points, but in particular, the Residuals vs Leverage plot\n        is used to determine outliers based on Cook's distance and we\n        can clearly see that Palm Beach (and DADE) are outside of the\n        red dashed lines.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Read in data\ndata(\"florida\", package = \"alr4\")\ndata <- florida\n\n# Define the regression model\nmodel5 <- lm(Buchanan ~ Bush, data = data)\n\n# Generate diagnostic plots\npar(mfrow = c(2, 2))\nplot(model5)\n```\n\n::: {.cell-output-display}\n![](HW5_MiguelCuriel_files/figure-pdf/unnamed-chunk-5-1.pdf){fig-pos='H'}\n:::\n:::\n\n\nB.  Take the log of both variables (Bush vote and Buchanan Vote) and\n    repeat the analysis in (A.) Does your findings change?\n\n    1.  Findings do slightly change - even though Palm Beach County\n        still is somewhat away from the rest of the data points, now it\n        is closer. Additionally, it is no longer outside of the red\n        dashed lines in the Residuals vs Leverage plot, which means that\n        using a logarithmic transformation makes it so that Palm Beach\n        is not an outlier.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Define the regression model\nmodel6 <- lm(log(Buchanan) ~ log(Bush), data = data)\n\n# Generate diagnostic plots\npar(mfrow = c(2, 2))\nplot(model6)\n```\n\n::: {.cell-output-display}\n![](HW5_MiguelCuriel_files/figure-pdf/unnamed-chunk-6-1.pdf){fig-pos='H'}\n:::\n:::\n",
    "supporting": [
      "HW5_MiguelCuriel_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": null,
    "postProcess": false
  }
}