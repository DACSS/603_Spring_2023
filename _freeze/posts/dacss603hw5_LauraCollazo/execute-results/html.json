{
  "hash": "97366c02ded643d712c594a70c6f5514",
  "result": {
    "markdown": "---\ntitle: \"DACSS 603 Homework 5\"\nauthor: \"Laura Collazo\"\ndescription: \"Homework 5 for DACSS 603\"\ndate: \"04/26/2023\"\nformat:\n  html:\n    toc: true\n    code-fold: true\n    code-copy: true\n    code-tools: true\ncategories:\n  - hw5\n---\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(smss)\nlibrary(alr4)\n```\n:::\n\n\n# Question 1\n\nThe answers for this question refer to the house.selling.price.2 data set in the smss package.  \n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(house.selling.price.2)\n\nmodel_1 <- lm(data = house.selling.price.2,P ~ S + Be + Ba + New)\n\nmodel_2 <- lm(data = house.selling.price.2,P ~ S + Ba + New)\n```\n:::\n\n\n\n## a\n\nUsing backward elimination, the first variable to be deleted would be beds as it has the highest p-value.\n\n## b\n\nUsing forward selection, the first variable to be added would be new as it has the lowest p-value. \n\n## c\n\nI think beds has a large p-value in the regression model, even though it's highly correlated with price, because multicollinearity exists between beds and size.\n\n## d\n\nBelow are 2 different models: one with all variables and one with beds removed as it did not have a significant p-value in the original model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(model_1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = P ~ S + Be + Ba + New, data = house.selling.price.2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-36.212  -9.546   1.277   9.406  71.953 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -41.795     12.104  -3.453 0.000855 ***\nS             64.761      5.630  11.504  < 2e-16 ***\nBe            -2.766      3.960  -0.698 0.486763    \nBa            19.203      5.650   3.399 0.001019 ** \nNew           18.984      3.873   4.902  4.3e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 16.36 on 88 degrees of freedom\nMultiple R-squared:  0.8689,\tAdjusted R-squared:  0.8629 \nF-statistic: 145.8 on 4 and 88 DF,  p-value: < 2.2e-16\n```\n:::\n\n```{.r .cell-code}\nsummary(model_2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = P ~ S + Ba + New, data = house.selling.price.2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-34.804  -9.496   0.917   7.931  73.338 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -47.992      8.209  -5.847 8.15e-08 ***\nS             62.263      4.335  14.363  < 2e-16 ***\nBa            20.072      5.495   3.653 0.000438 ***\nNew           18.371      3.761   4.885 4.54e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 16.31 on 89 degrees of freedom\nMultiple R-squared:  0.8681,\tAdjusted R-squared:  0.8637 \nF-statistic: 195.3 on 3 and 89 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n\n### R-squared\n\nIf using only the value for R-squared to determine the best fitting model I would select the model using all variables as it's higher, albeit barely, than the model without beds as seen above.\n\n### Adjusted R-squared\n\nIf using only the value for adjusted R-squared to determine the best fitting model I would select the model without beds as it's slightly higher than the model with all variables as seen above.\n\n### PRESS\n\nIf using only the PRESS statistic to determine the best fitting model, I would select the model without the variable beds. As seen below, `P ~ S + Ba + New` has the lowest PRESS statistic. \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# PRESS function\n\nPRESS <- function(model) {\n    i <- residuals(model)/(1 - lm.influence(model)$hat)\n    sum(i^2)\n}\n```\n:::\n\n\n`P ~ S + Be + Ba + New`\n\n\n::: {.cell}\n\n```{.r .cell-code}\nPRESS(model_1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 28390.22\n```\n:::\n:::\n\n\n`P ~ S + Ba + New`\n\n\n::: {.cell}\n\n```{.r .cell-code}\nPRESS(model_2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 27860.05\n```\n:::\n:::\n\n\n### AIC\n\nIf using only the value for AIC to determine the best fitting model, I would select the model without the variable beds. As seen in the summary below, `P ~ S + Ba + New` has the lowest AIC.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstep(object = model_1, direction = \"backward\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nStart:  AIC=524.7\nP ~ S + Be + Ba + New\n\n       Df Sum of Sq   RSS    AIC\n- Be    1       131 23684 523.21\n<none>              23553 524.70\n- Ba    1      3092 26645 534.17\n- New   1      6432 29985 545.15\n- S     1     35419 58972 608.06\n\nStep:  AIC=523.21\nP ~ S + Ba + New\n\n       Df Sum of Sq   RSS    AIC\n<none>              23684 523.21\n- Ba    1      3550 27234 534.20\n- New   1      6349 30033 543.30\n- S     1     54898 78582 632.75\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = P ~ S + Ba + New, data = house.selling.price.2)\n\nCoefficients:\n(Intercept)            S           Ba          New  \n     -47.99        62.26        20.07        18.37  \n```\n:::\n:::\n\n\n### BIC\n\nIf using only the value for BIC to determine the best fitting model, I would select the model without the variable beds. As seen below, `P ~ S + Ba + New` has the lowest BIC.\n\n`P ~ S + Be + Ba + New`\n\n\n::: {.cell}\n\n```{.r .cell-code}\nBIC(model_1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 805.8181\n```\n:::\n:::\n\n\n`P ~ S + Ba + New`\n\n\n::: {.cell}\n\n```{.r .cell-code}\nBIC(model_2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 801.7996\n```\n:::\n:::\n\n\n## e\n\nThe model I would select as best fitting is the one without beds, `P ~ S + Ba + New`. This model is the best fitting using each criterion, except when looking at R-squared. R-squared isn't the best measure to examine anyways, though, as it increases as predictor variables are added even if the variables aren't significant. I also believe there is multicollinearity between size and beds, so removing beds addresses this issue.\n\n# Question 2\n\nQuestion two uses the data set `trees`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(\"trees\")\n```\n:::\n\n\n## a\n\nBelow is the regression model `Volume ~ Girth + Height`. It has a very high adjusted R-squared and all variables have a significant p-value. This indicates a good fitting model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_2a <- lm(data = trees, Volume ~ Girth + Height)\n\nsummary(model_2a)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = Volume ~ Girth + Height, data = trees)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.4065 -2.6493 -0.2876  2.2003  8.4847 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -57.9877     8.6382  -6.713 2.75e-07 ***\nGirth         4.7082     0.2643  17.816  < 2e-16 ***\nHeight        0.3393     0.1302   2.607   0.0145 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.882 on 28 degrees of freedom\nMultiple R-squared:  0.948,\tAdjusted R-squared:  0.9442 \nF-statistic:   255 on 2 and 28 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n\n## b\n\nDiagnostic plots for this model are below. \n\nIn the Residuals vs Fitted plot, both the linearity and constant variance assumptions are violated as the line is not linear and the points are not equally horizontal around the line at 0.\n\nIn the Normal Q-Q plot, most of the points fall on the line, however towards the ends they begin to curve. This violates the normality assumption. \n\nThe Scale-Location plot shows a violation of the constant variance assumption and also indicates heteroskedasticity.\n\nThe Residuals vs Leverage plot shows an outlier outside of Cooks distance which violates the influential observation assumption.\n\nAfter examining these plots, it appears the model is actually not a great fit as it violates all assumptions.  \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(model_2a)\n```\n\n::: {.cell-output-display}\n![](dacss603hw5_LauraCollazo_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](dacss603hw5_LauraCollazo_files/figure-html/unnamed-chunk-12-2.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](dacss603hw5_LauraCollazo_files/figure-html/unnamed-chunk-12-3.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](dacss603hw5_LauraCollazo_files/figure-html/unnamed-chunk-12-4.png){width=672}\n:::\n:::\n\n\n# Question 3\n\nThis question examines if it's possible that voters in Palm Beach County voted for Buchanan when their intended choice was Gore due to the style of ballot that was used.\n\n## a\n\nBelow is the regression summary and diagnostic plots for the model `Buchanan ~ Bush`. Based on the diagnostic plots, Palm Beach County is an outlier in every plot as the point for it on each graph is labeled and falls far from all the others. \n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(\"florida\")\n\nmodel_3a <- (lm(data = florida, Buchanan ~ Bush))\n\nsummary(model_3a)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = Buchanan ~ Bush, data = florida)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-907.50  -46.10  -29.19   12.26 2610.19 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 4.529e+01  5.448e+01   0.831    0.409    \nBush        4.917e-03  7.644e-04   6.432 1.73e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 353.9 on 65 degrees of freedom\nMultiple R-squared:  0.3889,\tAdjusted R-squared:  0.3795 \nF-statistic: 41.37 on 1 and 65 DF,  p-value: 1.727e-08\n```\n:::\n\n```{.r .cell-code}\nplot(model_3a)\n```\n\n::: {.cell-output-display}\n![](dacss603hw5_LauraCollazo_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](dacss603hw5_LauraCollazo_files/figure-html/unnamed-chunk-13-2.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](dacss603hw5_LauraCollazo_files/figure-html/unnamed-chunk-13-3.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](dacss603hw5_LauraCollazo_files/figure-html/unnamed-chunk-13-4.png){width=672}\n:::\n:::\n\n\n## b\n\nThe log of both variables are used in the next model and diagnostic plots. Although this is a better fitting model, Palm Beach County is still an outlier in every diagnostic plot.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(\"florida\")\n\nmodel_3b <- (lm(data = florida, log(Buchanan) ~ log(Bush)))\n\nsummary(model_3b)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = log(Buchanan) ~ log(Bush), data = florida)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.96075 -0.25949  0.01282  0.23826  1.66564 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -2.57712    0.38919  -6.622 8.04e-09 ***\nlog(Bush)    0.75772    0.03936  19.251  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4673 on 65 degrees of freedom\nMultiple R-squared:  0.8508,\tAdjusted R-squared:  0.8485 \nF-statistic: 370.6 on 1 and 65 DF,  p-value: < 2.2e-16\n```\n:::\n\n```{.r .cell-code}\nplot(model_3b)\n```\n\n::: {.cell-output-display}\n![](dacss603hw5_LauraCollazo_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](dacss603hw5_LauraCollazo_files/figure-html/unnamed-chunk-14-2.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](dacss603hw5_LauraCollazo_files/figure-html/unnamed-chunk-14-3.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](dacss603hw5_LauraCollazo_files/figure-html/unnamed-chunk-14-4.png){width=672}\n:::\n:::\n",
    "supporting": [
      "dacss603hw5_LauraCollazo_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}