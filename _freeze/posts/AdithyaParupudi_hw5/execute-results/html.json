{
  "hash": "86557341e7e94299efb60f6969ef9931",
  "result": {
    "markdown": "---\ntitle: \"Homework 5\"\nauthor: \"Adithya Parupudi\"\ndescription: \"my homework 5\"\ndate: \"05/08/2023\"\nformat:\n  html:\n    toc: true\n    code-fold: true\n    code-copy: true\n    code-tools: true\ncategories:\n  - hw5\n  - Adithya Parupudi\n---\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(MPV)\nlibrary(alr4)\nlibrary(smss)\n\nknitr::opts_chunk$set(echo = TRUE)\n```\n:::\n\n\n# Question 1\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(house.selling.price.2)\nhouse.selling.price.2\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       P    S Be Ba New\n1   48.5 1.10  3  1   0\n2   55.0 1.01  3  2   0\n3   68.0 1.45  3  2   0\n4  137.0 2.40  3  3   0\n5  309.4 3.30  4  3   1\n6   17.5 0.40  1  1   0\n7   19.6 1.28  3  1   0\n8   24.5 0.74  3  1   0\n9   34.8 0.78  2  1   0\n10  32.0 0.97  3  1   0\n11  28.0 0.84  3  1   0\n12  49.9 1.08  2  2   0\n13  59.9 0.99  2  1   0\n14  61.5 1.01  3  2   0\n15  60.0 1.34  3  2   0\n16  65.9 1.22  3  1   0\n17  67.9 1.28  3  2   0\n18  68.9 1.29  3  2   0\n19  69.9 1.52  3  2   0\n20  70.5 1.25  3  2   0\n21  72.9 1.28  3  2   0\n22  72.5 1.28  3  1   0\n23  72.0 1.36  3  2   0\n24  71.0 1.20  3  2   0\n25  76.0 1.46  3  2   0\n26  72.9 1.56  4  2   0\n27  73.0 1.22  3  2   0\n28  70.0 1.40  2  2   0\n29  76.0 1.15  2  2   0\n30  69.0 1.74  3  2   0\n31  75.5 1.62  3  2   0\n32  76.0 1.66  3  2   0\n33  81.8 1.33  3  2   0\n34  84.5 1.34  3  2   0\n35  83.5 1.40  3  2   0\n36  86.0 1.15  2  2   1\n37  86.9 1.58  3  2   1\n38  86.9 1.58  3  2   1\n39  86.9 1.58  3  2   1\n40  87.9 1.71  3  2   0\n41  88.1 2.10  3  2   0\n42  85.9 1.27  3  2   0\n43  89.5 1.34  3  2   0\n44  87.4 1.25  3  2   0\n45  87.9 1.68  3  2   0\n46  88.0 1.55  3  2   0\n47  90.0 1.55  3  2   0\n48  96.0 1.36  3  2   1\n49  99.9 1.51  3  2   1\n50  95.5 1.54  3  2   1\n51  98.5 1.51  3  2   0\n52 100.1 1.85  3  2   0\n53  99.9 1.62  4  2   1\n54 101.9 1.40  3  2   1\n55 101.9 1.92  4  2   0\n56 102.3 1.42  3  2   1\n57 110.8 1.56  3  2   1\n58 105.0 1.43  3  2   1\n59  97.9 2.00  3  2   0\n60 106.3 1.45  3  2   1\n61 106.5 1.65  3  2   0\n62 116.0 1.72  4  2   1\n63 108.0 1.79  4  2   1\n64 107.5 1.85  3  2   0\n65 109.9 2.06  4  2   1\n66 110.0 1.76  4  2   0\n67 120.0 1.62  3  2   1\n68 115.0 1.80  4  2   1\n69 113.4 1.98  3  2   0\n70 114.9 1.57  3  2   0\n71 115.0 2.19  3  2   0\n72 115.0 2.07  4  2   0\n73 117.9 1.99  4  2   0\n74 110.0 1.55  3  2   0\n75 115.0 1.67  3  2   0\n76 124.0 2.40  4  2   0\n77 129.9 1.79  4  2   1\n78 124.0 1.89  3  2   0\n79 128.0 1.88  3  2   1\n80 132.4 2.00  4  2   1\n81 139.3 2.05  4  2   1\n82 139.3 2.00  4  2   1\n83 139.7 2.03  3  2   1\n84 142.0 2.12  3  3   0\n85 141.3 2.08  4  2   1\n86 147.5 2.19  4  2   0\n87 142.5 2.40  4  2   0\n88 148.0 2.40  5  2   0\n89 149.0 3.05  4  2   0\n90 150.0 2.04  3  3   0\n91 172.9 2.25  4  2   1\n92 190.0 2.57  4  3   1\n93 280.0 3.85  4  3   0\n```\n:::\n:::\n\n\n## A\n\nFor backward elimination, you fit a model using all possible explanatory values to predict the output. Then\none by one, you delete the least significant explanatory variable in the model, which would have the largest\np-value. In this example, we would delete `Beds` first, which has a p-value of 0.487.\n\n## B\n\nWith forward selection, you begin with no explanatory variables, then add one variable at a time to the model.\nThe variable you add should be the most significant one, based on it having the lowest P-value of the group of\npossible explanatory variables. In this example, the first variable to add to the model is `Size`, given its\nextremely small p-value < 2e-16.\n\n## C\n\nWhile the variable Beds does have a strong correlation with price, when adding additional variables using a\nregression model, the relationship significantly diminishes, thus the other variables may act as a control on\nthe bed variable.\n\n## D\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(lm(P ~ S, data = house.selling.price.2))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = P ~ S, data = house.selling.price.2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-56.407 -10.656   2.126  11.412  85.091 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -25.194      6.688  -3.767 0.000293 ***\nS             75.607      3.865  19.561  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 19.47 on 91 degrees of freedom\nMultiple R-squared:  0.8079,\tAdjusted R-squared:  0.8058 \nF-statistic: 382.6 on 1 and 91 DF,  p-value: < 2.2e-16\n```\n:::\n\n```{.r .cell-code}\nsummary(lm(P ~ S + New, data = house.selling.price.2))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = P ~ S + New, data = house.selling.price.2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-47.207  -9.763  -0.091   9.984  76.405 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -26.089      5.977  -4.365 3.39e-05 ***\nS             72.575      3.508  20.690  < 2e-16 ***\nNew           19.587      3.995   4.903 4.16e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 17.4 on 90 degrees of freedom\nMultiple R-squared:  0.8484,\tAdjusted R-squared:  0.845 \nF-statistic: 251.8 on 2 and 90 DF,  p-value: < 2.2e-16\n```\n:::\n\n```{.r .cell-code}\nsummary(lm(P ~ ., data = house.selling.price.2))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = P ~ ., data = house.selling.price.2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-36.212  -9.546   1.277   9.406  71.953 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -41.795     12.104  -3.453 0.000855 ***\nS             64.761      5.630  11.504  < 2e-16 ***\nBe            -2.766      3.960  -0.698 0.486763    \nBa            19.203      5.650   3.399 0.001019 ** \nNew           18.984      3.873   4.902  4.3e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 16.36 on 88 degrees of freedom\nMultiple R-squared:  0.8689,\tAdjusted R-squared:  0.8629 \nF-statistic: 145.8 on 4 and 88 DF,  p-value: < 2.2e-16\n```\n:::\n\n```{.r .cell-code}\nsummary(lm(P ~ . - Be, data = house.selling.price.2))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = P ~ . - Be, data = house.selling.price.2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-34.804  -9.496   0.917   7.931  73.338 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -47.992      8.209  -5.847 8.15e-08 ***\nS             62.263      4.335  14.363  < 2e-16 ***\nBa            20.072      5.495   3.653 0.000438 ***\nNew           18.371      3.761   4.885 4.54e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 16.31 on 89 degrees of freedom\nMultiple R-squared:  0.8681,\tAdjusted R-squared:  0.8637 \nF-statistic: 195.3 on 3 and 89 DF,  p-value: < 2.2e-16\n```\n:::\n\n```{.r .cell-code}\nsummary(lm(P ~ . - Be - Ba, data = house.selling.price.2))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = P ~ . - Be - Ba, data = house.selling.price.2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-47.207  -9.763  -0.091   9.984  76.405 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -26.089      5.977  -4.365 3.39e-05 ***\nS             72.575      3.508  20.690  < 2e-16 ***\nNew           19.587      3.995   4.903 4.16e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 17.4 on 90 degrees of freedom\nMultiple R-squared:  0.8484,\tAdjusted R-squared:  0.845 \nF-statistic: 251.8 on 2 and 90 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n\n### a. R^2\n\nAs expected, the model with the most explanatory variables has the highest R-squared value at 0.8689. Therefore, if you were to select a model solely based on maximizing the R-squared value, it would be: ŷ = -41.79 + 64.76(Size) - 2.77(Beds) + 19.2(Baths) + 18.98(New).\n\n### b. Adjusted R^2\n\nHowever, if you were to select a model based on adjusted R-squared, the best model for predicting selling price would exclude Beds and use Size, Baths, and New as explanatory variables. The adjusted R-squared value see a slight increase when Beds is removed (from 0.8629 to 0.8637). The model would be: ŷ  = -47.99 + 62.26(Size) + 20.07(Baths) + 18.37(New).\n\n### c. PRESS\n\n\n::: {.cell}\n\n```{.r .cell-code}\nPRESS(lm(P ~ ., data = house.selling.price.2))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 28390.22\n```\n:::\n\n```{.r .cell-code}\nPRESS(lm(P ~ . -Be, data = house.selling.price.2))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 27860.05\n```\n:::\n:::\n\n\nWhen considering PRESS, a smaller PRESS value indicates a better predictive model. Comparing the PRESS value of the model with all variables and the model excluding Bed, the PRESS values would lead us to select the model with Size, Baths, and New as variables for predicting selling price.\n\n### d. AIC\n\n\n::: {.cell}\n\n```{.r .cell-code}\nAIC(lm(P ~ ., data = house.selling.price.2))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 790.6225\n```\n:::\n\n```{.r .cell-code}\nAIC(lm(P ~ . -Be, data = house.selling.price.2))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 789.1366\n```\n:::\n:::\n\n\nWhen considering the AIC for both models, the value is slightly lower for the model that excludes Bed as a variable. Therefore, the AIC would lead us to use the model with Size, Baths, and New as explanatory variables to predicting selling price.\n\n### e. BIC\n\n\n::: {.cell}\n\n```{.r .cell-code}\nBIC(lm(P ~ ., data = house.selling.price.2))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 805.8181\n```\n:::\n\n```{.r .cell-code}\nBIC(lm(P ~ . -Be, data = house.selling.price.2))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 801.7996\n```\n:::\n:::\n\n\nLastly, like AIC, the BIC value is lower for the model that excludes Bed as a variable. Once again, we’d select the model that uses Size, Baths, and New as explanatory variables to predict selling price.\n\n## E\n\nGiven the results from the various criteria above, the model I would prefer to use to predict selling price is that which excludes Bed and includes Size, Bath, and New as variables: ŷ  = -41.79 + 64.76(Size) - 2.77(Beds) + 19.2(Baths) + 18.98(New). This is because each of the criterion indicate this model as slightly stronger in its predictive power than the model that includes all variables except R-squared, which cannot be used alone to determine model strength.\n\n# Question 2\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(\"trees\")\ntrees\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   Girth Height Volume\n1    8.3     70   10.3\n2    8.6     65   10.3\n3    8.8     63   10.2\n4   10.5     72   16.4\n5   10.7     81   18.8\n6   10.8     83   19.7\n7   11.0     66   15.6\n8   11.0     75   18.2\n9   11.1     80   22.6\n10  11.2     75   19.9\n11  11.3     79   24.2\n12  11.4     76   21.0\n13  11.4     76   21.4\n14  11.7     69   21.3\n15  12.0     75   19.1\n16  12.9     74   22.2\n17  12.9     85   33.8\n18  13.3     86   27.4\n19  13.7     71   25.7\n20  13.8     64   24.9\n21  14.0     78   34.5\n22  14.2     80   31.7\n23  14.5     74   36.3\n24  16.0     72   38.3\n25  16.3     77   42.6\n26  17.3     81   55.4\n27  17.5     82   55.7\n28  17.9     80   58.3\n29  18.0     80   51.5\n30  18.0     80   51.0\n31  20.6     87   77.0\n```\n:::\n:::\n\n\n## A\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel <- lm(Volume ~ Girth + Height, data = trees)\nsummary(model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = Volume ~ Girth + Height, data = trees)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.4065 -2.6493 -0.2876  2.2003  8.4847 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -57.9877     8.6382  -6.713 2.75e-07 ***\nGirth         4.7082     0.2643  17.816  < 2e-16 ***\nHeight        0.3393     0.1302   2.607   0.0145 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.882 on 28 degrees of freedom\nMultiple R-squared:  0.948,\tAdjusted R-squared:  0.9442 \nF-statistic:   255 on 2 and 28 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n\n## B\n\n\n::: {.cell}\n\n```{.r .cell-code}\npar(mfrow = c(2, 3)); plot(model, which = 1:6)\n```\n\n::: {.cell-output-display}\n![](AdithyaParupudi_hw5_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\nBased on the residuals vs. fitted values plot, the central points appear to roughly bounce randomly above and below 0, but the lowest and highest point appear to be very influential residuals. The red line should be flat along 0 horizontally, but it is U-shaped. This curvature may suggest a violation in the linearity assumption. With the normal Q-Q plot, it’s difficult to confidently say that the assumption of normality appears to be violated. The points generally run along the trend-line, but they do deviate above the line for the higher points. It’s a noteworthy deviation, but it’s difficult to make a certain decision based on the plot. In the scale-location plot, the line is not horizontal, thus suggesting a violation in the assumption of constant variance. Cook’s distance suggests that the 31st observation is above the threshold, meaning it is too influential as one observation.\n\n# Question 3\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(\"florida\")\nflorida\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n               Gore   Bush Buchanan\nALACHUA       47300  34062      262\nBAKER          2392   5610       73\nBAY           18850  38637      248\nBRADFORD       3072   5413       65\nBREVARD       97318 115185      570\nBROWARD      386518 177279      789\nCALHOUN        2155   2873       90\nCHARLOTTE     29641  35419      182\nCITRUS        25501  29744      270\nCLAY          14630  41745      186\nCOLLIER       29905  60426      122\nCOLUMBIA       7047  10964       89\nDADE         328702 289456      561\nDE SOTO        3322   4256       36\nDIXIE          1825   2698       29\nDUVAL        107680 152082      650\nESCAMBIA      40958  73029      504\nFLAGLER       13891  12608       83\nFRANKLIN       2042   2448       33\nGADSDEN        9565   4750       39\nGILCHRIST      1910   3300       29\nGLADES         1420   1840        9\nGULF           2389   3546       71\nHAMILTON       1718   2153       24\nHARDEE         2341   3764       30\nHENDRY         3239   4743       22\nHERNANDO      32644  30646      242\nHIGHLANDS     14152  20196       99\nHILLSBOROUGH 166581 176967      836\nHOLMES         2154   4985       76\nINDIAN RIVER  19769  28627      105\nJACKSON        6868   9138      102\nJEFFERSON      3038   2481       29\nLAFAYETTE       788   1669       10\nLAKE          36555  49963      289\nLEE           73560 106141      305\nLEON          61425  39053      282\nLEVY           5403   6860       67\nLIBERTY        1011   1316       39\nMADISON        3011   3038       29\nMANATEE       49169  57948      272\nMARION        44648  55135      563\nMARTIN        26619  33864      108\nMONROE        16483  16059       47\nNASSAU         6952  16404       90\nOKALOOSA      16924  52043      267\nOKEECHOBEE     4588   5058       43\nORANGE       140115 134476      446\nOSCEOLA       28177  26216      145\nPALM BEACH   268945 152846     3407\nPASCO         69550  68581      570\nPINELLAS     199660 184312     1010\nPOLK          74977  90101      538\nPUTNAM        12091  13439      147\nST. JOHNS     19482  39497      229\nST. LUCIE     41559  34705      124\nSANTA ROSA    12795  36248      311\nSARASOTA      72854  83100      305\nSEMINOLE      58888  75293      194\nSUMTER         9634  12126      114\nSUWANNEE       4084   8014      108\nTAYLOR         2647   4051       27\nUNION          1399   2326       26\nVOLUSIA       97063  82214      396\nWAKULLA        3835   4511       46\nWALTON         5637  12176      120\nWASHINGTON     2796   4983       88\n```\n:::\n:::\n\n\n## A\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel <- lm(formula = Buchanan ~ Bush, data = florida)\nsummary(model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = Buchanan ~ Bush, data = florida)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-907.50  -46.10  -29.19   12.26 2610.19 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 4.529e+01  5.448e+01   0.831    0.409    \nBush        4.917e-03  7.644e-04   6.432 1.73e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 353.9 on 65 degrees of freedom\nMultiple R-squared:  0.3889,\tAdjusted R-squared:  0.3795 \nF-statistic: 41.37 on 1 and 65 DF,  p-value: 1.727e-08\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\npar(mfrow = c(2, 3)); plot(model, which = 1:6)\n```\n\n::: {.cell-output-display}\n![](AdithyaParupudi_hw5_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n:::\n\n\nBased on the diagnostic plots, Palm Beach County is an outlier. First, when looking at the residuals vs fitted plot, the Palm Beach County residual is very large. When referring to the summary of the simple regression model, the third quartile for residuals is 12.26, yet the max is 2610.19. This is a significant jump and indicative of the value being an outlier. The normal Q-Q plot also indicates that the residuals for the model are generally normal except for the Palm Beach County residual, as it greatly deviates from the line in the plot. The Cook’s distance plot shows two points that may be of concern as outliers if you follow the metric of observations scoring over 1, which are DADE and Palm Beach at about 2. The residuals and leverages plot shows the Palm Beach County standardized residual value beyond the dashed line indicating Cook’s distance. This also suggests that the observation is an outlier and the observation has the potential to influence the regression model.\n\n## B\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel <- lm(formula = log(Buchanan) ~ log(Bush), data = florida)\nsummary(model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = log(Buchanan) ~ log(Bush), data = florida)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.96075 -0.25949  0.01282  0.23826  1.66564 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -2.57712    0.38919  -6.622 8.04e-09 ***\nlog(Bush)    0.75772    0.03936  19.251  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4673 on 65 degrees of freedom\nMultiple R-squared:  0.8508,\tAdjusted R-squared:  0.8485 \nF-statistic: 370.6 on 1 and 65 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\npar(mfrow = c(2, 3)); plot(model, which = 1:6)\n```\n\n::: {.cell-output-display}\n![](AdithyaParupudi_hw5_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n:::\n\n\nBased on the diagnostic plots, Palm Beach County is still an outlier. First, when looking at the residuals vs fitted plot, the Palm Beach County residual is still very large. The normal Q-Q plot also indicates that the residuals for the model are generally normal except for the Palm Beach County residual, as it greatly deviates from the line in the plot. The Cook’s distance plot shows that may be of concern as outlier if you follow the metric of observations scoring over 0.2, which is Palm Beach at about 0.3. The residuals and leverages plot shows the Palm Beach County standardized residual value beyond the dashed line indicating Cook’s distance. This also suggests that the observation is an outlier and the observation has the potential to influence the regression model.\n",
    "supporting": [
      "AdithyaParupudi_hw5_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}