{
  "hash": "62ea9acec053753c736f31095cea313f",
  "result": {
    "markdown": "---\ntitle: \"Homework 4\"\nauthor: \"Young Soo Choi\"\ndescription: \"hw4\"\ndate: \"04/25/2023\"\nformat:\n  html:\n    toc: true\n    code-fold: true\n    code-copy: true\n    code-tools: true\ncategories:\n  - hw4\n---\n\n\n# Question 1\n\n## (A)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx1=1240\nx2=18000\n-10536+53.8*x1+2.84*x2\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 107296\n```\n:::\n:::\n\nWhen the given values for size of home and size of lot are 1240 and 18000, the predicted selling price is 107296.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n145000-pred_price\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in eval(expr, envir, enclos): object 'pred_price' not found\n```\n:::\n:::\n\nAnd the residual is 37704. This means that, in the given model, the selling price that cannot be explained by size of home and los is 37704.\n\n## (B)\n\nIt is increasing by 53.8. Since the coefficient of x1(size of home) is 53.8, if x2(size of lot) is constant, y(predicted selling price) increases by 53.8.\n\n## (C)\n\nAs discussed above, when x2 is constant and x1 increases by 1 unit, y increases by 53.8. Likewise, if x1 is constant and x2 increases by one unit, y increases by 2.84, the coefficient of x2 in the above equation.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n53.8/2.84\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 18.94366\n```\n:::\n:::\n\n\nUsing this relationship, the amount of increase in x2 that gives the same effect as the increase in x1 by one unit is 18.94. That is, if x2 increases 18.94, y increases as x1 increases by one unit.\n\n# Question 2\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# data loading\nlibrary(alr4)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: package 'alr4' was built under R version 4.2.3\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: car\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: carData\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: effects\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: package 'effects' was built under R version 4.2.3\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nlattice theme set by effectsTheme()\nSee ?effectsTheme for details.\n```\n:::\n\n```{.r .cell-code}\ndata(salary)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ncolnames(salary)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"degree\" \"rank\"   \"sex\"    \"year\"   \"ysdeg\"  \"salary\"\n```\n:::\n:::\n\n\n## (A)\n\nIt is necessary to implement a t-test to verify the hypothesis that the average salary of men and women is the same.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nt.test(salary~sex, data=salary)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tWelch Two Sample t-test\n\ndata:  salary by sex\nt = 1.7744, df = 21.591, p-value = 0.09009\nalternative hypothesis: true difference in means between group Male and group Female is not equal to 0\n95 percent confidence interval:\n -567.8539 7247.1471\nsample estimates:\n  mean in group Male mean in group Female \n            24696.79             21357.14 \n```\n:::\n:::\n\n\nUsing r's t.test function, the alternative hypothesis is that the difference between the two will not be zero (that is, there is a salary difference between men and women), and the null hypothesis is that there is no salary difference between men and women. However, as a result of the t-test, the p value is greater than 0.05, so the null hypothesis cannot be rejected at the significance level of 0.05. This means that there is no salary difference between men and women.\n\n## (B)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsal.lm<-lm(salary~., data=salary)\nsummary(sal.lm)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = salary ~ ., data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4045.2 -1094.7  -361.5   813.2  9193.1 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 15746.05     800.18  19.678  < 2e-16 ***\ndegreePhD    1388.61    1018.75   1.363    0.180    \nrankAssoc    5292.36    1145.40   4.621 3.22e-05 ***\nrankProf    11118.76    1351.77   8.225 1.62e-10 ***\nsexFemale    1166.37     925.57   1.260    0.214    \nyear          476.31      94.91   5.018 8.65e-06 ***\nysdeg        -124.57      77.49  -1.608    0.115    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2398 on 45 degrees of freedom\nMultiple R-squared:  0.855,\tAdjusted R-squared:  0.8357 \nF-statistic: 44.24 on 6 and 45 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\nThe results of deriving multiple regression equations for wages using all variables were obtained. According to the above equation, salary is 1166.37 high if the sex is female.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nconfint(sal.lm, \"sexFemale\", level=0.95)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n              2.5 %   97.5 %\nsexFemale -697.8183 3030.565\n```\n:::\n:::\n\nUsing the above values, the 95% confidence interval of the sex difference between men and women is from -697.8 to 3030.6. Since this confidence interval includes 0, it can be said that the coefficient of the regression equation for sex is not significant at the 5% level of significance.\n\n## (C)\n\nIn summary, the multiple regression equation derived above can be expressed as follows.\n\nSalary = 15746.1 + 1388.6*degreePhD + 5292.4*rankAssoc + 11118.8*rankProf + 1166.4*sexFemale + 476.3*years -124.6*ysdeg\n\nPrior to the discussion, all significance levels are set at 0.05.\n\nFor degree, (a) salary difference between masters and phD is not statistically significant.(p-value is greater than 0.05) (b) And according to the coefficient, the salary of the phD is greater than masters' by 1388.7. Taken together, when other variables are the same, the Ph.D. owner is paid1388.7 higher than the master's, but this is not statistically significant.\n\nFor rank, (a) salary difference between Asst, Assoc, and Prof is statistically significant.(p-value is less than 0.05) (b) And according to the coefficient, the salary of the Assoc is greater than Asst's by 5292.4 and the salary of the Prof is greater than Asst's by 11118.8.  Overall, the associate professor receives 5292.4 higher salary than the assistant professor, and the professor receives 11118.8 higher salary than the assistant professor, when any other variables are the same. And it is statistically significant.\n\nFor sex, (a) salary difference between male and female is not statistically significant.(p-value is greater than 0.05) (b) And according to the coefficient, the salary of the female is greater than male's by 1166.4. In general, when other variables are constant, female professors are paid 1166.4 higher than male professors, but this is not statistically significant.\n\nFor year, (a) coefficient of year is statistically significant.(p-value is less than 0.05) (b) And according to the coefficient, when the number of years increases by 1, the salary increases by 476.3.\nIn other words, if the number of years of service increases by one year, the salary increases by 476.3 when other variables are constant, which is statistically significant.\n\nFor ysdeg, (a) coefficient of ysdeg is not statistically significant.(p-value is greater than 0.05) (b) And according to the coefficient, when the number of ysdeg increases by 1, the salary decreases by 124.6. In the end, according to this model, if years since highest degree increases when other variables are the same, income decreases by 124.6, but this is not statistically significant.\n\n## (D)\n\nThe baseline can be changed using the relevel function. Through this, Assoc was changed to a baseline instead of Asst and a multiple regression model was derived.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsalary$rank.a <- relevel(salary$rank, ref = \"Assoc\")\n\nsal.lm.2<-lm(salary~degree+rank.a+sex+year+ysdeg, data=salary)\n\nsummary(sal.lm.2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = salary ~ degree + rank.a + sex + year + ysdeg, data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4045.2 -1094.7  -361.5   813.2  9193.1 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 21038.41    1109.12  18.969  < 2e-16 ***\ndegreePhD    1388.61    1018.75   1.363    0.180    \nrank.aAsst  -5292.36    1145.40  -4.621 3.22e-05 ***\nrank.aProf   5826.40    1012.93   5.752 7.28e-07 ***\nsexFemale    1166.37     925.57   1.260    0.214    \nyear          476.31      94.91   5.018 8.65e-06 ***\nysdeg        -124.57      77.49  -1.608    0.115    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2398 on 45 degrees of freedom\nMultiple R-squared:  0.855,\tAdjusted R-squared:  0.8357 \nF-statistic: 44.24 on 6 and 45 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n5826.40-(-5292.36)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 11118.76\n```\n:::\n:::\n\n\nChecking the above coefficients, salary decreases by 5292.4 compared to assoc when it is asst. Prof has a higher salary by 5826.4 compared to assoc. Comparing prop to asst, the difference is 11118.8, which is the same as the difference when the baseline is not changed. In other words, even if the baseline is changed, only the form of the multiple regression model changes and the meaning is the same.\n\n## (E)\n\nExcept for the rank variable, multiple regression equations were derived again.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsal.lm.nr<-lm(salary~degree+sex+year+ysdeg, data=salary)\nsummary(sal.lm.nr)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = salary ~ degree + sex + year + ysdeg, data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8146.9 -2186.9  -491.5  2279.1 11186.6 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 17183.57    1147.94  14.969  < 2e-16 ***\ndegreePhD   -3299.35    1302.52  -2.533 0.014704 *  \nsexFemale   -1286.54    1313.09  -0.980 0.332209    \nyear          351.97     142.48   2.470 0.017185 *  \nysdeg         339.40      80.62   4.210 0.000114 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3744 on 47 degrees of freedom\nMultiple R-squared:  0.6312,\tAdjusted R-squared:  0.5998 \nF-statistic: 20.11 on 4 and 47 DF,  p-value: 1.048e-09\n```\n:::\n:::\n\n\nThere are many changes. First of all, Ph.D. ownership acts in the direction of reducing salaries, which is statistically significant (all significance levels are 0.05), and gender has also changed its direction of influence. It was found that women's salary was lower than that of men, but it was not statistically significant. The influence of ysdeg also changed, and when rank was included, this variable had a negative effect on salary, but now it has a positive effect. And this result is also significant. Finally, year did not change the direction of influence, but the coefficient was slightly reduced. be statistically significant as well.\n\n## (F)\n\nFirst, a new variable(ysdeg.dean) with an \"old\" when ysdeg exceeds 15 and a \"new\" if not was created.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(dplyr)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'dplyr'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following object is masked from 'package:car':\n\n    recode\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n```\n:::\n\n```{.r .cell-code}\nnew.sal<-salary %>% mutate(ysdeg.dean = ifelse(ysdeg>15, \"old\", \"new\"))\n```\n:::\n\n\nNext, in order to check the multicollinearity, the vif value was checked using the vif function. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(car)\nvif<-vif(lm(salary~degree+rank+sex+year+ysdeg.dean, data=new.sal))\nvif\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n               GVIF Df GVIF^(1/(2*Df))\ndegree     1.341872  1        1.158392\nrank       2.964200  2        1.312130\nsex        1.295820  1        1.138341\nyear       1.726209  1        1.313853\nysdeg.dean 2.678486  1        1.636608\n```\n:::\n:::\n\n\nHere, there is no value over 5, so it seems that there is no variable with strong multicollinearity. Therefore, I derived multiple regression equations including all variables and confirmed the effect of ysdeg reclassified into old and new.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsal.lm.on<-lm(salary~degree+rank+sex+year+ysdeg.dean, data=new.sal)\nsummary(sal.lm.on)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = salary ~ degree + rank + sex + year + ysdeg.dean, \n    data = new.sal)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3403.3 -1387.0  -167.0   528.2  9233.8 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   15491.84     806.32  19.213  < 2e-16 ***\ndegreePhD       818.93     797.48   1.027   0.3100    \nrankAssoc      4972.66     997.17   4.987 9.61e-06 ***\nrankProf      11096.95    1191.00   9.317 4.54e-12 ***\nsexFemale       907.14     840.54   1.079   0.2862    \nyear            434.85      78.89   5.512 1.65e-06 ***\nysdeg.deanold -2163.46    1072.04  -2.018   0.0496 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2362 on 45 degrees of freedom\nMultiple R-squared:  0.8594,\tAdjusted R-squared:  0.8407 \nF-statistic: 45.86 on 6 and 45 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n\nAs a result of the confirmation, if all other variables were the same, if ysdeg was old, the salary was 2163.5 lower than if it was new. In addition, the p value of this coefficient is 0.0496, so it is statistically significant at the significance level of 0.05. In other words, the perception that the new dean is more generous in salary is thought to be grounded.\n\n# Question 3\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(smss)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: package 'smss' was built under R version 4.2.3\n```\n:::\n\n```{.r .cell-code}\ndata(house.selling.price)\ncolnames(house.selling.price)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"case\"  \"Taxes\" \"Beds\"  \"Baths\" \"New\"   \"Price\" \"Size\" \n```\n:::\n\n```{.r .cell-code}\nnrow(house.selling.price)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 100\n```\n:::\n:::\n\n\n## (A)\n\nA multiple regression model was derived with Size and New as independent variables and price as dependent variables.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm.3.a<-lm(Price~Size+New, data=house.selling.price)\nsummary(lm.3.a)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = Price ~ Size + New, data = house.selling.price)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-205102  -34374   -5778   18929  163866 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -40230.867  14696.140  -2.738  0.00737 ** \nSize           116.132      8.795  13.204  < 2e-16 ***\nNew          57736.283  18653.041   3.095  0.00257 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 53880 on 97 degrees of freedom\nMultiple R-squared:  0.7226,\tAdjusted R-squared:  0.7169 \nF-statistic: 126.3 on 2 and 97 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\nAs a result, as the size increases by one unit, the price increases by 116.1, and in the case of New, the price is 57736.3 higher than that of Old. These are all statistically significant (significance level 0.05).\n\n## (B)\n\nFor new home: Price = 17505.42 + 116.132*Size\nFor old home; Price = -40230.867 + 116.132*Size\n\nThis relationship can also be seen in the figure below.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\nggplot(house.selling.price, aes(Size, Price, color=as.factor(New)),\n     xlab=\"Size\",\n     ylab=\"Price\") + \n  geom_point() +\n  scale_color_manual(values = c(\"red\", \"blue\"), label = c(\"Old\", \"New\")) +\n  geom_abline(slope=116.132, intercept = -40230.867, color =\"red\") +\n  geom_abline(slope=116.132, intercept = -40230.867+57736.283, color=\"blue\")\n```\n\n::: {.cell-output-display}\n![](HW4_young_files/figure-html/unnamed-chunk-17-1.png){width=672}\n:::\n:::\n\n\nIn interpretation, regardless of whether the house is new or old, the increase in price is the same as the unit increase in size. However, the intercept of new is 17505.42, which is as large as 57736.283 compared to the intercept of old. That is, the slope of the regression line representing each case is the same, but the new regression line is vertically higher than the old regression line by 57736.283.\n\n## (C)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nSize=3000\nnew.home.Price <- 17505.42 + 116.132*Size\nold.home.Price <- -40230.867 + 116.132*Size\nnew.home.Price\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 365901.4\n```\n:::\n\n```{.r .cell-code}\nold.home.Price\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 308165.1\n```\n:::\n:::\n\n\nAccording to fitted model, when size is 3000, predicted selling price for a new home is 365901.4 and for an old home is 308165.1\n\n## (D)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm.3.d<-lm(Price~Size*New, data=house.selling.price)\nsummary(lm.3.d)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = Price ~ Size * New, data = house.selling.price)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-175748  -28979   -6260   14693  192519 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -22227.808  15521.110  -1.432  0.15536    \nSize           104.438      9.424  11.082  < 2e-16 ***\nNew         -78527.502  51007.642  -1.540  0.12697    \nSize:New        61.916     21.686   2.855  0.00527 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 52000 on 96 degrees of freedom\nMultiple R-squared:  0.7443,\tAdjusted R-squared:  0.7363 \nF-statistic: 93.15 on 3 and 96 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n\nIn the regression model that includes the interaction term, the price is rather lowered when new(-78527.5). However, according to the interaction term between Size and new, in the case of new, the sales price increases by 61.916 as the size increases by one unit. Even if it is new, the price is lower than old until it reaches a specific size, and it can be seen that the price of new is higher than old only when it exceeds that specific size.\n\n## (E)\n\nFor new home: Price = -100755.3 + 166.354*Size\nFor old home; Price = -22227.808 + 104.438*Size\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(house.selling.price, aes(Size, Price, color=as.factor(New)),\n     xlab=\"Size\",\n     ylab=\"Price\") + \n  geom_point() +\n  scale_color_manual(values = c(\"red\", \"blue\"), label = c(\"Old\", \"New\")) +\n  geom_abline(slope=104.438, intercept = -22227.808, color =\"red\") +\n  geom_abline(slope=166.354, intercept = -100755.3, color=\"blue\")\n```\n\n::: {.cell-output-display}\n![](HW4_young_files/figure-html/unnamed-chunk-20-1.png){width=672}\n:::\n:::\n\n\nAs shown in the figure, in the case of new, the intercept is lower than old, but the slope of new is larger than that of old.\n\n## (F)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nSize=3000\nnew.home.Price.2 <- -100755.3 + 166.354*Size\nold.home.Price.2 <- -22227.808 + 104.438*Size\nnew.home.Price.2\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 398306.7\n```\n:::\n\n```{.r .cell-code}\nold.home.Price.2\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 291086.2\n```\n:::\n:::\n\n\nPredicted selling price of new home is 398306.7 and of new home is 291086.2.\n\n## (G)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nSize=1500\nnew.home.Price.2 <- -100755.3 + 166.354*Size\nold.home.Price.2 <- -22227.808 + 104.438*Size\nnew.home.Price.2\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 148775.7\n```\n:::\n\n```{.r .cell-code}\nold.home.Price.2\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 134429.2\n```\n:::\n:::\n\n\nPredicted selling price of new home is 148775.7 and of new home is 134429.2. The price difference is smaller than when the size is 3000. \nIn the case of new, the slope of the model is larger than that of old, so the larger the size, the higher the price of new. In other words, as the size increases, the price difference between new and old will gradually increase, which can be seen from the regression equation between the two and the graph above.\n\n## (H)\n\nLooking at the r-square value of each model, the r-square value of the model including the interaction term is larger(model with interaction term: 0.7443, model without it: 0.7226). Even looking at the adjusted r-square value that corrects the increased amount of explanation as the variable increasesr(model with interaction term: 0.7363, model without it: 0.7169), it can be said that the explanatory power of the model including the interaction term is higher.\nHowever, which model to use and prefer is not determined simply by these explanations. In this case, it was easy to compare the cases of new and old by simply modifying the expression, but if the number of variables increases or there are more than three categories of nominal variables, it will be increasingly difficult to derive or interpret the regression model.\nIn other words, in selecting models and variables, various variables such as interpretability as well as simple explanatory power should be considered.\n\nFor this case, I prefer a model with interaction terms. It is interesting that the older the house, the smaller the price increase due to the increase in its area, in that the two variables interact to change the influence of the variable.\n\n\n",
    "supporting": [
      "HW4_young_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}