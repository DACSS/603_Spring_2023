{
  "hash": "18d3f9bce73341f284ce41c86dcc0075",
  "result": {
    "markdown": "---\ntitle: \"Homework 4\"\nauthor: \"Justine Shakespeare\"\ndescription: \"Homework 4 for DACSS 603 Spring 2023\"\ndate: \"04/24/2023\"\nformat:\n  html:\n    toc: true\n    code-fold: true\n    code-copy: true\n    code-tools: true\ncategories:\n  - hw4\n  - multiple linear regression\n  - Justine Shakespeare\n---\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(alr4)\nlibrary(smss)\nlibrary(stargazer)\n```\n:::\n\n# Question 1\n\nŷ = −10,536 + 53.8x1 + 2.84x2\n\nx1 = size of home\n\nx2 = size of lot\n\n## A\n\n*A particular home of 1240 square feet on a lot of 18,000 square feet sold for $145,000. Find the predicted selling price and the residual, and interpret.*\n\nx1 = 1240\nx2 = 18000\ny = 145000\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx1 = 1240\nx2 = 18000\n\n-10536 + (53.8*x1) + (2.84*x2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 107296\n```\n:::\n:::\n\n\nThe predicted selling point is \\$107,296 for this house. Given that the actual selling value is \\$145,000, this model underpredicts the value of the house. \n\nThe residual for this data point is the difference between y_hat and y. In this case, that is \\$145,000 - \\$107,296, which is \\$37,704. This house sold for \\$37,704 more than the predicted value. \n\n\n::: {.cell}\n\n```{.r .cell-code}\n145000 - 107296\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 37704\n```\n:::\n:::\n\n\n## B \n\n*For fixed lot size, how much is the house selling price predicted to increase for each square-foot increase in home size? Why?*\n\nThe house selling price is predicted to increase \\$53.80 for each square-foot increase, controlling for lot size. 53.80 is the slope or coefficient of the house size.  \n\nŷ = −10,536 + 53.8x1 + 2.84x2\n\n## C\n\n*According to this prediction equation, for fixed home size, how much would lot size need to increase to have the same impact as a one-square-foot increase in home size?*\n\nA one square foot increase in the size of the home would lead to a \\$53.80 increase in the sales prices, and a one square foot increase in the size of the lot would lead to a \\$2.84 increase in the sales price. The find how much the lot size would need to increase to have the same impact as a one square foot increase in home size we can this equation 53.8=x*2.84 and solve for x.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n53.8/2.84\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 18.94366\n```\n:::\n:::\n\nThe lot would need to increase by over 18.94 square feet to increase the sales price as much as one square foot increase in home size. \n\n# Question 2\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(salary)\n\nglimpse(salary)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 52\nColumns: 6\n$ degree <fct> Masters, Masters, Masters, Masters, PhD, Masters, PhD, Masters,…\n$ rank   <fct> Prof, Prof, Prof, Prof, Prof, Prof, Prof, Prof, Prof, Prof, Pro…\n$ sex    <fct> Male, Male, Male, Female, Male, Male, Female, Male, Male, Male,…\n$ year   <int> 25, 13, 10, 7, 19, 16, 0, 16, 13, 13, 12, 15, 9, 9, 9, 7, 13, 1…\n$ ysdeg  <int> 35, 22, 23, 27, 30, 21, 32, 18, 30, 31, 22, 19, 17, 27, 24, 15,…\n$ salary <int> 36350, 35350, 28200, 26775, 33696, 28516, 24900, 31909, 31850, …\n```\n:::\n:::\n\n\n## A\n\n*Test the hypothesis that the mean salary for men and women is the same, without regard to\nany other variable but sex. Explain your findings.*\n\nTo test that hypothesis that the mean salary for men and women is the same, we'll run a two sample t-test.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nt.test(formula = salary ~ sex, data = salary)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tWelch Two Sample t-test\n\ndata:  salary by sex\nt = 1.7744, df = 21.591, p-value = 0.09009\nalternative hypothesis: true difference in means between group Male and group Female is not equal to 0\n95 percent confidence interval:\n -567.8539 7247.1471\nsample estimates:\n  mean in group Male mean in group Female \n            24696.79             21357.14 \n```\n:::\n:::\n\nThe p-value of this test is larger than 0.05, so if we choose that for our alpha value then we should retain the null hypothesis that the mean salary for men and women is the same. The 95% confidence interval also spans from -567.8539 to 7,247.1471, which includes 0 and indicates that the means are not significantly different from one another. \n\n## B\n\n*Run a multiple linear regression with salary as the outcome variable and everything else as\npredictors, including sex. Assuming no interactions between sex and the other predictors,\nobtain a 95% confidence interval for the difference in salary between males and females.*\n\n\n::: {.cell}\n\n```{.r .cell-code}\nQbLM <- (lm(formula = salary ~ degree + rank + sex + year + ysdeg, data = salary))\n\nstargazer(QbLM, data = salary, type = 'text')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n===============================================\n                        Dependent variable:    \n                    ---------------------------\n                              salary           \n-----------------------------------------------\ndegreePhD                    1,388.613         \n                            (1,018.747)        \n                                               \nrankAssoc                  5,292.361***        \n                            (1,145.398)        \n                                               \nrankProf                   11,118.760***       \n                            (1,351.772)        \n                                               \nsexFemale                    1,166.373         \n                             (925.569)         \n                                               \nyear                        476.309***         \n                             (94.914)          \n                                               \nysdeg                        -124.574          \n                             (77.486)          \n                                               \nConstant                   15,746.050***       \n                             (800.178)         \n                                               \n-----------------------------------------------\nObservations                    52             \nR2                             0.855           \nAdjusted R2                    0.836           \nResidual Std. Error     2,398.418 (df = 45)    \nF Statistic           44.239*** (df = 6; 45)   \n===============================================\nNote:               *p<0.1; **p<0.05; ***p<0.01\n\n===============================================\nStatistic N     Mean    St. Dev.   Min    Max  \n-----------------------------------------------\nyear      52   7.481      5.508     0      25  \nysdeg     52   16.115    10.222     1      35  \nsalary    52 23,797.650 5,917.289 15,000 38,045\n-----------------------------------------------\n```\n:::\n:::\n\n\nAfter running the multiple linear regression and using the stargazer() function we see in the results that the coefficient for the `sex` variable is 1166.37 for sex Female with a standard error of 925.569. We can also see from these results that there are 52 observations, so n = 52. With these figures we can calculate the 95% confidence interval.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# CI = (X bar) ± (t × s/sqrt(n)) = CI = (X bar) ± (t × se)\n\nt_score <- qt(.025, df = 45)\n\n1166.373 + (t_score * 925.569)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] -697.8187\n```\n:::\n\n```{.r .cell-code}\n1166.373 - (t_score * 925.569)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 3030.565\n```\n:::\n:::\n\n\nOr we can use the `confint()` function in R:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n  confint(QbLM) \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                 2.5 %      97.5 %\n(Intercept) 14134.4059 17357.68946\ndegreePhD    -663.2482  3440.47485\nrankAssoc    2985.4107  7599.31080\nrankProf     8396.1546 13841.37340\nsexFemale    -697.8183  3030.56452\nyear          285.1433   667.47476\nysdeg        -280.6397    31.49105\n```\n:::\n:::\n\n## C\n\n*Interpret your finding for each predictor variable; discuss (a) statistical significance, (b)\ninterpretation of the coefficient / slope in relation to the outcome variable and other variables*\n\nIn this regression analysis the `sex`, `ysdeg`, and `degree` variables are not statistically significant. Because these variables are not significant I will not interpret the coefficients.\n\nThe `rank` and `year` variables were found to be statistically significant at the level of 0.01 (1%). The coefficients indicate that with the \"Assoc\" rank, a person will make \\$5,292.361 more than a person with the \"Asst\" rank (since the \"Assoc\" rank was not included in the regression output we know it was the baseline). With the rank \"Prof\" a person will make \\$11,118.760 more than a person with \"Asst\" rank. \n\nWith each unit increase of the `year` variable a person will make \\$476.309 more in salary. \n\n## D\n\n*Change the baseline category for the rank variable. Interpret the coefficients related to rank\nagain.*\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsalary$rank <- relevel(salary$rank, ref = \"Prof\")\n\nsummary(lm(formula = salary ~ degree + rank + sex + year + ysdeg, data = salary))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = salary ~ degree + rank + sex + year + ysdeg, data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4045.2 -1094.7  -361.5   813.2  9193.1 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  26864.81    1375.29  19.534  < 2e-16 ***\ndegreePhD     1388.61    1018.75   1.363    0.180    \nrankAsst    -11118.76    1351.77  -8.225 1.62e-10 ***\nrankAssoc    -5826.40    1012.93  -5.752 7.28e-07 ***\nsexFemale     1166.37     925.57   1.260    0.214    \nyear           476.31      94.91   5.018 8.65e-06 ***\nysdeg         -124.57      77.49  -1.608    0.115    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2398 on 45 degrees of freedom\nMultiple R-squared:  0.855,\tAdjusted R-squared:  0.8357 \nF-statistic: 44.24 on 6 and 45 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\nI used the `relevel()` function to change the baseline value for the `rank` variable to \"Prof\". Now the results tell us that people with a \"Asst\" rank make \\$11,118.76 less than people with a \"Prof\" rank, and people with an \"Assoc\" rank make \\$5,826.40 than those with a \"Prof\" rank. Both of these findings are statistically significant. \n\n## E\n\n...\n*Exclude the variable rank, refit, and summarize how your findings changed, if they did.*\n\n\n::: {.cell}\n\n```{.r .cell-code}\nQbLM <- (lm(formula = salary ~ degree + rank + sex + year + ysdeg, data = salary))\n\nQbLM_noRank <- (lm(formula = salary ~ degree + sex + year + ysdeg, data = salary))\n\nstargazer(QbLM, QbLM_noRank, data = salary, type = 'text')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n=================================================================\n                                 Dependent variable:             \n                    ---------------------------------------------\n                                       salary                    \n                             (1)                    (2)          \n-----------------------------------------------------------------\ndegreePhD                 1,388.613             -3,299.349**     \n                         (1,018.747)            (1,302.520)      \n                                                                 \nrankAsst                -11,118.760***                           \n                         (1,351.772)                             \n                                                                 \nrankAssoc               -5,826.403***                            \n                         (1,012.933)                             \n                                                                 \nsexFemale                 1,166.373              -1,286.544      \n                          (925.569)             (1,313.089)      \n                                                                 \nyear                      476.309***             351.969**       \n                           (94.914)              (142.481)       \n                                                                 \nysdeg                      -124.574              339.399***      \n                           (77.486)               (80.621)       \n                                                                 \nConstant                26,864.810***          17,183.570***     \n                         (1,375.288)            (1,147.942)      \n                                                                 \n-----------------------------------------------------------------\nObservations                  52                     52          \nR2                          0.855                  0.631         \nAdjusted R2                 0.836                  0.600         \nResidual Std. Error  2,398.418 (df = 45)    3,743.502 (df = 47)  \nF Statistic         44.239*** (df = 6; 45) 20.107*** (df = 4; 47)\n=================================================================\nNote:                                 *p<0.1; **p<0.05; ***p<0.01\n\n===============================================\nStatistic N     Mean    St. Dev.   Min    Max  \n-----------------------------------------------\nyear      52   7.481      5.508     0      25  \nysdeg     52   16.115    10.222     1      35  \nsalary    52 23,797.650 5,917.289 15,000 38,045\n-----------------------------------------------\n```\n:::\n:::\n\nRemoving the `rank` variable from the analysis changes the significance of some variables. With the `rank` variable included in the model the variables `degree` and `ysdeg` were not significant and the variable `year` was significant at the 0.01 (1%) level. With the `rank` variable removed both the `degree` and `ysdeg` variables are now statistically significant, with `degree` at the 0.05 (5%) level and `ysdeg` at the 0.01 (1%) level. The `year` variable also dropped one significance level, from 0.01 (1%) to 0.05 (5%).\n\nThe sign also flipped for the variables `ysdeg`, `sexFemale`, and `degreePhD`.\n\n## F\n\n*Everyone in this dataset was hired the year they earned their highest degree. It is also\nknown that a new Dean was appointed 15 years ago, and everyone in the dataset who\nearned their highest degree 15 years ago or less than that has been hired by the new Dean.\nSome people have argued that the new Dean has been making offers that are a lot more\ngenerous to newly hired faculty than the previous one and that this might explain some of\nthe variation in Salary.*\n\n*Create a new variable that would allow you to test this hypothesis and run another multiple\nregression model to test this. Select variables carefully to make sure there is no\nmulticollinearity. Explain why multicollinearity would be a concern in this case and how\nyou avoided it. Do you find support for the hypothesis that the people hired by the new\nDean are making higher than those that were not?*\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nQ2F <- salary %>% \n  mutate(\"new_dean\" = case_when(\n    ysdeg <=15 ~ 1, \n    ysdeg >15 ~ 0))\n\nsummary(lm(salary ~ new_dean + degree + rank + sex + year, data = Q2F))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = salary ~ new_dean + degree + rank + sex + year, \n    data = Q2F)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3403.3 -1387.0  -167.0   528.2  9233.8 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  24425.32    1107.52  22.054  < 2e-16 ***\nnew_dean      2163.46    1072.04   2.018   0.0496 *  \ndegreePhD      818.93     797.48   1.027   0.3100    \nrankAsst    -11096.95    1191.00  -9.317 4.54e-12 ***\nrankAssoc    -6124.28    1028.58  -5.954 3.65e-07 ***\nsexFemale      907.14     840.54   1.079   0.2862    \nyear           434.85      78.89   5.512 1.65e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2362 on 45 degrees of freedom\nMultiple R-squared:  0.8594,\tAdjusted R-squared:  0.8407 \nF-statistic: 45.86 on 6 and 45 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\nSince the new variable, `new_dean`, was built from the `ysdeg` variable, to avoid multicollinearity I did not include the `ysdeg` variable in the regression model (since the `new_dean` variable was included). We can confirm that these two variables would likely cause multicollinearity by checking that their correlation is high. \n\n\n::: {.cell}\n\n```{.r .cell-code}\ncor.test(Q2F$new_dean, Q2F$ysdeg)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tPearson's product-moment correlation\n\ndata:  Q2F$new_dean and Q2F$ysdeg\nt = -11.101, df = 50, p-value = 4.263e-15\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.9074548 -0.7411040\nsample estimates:\n       cor \n-0.8434239 \n```\n:::\n:::\n\nIt appears that these two variables have a high negative correlation. \n\nThe output of the regression with the `new_dean` variable shows that if a person was hired by the new dean they would earn \\$2,163.46 more in salary than if they were hired by the old dean. This finding is significant at the 0.05 (5%) level. \n\n\n# Question 3\n\n## A\n\n*Using the house.selling.price data, run and report regression results modeling y = selling price (in dollars) in terms of size of home (in square feet) and whether the home is new (1 = yes; 0 = no). In particular, for each variable; discuss statistical significance and interpret the\nmeaning of the coefficient.*\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(house.selling.price)\nglimpse(house.selling.price)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 100\nColumns: 7\n$ case  <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 1…\n$ Taxes <int> 3104, 1173, 3076, 1608, 1454, 2997, 4054, 3002, 6627, 320, 630, …\n$ Beds  <int> 4, 2, 4, 3, 3, 3, 3, 3, 5, 3, 3, 3, 3, 3, 3, 2, 3, 3, 2, 3, 2, 4…\n$ Baths <int> 2, 1, 2, 2, 3, 2, 2, 2, 4, 2, 2, 2, 2, 2, 1, 1, 2, 1, 2, 2, 1, 3…\n$ New   <int> 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1…\n$ Price <int> 279900, 146500, 237700, 200000, 159900, 499900, 265500, 289900, …\n$ Size  <int> 2048, 912, 1654, 2068, 1477, 3153, 1355, 2075, 3990, 1160, 1220,…\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nhouseData <- (lm(Price ~ Size + New, data = house.selling.price))\nsummary(houseData)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = Price ~ Size + New, data = house.selling.price)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-205102  -34374   -5778   18929  163866 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -40230.867  14696.140  -2.738  0.00737 ** \nSize           116.132      8.795  13.204  < 2e-16 ***\nNew          57736.283  18653.041   3.095  0.00257 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 53880 on 97 degrees of freedom\nMultiple R-squared:  0.7226,\tAdjusted R-squared:  0.7169 \nF-statistic: 126.3 on 2 and 97 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\nThe regression output indicates that both the `Size` and the `New` variable are significant (Size at the .1% level and New at the 1% level.) The coefficients tell use that with an increase in size, the price will increase by \\$116.132, and that the price for new houses will be \\$57,736.283 more than old houses. \n\n## B\n\n*Report and interpret the prediction equation, and form separate equations relating selling price to size for new and for not new homes.*\n\nE(price) = -40,230.867 + 116.132 * Size + 57736.283 * New\n\n*New home:*\nE(price) = -40,230.867 + 116.132 * Size + 57736.283 * 1\n\nWe use 1 for x1 with a new home and get the following equation: \nE(price) = 17,505.42 + 116.132 * Size\n\n*Old home:*\nE(price) = -40,230.867 + 116.132 * Size + 57736.283 * 0\n\nWe use 0 for x1 with an old home and get the following equation:  \nE(price) = -40,230.867 + 116.132 * Size\n\n## C\n\n*Find the predicted selling price for a home of 3000 square feet that is (i) new, (ii) not new.*\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Equation: -40,230.867 + 116.132 * size + 57736.283 * new\n\n# new home\nsize <- 3000\nnew <- 1\n-40230.867 + 116.132 * size + 57736.283 * new\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 365901.4\n```\n:::\n\n```{.r .cell-code}\n# old home\nsize <- 3000\nnew <- 0\n-40230.867 + 116.132 * size + 57736.283 * new\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 308165.1\n```\n:::\n:::\n\n\nThe predicted price of a new 3,000 square foot home is \\$365,901.40 and the predicted price of an old 3,000 square foot home is \\$308,165.10.\n\n## D\n\n*Fit another model, this time with an interaction term allowing interaction between size and new, and report the regression results*\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(lm(Price ~ Size + New + Size*New, data = house.selling.price))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = Price ~ Size + New + Size * New, data = house.selling.price)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-175748  -28979   -6260   14693  192519 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -22227.808  15521.110  -1.432  0.15536    \nSize           104.438      9.424  11.082  < 2e-16 ***\nNew         -78527.502  51007.642  -1.540  0.12697    \nSize:New        61.916     21.686   2.855  0.00527 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 52000 on 96 degrees of freedom\nMultiple R-squared:  0.7443,\tAdjusted R-squared:  0.7363 \nF-statistic: 93.15 on 3 and 96 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\nThe regression results indicate that the `Size` variable is still significant at the 0.001 (.1%) level, but the `New` variable is no longer significant. The interaction term `Size*New` is significant at the 0.01 (1%) level.\n\n## E\n\n*Report the lines relating the predicted selling price to the size for homes that are (i) new, (ii) not new.*\n\nI assume by \"report the lines\" we mean interpret the coefficients here...\n\nequation: E(price) = -40230.867 + 104.438 * size - 78527.502 * new + 61.916(size*new)\n\n*New house:*\nWe replace \"new\" with 1. \n-22227.808 + 104.438 * size - 78527.502 * 1 + 61.916(size*1)\n\n-100755.3 + 166.354 * size \n\n*Old house:*\nWe replace \"new\" with 0\n-22227.808 + 104.438 * size - 78527.502 * 0 + 61.916(size*0)\n\n-22227.808 + 104.438 * size\n\n\n## F\n\n*Find the predicted selling price for a home of 3000 square feet that is (i) new, (ii) not new.*\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# new house\nsize <- 3000\nnew <- 1\n-22227.808 + 104.438 * size - 78527.502 * new + 61.916*(size*new)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 398306.7\n```\n:::\n\n```{.r .cell-code}\n# old house\nnew <- 0\n-22227.808 + 104.438 * size - 78527.502 * new + 61.916*(size*new)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 291086.2\n```\n:::\n:::\n\n\n(i)\nThe predicted price for a new house that is 3,000 square feet is \\$398,306.70.\n\n(ii) \nThe predicted price for an old house of 3,000 square feet is \\$291,086.20.\n\n## G\n\n*Find the predicted selling price for a home of 1500 square feet that is (i) new, (ii) not new. Comparing to (F), explain how the difference in predicted selling prices changes as the size of home increases.*\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# new house\nsize <- 1500\nnew <- 1\n-22227.808 + 104.438 * size - 78527.502 * new + 61.916*(size*new)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 148775.7\n```\n:::\n\n```{.r .cell-code}\n# old house\nnew <- 0\n-22227.808 + 104.438 * size - 78527.502 * new + 61.916*(size*new)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 134429.2\n```\n:::\n:::\n\n\n(i)\nThe predicted price for a new house that is 1,500 square feet is \\$148,775.70.\n\n(ii) \nThe predicted price for an old house of 1,500 square feet is \\$134,429.20.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# F\n398306.7 - 291086.2\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 107220.5\n```\n:::\n\n```{.r .cell-code}\n# G\n148775.7 - 134429.2\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 14346.5\n```\n:::\n:::\n\n\nThe price difference between old and new houses is greater for larger houses (\\$107,220.50) than for smaller houses (\\$14,346.50). This is captured in the interaction term. \n\n## H\n\n*Do you think the model with interaction or the one without it represents the relationship of size and new to the outcome price? What makes you prefer one model over another?*\n\n\n::: {.cell}\n\n```{.r .cell-code}\nHSPmodel <- (lm(Price ~ Size + New, data = house.selling.price))\n\nHSPmodel_ia <- (lm(Price ~ Size + New + Size*New, data = house.selling.price))\n\nstargazer(HSPmodel, HSPmodel_ia, type = 'text')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n==================================================================\n                                 Dependent variable:              \n                    ----------------------------------------------\n                                        Price                     \n                              (1)                    (2)          \n------------------------------------------------------------------\nSize                      116.132***              104.438***      \n                            (8.795)                (9.424)        \n                                                                  \nNew                      57,736.280***           -78,527.500      \n                         (18,653.040)            (51,007.640)     \n                                                                  \nSize:New                                          61.916***       \n                                                   (21.686)       \n                                                                  \nConstant                -40,230.870***           -22,227.810      \n                         (14,696.140)            (15,521.110)     \n                                                                  \n------------------------------------------------------------------\nObservations                  100                    100          \nR2                           0.723                  0.744         \nAdjusted R2                  0.717                  0.736         \nResidual Std. Error  53,880.950 (df = 97)    51,998.110 (df = 96) \nF Statistic         126.335*** (df = 2; 97) 93.151*** (df = 3; 96)\n==================================================================\nNote:                                  *p<0.1; **p<0.05; ***p<0.01\n```\n:::\n:::\n\n\nI prefer the model with the interaction term because the interaction term is significant and both the R^2 and adjusted R^2 figures are higher than for the model without the interaction term. The residual standard errors are also smaller for the model with the interaction term. ",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}