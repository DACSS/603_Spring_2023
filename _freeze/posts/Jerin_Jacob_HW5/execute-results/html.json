{
  "hash": "aa1acd009ea46cbc37e640e6417c886c",
  "result": {
    "markdown": "---\ntitle: \"Homework 5\"\nauthor: \"Jerin Jacob\"\ndesription: \"Homework 5- 603 Spring 2023\"\ndate: \"05/13/2023\"\nformat:\n  html:\n    toc: true\n    code-fold: true\n    code-copy: true\n    code-tools: true\ncategories:\n  - hw5\n  - challenge5\n  - Jerin Jacob\n  \n---\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(readxl)\nlibrary(dplyr)\nlibrary(magrittr)\nlibrary(alr4)\nlibrary(smss)\nknitr::opts_chunk$set(echo = TRUE)\n```\n:::\n\n\nQuestion 1:\nA)\nFor backward elimination, `Beds` would be deleted first because it has the largest p-value. .487 is a very high p-value based on any conventional level of significance.\n\nB)\nFor forward selection, I would add `Size` first. It has the highest correlation coefficient. Also, this variable is statistically significant.\n\nC)\nOnce other variables are controlled for, most of the explanation done by Beds is instead done by other variables. For example, houses with more beds are also larger in size. When Beds is the only explanatory variable, the model attributes the effect of the Size variable to Beds. However, once Size is also in the model and thus controlled for, the p-value of Beds gets larger as it doesn't really have much explanatory power when it comes to variation in Price that remains unexplained after accounting for Size.\n\n\nD)\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(\"house.selling.price.2\", package = \"smss\")\nhead(house.selling.price.2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      P    S Be Ba New\n1  48.5 1.10  3  1   0\n2  55.0 1.01  3  2   0\n3  68.0 1.45  3  2   0\n4 137.0 2.40  3  3   0\n5 309.4 3.30  4  3   1\n6  17.5 0.40  1  1   0\n```\n:::\n:::\n\nRunning all regression models\n\n::: {.cell}\n\n```{.r .cell-code}\nfull_model <- lm(P ~ ., data = house.selling.price.2)\nmodel_noBeds <- lm(P ~ .-Be, data = house.selling.price.2)\nmodel_noBeds_noBaths <- lm(P ~ S + New, data = house.selling.price.2)\nmodel_size_only <- lm(P ~ S, data = house.selling.price.2)\n```\n:::\n\n\nCreate a functions to get R-squared, Adjusted R-squared & PRESS \n\n::: {.cell}\n\n```{.r .cell-code}\nrsquared <- function(fit) summary(fit)$r.squared\nadj_rsquared <- function(fit) summary(fit)$adj.r.squared\nPRESS <- function(fit) {\n  pr <- residuals(fit)/(1-lm.influence(fit)$hat)\n  sum(pr^2)\n}\n```\n:::\n\n\nFor AIC and BIC, the functions AIC() and BIC() can be used\n\nNow, applying the functions to model objects\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodels <- list(full_model, model_noBeds, model_noBeds_noBaths, model_size_only)\ndata.frame(models = c('full_model', 'model_noBeds', 'model_noBeds&Baths', 'model_only_size'),\n           rSquared = sapply(models, rsquared),\n           adj_rSquared = sapply(models, adj_rsquared),\n           PRESS = sapply(models, PRESS),\n           AIC = sapply(models, AIC),\n           BIC = sapply(models, BIC)) |>\n  print()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n              models  rSquared adj_rSquared    PRESS      AIC      BIC\n1         full_model 0.8688630    0.8629022 28390.22 790.6225 805.8181\n2       model_noBeds 0.8681361    0.8636912 27860.05 789.1366 801.7996\n3 model_noBeds&Baths 0.8483699    0.8450003 31066.00 800.1262 810.2566\n4    model_only_size 0.8078660    0.8057546 38203.29 820.1439 827.7417\n```\n:::\n:::\n\nFor R-Squared and Adjusted R-Squared, larger values are preferred while for PRESS, AIC and BIC, smaller values are the best.\n\nHere, model using all the variables (full_model) has the highest R-Squared while model with no Beds(model_noBeds) has the highest Adjusted R-Squared.\n\nmodel_noBeds has the lowest PRESS, AIC and BIC\n\n\nE)\nSince R-Squared value always increase as the new variables are added, it is not usefull for our analysis. So, considering other 4 criteria, we can select the model with no Beds(model_noBeds) as it's Adjusted R-Squared is the highest and PRESS, AIC & BIC are the lowest.\n\n\nQuestion 2:\n\nLoading the data\n\n::: {.cell}\n\n```{.r .cell-code}\nhead(trees)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  Girth Height Volume\n1   8.3     70   10.3\n2   8.6     65   10.3\n3   8.8     63   10.2\n4  10.5     72   16.4\n5  10.7     81   18.8\n6  10.8     83   19.7\n```\n:::\n:::\n\nA)\nFitting a model with the Volume as outcome and Girth and Height as the explanatory variables\n\n::: {.cell}\n\n```{.r .cell-code}\ntree_model <- lm(Volume ~ Girth + Height, data = trees)\nsummary(tree_model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = Volume ~ Girth + Height, data = trees)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.4065 -2.6493 -0.2876  2.2003  8.4847 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -57.9877     8.6382  -6.713 2.75e-07 ***\nGirth         4.7082     0.2643  17.816  < 2e-16 ***\nHeight        0.3393     0.1302   2.607   0.0145 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.882 on 28 degrees of freedom\nMultiple R-squared:  0.948,\tAdjusted R-squared:  0.9442 \nF-statistic:   255 on 2 and 28 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n\nB)\nCreating regression diagnostic plots\n\n::: {.cell}\n\n```{.r .cell-code}\npar(mfrow=c(2,3))\nplot(tree_model, which = 1:6)\n```\n\n::: {.cell-output-display}\n![](Jerin_Jacob_HW5_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\nResiduals vs Fitted plot should bounce randomly around the '0' line. Here it is a curve which suggests the violation of linearity assumption. The Scale-Location plot should be approximately horizontal but here it is a curve, eventhough not a perfect one. This suggests the violation of the assumption of Constant Variance. Cook's distance for observation 31 is clearly larger than 4/n which is 4/31 = 0.129. This is a violation of the assumption of Influential Observation.\n\n\nQuestion 3:\nA)\n\n::: {.cell}\n\n```{.r .cell-code}\ndata('florida')\n\nflorida_model <- lm(Buchanan ~ Bush, data = florida)\npar(mfrow = c(2,3))\nplot(florida_model, which = 1:6)\n```\n\n::: {.cell-output-display}\n![](Jerin_Jacob_HW5_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\nFrom the plot, Palm Beach County is largely an outlier.\n\nB)\n\n::: {.cell}\n\n```{.r .cell-code}\nlog_florida_model <- lm(log(Buchanan) ~ log(Bush), data = florida)\npar(mfrow = c(2,3))\nplot(log_florida_model, which = 1:6)\n```\n\n::: {.cell-output-display}\n![](Jerin_Jacob_HW5_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\nWhen we did the log transformation for each variable, the Palm Beach County became less of an outlier but still stands out.\n\n\n\n\n\n\n\n\n",
    "supporting": [
      "Jerin_Jacob_HW5_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}