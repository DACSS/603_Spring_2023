{
  "hash": "678f06596ee472243a0d89ff2bcfc69f",
  "result": {
    "markdown": "---\ntitle: \"Homework 5\"\nauthor: \"Rosemary Pang\"\ndescription: \"Solutions to the fifth homework\"\ndate: \"05/12/2023\"\nformat:\n  html:\n    toc: true\n    code-fold: false \n    code-copy: true\n    code-tools: true\ncategories:\n  - hw5\n---\n\n\nPlease check your answers against the solutions.\n\nLoad the necessary packages:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(smss)\nlibrary(alr4)\nlibrary(magrittr)\n```\n:::\n\n\n## Question 1\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(\"house.selling.price.2\")\n# object name too long, rename\nhouse <- house.selling.price.2\nrm(house.selling.price.2)\n\n# also rename the variables to make them more intuitive\n\nnames(house) <- c('Price', 'Size', 'Beds', 'Baths', 'New')\n```\n:::\n\n\n### A\n\nThe variable $\\textrm{Beds}$ would be deleted first in backward elimination, because its p-value 0.487 is the largest of all the variables.\n\n### B\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Write a function that, for any given variable name, regresses that variable on Price (P) and gives the p-value:\n\nget_p_value <- function(variable) {\n  variable %>%\n    paste0(\"Price ~ \", .) %>%\n    as.formula() %>%\n    lm(data = house) %>%\n    summary() %>%\n    use_series('coefficients') %>%\n    extract(variable,'Pr(>|t|)') %>%\n    return()\n}\n\n# apply the function to all potential explanatory variables\nsapply(names(house)[-1], get_p_value)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n        Size         Beds        Baths          New \n2.346660e-34 4.758795e-10 9.839165e-16 4.514570e-04 \n```\n:::\n:::\n\n\nSize has the lowest p-value when regressed against Price, so it should be the first variable to be added in forward selection.\n\nThe correlation matrix given in the question gives us the same information: out of all the candidates, Size has the highest correlation with Price: 0.899.\nBecause all pair-wise correlations calculated in the correlation matrix are for samples of same size, the one with highest magnitude will also have the smallest p-value.\n\n\n### C\n\nOnce other variables are controlled for, most of the explanation done by Beds is instead done by other variables. For example, houses with more beds are also larger in size. When Beds is the only explanatory variable, the model attributes the effect of the Size variable to Beds. However, once Size is also in the model and thus controlled for, the p-value of Beds gets larger as it doesn't really have much explanatory power when it comes to variation in Price that remains unexplained after accounting for Size.\n\n\n### D\n\nThe question seems to suggest we should compare all possible models, which is possible but probably not a very good idea. When we do backward elimination, we get the model that only  excludes 'Beds.' So, let's compare the model that has Beds to the one that doesn't instead of comparing everything.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_full <- lm(Price ~ ., data = house)\nfit_nobeds <- lm(Price ~ . -Beds, data = house)\n```\n:::\n\n\nNow, let's write functions to gather each metric that we want to use:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrsq <- function(fit) summary(fit)$r.squared\nadj_rsq <- function(fit) summary(fit)$adj.r.squared\nPRESS <- function(fit) {\n    pr <- residuals(fit)/(1 - lm.influence(fit)$hat)\n    sum(pr^2)\n}\n# the functions for AIC and BIC are already AIC() and BIC()\n```\n:::\n\n\nApply the two functions to the two objects:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodels <- list(fit_full, fit_nobeds)\ndata.frame(models = c('fit_full', 'fit_nobeds'),\n           rsq = sapply(models, rsq),\n           adj.rsq = sapply(models, adj_rsq),\n           PRESS = sapply(models, PRESS),\n           AIC = sapply(models, AIC),\n           BIC = sapply(models, BIC)\n) %>% \n  print()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      models       rsq   adj.rsq    PRESS      AIC      BIC\n1   fit_full 0.8688630 0.8629022 28390.22 790.6225 805.8181\n2 fit_nobeds 0.8681361 0.8636912 27860.05 789.1366 801.7996\n```\n:::\n:::\n\n\nNote that, for R-squares and Adjusted R-squared, larger is \"better\" while for PRESS, AIC, and BIC, lower is \"better.\"\nHowever, because R-squared always increases with the addition of new variables, it's not very important for our purposes here.\nWith other metrics, the model without Beds is the better model since it has higher adjusted R-squared, lower PRESS, AIC, and BIC.\n\n### E\n\nThus I select the model without the Beds variable.\n\n## Question 2\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(trees)\n```\n:::\n\n\n### A\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntree_model <- lm(Volume ~ Girth + Height, data = trees)\nsummary(tree_model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = Volume ~ Girth + Height, data = trees)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.4065 -2.6493 -0.2876  2.2003  8.4847 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -57.9877     8.6382  -6.713 2.75e-07 ***\nGirth         4.7082     0.2643  17.816  < 2e-16 ***\nHeight        0.3393     0.1302   2.607   0.0145 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.882 on 28 degrees of freedom\nMultiple R-squared:  0.948,\tAdjusted R-squared:  0.9442 \nF-statistic:   255 on 2 and 28 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n\n### B \n\n\n::: {.cell}\n\n```{.r .cell-code}\npar(mfrow = c(2,3))\nplot(tree_model, which = 1:6)\n```\n\n::: {.cell-output-display}\n![](HW5_solutions_Pang_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\nThe most obvious violation, based on the first plot, is the violation of the linearity assumption. The red line in the fitted values vs residuals plot should be somewhat straight, but it is actually U-shaped. The main reason for this is that volume is related to $\\textrm{diameter}^2$, but we use the diameter itself in the model (Girth). Let's see if using a quadratic term fixes the issue.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntree_model_quad <- lm(Volume ~ Girth + I(Girth^2) + Height, data = trees)\nplot(tree_model_quad, which = 1)\n```\n\n::: {.cell-output-display}\n![](HW5_solutions_Pang_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\nThe line is much more straight now.\n\n\n## Question 3\n\n\n### A\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(\"florida\")\nflorida_model <- lm(Buchanan ~ Bush, data = florida)\npar(mfrow = c(2,3))\nplot(florida_model, which = 1:6)\n```\n\n::: {.cell-output-display}\n![](HW5_solutions_Pang_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n\n\nIn this model, Palm Beach County is a massive outlier. \n\n### B\n\nIf we fit a model where each variable is logged, Palm Beach County would be somewhat less of an outlier but would still stand out. Let's see:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(\"florida\")\nflorida_model_logged <- lm(log(Buchanan) ~ log(Bush), data = florida)\npar(mfrow = c(2,3))\nplot(florida_model_logged, which = 1:6)\n```\n\n::: {.cell-output-display}\n![](HW5_solutions_Pang_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n:::\n",
    "supporting": [
      "HW5_solutions_Pang_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}