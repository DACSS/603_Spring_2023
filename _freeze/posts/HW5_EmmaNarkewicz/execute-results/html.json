{
  "hash": "b82d7f28c55c059b298fbb73ec92dc3e",
  "result": {
    "markdown": "---\ntitle: \"Homework 5\"\nauthor: \"Emma Narkewicz\"\ndescription: \"Diagnostics\"\ndate: \"05/09/2023\"\nformat:\n  html:\n    toc: true\n    code-fold: true\n    code-copy: true\n    code-tools: true\ncategories:\n  - hw5\n  - emma_narkewicz\n  - diagnostics\n---\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.3.6      ✔ purrr   0.3.4 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.0      ✔ stringr 1.4.1 \n✔ readr   2.1.2      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n```\n:::\n\n```{.r .cell-code}\nlibrary(alr4)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: car\nLoading required package: carData\n\nAttaching package: 'car'\n\nThe following object is masked from 'package:dplyr':\n\n    recode\n\nThe following object is masked from 'package:purrr':\n\n    some\n\nLoading required package: effects\nlattice theme set by effectsTheme()\nSee ?effectsTheme for details.\n```\n:::\n\n```{.r .cell-code}\nlibrary(smss)\nlibrary(sjPlot)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLearn more about sjPlot with 'browseVignettes(\"sjPlot\")'.\n```\n:::\n\n```{.r .cell-code}\nlibrary(sjmisc)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nInstall package \"strengejacke\" from GitHub (`devtools::install_github(\"strengejacke/strengejacke\")`) to load all sj-packages at once!\n\nAttaching package: 'sjmisc'\n\nThe following object is masked from 'package:purrr':\n\n    is_empty\n\nThe following object is masked from 'package:tidyr':\n\n    replace_na\n\nThe following object is masked from 'package:tibble':\n\n    add_case\n```\n:::\n\n```{.r .cell-code}\nlibrary(stargazer)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nPlease cite as: \n\n Hlavac, Marek (2022). stargazer: Well-Formatted Regression and Summary Statistics Tables.\n R package version 5.2.3. https://CRAN.R-project.org/package=stargazer \n```\n:::\n\n```{.r .cell-code}\nlibrary(broom)\nlibrary(qpcR)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: MASS\n\nAttaching package: 'MASS'\n\nThe following object is masked from 'package:dplyr':\n\n    select\n\nLoading required package: minpack.lm\nLoading required package: rgl\nLoading required package: robustbase\n\nAttaching package: 'robustbase'\n\nThe following object is masked from 'package:alr4':\n\n    cloud\n\nLoading required package: Matrix\n\nAttaching package: 'Matrix'\n\nThe following objects are masked from 'package:tidyr':\n\n    expand, pack, unpack\n```\n:::\n:::\n\n\n# Question 1\n\nI started out by reading in the house.selling.price.2 data set & recreating the correlation matrix & regression output for home price.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#Reading in Data\ndata(\"house.selling.price.2\")\nhead(house.selling.price.2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      P    S Be Ba New\n1  48.5 1.10  3  1   0\n2  55.0 1.01  3  2   0\n3  68.0 1.45  3  2   0\n4 137.0 2.40  3  3   0\n5 309.4 3.30  4  3   1\n6  17.5 0.40  1  1   0\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n#Correlation matrix\ncor(house.selling.price.2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n            P         S        Be        Ba       New\nP   1.0000000 0.8988136 0.5902675 0.7136960 0.3565540\nS   0.8988136 1.0000000 0.6691137 0.6624828 0.1762879\nBe  0.5902675 0.6691137 1.0000000 0.3337966 0.2672091\nBa  0.7136960 0.6624828 0.3337966 1.0000000 0.1820651\nNew 0.3565540 0.1762879 0.2672091 0.1820651 1.0000000\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n#Regression Output all variables explanatory\nsummary(lm(P ~ S + Be + Ba + New, data = house.selling.price.2))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = P ~ S + Be + Ba + New, data = house.selling.price.2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-36.212  -9.546   1.277   9.406  71.953 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -41.795     12.104  -3.453 0.000855 ***\nS             64.761      5.630  11.504  < 2e-16 ***\nBe            -2.766      3.960  -0.698 0.486763    \nBa            19.203      5.650   3.399 0.001019 ** \nNew           18.984      3.873   4.902  4.3e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 16.36 on 88 degrees of freedom\nMultiple R-squared:  0.8689,\tAdjusted R-squared:  0.8629 \nF-statistic: 145.8 on 4 and 88 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n\n### A\n\nFor backward elimination, which variable would be deleted first? Why?\n\nBeds would be the first variable to be deleted, as it has the largest P-value of 0.48673 of the 4 explanatory variables and is the only explanatory variable that is not statistically significant at any level (0.1, 0.05, 0.001)\n\n### B\n\nFor forward selection, which variable would be added first? Why?\n\nIt was difficult to determine from the Regression Output table on the HW5 assignment the difference between a p-value 0 & 0.00000. Pulling up the regression summary in R gives me more precise p-values for Size and New, which shows that Size is more statistically significant than New with a smaller p-value. \\<2e-16 is less than 4.3e-06. Size is also the most correlated with Price of any of the explanatory variables in the mode. Therefor in forward selection Size would be added first.\n\n### C\n\nWhy do you think that BEDS has such a large P-value in the multiple regression model, even though it has a substantial correlation with PRICE?\n\nI think that BEDS has such a large a p-value in the multiple regression model (making it not statistically significant) even though it has a substantial correlation with PRICE because of Size serving as a confounder. Size is the most statistically significant explanatory variable in the model, and the larger in size a house, the more likely it is to have more bedrooms.\n\n### D\n\nI created the first Models 1 & 2 using the backwards selection method and Model 3 using the forward selection method\n\nModel 1: All 4 explanatory variables Size, Beds, Baths, & New\n\nModel 2: 3 explanatory variables Size, Bath, & New\n\nModel 3: 2 explanatory variables Bath & New\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#Model 1\nmodel_1 <- lm(P ~ S + Be + Ba + New, data = house.selling.price.2)\nsummary(model_1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = P ~ S + Be + Ba + New, data = house.selling.price.2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-36.212  -9.546   1.277   9.406  71.953 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -41.795     12.104  -3.453 0.000855 ***\nS             64.761      5.630  11.504  < 2e-16 ***\nBe            -2.766      3.960  -0.698 0.486763    \nBa            19.203      5.650   3.399 0.001019 ** \nNew           18.984      3.873   4.902  4.3e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 16.36 on 88 degrees of freedom\nMultiple R-squared:  0.8689,\tAdjusted R-squared:  0.8629 \nF-statistic: 145.8 on 4 and 88 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n#Model 2\nmodel_2 <- lm(P ~ S +  Ba + New, data = house.selling.price.2)\nsummary(model_2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = P ~ S + Ba + New, data = house.selling.price.2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-34.804  -9.496   0.917   7.931  73.338 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -47.992      8.209  -5.847 8.15e-08 ***\nS             62.263      4.335  14.363  < 2e-16 ***\nBa            20.072      5.495   3.653 0.000438 ***\nNew           18.371      3.761   4.885 4.54e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 16.31 on 89 degrees of freedom\nMultiple R-squared:  0.8681,\tAdjusted R-squared:  0.8637 \nF-statistic: 195.3 on 3 and 89 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n#Model 3\nmodel_3 <- lm(P ~ S + New , data = house.selling.price.2) \nsummary(model_3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = P ~ S + New, data = house.selling.price.2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-47.207  -9.763  -0.091   9.984  76.405 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -26.089      5.977  -4.365 3.39e-05 ***\nS             72.575      3.508  20.690  < 2e-16 ***\nNew           19.587      3.995   4.903 4.16e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 17.4 on 90 degrees of freedom\nMultiple R-squared:  0.8484,\tAdjusted R-squared:  0.845 \nF-statistic: 251.8 on 2 and 90 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n\n-   \n\n    1.  R squared\n\n    -   Model 1 R-squared = 0.8689\n    -   Model 2 R-squared = 0.8681\n    -   Model 3 R-squared = 0.8484\n\nBased on the R-squared, Model 1 would be selected due to having the largest R-squared of the 3 models. R-squared measures how well variation in the dependent variable is explained by variation in the independent variables.\n\n-   \n\n    2.  Adjusted R-Squared\n\n    -   Model 1 Adjusted R-squared = 0.8629\n    -   Model 2 Adjusted R-squared = 0.8637\n    -   Model 3 Adjusted R-squared = 0.845\n\nBased on the Adjusted R-squared, Model 2 would be selected due to having the largest R-squared of the 3 models. Adjusted R-squared measures how well variation in the dependent variable is explained by variation in the independent variables, but penalizes the addition of multiple explanatory variables.\n\n-   \n\n    3.  PRESS\n\n-   Model 1 PRESS = 28390.22\n\n-   Model 2 PRESS = 27860.05\n\n-   Model 3 PRESS = 31066\n\nBased on the PRESS Statistics, Model 2 would be selected due to having the smallest PRESS of the 3 models. The PRESS statistic also penalizes the addition of more explanatory variables. The smaller the PRESS statistic the better the predictive power of the model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#Calculating PRESS Statistics\n\npr <- resid(model_1)/(1 - lm.influence(model_1)$hat)\nsum(pr^2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 28390.22\n```\n:::\n\n```{.r .cell-code}\npr <- resid(model_2)/(1 - lm.influence(model_2)$hat)\nsum(pr^2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 27860.05\n```\n:::\n\n```{.r .cell-code}\npr <- resid(model_3)/(1 - lm.influence(model_3)$hat)\nsum(pr^2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 31066\n```\n:::\n:::\n\n\n-   \n\n    4.  AIC\n\n    -   Model 1 AIC = 790.6225\n    -   Model 2 AIC = 789.1366\n    -   Model 3 AIC = 800.1262\n\nBased on the AIC, Model 2 would be selected due to having the smallest AIC of the 3 models. The AIC statistic also penalizes the addition of more explanatory variables and the lower value the better the model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n##Calculating AICs\nglance(model_1)$AIC\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 790.6225\n```\n:::\n\n```{.r .cell-code}\nglance(model_2)$AIC\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 789.1366\n```\n:::\n\n```{.r .cell-code}\nglance(model_3)$AIC\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 800.1262\n```\n:::\n:::\n\n\n-   \n\n    5.  BIC\n\n    -   Model 1 BIC = 805.8181\n    -   Model 2 BIC = 801.7996\n    -   Model 3 BIC = 810.2566\n\nBased on the BIC, Model 2 would be selected due to having the smallest BIC of the 3 models. The BIC statistic also penalizes the addition of more explanatory variables and the lower the value the better the model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglance(model_1)$BIC\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 805.8181\n```\n:::\n\n```{.r .cell-code}\nglance(model_2)$BIC\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 801.7996\n```\n:::\n\n```{.r .cell-code}\nglance(model_3)$BIC\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 810.2566\n```\n:::\n:::\n\n\n### E\n\nBased on the findings in the prior question, I prefer Model 2 out of the 3 models because it has the lowest AIC, BIC, & PRESS statistics & highest adjusted R-squared of the 3 models. These all suggest that Model 2, that variance in the explanatory variables Baths, Size, & New, explain the most variance in the outcome variable home price (highest adjusted R-squared), without gratuitous addition of explanatory variables to the model (lowest PRESS, AIC, BIC). I also like that all 3 explanatory variables Size (p-value = \\< 2e-16), Bath (p-value = 0.000438), & New (p-value = 4.54e-06) are statistically significant in the model, which suggests they are all indeed explanatory variables for the outcome Price.\n\n## Question 2\n\nFirst I read in the trees data set below, containing the measures of diameter, height, & volume of 31 black cherry trees that were cut down for lumber.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(\"trees\")\nhead(trees)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  Girth Height Volume\n1   8.3     70   10.3\n2   8.6     65   10.3\n3   8.8     63   10.2\n4  10.5     72   16.4\n5  10.7     81   18.8\n6  10.8     83   19.7\n```\n:::\n:::\n\n\n### A\n\nFit a multiple regression model with the Volume as the outcome and Girth and Height as the explanatory variables\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#lm volume as outcome, girth & height as explanatory\n tree <-lm(Volume ~ Height + Girth, data = trees)\nsummary(tree)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = Volume ~ Height + Girth, data = trees)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.4065 -2.6493 -0.2876  2.2003  8.4847 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -57.9877     8.6382  -6.713 2.75e-07 ***\nHeight        0.3393     0.1302   2.607   0.0145 *  \nGirth         4.7082     0.2643  17.816  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.882 on 28 degrees of freedom\nMultiple R-squared:  0.948,\tAdjusted R-squared:  0.9442 \nF-statistic:   255 on 2 and 28 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n\nThis linear regression model has a very high adjusted R-squared of 0.9442, suggesting that most of the variation in tree volume is explained by variation in tree height & girth. This isn't surprising, as volume is a function of dimension & height. Height is statistically significant at the 0.05 level with a p-value of 0.0145. Girth is statistically significant at the 0.001 value with a p-value of \\< 2e-16.\n\n### B\n\nRun regression diagnostic plots on the model. Based on the plots, do you think any of the regression assumptions is violated?\n\n\n::: {.cell}\n\n```{.r .cell-code}\n##Diagnostic Plots\npar(mfrow = c(2,3))\nplot(tree, which = 1:6)\n```\n\n::: {.cell-output-display}\n![](HW5_EmmaNarkewicz_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n:::\n\n\nBased on the diagnostic plots, I think the regression assumptions of linearity, constant variance, and influential observation are violated by the model for tree volume. The curvature of the first plot of residuals vs. fitted values shows a lack of linearity. The steep increase in decrease in the scale location plot suggests the linear regression model violates the assumption of constant variance, The final three diagnostic plots suggest violation of influential observation assumption.\n\nIn Plot 4, the 31st observation has a Cook's distance of 0.6, which is \\< 1, but greater than 4/n, which is 0.13 with n = 31. In the residuals vs. leverage plot, this same observation falls outside of the red curved line further supporting the violation of influential outliers. In the Cook's distance vs. leverage plot, the 31st observation has a high Cook's distance (0.6) and a high leverage (0.2), again suggesting the violation of the influential observation.\n\n## Question 3\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#Read in the data\ndata(\"florida\")\nhead(florida)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n           Gore   Bush Buchanan\nALACHUA   47300  34062      262\nBAKER      2392   5610       73\nBAY       18850  38637      248\nBRADFORD   3072   5413       65\nBREVARD   97318 115185      570\nBROWARD  386518 177279      789\n```\n:::\n:::\n\n\n### A\n\nRun a simple linear regression model where the Buchanan vote is the outcome and the Bush vote is the explanatory variable. Produce the regression diagnostic plots. Is Palm Beach County an outlier based on the diagnostic plots? Why or why not?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nflo <- lm(Buchanan ~ Bush, data = florida)\nsummary(flo)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = Buchanan ~ Bush, data = florida)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-907.50  -46.10  -29.19   12.26 2610.19 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 4.529e+01  5.448e+01   0.831    0.409    \nBush        4.917e-03  7.644e-04   6.432 1.73e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 353.9 on 65 degrees of freedom\nMultiple R-squared:  0.3889,\tAdjusted R-squared:  0.3795 \nF-statistic: 41.37 on 1 and 65 DF,  p-value: 1.727e-08\n```\n:::\n:::\n\n\nThe adjusted R-squared value of the model is 0.3795, which suggests a moderate amount of variance in Buchanan vote is explained by variance in the Bush vote. The explanatory variable of Bush vote is statistically significant as an explanatory variable in the model with a p-value of 1.73e-08.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n##Diagnostic Plots\npar(mfrow = c(2,3))\nplot(flo, which = 1:6)\n```\n\n::: {.cell-output-display}\n![](HW5_EmmaNarkewicz_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n:::\n\n\nPalm Beach county does appear to be an outlier based on the regression diagnostic plots. It has a Cook's distance of 2.0, suggesting it has a high influence on the plot. Palm beach is also outside the dotted lined on the residuals vs leverage plot, supporting it is an outlier. The final Cook's distance vs. Leverage plot shows that while Palm Beach county has a high Cook's distance, it has a low leverage of 0.05, suggesting that while Palm Beach County is an outlier it does not strongly influence the model.\n\n### B\n\nTake the log of both variables (Bush vote and Buchanan Vote) and repeat the analysis in (A.) Does your findings change?\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#logs\nflo_log <- lm(log(Buchanan) ~ log(Bush), data = florida)\nsummary(flo_log)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = log(Buchanan) ~ log(Bush), data = florida)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.96075 -0.25949  0.01282  0.23826  1.66564 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -2.57712    0.38919  -6.622 8.04e-09 ***\nlog(Bush)    0.75772    0.03936  19.251  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4673 on 65 degrees of freedom\nMultiple R-squared:  0.8508,\tAdjusted R-squared:  0.8485 \nF-statistic: 370.6 on 1 and 65 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n\nThe first plot had an adjusted R-squared of 0.3795, while the plot with log of both variable has a much higher adjusted R-squared of 0.8485. log(Bush) is still very statistically significant as an explanatory variable, with a p-value of \\< 2e-16.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n##Diagnostic Plots\npar(mfrow = c(2,3))\nplot(flo_log, which = 1:6)\n```\n\n::: {.cell-output-display}\n![](HW5_EmmaNarkewicz_files/figure-html/unnamed-chunk-18-1.png){width=672}\n:::\n:::\n\n\nLooking at the diagnostic plots, in the model taking the log of both Buchanan and Bush variables, Palm Beach the Cook's distance decreased from 2 to 0.35, making it no an longer influential observation. Palm beach no longer shows as an outlier on the Residual vs Leverage, and continues to have low leverage on the Cook's distance vs. leverage plot. Repeating my analysis after taking the log of both the outcome Buchanan & explanatory Bush variables does change my findings, with Palm Beach no longer an outlier in this model.\n",
    "supporting": [
      "HW5_EmmaNarkewicz_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}