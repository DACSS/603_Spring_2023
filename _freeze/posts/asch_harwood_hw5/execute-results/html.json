{
  "hash": "e4203936603e645608805294538efe8b",
  "result": {
    "markdown": "---\ntitle: \"Homework 5\"\nauthor: \"Asch Harwood\"\ndescription: \"Homework 5\"\ndate: \"5/9/2023\"\nformat:\n  html:\n    toc: true\n    code-fold: true\n    code-copy: true\n    code-tools: true\ncategories:\n  - hw5\n---\n\n\n\n\n# Question 1\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(house.selling.price.2)\n```\n:::\n\n\n### A\n\nBeds would be deleted first. It has the highest p-value of 0.487, which also happens to not be statistically significant.\n\n### B\n\nThe first fit would be an intercept only model, which means there is no explanatory variable. This becomes our 'baseline' against which we can evaluate our model as we add explanatory variables.\n\n### C\n\nBeds has relatively strong relationship with size, which also has a strong relationship with price. This means that this current model suffers from multicollinearity, which is obscuring the relationship between size and price.\n\n### D\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_S <- lm(P ~ S + Ba + New, data = house.selling.price.2)\nfit_Be <- lm(P ~ S + New, data = house.selling.price.2)\nfit_Ba <- lm(P ~ New, data = house.selling.price.2)\nfit_New <- lm(P ~ 1, data = house.selling.price.2)\n```\n:::\n\n\n#### R2\n\n-   With an R2 of 0.87: S + Ba + New\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(fit_S)$r.squared\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.8681361\n```\n:::\n\n```{.r .cell-code}\nsummary(fit_Be)$r.squared\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.8483699\n```\n:::\n\n```{.r .cell-code}\nsummary(fit_Ba)$r.squared\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.1271307\n```\n:::\n\n```{.r .cell-code}\nsummary(fit_New)$r.squared\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0\n```\n:::\n:::\n\n\n#### Adjusted R2\n\n-   With an Adjusted R2 of 0.86: S + Ba + New\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(fit_S)$adj.r.squared\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.8636912\n```\n:::\n\n```{.r .cell-code}\nsummary(fit_Be)$adj.r.squared\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.8450003\n```\n:::\n\n```{.r .cell-code}\nsummary(fit_Ba)$adj.r.squared\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.1175388\n```\n:::\n\n```{.r .cell-code}\nsummary(fit_New)$adj.r.squared\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0\n```\n:::\n:::\n\n\n#### PRESS\n\n-   Again, S + Ba + New has the smallest PRESS, which means it has the best 'predictive' power compared to the other models.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npress_stat <- function(model) {\n  # Calculate PRESS residuals\n  pr <- resid(model) / (1 - lm.influence(model)$hat)\n  \n  # Compute the PRESS statistic\n  press <- sum(pr^2)\n  \n  return(press)\n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\npress_stat(fit_S)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 27860.05\n```\n:::\n\n```{.r .cell-code}\npress_stat(fit_Be)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 31066\n```\n:::\n\n```{.r .cell-code}\npress_stat(fit_Ba)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 164039.3\n```\n:::\n\n```{.r .cell-code}\npress_stat(fit_New)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 183531.6\n```\n:::\n:::\n\n\n#### AIC\n\n-   Again, S + Ba + New has the smallest AIC, which suggests it does a better job of fitting the data without overfitting.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nAIC(fit_S)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 789.1366\n```\n:::\n\n```{.r .cell-code}\nAIC(fit_Be)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 800.1262\n```\n:::\n\n```{.r .cell-code}\nAIC(fit_Ba)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 960.908\n```\n:::\n\n```{.r .cell-code}\nAIC(fit_New)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 971.5532\n```\n:::\n:::\n\n\n#### BIC\n\n-   Again, S + Ba + New has the smallest BIC, which suggests it does a better job of fitting the data without overfitting\n\n\n::: {.cell}\n\n```{.r .cell-code}\nBIC(fit_S)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 801.7996\n```\n:::\n\n```{.r .cell-code}\nBIC(fit_Be)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 810.2566\n```\n:::\n\n```{.r .cell-code}\nBIC(fit_Ba)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 968.5058\n```\n:::\n\n```{.r .cell-code}\nBIC(fit_New)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 976.6184\n```\n:::\n:::\n\n\n### E\n\nI prefer P \\~ S + Ba + New. All coefficients are statistically significant. It outperforms the 'less complex' models on all metrics. It also make sense that several different factors influence home price.\n\n# Question 2\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(trees)\nhead(trees)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  Girth Height Volume\n1   8.3     70   10.3\n2   8.6     65   10.3\n3   8.8     63   10.2\n4  10.5     72   16.4\n5  10.7     81   18.8\n6  10.8     83   19.7\n```\n:::\n\n```{.r .cell-code}\nstr(trees)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n'data.frame':\t31 obs. of  3 variables:\n $ Girth : num  8.3 8.6 8.8 10.5 10.7 10.8 11 11 11.1 11.2 ...\n $ Height: num  70 65 63 72 81 83 66 75 80 75 ...\n $ Volume: num  10.3 10.3 10.2 16.4 18.8 19.7 15.6 18.2 22.6 19.9 ...\n```\n:::\n:::\n\n\n### A\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit <- lm(Volume ~ Girth + Height, data = trees)\nsummary(fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = Volume ~ Girth + Height, data = trees)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.4065 -2.6493 -0.2876  2.2003  8.4847 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -57.9877     8.6382  -6.713 2.75e-07 ***\nGirth         4.7082     0.2643  17.816  < 2e-16 ***\nHeight        0.3393     0.1302   2.607   0.0145 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.882 on 28 degrees of freedom\nMultiple R-squared:  0.948,\tAdjusted R-squared:  0.9442 \nF-statistic:   255 on 2 and 28 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n\n### B\n\n**Residuals vs Fitted**\n\nThe curved shape of the line suggests this model violates our assumption of linearity that the relationship between the independent and dependent variables is linear.\n\n**Scale-Location**\n\nThe curved shape of the line suggests this model violates our assumption of constant variance, or homoscedasticity, which affects the statistical significance of the model.\n\n**Cooks Distance, Residuals vs Leverage, Cooks dist vs Leverage**\n\nAll three charts show there is single, potentially influence point, which can impact whether our model meets the assumptions of linear regression and can disproportionately affect our regression coefficients.\n\n\n::: {.cell hash='asch_harwood_hw5_cache/html/unnamed-chunk-11_c4a4b13155c9d8129b22609f9e2619ea'}\n\n```{.r .cell-code}\npar(mfrow = c(2,3))\nplot(fit, which = 1:6)\n```\n\n::: {.cell-output-display}\n![](asch_harwood_hw5_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n\n# Question 3\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(\"florida\")\n```\n:::\n\n\n### A\n\nPalm Beach is clearly an outlier. For all other counties, there is a relatively weak but clear relationship between the number of Bush votes and Buchanan votes. The diagnostic plots show that the model largely obeys the relevant assumptions for linear regression of homoscedasticity, linearity, and normality of errors. However, it also highlights how the pattern observed in most counties in Florida does not hold in Palm Beach.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit <- lm(Buchanan ~ Bush, data = florida)\nsummary(fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = Buchanan ~ Bush, data = florida)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-907.50  -46.10  -29.19   12.26 2610.19 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 4.529e+01  5.448e+01   0.831    0.409    \nBush        4.917e-03  7.644e-04   6.432 1.73e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 353.9 on 65 degrees of freedom\nMultiple R-squared:  0.3889,\tAdjusted R-squared:  0.3795 \nF-statistic: 41.37 on 1 and 65 DF,  p-value: 1.727e-08\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\npar(mfrow = c(2,3))\nplot(fit, which = 1:6)\n```\n\n::: {.cell-output-display}\n![](asch_harwood_hw5_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n:::\n\n\n### B\n\nWhile taking the log of the independent and dependent variable increases the r-squared and reduces the p-value, Palm Beach continues to be an outlier, which suggests there is something 'different' about that county, compared to other counties in Florida.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit <- lm(log(Buchanan) ~ log(Bush), data=florida)\nsummary(fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = log(Buchanan) ~ log(Bush), data = florida)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.96075 -0.25949  0.01282  0.23826  1.66564 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -2.57712    0.38919  -6.622 8.04e-09 ***\nlog(Bush)    0.75772    0.03936  19.251  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4673 on 65 degrees of freedom\nMultiple R-squared:  0.8508,\tAdjusted R-squared:  0.8485 \nF-statistic: 370.6 on 1 and 65 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\npar(mfrow = c(2,3))\nplot(fit, which = 1:6)\n```\n\n::: {.cell-output-display}\n![](asch_harwood_hw5_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}