{
  "hash": "57cddcbbcccaf56f8c257e1414aa49df",
  "result": {
    "markdown": "---\ntitle: \"Homework 4\"\nauthor: \"Miguel Curiel\"\ndescription: \"Multiple linear regression for DACSS 603.\"\ndate: \"04/25/2023\"\nformat:\n  html:\n    toc: true\n    code-fold: true\n    code-copy: true\n    code-tools: true\ncategories:\n  - hw4\n  - linear regression\n  - multiple linear regression\neditor: \n  markdown: \n    wrap: 72\n---\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# load necessary packages\nlibrary(tidyverse)\nlibrary(alr4)\nlibrary(smss)\n```\n:::\n\n\n# Question 1\n\nFor recent data in Jacksonville, Florida, on y = selling price of home\n(in dollars), x1 = size of home (in square feet), and x2 = lot size (in\nsquare feet), the prediction equation is\n\nŷ = −10,536 + 53.8x1 + 2.84x2.\n\nA.  A particular home of 1240 square feet on a lot of 18,000 square feet\n    sold for \\$145,000. Find the predicted selling price and the\n    residual, and interpret.\n\n    a.  If ŷ = −10536 + 53.8x1 + 2.84x2, then by replacing the given\n        values we have ŷ = -10536 + (53.8\\*1240) + (2.84\\*18000).\n        Solving for that, the predicted selling price is \\$107,296 and\n        the residual is \\$37,704. This means that the actual selling\n        price was more than what the model would have predicted.\n\nB.  For fixed lot size, how much is the house selling price predicted to\n    increase for each square-foot increase in home size? Why?\n\n    a.  Selling price increases by 53.8 dollars for each square-foot\n        increase because that is the coefficient assigned to x1. In\n        other words, x1 is the effect of home size on the selling price\n        when holding other factors constant.\n\nC.  According to this prediction equation, for fixed home size, how much\n    would lot size need to increase to have the same impact as a\n    one-square-foot increase in home size?\n\n    a.  Lot size would need to increase 18.94 times to have the same\n        impact as a one-square-foot increase in home size. This can be\n        found by dividing x1 over x2, i.e. 53.8 / 2.84 = 18.94366.\n\n# Question 2\n\n(Data file: salary in **alr4** R package). The data file concerns salary\nand other characteristics of all faculty in a small Midwestern college\ncollected in the early 1980s for presentation in legal proceedings for\nwhich discrimination against women in salary was at issue. All persons\nin the data hold tenured or tenure track positions; temporary faculty\nare not included. The variables include degree, a factor with levels PhD\nand MS; rank, a factor with levels Asst, Assoc, and Prof; sex, a factor\nwith levels Male and Female; Year, years in current rank; ysdeg, years\nsince highest degree, and salary, academic year salary in dollars.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(\"salary\", package = \"alr4\")\nsalary <- salary\nhead(salary)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   degree rank    sex year ysdeg salary\n1 Masters Prof   Male   25    35  36350\n2 Masters Prof   Male   13    22  35350\n3 Masters Prof   Male   10    23  28200\n4 Masters Prof Female    7    27  26775\n5     PhD Prof   Male   19    30  33696\n6 Masters Prof   Male   16    21  28516\n```\n:::\n:::\n\n\nA.  Test the hypothesis that the mean salary for men and women is the\n    same, without regard to any other variable but sex. Explain your\n    findings.\n\n    a.  Since we are dealing with the mean of numerical data (salary)\n        between two groups (male and female), we can run a two-sample\n        t-test. As seen from the results below, even though the mean\n        salary of men is higher, according to the p-value (0.09), it is\n        not statistically significant, therefore we fail to reject the\n        null hypothesis. In other words, we do not have enough evidence\n        to say that male salaries are greater than female salaries.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# extract the salary data and sex variable\nsalary <- alr4::salary\nsex <- salary$sex\n\n# calculate the mean salaries for men and women\nmale_salaries <- salary$salary[sex == \"Male\"]\nfemale_salaries <- salary$salary[sex == \"Female\"]\nmean_male_salary <- mean(male_salaries)\nmean_female_salary <- mean(female_salaries)\n\n# perform the t-test\nt_test <- t.test(male_salaries, female_salaries)\n\n# print the results\ncat(\"Mean salary for men:\", mean_male_salary, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMean salary for men: 24696.79 \n```\n:::\n\n```{.r .cell-code}\ncat(\"Mean salary for women:\", mean_female_salary, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMean salary for women: 21357.14 \n```\n:::\n\n```{.r .cell-code}\ncat(\"p-value:\", t_test$p.value, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\np-value: 0.09009406 \n```\n:::\n:::\n\n\nB.  Run a multiple linear regression with salary as the outcome variable\n    and everything else as predictors, including sex. Assuming no\n    interactions between sex and the other predictors, obtain a 95%\n    confidence interval for the difference in salary between males and\n    females.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# fit a multiple linear regression model\nmodel <- lm(salary ~ ., data = salary)\n\n# print the model summary\nsummary(model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = salary ~ ., data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4045.2 -1094.7  -361.5   813.2  9193.1 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 15746.05     800.18  19.678  < 2e-16 ***\ndegreePhD    1388.61    1018.75   1.363    0.180    \nrankAssoc    5292.36    1145.40   4.621 3.22e-05 ***\nrankProf    11118.76    1351.77   8.225 1.62e-10 ***\nsexFemale    1166.37     925.57   1.260    0.214    \nyear          476.31      94.91   5.018 8.65e-06 ***\nysdeg        -124.57      77.49  -1.608    0.115    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2398 on 45 degrees of freedom\nMultiple R-squared:  0.855,\tAdjusted R-squared:  0.8357 \nF-statistic: 44.24 on 6 and 45 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# obtain a 95% confidence interval for the difference in salary between males and females\nconfint(model, \"sexFemale\", level = 0.95)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n              2.5 %   97.5 %\nsexFemale -697.8183 3030.565\n```\n:::\n:::\n\n\nC.  Interpret your finding for each predictor variable; discuss (a)\n    statistical significance, (b) interpretation of the coefficient /\n    slope in relation to the outcome variable and other variables.\n\n    a.  Statistical significance of each variable. There are several\n        variables that turn out to be statistically significant, in\n        particular rank and year. This makes sense as it is commonly\n        assumed that people with higher ranking positions have better\n        salaries. Similarly, people with more tenure or years in a\n        company should correlate to greater salaries. We had already\n        determined that gender does not play a significant role, but it\n        is interesting to see that neither level of education or years\n        since graduation play a significant role.\n\n    b.  Coefficient / slope of each predictor variable in relation to\n        the outcome variable and other variables. Some variables that\n        immediately draw attention are rank and years since graduation -\n        rank has a high coefficient, meaning that it plays an important\n        role in increasing salary. Years, on the other side, has a\n        negative coefficient, meaning that more years actually equates\n        to less salary (which is a positive outcome for recent grads,\n        but not for people that have several years in the workforce).\n        The remaining variables - gender, degree, and years in a\n        position - have positive relations, however they are not as high\n        as rank.\n\nD.  Change the baseline category for the rank variable. Interpret the\n    coefficients related to rank again.\n\n    a.  After experimenting with the three possible ranks, coefficients\n        do not change - what can change is the direction of the\n        relationship. For example, if you use \"Prof\" as the baseline,\n        now assistants and associates have a negative relationship,\n        meaning that professors see an increase in salary but assistants\n        and associates do not.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# change the baseline category for the rank variable to \"Asst\"\nsalary$rank <- relevel(salary$rank, ref = \"Prof\")\n\n# fit a new multiple linear regression model with a different baseline category for rank\nmodel2 <- lm(salary ~ ., data = salary)\n\n# print the model summary\nsummary(model2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = salary ~ ., data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4045.2 -1094.7  -361.5   813.2  9193.1 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  26864.81    1375.29  19.534  < 2e-16 ***\ndegreePhD     1388.61    1018.75   1.363    0.180    \nrankAsst    -11118.76    1351.77  -8.225 1.62e-10 ***\nrankAssoc    -5826.40    1012.93  -5.752 7.28e-07 ***\nsexFemale     1166.37     925.57   1.260    0.214    \nyear           476.31      94.91   5.018 8.65e-06 ***\nysdeg         -124.57      77.49  -1.608    0.115    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2398 on 45 degrees of freedom\nMultiple R-squared:  0.855,\tAdjusted R-squared:  0.8357 \nF-statistic: 44.24 on 6 and 45 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n\nE.  Finkelstein (1980), in a discussion of the use of regression in\n    discrimination cases, wrote, \"\\[a\\] variable may reflect a position\n    or status bestowed by the employer, in which case if there is\n    discrimination in the award of the position or status, the variable\n    may be 'tainted.'\"Thus, for example, if discrimination is at work in\n    promotion of faculty to higher ranks, using rank to adjust salaries\n    before comparing the sexes may not be acceptable to the courts.\n    (Exclude the variable rank, refit, and summarize how your findings\n    changed, if they did.)\n\n    a.  This results in significant changes. With this new model, all\n        variables included are statistically significant. This means\n        that gender, degree, and years since graduation do play an\n        important role in salary. In particular, females see a decrease\n        in salary, as do PhD graduates, while years since graduation is\n        the only variable with a positive influence on salary.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# fit a new multiple linear regression model without rank\nmodel3 <- lm(salary ~ sex + degree + ysdeg, data = salary)\n\n# print the model summary\nsummary(model3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = salary ~ sex + degree + ysdeg, data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8328.5 -2621.9  -864.5  2987.3 11025.5 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  18325.0     1105.3  16.580  < 2e-16 ***\nsexFemale    -2730.2     1236.8  -2.207  0.03210 *  \ndegreePhD    -4228.4     1311.7  -3.224  0.00228 ** \nysdeg          476.0       61.7   7.716 5.94e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3937 on 48 degrees of freedom\nMultiple R-squared:  0.5833,\tAdjusted R-squared:  0.5572 \nF-statistic: 22.39 on 3 and 48 DF,  p-value: 3.272e-09\n```\n:::\n:::\n\n\nF.  Everyone in this dataset was hired the year they earned their\n    highest degree. It is also known that a new Dean was appointed 15\n    years ago, and everyone in the dataset who earned their highest\n    degree 15 years ago or less than that has been hired by the new\n    Dean. Some people have argued that the new Dean has been making\n    offers that are a lot more generous to newly hired faculty than the\n    previous one and that this might explain some of the variation in\n    Salary. (Create a new variable that would allow you to test this\n    hypothesis and run another multiple regression model to test this.\n    Select variables carefully to make sure there is no\n    multicollinearity. Explain why multicollinearity would be a concern\n    in this case and how you avoided it. Do you find support for the\n    hypothesis that the people hired by the new Dean are making higher\n    than those that were not?)\n\n    a.  I created a new binary variable to distinguish between people\n        hired before and after the new dean. To check for\n        multicollinearity, I created a correlation matrix - and, as\n        expected, years since graduation is somewhat correlated. This\n        makes sense as people with more recent graduation dates are more\n        likely to have been hired by the new dean - therefore, \"ysdeg\"\n        was removed from the model. After fitting the model, most\n        variables remain relatively the same and the people hired\n        before/after the new dean do not seem to have a statistically\n        significant salary difference.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create a new variable indicating whether the person was hired by the new Dean or not\nsalary$newDean <- ifelse(salary$year >= 15, 0, 1)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calculate the correlation matrix\ncor(salary[, c(\"salary\", \"ysdeg\", \"newDean\")])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n            salary      ysdeg    newDean\nsalary   1.0000000  0.6748542 -0.3246869\nysdeg    0.6748542  1.0000000 -0.2931746\nnewDean -0.3246869 -0.2931746  1.0000000\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Fit the multiple regression model\nmodel4 <- lm(salary ~ sex + rank + degree + year + newDean, data=salary)\n\n# Check the summary of the model\nsummary(model4)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = salary ~ sex + rank + degree + year + newDean, data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3514.8 -1641.7  -263.6   895.5  8867.9 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  22870.5     2218.1  10.311 1.98e-13 ***\nsexFemale      550.0      838.5   0.656    0.515    \nrankAsst     -9198.9      948.3  -9.700 1.34e-12 ***\nrankAssoc    -5121.3      963.7  -5.314 3.21e-06 ***\ndegreePhD      163.7      785.8   0.208    0.836    \nyear           478.2      106.0   4.512 4.58e-05 ***\nnewDean       1931.1     1514.0   1.275    0.209    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2423 on 45 degrees of freedom\nMultiple R-squared:  0.8521,\tAdjusted R-squared:  0.8323 \nF-statistic:  43.2 on 6 and 45 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n\n# Question 3\n\n(Data file: house.selling.price in **smss** R package)\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(\"house.selling.price\", package = \"smss\")\nprices <- house.selling.price\nhead(prices)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  case Taxes Beds Baths New  Price Size\n1    1  3104    4     2   0 279900 2048\n2    2  1173    2     1   0 146500  912\n3    3  3076    4     2   0 237700 1654\n4    4  1608    3     2   0 200000 2068\n5    5  1454    3     3   0 159900 1477\n6    6  2997    3     2   1 499900 3153\n```\n:::\n:::\n\n\nA.  Using the house.selling.price data, run and report regression\n    results modeling y = selling price (in dollars) in terms of size of\n    home (in square feet) and whether the home is new (1 = yes; 0 = no).\n    In particular, for each variable; discuss statistical significance\n    and interpret the meaning of the coefficient.\n\n    a.  After running the model, both variables are statistically\n        significant, although size is much more significant than new\n        status (2e-16 \\> 0.00257). However, the coefficient is much\n        higher for new status than it is fore size (57736 \\>116).\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Fit the multiple regression model\nmodel5 <- lm(Price ~ Size + New, data=prices)\n\n# Check the summary of the model\nsummary(model5)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = Price ~ Size + New, data = prices)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-205102  -34374   -5778   18929  163866 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -40230.867  14696.140  -2.738  0.00737 ** \nSize           116.132      8.795  13.204  < 2e-16 ***\nNew          57736.283  18653.041   3.095  0.00257 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 53880 on 97 degrees of freedom\nMultiple R-squared:  0.7226,\tAdjusted R-squared:  0.7169 \nF-statistic: 126.3 on 2 and 97 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n\nA.  Report and interpret the prediction equation, and form separate\n    equations relating selling price to size for new and for not new\n    homes.\n\n    a.  The equation would be ŷ = -40230.867 + 116.132\\*size +\n        57736.283\\*new. What this means is that, if a house is new, it's\n        selling price will increase by \\$57,736.\n\nB.  Find the predicted selling price for a home of 3000 square feet that\n    is (i) new, (ii) not new.\n\n    a.  Using the above equation and replacing for the values given, we\n        have the two following equations: ŷ(new) = -40230.867 +\n        116.132\\*3000 + 57736.283\\*1 and ŷ(not new) = -40230.867 +\n        116.132\\*3000 + 57736.283\\*0. This results in a selling price of\n        \\$365,901.4 for the new home and \\$308,165.1 for one that is not\n        new.\n\nC.  Fit another model, this time with an interaction term allowing\n    interaction between size and new, and report the regression results.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Fit the multiple regression model\nmodel6 <- lm(Price ~ Size * New, data=prices)\n\n# Check the summary of the model\nsummary(model6)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = Price ~ Size * New, data = prices)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-175748  -28979   -6260   14693  192519 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -22227.808  15521.110  -1.432  0.15536    \nSize           104.438      9.424  11.082  < 2e-16 ***\nNew         -78527.502  51007.642  -1.540  0.12697    \nSize:New        61.916     21.686   2.855  0.00527 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 52000 on 96 degrees of freedom\nMultiple R-squared:  0.7443,\tAdjusted R-squared:  0.7363 \nF-statistic: 93.15 on 3 and 96 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n\nD.  Report the lines relating the predicted selling price to the size\n    for homes that are (i) new (ii) not new.\n\n    a.  The new formula would be ŷ = -22227.808 + 104.438\\*size -\n        78527.502\\*new + 61.916\\*size\\*new. We would have to replace\n        with a 1 or 0 depending where it says \"new\" in the previous\n        equation depending on a house's status.\n\nE.  Find the predicted selling price for a home of 3000 square feet that\n    is (i) new, (ii) not new.\n\n    a.  Replacing the given values in the previous equation, we would\n        have the two following formulas: ŷ(new)= -22227.808 +\n        104.438\\*3000 - 78527.502\\*1 + 61.916\\*3000\\*1 and ŷ(not new)=\n        -22227.808 + 104.438\\*3000 - 78527.502\\*0 + 61.916\\*3000\\*0.\n        This results in a selling price of \\$398,306.7 for the new house\n        and \\$291,086.2 for one that is not new.\n\nF.  Find the predicted selling price for a home of 1500 square feet that\n    is (i) new, (ii) not new. Comparing to (F), explain how the\n    difference in predicted selling prices changes as the size of home\n    increases.\n\n    a.  Following the same steps, we now have these formulas: ŷ(new)=\n        -22227.808 + 104.438\\*1500 - 78527.502\\*1 + 61.916\\*1500\\*1 and\n        ŷ(not new)= -22227.808 + 104.438\\*1500 - 78527.502\\*0 +\n        61.916\\*1500\\*0. This results in a selling price of \\$148,775.7\n        for the new house and \\$134,429.2 for one that is not new.\n        Compared to the prices in (F), there seems to be an\n        exponentiation effect on the selling price as houses increase in\n        size.\n\nG.  Do you think the model with interaction or the one without it\n    represents the relationship of size and new to the outcome price?\n    What makes you prefer one model over another?\n\n    a.  Even though it is tempting to say the the model without\n        interaction performs better because of the lower R-squared, when\n        comparing across other metrics (Root Mean Squared Error, Mean\n        Absolute Error, and Akaike Information Criterion), it turns out\n        the model with interaction performs better. Additionally, from\n        the examples above, the prices predicted by the model with\n        interaction does seem to be closer to the predicted prices.\n        Therefore, I would chose the model with interaction.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create a function to calculate the evaluation metrics\neval_metrics <- function(model) {\n  # Calculate the R-squared value\n  rsq <- summary(model)$r.squared\n  \n  # Calculate the RMSE value\n  predicted <- predict(model, newdata = house.selling.price)\n  actual <- house.selling.price$Price\n  rmse <- sqrt(mean((predicted - actual)^2))\n  \n  # Calculate the MAE value\n  mae <- mean(abs(predicted - actual))\n  \n  # Calculate the AIC value\n  aic <- AIC(model)\n  \n  # Return a data frame with the evaluation metrics\n  data.frame(R2 = rsq, RMSE = rmse, MAE = mae, AIC = aic)\n}\n\n# Calculate the evaluation metrics for each model\nmetrics1 <- eval_metrics(model5)\nmetrics2 <- eval_metrics(model6)\n\n# Combine the metrics into a table\nmetrics_table <- rbind(metrics1, metrics2)\nrownames(metrics_table) <- c(\"Model without Interaction\"\n                             , \"Model with Interaction\")\n\n# Print the table\nmetrics_table\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                                 R2     RMSE      MAE      AIC\nModel without Interaction 0.7225963 53066.58 38015.03 2467.648\nModel with Interaction    0.7443085 50947.53 35200.58 2461.498\n```\n:::\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}