{
  "hash": "bb67b4b6db9089d1b2d90087aca1e89e",
  "result": {
    "markdown": "---\ntitle: \"HW_5_Diana_Rinker\"\nauthor: \"Diana_Rinker\"\neditor: visual\n---\n\n\n------------------------------------------------------------------------\n\n# DACSS 603, spring 2023\n\n# Homework 5, Diana Rinker.\n\nLoading necessary libraries:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(dplyr)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'dplyr'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n```\n:::\n\n```{.r .cell-code}\nlibrary(ggplot2)\nlibrary(alr4)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: car\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: carData\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'car'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following object is masked from 'package:dplyr':\n\n    recode\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: effects\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nlattice theme set by effectsTheme()\nSee ?effectsTheme for details.\n```\n:::\n\n```{.r .cell-code}\nlibrary(smss)\ndata(house.selling.price.2)\n```\n:::\n\n\n# Question 1\n\n## A. For backward elimination, which variable would be deleted first? Why?\n\n\"Beds\" variable would be deleted first because it has the highest p-value of 0.487, meaning no statistical significance.\n\n## B. For forward selection, which variable would be added first? Why?\nI will sstart with the value that is most significant,which is size. Size variable also has the highest correlation coefficient on the matrix.\n\n## C. Why do you think that BEDS has such a large P-value in the multiple regression model,even though it has a substantial correlation with PRICE?\n\nI think that is because the number of beds is highly correlated with the size of the house and the number of baths in the house, representing almost the same thing.\n\n## D. Using software with these four predictors, find the model that would be selected using each criterion:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncor.matrix <-   cor(house.selling.price.2)\ncor.matrix\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n            P         S        Be        Ba       New\nP   1.0000000 0.8988136 0.5902675 0.7136960 0.3565540\nS   0.8988136 1.0000000 0.6691137 0.6624828 0.1762879\nBe  0.5902675 0.6691137 1.0000000 0.3337966 0.2672091\nBa  0.7136960 0.6624828 0.3337966 1.0000000 0.1820651\nNew 0.3565540 0.1762879 0.2672091 0.1820651 1.0000000\n```\n:::\n\n```{.r .cell-code}\ncolnames(house.selling.price.2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"P\"   \"S\"   \"Be\"  \"Ba\"  \"New\"\n```\n:::\n\n```{.r .cell-code}\nmodel.1 <-lm(P~ S +Be+Ba+New, data = house.selling.price.2)\nmodel.2 <-lm(P~ S +Ba+New, data = house.selling.price.2)\n\n\nlibrary(stargazer)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nPlease cite as: \n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n Hlavac, Marek (2022). stargazer: Well-Formatted Regression and Summary Statistics Tables.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n R package version 5.2.3. https://CRAN.R-project.org/package=stargazer \n```\n:::\n\n```{.r .cell-code}\nstargazer(model.1, model.2, type = 'text')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n===================================================================\n                                  Dependent variable:              \n                    -----------------------------------------------\n                                           P                       \n                              (1)                     (2)          \n-------------------------------------------------------------------\nS                          64.761***               62.263***       \n                            (5.630)                 (4.335)        \n                                                                   \nBe                          -2.766                                 \n                            (3.960)                                \n                                                                   \nBa                         19.203***               20.072***       \n                            (5.650)                 (5.495)        \n                                                                   \nNew                        18.984***               18.371***       \n                            (3.873)                 (3.761)        \n                                                                   \nConstant                  -41.795***              -47.992***       \n                           (12.104)                 (8.209)        \n                                                                   \n-------------------------------------------------------------------\nObservations                  93                      93           \nR2                           0.869                   0.868         \nAdjusted R2                  0.863                   0.864         \nResidual Std. Error    16.360 (df = 88)        16.313 (df = 89)    \nF Statistic         145.763*** (df = 4; 88) 195.313*** (df = 3; 89)\n===================================================================\nNote:                                   *p<0.1; **p<0.05; ***p<0.01\n```\n:::\n:::\n\n\n1\\. R2\nThe first model has higher R^2.\n\n2\\. Adjusted R2\nThe second model has better adjusted R^2,since unsignificant  IV was remoded (bedrooms) \n\n3\\. PRESS\n\n\n::: {.cell}\n\n```{.r .cell-code}\ny_pred.1 <- predict(model.1, newdata = house.selling.price.2)\nPRESS.1 <- sum((house.selling.price.2$P - y_pred.1)^2)\ny_pred.2 <- predict(model.2, newdata = house.selling.price.2)\nPRESS.2 <- sum((house.selling.price.2$P - y_pred.1)^2)\n\nPRESS.1\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 23552.98\n```\n:::\n\n```{.r .cell-code}\nPRESS.2\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 23552.98\n```\n:::\n:::\n\nPRESS values are identical for tho models. \n\n4\\. AIC\n\n\n::: {.cell}\n\n```{.r .cell-code}\nAIC(model.1) \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 790.6225\n```\n:::\n\n```{.r .cell-code}\nAIC(model.2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 789.1366\n```\n:::\n:::\n\n I select model 2, as it's AIC is lower \n \n5\\. BIC\n\n::: {.cell}\n\n```{.r .cell-code}\nBIC(model.1) \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 805.8181\n```\n:::\n\n```{.r .cell-code}\nBIC(model.2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 801.7996\n```\n:::\n:::\n\nBIC is also lower in the second model, so I prefer the second.  \n\n## E. Explain which model you prefer and why.\n\n\nSecond model has fewer variables  and therefore lower AIC, BIC and adjusted R^2. All its independent variables have  high significance, therefore i select the second model. \n\n# Question 2\n(Data file: trees from base R)\nFrom the documentation:\n“This data set provides measurements of the diameter, height and volume of timber in 31 felled\nblack cherry trees. Note that the diameter (in inches) is erroneously labeled Girth in the data. It is\nmeasured at 4 ft 6 in above the ground.”\nTree volume estimation is a big deal, especially in the lumber industry. Use the trees data to build\na basic model of tree volume prediction. In particular,\n### A. Fit a multiple regression model with the Volume as the outcome and Girth and Height as the explanatory variables\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(trees)\ncolnames(trees)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Girth\"  \"Height\" \"Volume\"\n```\n:::\n\n```{.r .cell-code}\nmodel.1 <- lm(Volume~ Height + Girth + Height*Girth, data =trees)\nsummary (model.1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = Volume ~ Height + Girth + Height * Girth, data = trees)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.5821 -1.0673  0.3026  1.5641  4.6649 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  69.39632   23.83575   2.911  0.00713 ** \nHeight       -1.29708    0.30984  -4.186  0.00027 ***\nGirth        -5.85585    1.92134  -3.048  0.00511 ** \nHeight:Girth  0.13465    0.02438   5.524 7.48e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.709 on 27 degrees of freedom\nMultiple R-squared:  0.9756,\tAdjusted R-squared:  0.9728 \nF-statistic: 359.3 on 3 and 27 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n### B. Run regression diagnostic plots on the model. Based on the plots, do you think any of the regression assumptions is violated?\n\n\n::: {.cell}\n\n```{.r .cell-code}\npar(mfrow = c(2,3))\nplot(model.1, which = 1:6)\n```\n\n::: {.cell-output-display}\n![](HW_5_Diana_Rinker_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\nNone of diagnostic plots suggest violation of regression assumptions.\n\n\n# Question 3\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(florida)\n```\n:::\n\n\nIn the 2000 election for U.S. president, the counting of votes in Florida was controversial. In Palm\nBeach County in south Florida, for example, voters used a so-called butterfly ballot. Some believe\nthat the layout of the ballot caused some voters to cast votes for Buchanan when their intended\nchoice was Gore.\nThe data has variables for the number of votes for each candidate—Gore, Bush, and Buchanan.\n\n\n### A. Run a simple linear regression model where the Buchanan vote is the outcome and the Bush vote is the explanatory variable. Produce the regression diagnostic plots. Is Palm Beach County an outlier based on the diagnostic plots? Why or why not?\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(florida)\ncolnames(florida)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Gore\"     \"Bush\"     \"Buchanan\"\n```\n:::\n\n```{.r .cell-code}\nmodel.1 <- lm(Buchanan~ Bush, data= florida)\nsummary(model.1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = Buchanan ~ Bush, data = florida)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-907.50  -46.10  -29.19   12.26 2610.19 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 4.529e+01  5.448e+01   0.831    0.409    \nBush        4.917e-03  7.644e-04   6.432 1.73e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 353.9 on 65 degrees of freedom\nMultiple R-squared:  0.3889,\tAdjusted R-squared:  0.3795 \nF-statistic: 41.37 on 1 and 65 DF,  p-value: 1.727e-08\n```\n:::\n\n```{.r .cell-code}\npar(mfrow = c(2,3))\nplot(model.1, which = 1:6)\n```\n\n::: {.cell-output-display}\n![](HW_5_Diana_Rinker_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\nScale-location plot \n\nYes, Palm Beach and Dade are clear outliers for the model, as demonstrated by cooks distance. The scale-location plot also showing upward trend meaning that the residuals are larger with larger fitted values. We can see that Palm Beach and Dade are the observations that pull the line upwards. \n\n\n### B. Take the log of both variables (Bush vote and Buchanan Vote) and repeat the analysis in (A.) Does your findings change?\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(florida)\ncolnames(florida)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Gore\"     \"Bush\"     \"Buchanan\"\n```\n:::\n\n```{.r .cell-code}\nmodel.2 <- lm(log(Buchanan) ~ log(Bush), data= florida)\nsummary(model.2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = log(Buchanan) ~ log(Bush), data = florida)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.96075 -0.25949  0.01282  0.23826  1.66564 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -2.57712    0.38919  -6.622 8.04e-09 ***\nlog(Bush)    0.75772    0.03936  19.251  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4673 on 65 degrees of freedom\nMultiple R-squared:  0.8508,\tAdjusted R-squared:  0.8485 \nF-statistic: 370.6 on 1 and 65 DF,  p-value: < 2.2e-16\n```\n:::\n\n```{.r .cell-code}\npar(mfrow = c(2,3))\nplot(model.2, which = 1:6)\n```\n\n::: {.cell-output-display}\n![](HW_5_Diana_Rinker_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\nwhile Palm Beach still standing out as one of the most extreme values and can be considered an outlier, it is now within acceptable boundaries on residuals vs leverage plot.  Scale-location plot looks more even as well. \n\nThe model shows that there is a connection between Bunchian and Bush, and that logarithmic transformation made the model a better fit. \n",
    "supporting": [
      "HW_5_Diana_Rinker_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}